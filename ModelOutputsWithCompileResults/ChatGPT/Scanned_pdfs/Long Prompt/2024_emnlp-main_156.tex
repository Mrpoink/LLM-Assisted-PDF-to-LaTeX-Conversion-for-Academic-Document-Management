=====FILE: main.tex=====
\documentclass[11pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}

\title{To Word Senses and Beyond: Inducing Concepts with Contextualized Language Models}
\author{Bastien Li'etard \and Pascal Denis \and Mikaela Keller}
\date{}

\begin{document}
\maketitle

\begin{abstract}
Polysemy and synonymy are two crucial inter-
related facets of lexical ambiguity. While both
phenomena are widely documented in lexical
resources and have been studied extensively
in NLP, leading to dedicated systems, they are
often being considered independently in prac-
tictal problems. While many tasks dealing with
polysemy (e.g. Word Sense Disambiguiation or
Induction) highlight the role ofword's senses,
the study of synonymy is rooted in the study
of concepts, i.e. meanings shared across the
lexicon. In this paper, we introduce Concept
Induction, the unsupervised task of learning
a soft clustering among words that defines a
set of concepts directly from data. This task
generalizes Word Sense Induction. We pro-
pose a bi-level approach to Concept Induction
that leverages both a local lemma-centric view
and a global cross-lexicon view to induce con-
cepts. We evaluate the obtained clustering on
SemCor's annotated data and obtain good per-
formance (BCubed F1 above 0.60). We f,nd
that the local and the global levels are mutually
beneficial to induce concepts and also senses
in our setting. Finally, we create static embed-
dings representing our induced concepts and
use them on the Word-in-Context task, obtain-
ing competitive performance with the State-of-
the-Art.

\end{abstract}

\section{Introduction}
A crucial challenge in understanding natural language comes from the fact that the mapping between word forms and lexical meanings is many to-many, due to polysemy (i.e., the multiplicity of meanings for a given form)\footnote{we take polysemy in its most comprehensive definition, also including homonymy.} and synonymy (i.e., the multiplicity of forms for expressing a given meaning). Both polysemy and synonymy have been thoroughly studied in NLP, but mostly as independent problems, giving rise to dedicated systems. Thus, Word Sense Disambiguiation (WSD) aims at correctly mapping word occurrences to one of its senses (Raganato et al., 2017), while Word Sense Induction (WSI), its unsupervised counterpart, aims at clustering word occurrences into latent senses directly from data (Manandhar et al., 2010; Jurgens and Klapaftis, 2013). More recently, researchers have proposed the task of Word-in-Context (WiC), which consists in classifying pairs of word occurrences depending on whether they realize the same sense or not (Pilehvar and Camacho-Collados, 2019). All these works take a word centric view, which aims at identifying or characterizing the different senses of a given word, where these senses are bound to a word. Another line of work, which takes a broader lexicon-wide perspective, is concerned with identifying synonyms, which are equivalence classes over different words that point to the same concept (Zhang et al., 2021; Manem et al., 2023), where concepts are semantic entities that are not bound to a word. In WordNet (Miller, 1995; Fellbaum, 1998), concepts are called synsets, defined as sets of synonyms. However, outside of lexical resources, synonymy and polysemy are usually considered as independent problems in the NLP literature. Yet, these two views are complementary. In lexicology, they correspond to two perspectives on the word-meaning mapping: semasiology and onomasiology. The former is the word-to-meanings view, where one can observe polysemy by looking at the different meanings a given word has. The latter is the meaning-to-words view, in which one can study synonymy by looking at the inventory of words that speakers use to express the same meaning.

In this paper, we propose a new task, called Concept Induction, that directly aims at learning concepts in an unsupervised manner from raw text. More precisely, this task aims at learning a soft clustering over a target lexicon (i.e., a set of words), such a way that each cluster corresponds to a (latent) concept. Thus, this task both addresses polysemy (since polysemous words should appear in multiple clusters) and synonymy (since synonymous words should appear in the same cluster(s)). Inducing concepts can be interesting for many external applications, like building lexical resources for low-resources languages (Velasco et al., 2023) and can bring a different perspective in computational studies of meaning, moving the usual word-centric focus to a more meaning-centric state.

Our approach to Concept Induction relies on word occurrences for a target lexicon, represented as word embeddings derived from a Contextualized Language Model (in this case, BERT Large (Devlin et al., 2019)), which are then grouped, using hard clustering algorithms, into concept denoting clusters. While these concept clusters could in principle be obtained directly from word occurrences, we propose a bi-level methodology that leverages both a local, lemma-centric clustering (i.e., operating on only specific word occurrences), and a global, cross-lexicon clustering (i.e., operating on all words occurrences). From this perspective, our approach generalizes, and in fact builds upon classical Word Sense Induction, in that word senses are learned jointly alongside with concepts. We hypothesize that an approach taking both complementary resolutions in account will lead to improved Concept Induction and Word Sense Induction, i.e. that the two objectives can be mutually beneficial.

To validate our approach, we carried out experiments on the SemCor dataset, which provides a set of concepts (taking the form of WordNet synsets) related to word occurrences. We found that our bi-level clustering approach accurately learn concepts, achieving F1 scores above 0.60 on the task of Concept Induction compared to WordNet's synsets, outperforming competing approaches that use only local and global views. This demonstrates the benefits of our bi-level approach and its ability to leverage both local and global views when inducing concepts. Interestingly, we show that the benefits go both ways: our proposed approach outperforms lemma-centric approaches when evaluated for WSI. Finally, we show that concept-aware static embeddings derived from our approach are also competitive with state-of-the-art approaches efficient on the Word-in-Context task while using less training data. Through the new task of concept induction, we also contribute in new way to the ongoing debate regarding the ability to align vector representations extracted from Contextualized Language Models to the semantic representations posited by (psycho-)linguists. In this vein, we conduct a qualitative evaluation of obtained clusters to ensure they indeed reflect concepts and gather synonyms. The source code used for experiments is available at \url{[ILLEGIBLE]}.


\section{Related Work}

\subsection{Lexical resources for concepts}

Princeton's WordNet (PWN) (Miller, 1995; Fellbaum, 1998) is a lexical database that has been been the most widely used as a reference for most wordsense-related tasks for many years. In WordNet, the entry corresponding to a lemma has different wordsenses, each of them mapping to a synset. Synsets are WordNet's equivalents of our concepts. Lemmas whose wordsenses belong to the same synset are synonymous. WordNet 3.0 contains 117,659 synsets and is built from the work of psycholinguists and lexicographers, that not only describes synonymy but also other lexical relations such as hypernymy/hyponymy, antonymy, meronymy/holonymy, etc. But the amount of resources needed to create such lexical databases with human experts is considerable, making them a very rare and precious resource. They are not available for a large number of active languages, and even more rare for dead languages (Bizzoni etal., 2014; Khan et a1., 2022).

\subsection{Word senses with Language Models}

With the recent development of neural Contextualized Language Models (CLM), several work use their hidden-layers to extract vector representations of word usages and retrieve word senses. These representations are fed to a classification (for WSD) or a clustering (in the case of WSI) algorithm to distinguish the word's senses (Scarlini et al., 2020; Nair et a1., 2020; Saidi and Jarray, 2023). These embeddings-based approaches have applications in other flelds: Kutuzov and Giulianelli (2020) and Martinc et al. (2020) use sense clusters found using CLM embeddings to study the change in meaning of words, and Chronis and Erk (2020) propose a many-Kmeans method to investigate semantic similarity and relatedness. Another line of work uses list of substitute tokens sampled from the CLM head to infer senses (Amrami and Goldberg, 2019; Eyal et a1., 2022) and are sucessful on WSI benchmarks like Manandhar et al. (2010) and Jurgens and Klapaftis (2013).

\subsection{Structures of Meaning in CLM}

Recent research probes neural CLMs for alignements between representations from their latent spaces and semantic patterns and relations. Section7.2 of Haber and Poesio (2024) summarizes findings about polysemy in contextualized CLMs, showing that these models were able to detect polysemy and in some cases distinguish actual polysemy from homonymy. They report that representations from different senses may however overlap.

Hanna and Maredek (2021) shows that pretrained BERT embed knowledge of [ILLEGIBLE] but is limited to the more common hyponyms.

Velasco et al. (2023) build on top of WSI techniques in an attempt to automatically construct a WordNet for Filipino, thus proposing a modeling of synonymy in this language. However, the evaluation of the synsets they obtained is limited by the lack of sense-annotated data for Filipino, and they could not evaluate the impact of their methodology on the two levels (senses and concepts).

Works like Ethayarajh (2019) and Chronis and Erk (2020) study the kind of information that was distributed across layers. The former concludes that syntactic and word-order information are distributed in the first layers while in deeper layers, representations are heavily influenced by contexts. The latter demonstrates, with a multi-prototypes embedding approach, that semantic similarity is best found in moderately late layers, wtile relatedness is best found in last layers.


\section{Concept Induction}
Our main motivation behind Concept Induction is to present a view of the mapping between words and their meaning(s).\footnote{This mapping is calledpatterns of lexificationby Frangois (2022); see also coexpression and synexpression it the terminology proposed by Haspelmath (2023).} This view is systemic, meaning that it should not be defined for individual words neither for individual concepts, but rather acknowledging these as a whole with interactions and relations. This extends beyond the primary objective of WSI, which defines word senses as pertaining to individual words only and does not explore relations between lemmas or concepts.

\subsection{Basic notions}

Consider a set of target words (or lemmas) and for each lemma, we have a set of occurrences of this word in a context (e,g, a sentence or a phrase). The set of target lemmas is referred to as the lexicon, while the corpus is the set of all occurrences. Our goal is to study the meaning of target words as they are used in the corpus.

In this study we call sense of a word its usage to refer to a concept. A polysemous word has multiple senses, each of them referring to a distinct concept. Two words are said to be synonyms for a given concept when each of them has one of their senses referring to this shared concept. Senses are defined `locally'', i.e. bound to an individual word of the lexicon, as opposed to concepts which are defined `globally'', i.e. across the whole lexicon. An occurrence of a word $w$ realizes one of its senses.

Consider the words `test'' and and the following corpus: (A) the jury found them guilty in afair trial. (B) candidates competed in a trial of skill. (C) the hero underwent a test of strength. The corpus is composed of two occurrences of `trial'' and one occulTence of `test.'' In the corpus , `tial'' is polysemous. Its first sense, illustrated in A, refers to a process of law,Its second sense, in B, refers to the concept of the act of undergoing testing. T\e sense of `test'' in sentence C also corresponds to this concept: it's a case where `test'' and ``trials'' are synonymous. Shifting the focus from senses to concepts, we will say that B and C instantiate the same concept, while A is an instance of a different concept.

\subsection{Task definition}

The goal of Concept Induction (CI) is to automatically learn a set of concepts directly from the data, i.e. learning a soft clustering $C_w$ in the set of target words $W$ that should correspond to the multiple concepts instantiated by occurrences of the corpus. $C_w$ is a sol clustering because a word can be assigned to several clusters (when it is polysemous). Using a different perspective than WSI, the framework of Concept Induction provides a more complete view on meaning across the lexicon. Both WSI and CI capture polysemy, but CI also reveals synonymy across the lexicon. Like WSI, Concept Induction does not require a pre-defined set of concepts.

\subsection{Formal framework}

Let $W$ be the lexicon. For all word $w$ in $W$, we denote $o_i^w$ the $i$-th occurrence of $w$ in the corpus. We define $O_w := {o_i^w}_{1 \le i \le n_w}$, the set of $n_w$ occurrences of $w$. The corpus, denoted $O$, is the union of all $O_w$.

\begin{figure}[t]
\centering
%%%PLACEHOLDER: FIG_0001%%%
\caption{Illustration of our framework. The word `trial'' is polysemous and has two senses corresponding to two different concepts, and is synonym with `test'' for this second meaning.}
\end{figure}

For a given word $w \in W$, the set $O_w$ can be partitionned according to its different senses. We denote $s_j^w$ the part of occurrences of $w$ in the corpus corresponding to the $j$-th sense of $w$. We refer to these groups of occurrences as the sense clusters of $w$. The set $S_w := {s_j^w}*{1 \le j \le t_w}$ forms a partition of $O_w$, and we call $S$ the set of all sense clusters of all words, i.e. $S = \bigcup*{w \in W} S_w$. $S$ is a ``local'' (lemma-centric) partition of the whole $O$. The task of Word Sense Induction aims at learning the partition $S$ given a corpus $O$.

In this work, we aim at dividing the corpus into concepts instead of senses. We denote $c_k$ the group of occurrences of words corresponding to the concept indexed by $k$, and ${c_k}_{1 \le k \le p}$ the partition of $O$ in $p$ concept clusters. Unlike sense clusters, a concept cluster $c_k \in C$ can gather occurrences of different words: $C$ is a ``global'' partition. Each occurrence $o_i^w$ of a word $w \in W$ is associated to a sense cluster $s_j^w$ and a concept cluster $c_k \in C$. We can say that a concept corresponding to $c_k$ is instantiated by occurrence $o_i^w$ through the sense corresponding to $s_j^w$, or conversely that $o_i^w$ uses the sense reflected in $s_j^w$ to mean the concept described by concept cluster $c_k$. All occurrences of sense cluster $s_j^w \in S$ appeax in the same concept cluster $c_k \in C$.

In summary, $S$ and $C$ are partitions of $O$ and are naturally constrained as follows:
\begin{enumerate}
\item By definition, a sense in $S$ is associated to one and only one word $w \in W$.
\item An occurrence $o_i^w$ realizes exactly one sense $s \in S$.
\item An occurrence $o_i^w$ instantiates exactly one concept $c \in C$.
\item In a given sense $s \in S$, all occurrences are assigned to the same concept $c \in C$.
\item All $s_i \in S_w$ (i.e. same word) refer to distinct concepts.
\end{enumerate}

From the partition $C$ on occurrences, one can derive $C_w$, a clustering of the set of words $W$ into concepts. To each concept cluster $c \in C$ we associate a cluster in $C_w$ that contains all lemmas of $W$ whose occurrences were assigned to $c$. In $C_w$, apolysemous word with $n$ senses appears in $n$ distinct clusters (one per sense), and synonyms appear in at least one common cluster (one per shared concept).

We denote $\Theta_w$ the word-level soft-clustering and $C$ the partition of occurrences that ue leamed on the data.

In Figure~1 we illustrate this framework, using a corpus of occurrences of the words `test'' and `trial''. In this scenario, $W = {\text{`test''}, \text{`trial''}}$ and two concepts are instantiated: a process of law to determine someone's guilt and a challenge to evaluate a skill. The lemma `trial'' exhibits two senses as it has occurrences corresponding to both concepts: `trial'' is polysemous. The second concept is also instantiated by occurrences of `test'', therefore `trial'' and ``test'' show synonymy in this case. This toy example also follows all constraints formulated above.


\section{Methodology}
In this section we describe the methods we propose and evaluate for Concept Induction. We learn 0w a clustering drawinginspiration from the relations between O, S, C and Cw. In particular, the overall objective of our methodology consist in finding C (i.e. partition occurrences into concept clusters) to derive Cw. Section 3.3 highlighted that there are two levels of partitions: a local level (senses) and a global one (concepts). The proposed approaches rely on both levels and the use of a Contextualized Language Model (CLM) to gather representations of occurrences influenced by the context.

\subsection{Proposed Bi-level Method}

\paragraph{Local (lemma-centric) clustering} Firstly, we propose to learn a word-sense partition for each target words individually. Using the CLM hidden layers, we extract a vector representation (the occurrence embedding) of every occurrence of. We then learn a partition of each O* using a [ILLEGIBLE] clustering algorithm on the embeddings. Each [ILLEGIBLE] describes the locally estimated sense clusters of tl. word Jointly considering these partitions for a7l w e W, we obtain a partition S of the whole set of occurrences O. This partition is local in the sense that each word has its occurrences clustered independently from other words.

\paragraph{Global (cross-lexicon) clustering} Once we have [ILLEGIBLE] a local clustering ,9, turn from considering words independently to consider all words together. In this step, we learn a global clustering by merging local clusters ofoccurrences. To do so, we average embeddings of all occurrences in the same local cluster to get a single embedding representing each local cluster. Then we run a second clustering algorithm, this time using the averaged embeddings of local clusters. This global clustering defines a new partition C of the the corpus O: when two local clusters ,iI1 and 3Y,2 are merged into the same J J' global cluster d7, (because their embeddings were clustered together), all their occurrences are assigned to global cluster 66. From this global oc [ILLEGIBLE] currence partition C [ILLEGIBLE] easily extracl0w , a word-level soft-clustering of lemmas whose occurrences appear in the same 6r. This Bi-level method directly implements the system of contraints described in Section 3.3. Only constraint 5 is not enforced by design. Indeed, our local clusters being learned and not informed by an expert, the local clustering step may make errors, especially if the data for a given word are sparse. Allowing the global clustering to merge local clusters enables the correction of local clustering's recall errors using information from the global level. We also want to highlight that the proposed methodology is generic, in the sense that it is not tied to a specific choice of clustering algorithm.

\subsection{Local-only and Global-only}

Sense-inducing systems (WSI approaches) that create only local clusters of occurrences for each word are said to be Local-only systems. We use them as baseline models that only produce word-level clusters of size I and do not reflect synonymy, but still learn polysemy. On the other hand, consider a system in which each occurrence is mapped to its own local cluster (i,e. no actual local clustering step), and the global step divides occurrences directly into global clusters. We refer to this kind of system as Global-only approaches. They allow to evaluate how useful the local clustering step is in the process: we hypothesize that the local step in Bi-level will reduce potential variance in occurrences by aggregating them, increasing Precision compared to Global-only.


\section{Experiments}
In this section, we evaluate the abilities of the proposed methods to induce concepts and compare the proposed bi-level approach to other methods. We investigate the advantages of the bi-level approach not only for the global viewpoint but also in the local setting.

\subsection{Settings}

\paragraph{Data.}
We choose to use the annotated part of the SemCor 3.03 corpus. This dataset contains occurrences for a wide number of words, and morpho-syntactic annotations provide their lemma and their Part-of-Speech tag. Among all lemmas having at least 10 annotated occurrences, we keep only nouns (excluding proper nouns)\footnote{[ILLEGIBLE]} composed only of alphabetical characters with a minimum length of 3 letters.\footnote{For the sake of simplicity and clarity, this study is focused only on nouns. Indeed, other Parts-of-Speech induce extra difficulties. Verbs for instance required extra preprocessing steps and decisions (e.g. include or exclude gerundive uses, past participle employed like adjectives, etc.). Extension of experiments to other PoS is left to future work.} The resulting lexicon $W$ contains 1{,}560 different lemmas, for which we gather a corpus $O$ containing a total of 52{,}997 occurrences. These occurrences cover 3{,}855 different concepts (WordNet's synsets) in the annotated SemCor data. We also select a subset of these occurrences that are annotated as part of a synset whose size is greater than 1 (i.e., for which the synset contains at least 2 lemmas). We refer to this subset as the \emph{Synon.} subset.

\paragraph{Evaluation of Concept Induction.}
We compare the learned word clustering $\Theta_w$ to the reference $C_w$. We choose to use the BCubed metrics (Bagga and Baldwin, 1998), obtaining Precision and Recall for the evaluated clustering compared to the reference, as well as an $F_1$ score. To account for overlapping clusters, we use the Extended BCubed metrics proposed by Amig{'o} et al. (2009), which has already been used in previous work for overlapping cluster evaluation.

\paragraph{Development split.}
We use a development split of the corpus to select the hyperparameters of our clustering algorithms. This split is built from a subset of polysemous words, and is used to tune hyperparameters for Concept Induction systems based on Concept Induction $F_1$, while Local-only systems tune hyperparameters based on a WSI objective.

\subsection{Systems and baselines}

We evaluate the Local-only, Global-only and Bi-level systems described in Section~4. We use two clustering algorithms: Kmeans and Agglomerative clustering (Agglo). For Kmeans we use the number of clusters as a hyperparameter; for Agglo we use a distance threshold. For each system, we run 5 times with different random seeds and report averaged scores.

\begin{table}[t]
\centering
\caption{Concept Induction BCubed Precision (P), Recall (R) and $F_1$ on the SemCor data averaged over 5 runs.}
\begin{tabular}{l}
[ILLEGIBLE]
\end{tabular}
\end{table}

\paragraph{Sense-inducing systems.}
Comparison to Local-only systems will give a (strong) baseline just by inducing senses without aiming at concepts. We used the same clustering algorithms. We also implement the WSI method proposed by Eyal et al. (2022). It relies on a different paradigm, using the Language Model for substitution instead of word embeddings. From lists of substitutes, they build a graph of substitutes in which they find communities and then assign each occurrence to a community of substitutes to find the word senses. Because Local-only methods only induce senses, their hyperparameters are chosen to maximize a WSI objective on polysemous words of the dev split.

\paragraph{Baselines.}
We construct a candidate clustering $\Theta_w$ where each lemma has its own cluster. This baseline model is referred to as the `Lemmas'' baseline. This is to evaluate the extent to which the information contained by the lemma alone can be used to induce concepts without any knowledge on word senses neither on context. As a second baseline, we create for each lemma as many singletons as the number of different concepts its occurrences are annotated with. All created clusters are of size 1: we account perfectly for polysemy but not at all for synonymy. This second baseline is dubbed `Oracle WSI''.

\subsection{Concept Induction in SemCor}

In Table~1 we display the Concept Induction scores ($F_1$) of proposed baselines and systems on the full SemCor data and on the \emph{Synon.} subset. We observe that Global-only methods outperform Local-only methods on Concept Induction, supporting the need for a cross-lexicon view when learning concepts. Among CI systems, Bi-level approaches generally outperform Global-only approaches, indicating that incorporating local clustering before global clustering is beneficial. Overall, the best scores are obtained by Agglo Bi-level on both settings.

We also observe that the Lemmas baseline obtains a relatively strong performance on the full data but lower performance on the \emph{Synon.} subset, showing that the lemma alone captures some information relevant to concepts but is not sufficient to account for synonymy. Conversely, the Oracle WSI baseline performs poorly on Concept Induction, as it does not capture synonymy at all.

\subsection{Qualitative Analysis of Concepts Clusters}

We manually annotate word clusters (obtained with our best-performing approach, the Agglo Bi-level system) containing at least 2 lemmas according to the semantic similarity between lemmas. Distribution of cluster sizes (in number of lemmas) can be found in Appendix D. We distinguish four categories: synonyms when lemmas are cognitive synonyms (e.g. `necessity'' and `need''), near-synonyms for lemmas close to be synonyms but showing slight difference in meaning (e.g. `duty'' and `task'', the former being stronger than the latter),\footnote{Notions of cognitive synonymy and near-synonymy are discussed by Stanojevic (2009).} related when lemmas show a topical (e.g. `dirt'', `sand'' and `mud'') or lexical relations (e.g. antonyms like `man'' and `woman'') and invalid clusters when lemmas show no semantic relation (e.g. `child'' and ``idea'').

\begin{table}[t]
\centering
\caption{[ILLEGIBLE]}
\begin{tabular}{l}
[ILLEGIBLE]
\end{tabular}
\end{table}

Proportions of these annotations are displayed in Table~2 with respect to the cluster size, the number of lemmas in the cluster. For a given cluster size, if the number of clusters exceeds 50, we randomly sample 50 clusters to be annotated. Overall, the proportion of synonyms and near-synonyms is generally above [ILLEGIBLE] and less than [ILLEGIBLE] of clusters are invalid, indicating that most learned concepts are reliable and meaningful. We argue that the remaining related term clusters, while not synonyms, may still be interesting in less fine-grained studies. The portion of related clusters is in line with findings from previous work showing that BERT was also reflective of other lexical relations, such as hypernymy (Hanna and Marecek, 2021).

\subsection{Benefits at the Local Level}

We now turn back to the local level and assess whether the information brought at the global level helps distinguishing senses of individual words. Here we do not evaluate the word-level soft clustering, but the occurrence-level division of SemCor's data, considering each word independently. In other words, we evaluate WSI in SemCor using annotations as the reference sense clustering.

\paragraph{Evaluation of induced senses.}
For each word $w \in W$, we compare how its set of occurrences $O_w$ is divided in $\Theta$ to how it is divided in the reference $C$ provided by annotations using BCubed metrics, and we average scores obtained across $W$. We display the WSI BCubed $F_1$, as in previous WSI tasks like Jurgens and Klapaftis (2013). Following Amrami and Goldberg (2019), we report $\rho$ the Spearman correlation coefficient between the number of clusters a lemma is assigned to and its number of senses according to annotations, to ensure that the number of created senses actually scales with the actual degree of polysemy. Note that, for CI systems, we evaluate the division of occurrences provided by the final clustering $C$ (i.e. how occurrences are clustered after the global step and its potential merge operations). The quality of sense clusters induced by the local-step only is actually evaluated with Local-only systems.

\begin{table}[t]
\centering
\caption{WSI BCubed $F_1$ and sense number correlation coefficient $\rho$ on SemCor full data. Not computed for Kmeans because the number of cluster is constant.}
\begin{tabular}{l}
[ILLEGIBLE]
\end{tabular}
\end{table}

\paragraph{Local results.}
Results of this local evaluation are displayed in Table~3. Let us recall that Local-only systems' hyperparameters are chosen to maximize the WSI $F_1$ on the dev split, while those of CI systems maximize the Concept Induction $F_1$. Nonetheless, one can observe that all CI systems outperform their Local-only counterparts, achieving higher WSI $F_1$ and $\rho$ even though their hyperparameters are not chosen to match the WSI itself. This indicates that the information brought at the global level by considering cross-lexicon relations may indeed help improving WSI, and benefits between local and global levels go both ways.

We explain the relatively poor performance of the State-of-the-Art WSI system by the fact that we are in a particular setting, where the number of occurrences per lemma is relatively low in SemCor (30 per lemma on average) and so is the average number of occurrences per concept. Data sparsity is a favorable ground for word senses to be misrepresented. As such, methods meant to be applied on larger datasets like the one of Eyal et al. (2022) may not work as well as expected. Our results show the limitations of these systems when the amount of training data is low and the interest of aiming at areas where data are not available in large quantities and still require to induce senses. In the case of the study of Lexical Semantic Change (the evolution of word meanings over time), recent works often perform WSI in diachronic corpora that are unbalanced and small (Tahmasebi et al., 2021).

\begin{table}[t]
\centering
\caption{Accuracy scores on the nouns of the WiC dataset (Pilehvar and Camacho-Collados, 2019).}
\begin{tabular}{l}
[ILLEGIBLE]
\end{tabular}
\end{table}


\section{Extrinsic Evaluation with Concept-aware Embeddings}
In their work, Eyal et al.\ (2022) derive sense-aware static embeddings from their WSI method, training them on the Wikipedia dataset and used them for the Word-in-Context (WiC) task. They achieve nearly-SotA results on the dataset proposed by Pilehvar and Camacho-Collados (2019), and report to be outperformed only by methods using external lexical knowledge and resources. We proceed to the same extrinsic evaluation of our work, constructing concept-aware embeddings using concept clusters of Concept Induction systems (Global-only and Bi-level Agglo). To obtain such embeddings, we average all vectors representating occurrences in SemCor contained each global cluster to get one vector per concept cluster.

The WiC task consists of determining whether two occurrences of a target lemma $w$ correspond to the same sense. The WiC dataset's target words are nouns and verbs, but like in the rest of this paper, we restrict our scope to nouns.

To solve the task, we use BERT Large to create representations of the two target occurrences. Each of them is assigned to a concept by finding the closest concept-aware [ILLEGIBLE] using cosine distance. The decision depends on whether the two occurrences are mapped to the same concept (true) or to distinct ones (false). Results are displayed in Table 4.

Our concept-aware embeddings obtain very similar results to those of their sense-aware embeddings, with ours derived from our bi-level approach even outperforming their CROW method. Interestingly, our embeddings were trained with far fewer resources than theirs, as we used 52 997 occurrences from the SemCor dataset while they used a dump of Wikipedia, gathering millions of occurrences. This emphasizes the value of concept-aware embeddings: the use of cross-lexicon information allows competitive results with fewer resources.


\section{Conclusion}
In this paper, we argued that, while word senses allow to investigate polysemy, concepts are a larger perspective that allows the study of polysemy as well as synonymy. We defined Concept Induction, the unsupervised task to learn a soft-clustering of words in a large lexicon, directly from their in-
context occurrences in a corpus. Then, we pro-
posed a formulation of this problem in terms of
[ILLEGIBLE]
complementary views, and tested an approach that
uses information from both levels using [ILLEGIBLE]
Language Models' on concept-annotated [ILLEGIBLE]

[ILLEGIBLE] we found that this bi-level view [ILLEGIBLE] for concept Induction, and even for [ILLEGIBLE] with manual annotations, ensuring that clusters [ILLEGIBLE] to actual synonyms and con-
cepts. finally, we showcased an external applica-
tion of [ILLEGIBLE] methodology to create concept-aware
embeddings [ILLEGIBLE] be competitive to other meth-
ods on semantic tasks, such as word-in-context. concept Induction opens the way for a different
perspective [ILLEGIBLE] on lexical semantics in NLP, and can
be [ILLEGIBLE] a basis [ILLEGIBLE] studies of lexical meanings as
it is expressive enough to reflect relations on both
sides of the word-meaning mapping.


\section{Limitations}
The formal framework we defined uses terminology and notions from rather structuralist/relational assumptions of the language's lexical system (e.g. senses, discrete concepts, etc.). We made this choice based on how lexical databases like WordNet (and its derivatives), or other like the Historical Thesaurus of English for instance, are designed

using the `word/sense/concept'' structure. From a purely practical point of view, this choice makes sense as these resources would be the primary [ILLEGIBLE] source for task data's annotations. Conceptually, senses are also a notion widely used in computational linguistics and we wanted to propose Concept Induction as a step `beyond'' [ILLEGIBLE] conventional aspect and its related tasks. Future research may explore definitions/extensions of Concept Induction outside of this structuralist/relational framework, towards cognitive semantics for instance (Geeraerts,2010). Evaluating Concept Induction is mainly limited by the low amount of suitable annotated corpora. Not only the data need to be annotated in concepts, but these annotations must cover a wide variety of lemmas for synonymy to be sufficiently represented in the corpus. Future work may find or create datasets meeting these requirements to evaluate Concept Induction outside of SemCor. For now, the study is limited to nouns. performances of benchmarked algorithms and systems may change with other Part-of-speech tags. Our Bi-level method allows the global clustering to merge local clusters, leveraging lexicon-level information to be used to correct Word Sense Induction errors at the [ILLEGIBLE]. By its sequential nature, our method does not allow to split local clusters using global-level information, which could lead to better results. Further research directions include creating an iterative version of our methodology (alternating local and global clustering), or attempting to tackle both clustering objectives simultaneously with bi-level constrained clustering. Our results about sense-induction at the local level showed that usual WSI methods may not be robust in our setting where there are few occurrences for some lemmas. We demonstrated that, in this setting, concept-inducing methods provided better division in word senses. In many fields of linguistics, corpora are not very large and do not contain hundreds of occurrences for each word. Nonetheless, it is still uncertain if this observed advantage of CI systems would still hold on bigger datasets with many [ILLEGIBLE] per lemma, a setting better-suited for usual WSI methods. In this paper, we limited our study to Nouns, the morpho-syntactic class exhibiting the most prominent semantic features. We leave to further research the study of Concept Induction for Verbs, Adjectives, or the heterogeneous family of Adverbs.


\section{Ethical Considerations}
Our methodology uses pretrained Contextualized Language Models, which are know to encode and replicate social biases contained in their training data and sometimes amplify them. While we do not observe surface-level biases arising when manually annotating concept clusters, it is still an open question of how these social biases may influence or even change results when inducing concepts in SemCor.


\section*{Acknowledgements}
We gratefully thank the anynymous reviewers for their insightful comments. This research was funded by Inria Exploratory Action COMANCHE.


\appendix

\section{Extended BCubed to Evaluate CI and WSI}
\section{Extended BCubed to Evaluate CI and WSI}

The extension of BCubed for overlapping clusters rely on two quantities, Multiplicity Precision (MP) and Multiplicity Recall (MR). In the case of Concept Induction, MP and MR between two lemmas are defined as follows:

\begin{equation}
\mathrm{MP}(w_1,w_2)=\frac{\min\left(\left|f(w_1)\cap f(w_2)\right|,\left|g(w_1)\cap g(w_2)\right|\right)}{\left|f(w_1)\cap f(w_2)\right|}
\end{equation}

\begin{equation}
\mathrm{MR}(w_1,w_2)=\frac{\min\left(\left|f(w_1)\cap f(w_2)\right|,\left|g(w_1)\cap g(w_2)\right|\right)}{\left|g(w_1)\cap g(w_2)\right|}
\end{equation}

with $w_1$ and $w_2$ two lemmas, and $g$ a reference clustering function and $f$ the clustering function we want to evaluate. MP (resp.\ MR) can be computed for every lemma $w_1$ with every other lemma $w_2$ sharing at least one cluster with $w_1$ in $f$ (resp.\ in $g$). We denote $\mathrm{MP}(w_1,\cdot)$ and $\mathrm{MP}(w_1,\cdot)$ the obtained averages. In the case of non-overlapping clusters, this formulation gives the same result as the original (non-extended) BCubed. To evaluate WSI, the formulation is the same but we do not evaluate at the word-level but at the occurrence-level.

Precision, Recall and F-score are obtained as follows:

\begin{equation}
\mathrm{Precision}=\frac{1}{|W|}\sum_{w\in W}\mathrm{MP}(w,\cdot)
\end{equation}

\begin{equation}
\mathrm{Recall}=\frac{1}{|W|}\sum_{w\in W}\mathrm{MR}(w,\cdot)
\end{equation}

\begin{equation}
F_{\beta}=(1+\beta^{2})\frac{\mathrm{Recall}\times \mathrm{Precision}}{\beta^{2}\times \mathrm{Precision}+\mathrm{Recall}}.
\end{equation}

By default we fix $\beta = 1$, as we compare the learned clustering and the reference clustering as equals and therefore do not find that Precision and Recall should be weighted differently.

Amig'o et al.\ (2009) showed that the benefits of BCubed over other clustering scores. For instance, Rand Index does not handle well the case of many small clusters, which is likely to be the case for Concept Induction. We also prefer Extended BCubed over Overlapping Normalized Mutual Information (McDaid et al., 2011) as the latter is matching-based. That is, the repetition (or non-repetition) of identical clusters will have no impact on the measure. However, we can easily imagine identical clusters of words to be repeated as they may correspond to distinct concepts. In Extended BCubed, repeated clusters are taken in account as we measure the number of times two lemmas are clustered together. The denominator of MP ensures that over-estimating the number of common clusters is also penalized, and those of MR ensures that under-estimating is penalized. $\min$ operators are there to prevent both quantities to grow over 1.


\section{Splits and dataset statistics}

In Table 5 we display statistics over the different splits we used. Dev is a subset containing a sample of 10% of concepts and their occurrences. Synon. is a subset containing only concepts instantiated with 2 lemmas or more, and their occurrences.

\begin{table}[t]
\centering
\begin{tabular}{lrrrrrrr}
\toprule
& #Occs & #Lemmas & #Concepts & #Occs/Concept & #Occs/Lemmas & $d_{\mathrm{Lex}}$ & $d_{\mathrm{Polysemy}}$ \
\midrule
Full data & 52'997 & 1'560 & 3'855 & 13.75 & 33.97 & 1.14 & 2.83 \
Dev & 4'795 & 389 & 386 & 12.42 & 12.33 & 1.14 & 1.13 \
Synon & 13'158 & 630 & 447 & 29.44 & 20.89 & 2.24 & 1.59 \
\bottomrule
\end{tabular}
\caption{Statistics on the different data splits in annotated SemCor. The split ``Synon'' only contains occurrences of concepts instantiated with multiple lemmas (cases of synonymy). $d_{\mathrm{Lex}}$ is the average number of unique lemmas per concept, $d_{\mathrm{Polysemy}}$ is the average number of distinct concepts per lemma.}
\end{table}


\section{Used hyperparameters and layers}

\subsection{CLM layers}

Prior work like Ethayarajh (2019) showed that later layers usually correlates with deeper levels of contextualization and more semantic information, Chronis and Erk (2020) showed that moderately-late were preferred for lexical similarity while very last layers were preferred for semantic relatedness. To get embeddings, we try 4 sets of layers corresponding to different depths: first layers (1 to 4), moderately early layers (8 to 11), moderately late (14 to 17), and last layers (21 to 24). To get the representation of a word's occurrence, we simply average its embeddings from the four chosen layers into one single 1024-dimensional embedding. For Concept Induction, we find that best results were obtained using layers 14 to 17, that are the reported results.

\subsection{Hyperparameters}

For Eyal et al.\ (2022), we tried different resolution, varying it from 1e-3 to 10, for the Louvain clustering but found very little to no effect.

For Kmeans at the local level, we varied the number of clusters $k$ between 2 and 10. For Agglomerative clustering at both levels, we tried single, average and complete linkage.

The distance threshold in Agglo $\tau$ was indexed on the distribution of distances. We fixed an hyperparameter $\nu$ and derived $\tau = \mathrm{avg}(d) - \nu \cdot \mathrm{std}(d)$ with $d$ the distribution of distances between clustered instances. We made $\nu$ vary between $-4$ and $+8$. For global Kmeans, the number of clusters was indexed using a proportion $\pi$ on the number of lemmas (e.g.\ $120% \times W$), $\pi$ varying from $40%$ to $400%$. This may help transfering hyperparameters to other dataset in future research.

Best hyperparameters choices are in Table 6

\begin{table}[t]
\centering
\begin{tabular}{ll}
\toprule
Systems & Best hyperparameters \
\midrule
Local-only Kmeans & $k = 3$ \
Local-only Agglo & $\mathrm{linkage} = \mathrm{average}, \nu = 1.0$ \
Global-only Kmeans & $\pi = 120%$ \
Global-only Agglo & $\mathrm{linkage} = \mathrm{average}, \nu = 3.5$ \
Bi-level Kmeans & $k = 8, \pi = 120%$ \
Bi-level Agglo & $\mathrm{linkage} = \mathrm{average}\ \mathrm{(both)}, \nu_{\mathrm{local}} = 0.0, \nu_{\mathrm{global}} = 4.5$ \
Bi-level Kmeans (local Agglo) & $\mathrm{linkage} = \mathrm{average}\ \nu_{\mathrm{local}} = 0.0, \pi = 120%$ \
Bi-level Agglo (local Kmeans) & $k = 10, \mathrm{linkage} = \mathrm{average}, \nu_{\mathrm{global}} = 4.5$ \
\bottomrule
\end{tabular}
\caption{Best hyperparameters on the \emph{Dev} split.}
\end{table}


\section{Concept Clusters Size Distribution}
The distribution of the concept cluster size (in number of lemmas) obtained with Bi-level Agglo system can be found in Figure 2.


\section{Scientific Artifacts}
We release our source code for reproducibility purposes at \url{[https://github.com/lbastien/concept-induction}](https://github.com/lbastien/concept-induction}).


\end{document}
=====END FILE=====
