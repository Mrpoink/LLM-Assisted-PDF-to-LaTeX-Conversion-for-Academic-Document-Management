=====FILE: main.tex=====
\documentclass[11pt]{article}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage{microtype}
\usepackage{hyperref}

\title{Academics Can Contribute to Domain-Specialized Language Models}
\author{Mark Dredze \and Genta Indra Winata \and Prabhanjan Kambadur \and Shilie Wu \and Ozan Irsoy \and Steven Lu \and Yadim Dabravolski \and David S. Rosenberg \and Sebastian Gehrmann}
\date{}

\begin{document}
\maketitle

\begin{abstract}
Commercially available models dominate academic leaderboards. While impressive, this has concentrated research on creating and adapting general-purpose models to improve NLP leaderboard standings for large language models. However, leaderboards collect many individual tasks and general-purpose models often underperform in specialized domains; domain-specific or adapted models yield superior results. This focus on large general-purpose models excludes many academics and draws attention away from areas where they can make important contributions. We advocate for a renewed focus on developing and evaluating domain- and task-specific models, and highlight the unique role of academics in this endeavor.

[ILLEGIBLE] solved with a chat-like interface. Second, the best-performing LLMs are often commercial systems, which are sometimes opaque about training data, system architecture, and training details. Third, frequent model updates hinder reproducibility.

The resources required to train large general language models naturally constrain research to large organizations, and researchers (or academics) outside of these organizations have become dependent on closed commercial systems, or open systems with limited transparency regarding their training data. This is partly reflected in broader AI trends: Zhang et al. (2021) found that roughly 30% of papers at AI conferences (including *CL) have a Fortune 500 tech affiliation. Increased resources contribute to the success of transformer-

\end{abstract}

\section{Introduction}
Natural language processing (NLP) research has historically produced domain- and task-specific supervised models. The field has shifted course in the past few years, with a singular focus on general-purpose generative large language models (LLMs) that, rather than focusing on a single task or domain, do well across many tasks (Brown et al., 2020; Chowdhery et al., 2022; Workshop et al., 2022; Zhang et al., 2022; Touvron et al., 2023b). By training on massive amounts of data from many sources, these models can do well on extremely broad professional and linguistic examinations (Achiam et al., 2023; Anil et al., 2023), college-level knowledge questions (Hendrycks et al., 2021; Lai et al., 2023), and collections of reasoning tasks (Suzgun et al., 2023).

While the trend to develop a single, general-purpose generative model is a net positive change that has resulted in impressive results, it has also slowed down progress in other areas of NLP. First, we are less focused on problems that cannot be solved with a chat-like interface. Second, the best-performing LLMs are often commercial systems, which are sometimes opaque about training data, system architecture, and training details. Third, frequent model updates hinder reproducibility.

The resources required to train large general language models naturally constrain research to large organizations, and researchers (or academics) outside of these organizations have become dependent on closed commercial systems, or open systems with limited transparency regarding their training data. This is partly reflected in broader AI trends: Zhang et al. (2021) found that roughly 30% of papers at AI conferences (including *CL) have a Fortune 500 tech affiliation. Increased resources contribute to the success of transformer-based LLMs (Vaswani et al., 2017), with available hardware (Hooker, 2021) and benchmarks (Dehghani et al., 2021) both playing a deciding role in what models end up being developed. By optimizing the average score across hundreds of shallow tasks, we are smoothing out any signal that would be gained from deeply engaging with individual tasks. Developing domain-specific models can help identify model and training choices that yield improvements on tasks within those domains.

In this paper, we argue for renewed attention to domain-specific models with rigorous and domain-expert informed evaluations. Because many academics are excluded from LLM development due to resource constraints, attention has been drawn away from research areas where academics can make the greatest contributions: deep dives on specific challenging problems. Thus, we propose several research questions to reorient the research community towards developing domain-specific models and applications, where academics are uniquely suited to lead.


\section{LLMs: A Brief History}
While modern LMs date back to Jelinek (1976), we summarize very recent history to describe the current environment. In the wake of the popularization of neural word embeddings by word2vec (Mikolov et al., 2013), contextualized representations of language as features for supervised systems were realized by ELMo (Peters et al., 2018) followed by BERT (Devlin et al., 2019; Liu et al., 2019). BERT and subsequent models became the base models for supervised systems utilizing task-specific fine-tuning and continued pre-training for new domains (Gururangan et al., 2020), e.g., for clinical tasks ELMo (Schumacher and Dredze, 2019) and clinicalBERT (Huang et al., 2019).

Parallel work utilized transformers for autoregressive LLMs, resulting in GPT (Radford et al., 2018), GPT-2 (Radford et al., 2019), BART (Lewis et al., 2020a; Liu et al., 2020), CTRL (Keskar et al., 2019), T5 (Raffel et al., 2020; Xue et al., 2021), and XGLM (Lin et al., 2021). These models had some few-shot capabilities, but they could each be adapted (fine-tuned) for a specific task of interest. Some models were available to academics, though training a new model was beyond reach for many.

GPT-3 (Brown et al., 2020) greatly increased model size and changed our understanding of LLMs. Impressive in-context (few-shot) learning pushed the idea that a single large model could solve a wide range of tasks. While the cost of resources meant training was restricted to a few groups, work focused on training bigger models (Chowdhery et al., 2022; Anil et al., 2023; Zhang et al., 2022; Touvron et al., 2023a; Rae et al., 2021).

While only a few could train large models, many studied how best to use them: prompt engineering (Liu et al., 2023), prompt tuning (Han et al., 2022; Wei et al., 2022), evaluation (Liang et al., 2022), among many other topics. Commercial LLM APIs, and eventually open source models (Zhang et al., 2022; Workshop et al., 2022; Touvron et al., 2023a,b; Groeneveld et al., 2024), facilitated this work. Ignat et al. (2024) noted the massive research shift to LLMs reflected in Google Scholar citations. Subsequent work in instruction tuning (Ouyang et al., 2022) and fine-tuning (Wei et al., 2022; Chung et al., 2022; Longpre et al., 2023) have further centralized research around general-purpose models. Many consider fine-tuning for specific applications to be obsolete: \emph{why would you tune a model for a specific task when you could tune a single model to do well on all tasks?}\footnote{Distillation for task-specific models remains popular if smaller models are desired (Hsieh et al., 2023).}

Despite this view, multiple domain-specific LLMs have demonstrated that domain-specific data leads to models that outperform much larger models (Wu et al., 2023; Taylor et al., 2022). Med-PaLM has shown that adapting even giant LLMs to a specific domain leads to vastly increased performance (Singhal et al., 2022, 2023).\footnote{We acknowledge that the biomedical domain is a rapidly developing area, and GPT-4 without fine-tuning was reported to surpass MedPaLM 2 (Nori et al., 2023).} Furthermore, the release of LLaMA (Touvron et al., 2023a) led quickly to Alpaca (Taori et al., 2023) and a wave of new fine-tuned versions of LLaMA for specific tasks. This trend strongly indicates that domain-specific models, especially for constrained sizes, are still highly relevant.

To be clear, our concern is not with closed models, which play an important role in the model ecosystem. Models range from full to limited to no access, with some closed models providing incredibly detailed information (Hoffmann et al., 2022; Rae et al., 2019; Wu et al., 2023) and others providing none (Achiam et al., 2023). Our lament over this focus on general models, either open or closed, is that it draws attention away from work on task- and domain-specific models and evaluations. Academics have become product testers, instead of focusing on tasks where they can play a unique role. Moreover, existing academic benchmarks increasingly serve a reduced purpose for commercial models; we are hill-climbing on benchmarks without a way to ensure existing LLMs have not been trained to excel on these benchmarks (Dodge et al., 2021). Furthermore, we rely on benchmarks in place of deep engagement with an application and its stakeholders.


\section{The Need for Domain-Specific LLMs}
In general, web data does not reflect the needs of all NLP systems. Historically, the community has developed systems for specialized domains such as finance, law, bio-medicine, and science. Accordingly, there have been efforts to build LLMs for these domains (Wu et al., 2023; Taylor et al., 2022; Singhal et al., 2022; Bolton et al., 2023; Luo et al., 2022; Lehman et al., 2023; Garcia-Ferrero et al., 2024). We need a deep investment in how best to develop and evaluate these models in partnership with domain experts. \emph{How should we best integrate insights gained from the development of general-purpose models with these efforts?} We propose several research directions.

\textbf{How can general-purpose models inform domain-specific models?} Building domain-specific models should benefit from insights and investments into general-purpose models. There are several strategies: training domain-specific models from scratch (Taylor et al., 2022; Bolton et al., 2023), mixing general and domain-specific data (Wu et al., 2023), and fine-tuning existing models (Singhal et al., 2022, 2023). Focusing on domain-specific needs, applications, and knowledge with guidance from topic experts will benefit us in acquiring a better model for specific NLP tasks. \emph{Which approach yields the best results for task performance and overall cost?}

\textbf{What is the role of in-context learning and fine-tuning?} Both LIMA (Zhou et al., 2023) and Med-PaLM (Singhal et al., 2022) use a small number of examples to tune a model. With expanding context size, we may soon rely entirely on in-context learning (Petroni et al., 2020). This blurs the lines between changing model parameters and conditioning during inference. Beyond inference speed tradeoffs between the two, there may be value in tuning on tens of thousands (or more) of examples. \emph{Which domain-specific examples are the most effective to include and in what manner?}

\textbf{How can LLMs be integrated with domain-specific knowledge?} Specialized knowledge is key in many domains. RAG (Lewis et al., 2020b; Guu et al., 2020) and KILT-derived works (Petroni et al., 2021) focus on knowledge-intensive tasks by including retrieval steps. Work on attributed QA (Bohnet et al., 2022) takes a similar approach, as do search LLMs that require interaction with retrieved data (Nakano et al., 2021). Rich updated knowledge sources will always exist beyond the model, especially in environments like medicine, finance, and many academic disciplines.


\section{Evaluation of Domain-Specific Models}
The evaluation of NLP systems is at a crossroads, and the downstream usage of LLMs and evaluation approaches have diverged. Benchmarks assume that their results translate to insights into similar tasks and usefulness for commercial applications. But benchmarks have become increasingly narrow in scope, oftentimes assessing one metric on a single, often flawed, dataset (Mitchell et al., 2019; Kiela et al., 2021; Ethayarajh and Jurafsky, 2020). The primary evaluation approach for LLMs has been to evaluate on a broad set of these narrow benchmarks (Liang et al., 2022, HELM) (Srivastava et al., 2022, BIG-Bench). High average performance argues for a broad range of capabilities; however, one size may not fit all. Since specific uses of LLMs are typically much more narrow, we identify three major issues and associated research opportunities with this approach.

\textbf{Depth-first Evaluation} Current approaches focus on a single model doing everything well on average instead of being useful in a single domain. However, it is widely acknowledged that the standard benchmarks for most tasks are insufficient (e.g., for summarization, Fabbri et al., 2021; Goyal et al., 2022). Task-specific evaluations have thus adopted additional protocols that measure how well models transfer to different domains, how robust they are, and whether they stand up to concept drift (Mille et al., 2021; Dhole et al., 2021). These details disappear when benchmarking on 100+ tasks. Yet, a model’s usefulness is not solely defined by doing okay on everything but rather by how well it performs in specific and narrow tasks that provide value. This value is only realized if the model does not suffer from catastrophic failures.

Exemplar studies that perform deep dives on LLMs for specific tasks exist in healthcare (Zack et al., 2024; Eriksen et al., 2023; Ayers et al., 2023; Han et al., 2024; Chen et al., 2024; Strong et al., 2023), law (Blair-Stanek et al., 2023a,b; Magesh et al., 2024), and physics (Kim et al., 2024), among other areas. We encourage more work on evaluation practices for specific tasks that can handle various model setups and yield informative insights (Zhang et al., 2023; Liang et al., 2022).

\textbf{Sound Metrics} For convenience, most benchmark tasks are formulated as multiple choice question answering or classification. This is not how LLMs are often used. For much more common generation tasks, researchers have been ringing alarms about broken evaluations (Gehrmann et al., 2023). It is dubious whether we gain insights into non-task-specific generation through NLU benchmarks. If we are performing the depth-first evaluation of a generation task, a remaining hurdle---and why researchers fall back to NLU tasks---is the lack of robust metrics. While there is much recent work on better metrics (Celikyilmaz et al., 2020; Gehrmann et al., 2023), a troubling trend is the use of LLMs as evaluators (e.g., Sellam et al., 2020; Chiang et al., 2023). This approach poses many risks, including the implicit assumption that the evaluation model has access to the ground truth judgment. While there are some promising results, using an LLM out of the box should be avoided (e.g., Wang et al., 2023a,b). Moreover, it is unclear how to evaluate the evaluator when it is a non-deterministic API, or how to scale the development of learned metrics and quantify the strength of a metric.

\textbf{Products are not Baselines} If we really do want to evaluate 100+ tasks, there are many issues with the soundness of evaluation setups. At this scope, it is impossible to run careful ablation studies or to assess the effect of changes to methodology in a causal manner. Moreover, different LLMs respond differently to prompts. The BLOOM evaluation averaged over multiple prompts and found significant variance (Workshop et al., 2022). This variance leads to a lack of reproducibility: LLaMA (Touvron et al., 2023a) claimed high MMLU (Hendrycks et al., 2021) performance but didn’t release the prompts that led to them.\footnote{Similarly, the evaluation scheme makes a difference (Liang et al., 2022, Fig. 33).} High evaluation costs mean benchmarks pick a small number of setups (sometimes only one) for each task, which introduces further bias, making it hard to construct fair benchmarks on many tasks.

An additional issue with the current benchmarking approach is that the best-performing models are often commercial APIs. With limited transparency regarding data and training, we cannot fairly evaluate these models (e.g., data leakage). Furthermore task-specific tuning may have been selected based on these specific benchmarks. Moreover, the underlying models change frequently, so it is unclear whether a result will hold for long.

These evaluation issues prompt significant open questions: 1) How do we develop consistent evaluation setups across models that give true measures of performance? 2) How do we develop evaluation setups and metrics more closely aligned with downstream usage? 3) How do we develop evaluation suites that support depth-first evaluation and not breadth-first benchmarking?\footnote{There was significant confusion surrounding model evaluation: \url{[https://huggingface.co/blog/open-llm-leaderboard-mmlu}}](https://huggingface.co/blog/open-llm-leaderboard-mmlu}})


\section{The Role of Academics}
A focus on general-purpose LLMs has forced academics to work with large base models and perhaps, shifted the focus to solve problems of immediate industrial interest. Many academics feel excluded from current research trends (Ignat et al., 2024) and the academic and industry relationship is changing (Littman et al., 2022). Shifting attention back to domain-specific applications emphasizes areas where academics hold an advantage: partnerships with domain experts to invest in specific tasks, and consideration of broader societal needs.

Developing domain-specific models requires domain expertise and universities are diverse academic environments that house experts in many domains. Collaborations with these experts can identify data sources, tasks, and challenges important within each domain. Furthermore, these collaborations are the best avenues for better alignment of evaluations with use cases (Winata et al., 2024), and can support the development of proper metrics. These collaborations are necessary to explore wide open interdisciplinary topics, such as models for protein structure prediction (Tunyasuvunakool et al., 2021; Vig et al., 2021) and games as proxies for reasoning (Silver et al., 2016; Agostinelli et al., 2019; Schrittwieser et al., 2020). This includes developing domain-specific resources, which require domain experts to properly design and construct the datasets. Further, areas where industry underinvests are those where academics could focus attention. For example, low-resource languages are not served by a general-purpose multilingual LLM, nor will we reasonably have enough data to support current LLM training methods. Dialects and variations in languages are still wide open topics (Aji et al., 2022; Winata et al., 2023; Nicholas and Bhatia, 2023).

General-purpose LLMs are unlikely to solve problems in many important domains, with many open research problems that can only be solved by domain-specific approaches. Focusing on domain-specific knowledge will benefit us in acquiring a better model and developing application strategies more aligned with how humans learn domain-specific knowledge (Tricot and Sweller, 2014). For many interdisciplinary areas, subject matter experts are essential, and the problems must be defined clearly. The first pass from an LLM is often impressive, but it hides the trenches and areas where things are most interesting. We need a renewed focus on developing and evaluating domain-specific models and applications, an area where academics can play a leading role. Let us not be distracted by claims that a single model solves all tasks, and instead deeply explore and understand the needs and challenges of specific domains.


\section*{Limitations}
The literature that we explored in this opinion paper is limited to the area of LLMs. We study the history of LLMs from the literature on word embeddings, encoder-only, and generative transformers to the latest advancement of APl-based LLMs.


\section*{Ethics Statement}
We confirm that all ethical concerns are addressed.


\section*{References}
{\small
\noindent Sy s tems, 33:181 1 -1901.\par
\noindent Josh Achiam, Steven Adler, Sandhini Agarwal, Lama\par
\noindent Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Asli Celikyilmaz,Elizabeth Clark, and Jianfeng Gao.\par
\noindent 2020. Evahntion of text generation: A survey. arXlv,\par
\noindent Diogo Almeida, Janko Altenschmidt, Sam Altman,\par
\noindent 2006.14799.\par
\noindent Shyamal Anadkat, et al. 2023. Gpt-4 technical report.\par
\noindent arXiv p re p rint arXiv : 2 3 0 3.0877 4. Hanjie Chen, Zhouxiang Fang, Yash Singla, and Mark\par
\noindent Forest Agostinelli, Stephen McAleer, Alexander Dredze. 2024. Benchmarking large language models\par
\noindent on answering and explaining challenging medical\par
\noindent Shmakov, and Pierre Baldi. 2019. Solving the Ru-\par
\noindent bik's Cube with deep reinforcement learning and questions. arXiv preprint arXiv : 2402. I 8060.\par
\noindent search. N ature M achine Int e ll i g enc e, 1 (8) : 3 56-3 63. Wei-Lin Chiang, Zhuohan L| Zi Lin, Ying Sheng,\par
\noindent Alham Aji, Genta Indra Winata, Fajri Koto, Samuel Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\par
\noindent Zhrrang, Yonghao Zhtang, Joseph E Gonzalez, et al.\par
\noindent Cahyawijaya, Ade Romadhony, Rahmad Mahendra,\par
\noindent 2023. Yicrlna: An open-source chatbot impressing\par
\noindent Kemal Kurniawan, David Moeljadi, Radityo Eko Pra-\par
\noindent sojo, Timothy Baldwin, et al. 2022. One country, gpt-4 with 90Vo*\par
\noindent many languages: Building the indonesian nlp bench-\par
\noindent mark dataset and model. In Proceedings of the 60th\par
\noindent Annual Meeting of the Association for Computational\par
\noindent Linguistics (Volume 1: Long Papers), pages 7265-\par
\noindent 7279.\par
\noindent Rohan Anil, Gabriel Pereyra, Alexandre Passos,\par
\noindent Robert L Johnson, Cesar Cibeira, Mahmoud\par
\noindent Mostafa, et al. 2023. Palm 2 technical report. arXiv\par
\noindent preprint arXiv:2305.10403.\par
\noindent John W Ayers, Adam Poliak, Mark Dredze, Eric C\par
\noindent Leas, Jennifer J Zhu, Jessica B Kelley, Rachna\par
\noindent Reddy, Joseph A Ha, C Yiu Cho, and Davey M\par
\noindent Smith. 2023. Comparing physician and artificial\par
\noindent intelligence chatbot responses to patient questions\par
\noindent posted to a public social media forum. \emph{JAMA In-\par
\noindent ternal Medicine}, 183(6):589--596.\par
\noindent Iz Beltagy, Arman Cohan, and Kyle Lo. 2020. Long-\par
\noindent former: The long-document transformer. arXiv preprint\par
\noindent arXiv:2004.05150.\par
\noindent Andrew Blair-Stanek, Nils Holzenberger, and Benjamin\par
\noindent Van Durme. 2023a. The humans pretend and the\par
\noindent ai acts: A simulation of legal education with gener-\par
\noindent ative language models. In \emph{Proceedings of the 2023}\par
\noindent \emph{Conference on Empirical Methods in Natural Language}\par
\noindent \emph{Processing}, pages 6056--6077.\par
\noindent Andrew Blair-Stanek, Nils Holzenberger, and Benjamin\par
\noindent Van Durme. 2023b. Openai cribbed our tax exam-\par
\noindent ple, but can gpt-4 really do tax? arXiv preprint\par
\noindent arXiv:2309.09992.\par
\noindent Bernd Bohnet, Mnh Q. Tran, Pat Verga, Roee Aharoni,\par
\noindent Daniel Andor, Livio Baldini Soares, and Michael\par
\noindent Collins. 2022. Attributed question answering: Eval-\par
\noindent uating the attribution of answers to their sources. In\par
\noindent \emph{Proceedings of the 60th Annual Meeting of the Associa-}\par
\noindent \emph{tion for Computational Linguistics (Volume 1: Long Papers)},\par
\noindent pages 4621--4632.\par
\noindent Elliot Bolton, Shubham Gupta, and Christopher D.\par
\noindent Manning. 2023. Sapiens: A system for fine-\par
\noindent tuning language models for reasoning in science.\par
\noindent arXiv preprint arXiv:2306.11470.\par
\noindent Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie\par
\noindent Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\par
\noindent Neelakantan, Pranav Shyam, Girish Sastry, Amanda\par
\noindent Askell, et al. 2020. Language models are few-shot\par
\noindent learners. In \emph{Advances in Neural Information Processing}\par
\noindent \emph{Systems}, 33:1877--1901.\par
\noindent Asli Celikyilmaz, Elizabeth Clark, and Jianfeng Gao.\par
\noindent 2020. Evaluation of text generation: A survey. arXiv,\par
\noindent 2006.14799.\par
\noindent Hanjie Chen, Zhouxiang Fang, Yash Singla, and Mark\par
\noindent Dredze. 2024. Benchmarking large language models\par
\noindent on answering and explaining challenging medical\par
\noindent questions. arXiv preprint arXiv:2402.18060.\par
\noindent Danqi Chen, Adam Fisch, Jason Weston, and Antoine\par
\noindent Bordes. 2017. Reading Wikipedia to answer open-\par
\noindent domain questions. In \emph{Proceedings of the 55th Annual}\par
\noindent \emph{Meeting of the Association for Computational Linguistics}\par
\noindent \emph{(Volume 1: Long Papers)}, pages 1870--1879.\par
\noindent Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,\par
\noindent Henrique Ponde de Oliveira Pinto, Jared Kaplan,\par
\noindent Harri Edwards, Yuri Burda, Nicholas Joseph, Greg\par
\noindent Brockman, et al. 2021. Evaluating large language\par
\noindent models trained on code. arXiv preprint arXiv:2107.03374.\par
\noindent Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\par
\noindent Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao\par
\noindent Zhuang, Joseph E Gonzalez, and Ion Stoica. 2023.\par
\noindent Vicuna: An open-source chatbot impressing gpt-4\par
\noindent with 90%* chatgpt quality. \emph{[ILLEGIBLE]}\par
\noindent Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,\par
\noindent Maarten Bosma, Gaurav Mishra, Adam Roberts,\par
\noindent Paul Barham, Hyung Won Chung, Charles Sutton,\par
\noindent Sebastian Gehrmann, and others. 2022. PaLM:\par
\noindent Scaling language modeling with pathways. arXiv\par
\noindent preprint arXiv:2204.02311.\par
\noindent Hyung Won Chung, Le Hou, Shayne Longpre, Bar-\par
\noindent ret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\par
\noindent Wang, Mostafa Dehghani, Siddhartha Brahma, et al.\par
\noindent 2022. Scaling instruction-finetuned language models.\par
\noindent arXiv, 2210.11416.\par
\noindent Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\par
\noindent Kristina Toutanova. 2019. BERT: Pre-training of\par
\noindent deep bidirectional transformers for language under-\par
\noindent standing. In \emph{Proceedings of the 2019 Conference of the}\par
\noindent \emph{North American Chapter of the Association for Computational}\par
\noindent \emph{Linguistics: Human Language Technologies, Volume 1 (Long}\par
\noindent \emph{and Short Papers)}, pages 4171--4186.\par
\noindent Mostafa Dehghani, Alexey Gritsenko, Mario N.\par
\noindent Belk, and others. 2021. Scaling vision with sparse\par
\noindent mixture of experts. arXiv preprint arXiv:2106.05974.\par
\noindent Bhavya Dhole, Gaurav Kumar, and Romain Paulus.\par
\noindent 2021. Robustness and domain transferability of\par
\noindent neural summarization. In \emph{Proceedings of the 2021}\par
\noindent \emph{Conference on Empirical Methods in Natural Language Processing},\par
\noindent pages 10358--10375.\par
\noindent Jesse Dodge, Suchin Gururangan, Dallas Card, Roy\par
\noindent Schwartz, and Noah A. Smith. 2021. Documenting\par
\noindent large webtext corpora: A case study on the colosal\par
\noindent clean crawled corpus. arXiv preprint arXiv:2104.08758.\par
\noindent Kawin Ethayarajh and Dan Jurafsky. 2020. Utility is\par
\noindent in the eye of the user: A critique of nlp leaderboards.\par
\noindent arXiv preprint arXiv:2009.13888.\par
\noindent Emma Eriksen, Michael W. Vowels, and others. 2023.\par
\noindent GPT-4 on medical licensing exams. \emph{[ILLEGIBLE]}\par
\noindent Alexander R. Fabbri, Wojciech Kryscinski, Bryan\par
\noindent McCann, Caiming Xiong, Richard Socher, and Dragomir\par
\noindent Radev. 2021. SummEval: Re-evaluating summarization\par
\noindent evaluation. \emph{Transactions of the Association for Computational Linguistics}, 9:391--409.\par
\noindent Iker Garcia-Ferrero, Rodrigo Agerri, Aitziber Atutxa\par
\noindent Salazar, Elena Cabrio, Iker de la Iglesia, Alberto\par
\noindent Lavelli, Bernardo Magnini, Benjamin Molinet, Jo-\par
\noindent hana Ramirez-Romero, German Rigau, et al. 2024.\par
\noindent Medical mT5: an open-source multilingual text-to-\par
\noindent text LLM for the medical domain. arXiv preprint\par
\noindent arXiv:2402.[ILLEGIBLE].\par
\noindent Sebastian Gehrmann, Elizabeth Clark, and Thibault\par
\noindent Sellam. 2023. Repairing the broken evaluation of\par
\noindent text generation. \emph{Journal of Machine Learning Research},\par
\noindent 24(168):1--76.\par
\noindent Tanya Goyal, Junyi Jessy Li, and Greg Durrett.\par
\noindent 2022. NewsSumm: A dataset for summarizing\par
\noindent news. \emph{[ILLEGIBLE]}\par
\noindent Dirk Groeneveld, Kyle Lo, and others. 2024. OLMo:\par
\noindent Accelerating the science of language models. \emph{[ILLEGIBLE]}\par
\noindent Suchin Gururangan, Ana Marasovic, Swabha Swayamdipta,\par
\noindent Kyle Lo, Iz Beltagy, Doug Downey, and Noah A.\par
\noindent Smith. 2020. Don’t stop pretraining: Adapt\par
\noindent language models to domains and tasks. In \emph{Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics}, pages 8342--8360.\par
\noindent Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat,\par
\noindent and Ming-Wei Chang. 2020. REALM: Retrieval-augmented\par
\noindent language model pre-training. In \emph{Proceedings of the 37th International Conference on Machine Learning}, pages 3929--3938.\par
\noindent Naman Goyal, and others. 2022. \emph{[ILLEGIBLE]}\par
\noindent Xiao Han, Weilin Xia, and others. 2022. Prompt tuning\par
\noindent is competitive. \emph{[ILLEGIBLE]}\par
\noindent Eric Han, and others. 2024. \emph{[ILLEGIBLE]}\par
\noindent Dan Hendrycks, Collin Burns, Steven Basart, Andy\par
\noindent Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.\par
\noindent 2021. Measuring massive multitask language understanding.\par
\noindent In \emph{Proceedings of the International Conference on Learning Representations}.\par
\noindent Leo L. Hooker. 2021. The hardware lottery. \emph{Communications of the ACM}, 64(12):58--65.\par
\noindent Jordan Hoffmann, Sebastian Borgeaud, and others.\par
\noindent 2022. Training compute-optimal large language models.\par
\noindent arXiv preprint arXiv:2203.15556.\par
\noindent Kuan-Hao Huang, Jiaan-Der Chen, and others. 2019.\par
\noindent ClinicalBERT: Modeling clinical notes and predicting\par
\noindent hospital readmission. \emph{[ILLEGIBLE]}\par
\noindent Cheng-Yu Hsieh, Chun-Liang Li, Chih-kuan Yeh,\par
\noindent Hootan Nakhost, Yasuhisa Fujii, Alex Ratner, Ranjay\par
\noindent Krishna, Chen-Yu Lee, and Tomas Pfister. 2023. Dis-\par
\noindent tilling step-by-step! outperforming larger language\par
\noindent models with less training data and smaller model\par
\noindent sizes. \emph{[ILLEGIBLE]}\par
\noindent Nicolas Ignat, and others. 2024. \emph{[ILLEGIBLE]}\par
\noindent Frederick Jelinek. 1976. Continuous speech recognition\par
\noindent by statistical methods. \emph{Proceedings of the IEEE}, 64(4):532--556.\par
\noindent Nitish Shirish Keskar, Bryan McCann, Lav R. Varshney,\par
\noindent Caiming Xiong, and Richard Socher. 2019. CTRL:\par
\noindent A conditional transformer language model for controllable generation.\par
\noindent arXiv preprint arXiv:1909.05858.\par
\noindent Douwe Kiela, and others. 2021. Dynabench: Rethinking benchmark.\par
\noindent arXiv preprint arXiv:2104.14337.\par
\noindent Jungwoo Kim, and others. 2024. \emph{[ILLEGIBLE]}\par
\noindent Steven Lu, and others. \emph{[ILLEGIBLE]}\par
\noindent Qing Lai, and others. 2023. \emph{[ILLEGIBLE]}\par
\noindent Mike Lewis, Yinhan Liu, Naman Goyal, Marjan\par
\noindent Ghazvininejad, Abdelrahman Mohamed, Omer Levy,\par
\noindent Veselin Stoyanov, and Luke Zettlemoyer. 2020a.\par
\noindent BART: Denoising sequence-to-sequence pre-training\par
\noindent for natural language generation, translation, and com-\par
\noindent prehension. In \emph{Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics}, pages 7871--7880.\par
\noindent Patrick Lewis, and others. 2020b. Retrieval-augmented\par
\noindent generation for knowledge-intensive nlp tasks. \emph{[ILLEGIBLE]}\par
\noindent Fangzhao Liu, and others. 2023. \emph{[ILLEGIBLE]}\par
\noindent Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du,\par
\noindent Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\par
\noindent Luke Zettlemoyer, and Veselin Stoyanov. 2019.\par
\noindent RoBERTa: A robustly optimized BERT pretraining\par
\noindent approach. arXiv.\par
\noindent Yinhan Liu, and others. 2020. \emph{[ILLEGIBLE]}\par
\noindent Percy Liang, and others. 2022. \emph{[ILLEGIBLE]}\par
\noindent Xi Victoria Lin, and others. 2021. XGLM: A multilingual\par
\noindent autoregressive language model. arXiv preprint\par
\noindent arXiv:2112.10668.\par
\noindent Littman, and others. 2022. \emph{[ILLEGIBLE]}\par
\noindent Shayne Longpre, Le Hou, Tu Vu, Albert Webson,\par
\noindent Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V\par
\noindent Le, Barret Zoph, Jason Wei, et al. 2023. The FLAN\par
\noindent collection: Designing data and methods for effective\par
\noindent instruction tuning. arXiv preprint arXiv:2301.13688.\par
\noindent Yi Luo, and others. 2022. \emph{[ILLEGIBLE]}\par
\noindent Magesh, and others. 2024. \emph{[ILLEGIBLE]}\par
\noindent Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey\par
\noindent Dean. 2013. Efficient estimation of word representations\par
\noindent in vector space. arXiv preprint arXiv:1301.3781.\par
\noindent Rodolfo Mille, and others. 2021. \emph{[ILLEGIBLE]}\par
\noindent Margaret Mitchell, and others. 2019. Model cards for model reporting.\par
\noindent In \emph{Proceedings of the Conference on Fairness, Accountability, and Transparency}, pages 220--229.\par
\noindent Nicholas and Bhatia. 2023. \emph{[ILLEGIBLE]}\par
\noindent Nori, and others. 2023. \emph{[ILLEGIBLE]}\par
\noindent OpenAI. 2023. \emph{[ILLEGIBLE]}\par
\noindent Long Ouyang, and others. 2022. Training language models to follow instructions with human feedback.\par
\noindent In \emph{Advances in Neural Information Processing Systems}, 35.\par
\noindent Fabio Petroni, Tim Rockt"aschel, and others. 2020.\par
\noindent How contextual are contextualized word representations?\par
\noindent \emph{[ILLEGIBLE]}\par
\noindent Fabio Petroni, and others. 2021. KILT: A benchmark for knowledge intensive language tasks.\par
\noindent In \emph{Proceedings of NAACL}.\par
\noindent Alec Radford, Karthik Narasimhan, Tim Salimans,\par
\noindent and Ilya Sutskever. 2018. Improving language understanding\par
\noindent by generative pre-training. \emph{[ILLEGIBLE]}\par
\noindent Alec Radford, Jeffrey Wu, Rewon Child, David Luan,\par
\noindent Dario Amodei, and Ilya Sutskever. 2019. Language\par
\noindent models are unsupervised multitask learners. \emph{[ILLEGIBLE]}\par
\noindent Jack Rae, Sebastian Borgeaud, and others. 2021.\par
\noindent Scaling language models: Methods, analysis & insights.\par
\noindent arXiv preprint arXiv:2112.10684.\par
\noindent Jack Rae, and others. 2019. \emph{[ILLEGIBLE]}\par
\noindent Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,\par
\noindent Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li,\par
\noindent and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer.\par
\noindent \emph{Journal of Machine Learning Research}, 21(140):1--67.\par
\noindent Schrittwieser, and others. 2020. \emph{[ILLEGIBLE]}\par
\noindent Thibault Sellam, Dipanjan Das, and Ankur P. Parikh.\par
\noindent 2020. BLEURT: Learning robust metrics for text generation.\par
\noindent In \emph{Proceedings of ACL}.\par
\noindent Singhal, and others. 2022. Large language models encode clinical knowledge.\par
\noindent \emph{[ILLEGIBLE]}\par
\noindent Singhal, and others. 2023. Towards expert-level medical question answering with large language models.\par
\noindent \emph{[ILLEGIBLE]}\par
\noindent David Silver, Aja Huang, Chris J Maddison, Arthur\par
\noindent Guez, Laurent Sifre, George Van Den Driessche, Ju-\par
\noindent lian Schrittwieser, Ioannis Antonoglou, Veda Pan-\par
\noindent neershelvam, Marc Lanctot, et al. 2016. Mastering\par
\noindent the game of Go with deep neural networks and tree\par
\noindent search. Nature,\par
\noindent Srivastava, and others. 2022. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models.\par
\noindent arXiv preprint arXiv:2206.04615.\par
\noindent Strong, and others. 2023. \emph{[ILLEGIBLE]}\par
\noindent Mirac Suzgun, Nathan Scales, Nathanael Sch"arli,\par
\noindent Sebastian Gehrmann, Yi Tay, Hyung Won Chung,\par
\noindent Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny\par
\noindent Zhou, et al. 2023. Challenging big-bench tasks and\par
\noindent whether chain-of-thought can solve them. In Find-\par
\noindent ings of the Association for Computational Linguistics:\par
\noindent EMNLP 2023, pages 9699--9717.\par
\noindent Taylor, and others. 2022. Galactica: A large language model for science.\par
\noindent arXiv preprint arXiv:2211.09085.\par
\noindent Taori, and others. 2023. Alpaca: A strong, replicable instruction-following model.\par
\noindent \emph{[ILLEGIBLE]}\par
\noindent Tricot and Sweller. 2014. Domain-specific knowledge and learning.\par
\noindent \emph{[ILLEGIBLE]}\par
\noindent Touvron, and others. 2023a. LLaMA: Open and efficient foundation language models.\par
\noindent arXiv preprint arXiv:2302.13971.\par
\noindent Touvron, and others. 2023b. Llama 2: Open foundation and fine-tuned chat models.\par
\noindent arXiv preprint arXiv:2307.09288.\par
\noindent Tunyasuvunakool, and others. 2021. Highly accurate protein structure prediction with AlphaFold.\par
\noindent Nature, 596:583--589.\par
\noindent Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\par
\noindent Uszkoreit, Llion Jones, Aidan N. Gomez, \L ukasz\par
\noindent Kaiser, and Illia Polosukhin. 2017. Attention is all\par
\noindent you need. In \emph{Advances in Neural Information Processing Systems}, 30.\par
\noindent Vig, and others. 2021. \emph{[ILLEGIBLE]}\par
\noindent Wang, and others. 2023a. \emph{[ILLEGIBLE]}\par
\noindent Wang, and others. 2023b. \emph{[ILLEGIBLE]}\par
\noindent Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu,\par
\noindent Adams Wei Yu, Brian Lester, Nan Du, Andrew M.\par
\noindent Dai, and Quoc V. Le. 2022. Finetuned language\par
\noindent models are zero-shot learners. In \emph{International Conference on Learning Representations}.\par
\noindent Workshop, and others. 2022. BLOOM: A 176B-parameter open-access multilingual language model.\par
\noindent arXiv preprint arXiv:2211.05100.\par
\noindent Shilie Wu, and others. 2023. BloombergGPT: A large language model for finance.\par
\noindent arXiv preprint arXiv:2303.17564.\par
\noindent Xue, and others. 2021. mT5: A massively multilingual pre-trained text-to-text transformer.\par
\noindent In \emph{Proceedings of NAACL}.\par
\noindent Zack, and others. 2024. \emph{[ILLEGIBLE]}\par
\noindent Tianyi Zhang, and others. 2021. \emph{[ILLEGIBLE]}\par
\noindent Tianyi Zhang, Faisal Ladhak, Esin Durmus, percy Liang,\par
\noindent Kathleen R. McKeown, and Tatsunori B. Hashimoto.\par
\noindent 2023. Benchmarking large language models for news\par
\noindent summarization. CoRR, abs/2301.13949.\par
\noindent Winata, and others. 2023. \emph{[ILLEGIBLE]}\par
\noindent Winata, and others. 2024. \emph{[ILLEGIBLE]}\par
\noindent Zhou, and others. 2023. LIMA: Less is more for alignment.\par
\noindent arXiv preprint arXiv:2305.11206.\par
}


\end{document}
=====END FILE=====
