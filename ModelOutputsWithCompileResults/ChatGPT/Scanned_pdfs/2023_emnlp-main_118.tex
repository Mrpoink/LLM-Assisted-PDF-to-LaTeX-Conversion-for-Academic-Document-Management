=====FILE: main.tex=====
% 
\documentclass[11pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage{times}
\usepackage{latexsym}
\usepackage{microtype}
\usepackage{amsmath,amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{url}
\usepackage[hidelinks]{hyperref}

\title{Query-as-context Pre-training for Dense Passage Retrieval}

\author{
Xing Wu$^{1,2}$\thanks{The first two authors contributed equally to this work.}
\and
Guangyuan Ma$^{1,2}$\footnotemark[1]
\and
Wanhui Qian$^{3}$
\and
Zijia Lin$^{[MISSING]}$
\and
Songlin Hu$^{1,2}$\thanks{Corresponding authors.}
\
\
$^{1}$ Institute of Information Engineering, Chinese Academy of Sciences\
$^{2}$ School of Cyber Security, University of Chinese Academy of Sciences\
$^{3}$ Du Xiaoman Financial, a Kuaishou Technology\
$^{[MISSING]}$ [MISSING]\
\
\texttt{{wuxing, maguangyuan, husonglin}@iie.ac.cn}\
\texttt{[qianwanhui@duxiaoman.com](mailto:qianwanhui@duxiaoman.com)}\
\texttt{[linzijia07@tsinghua.org.cn](mailto:linzijia07@tsinghua.org.cn)}
}

\date{}

\begin{document}
\maketitle

\begin{abstract}
Recently, methods have been developed to improve the performance of dense passage retrieval by using context-supervised pre-training.
These methods simply consider two passages from the same document to be relevant, without taking into account the potential negative impacts of weakly correlated pairs.
Thus, this paper proposes query-as-context pre-training, a simple yet effective pre-training technique to alleviate the issue.
Query-as-context pre-training assumes that the query derived from a passage is more likely to be relevant to that passage and forms a passage-query pair.
These passage-query pairs are then used in contrastive or generative context-supervised pre-training.
The pre-trained models are evaluated on large-scale passage retrieval benchmarks and out-of-domain zero-shot benchmarks.
Experimental results show that query-as-context pre-training brings considerable gains for retrieval performances, demonstrating its effectiveness and efficiency.
\end{abstract}

\section{Introduction}
Passage retrieval is the process of retrieving relevant passages from a large corpus in response to a query, which is useful in a variety of downstream applications such as web search (Fan et al., 2021; Guo et al., 2022; Lin et al., 2021a), question answering (Karpukhin et al., 2020; Lee et al., 2020; Zhu et al., 2021) and dialogue systems (Gao et al., 2022a; Yu et al., 2021). The success of pre-trained language models (PLMs) (Devlin et al., 2018; Liu et al., 2019) has led to the development of more powerful PLM-based dense and sparse passage retrieval approaches.

PLM-based dense retrieval methods (Xiong et al., 2020; Lu et al., 2021; Hofst"{a}tter et al., 2021; Gao and Callan, 2021b; Ren et al., 2021b; Ma et al., 2022; Liu and Shao, 2022; Wu et al., 2022; Wang et al., 2022) use PLMs to encode queries and passages into a shared semantic space. The semantic relationships between query and passage representations are then measured by dot product or cosine similarities. Pre-training and fine-tuning techniques have been developed to improve the performance of dense retrieval models. Pre-training processes for dense retrieval aim to improve the text representation modeling ability of the encoder through auxiliary self-supervised or context-supervised tasks.

Context-supervised pre-training (Gao and Callan, 2021b; Wu et al., 2022) assumes that two passages\footnote{A passage refers to a long text span consisting of consecutive sentences within a much longer document.} within the same document are contextual or related to each other and can therefore be used for contrastive learning or contextual decoding. However, context-supervised pre-training ignores the fact that the passages within a document may be weakly related or even irrelevant in many cases. As shown in Figure~\ref{fig:fig1}, two passages within a document from the MS-MARCO corpus (Nguyen et al., 2016) are not directly related in content. According to our statistic results via human annotation, only 35.5% of passage pairs in the training data of coCondenser (Gao and Callan, 2022) have high correlation. For statistical details, please refer to Appendix~A. These weakly correlated or irrelevant passages do not align with the assumptions on which context-supervised pre-training is based, and are likely to be detrimental to context-supervised pre-training.

\begin{figure}[t]
\centering
\fbox{\parbox{0.95\linewidth}{\centering IMAGE NOT PROVIDED}}
\caption{An example of low-relevance passages within a document from the MS-MARCO corpus. The two passages are weakly correlated in content.}
\label{fig:fig1}
\end{figure}

In contrast to dense retrieval, sparse retrieval is based on the ``bag-of-words'' assumption and represents passages and queries as sparse term-based vectors. PLM-based sparse retrieval (Nogueira and Lin, 2019; Dai and Callan, 2019; Mao et al., 2020; Bai et al., 2020; Formal et al., 2021b,a; Mallia et al., 2021; Shen et al., 2022) uses PLM to improve sparse vectors. One representative technique is Query Prediction (Nogueira and Lin, 2019), which predicts a set of relevant queries to enrich the passage's content and thus alleviates the mismatch problem. Query prediction has been shown to be effective in sparse retrieval, but has not yet been explored in the context of dense retrieval, especially in the pre-training process. This raises the question of whether the query prediction technique can benefit the pre-training process tailored for dense retrieval.

The observation that predicted queries align better with the content of a passage in our statistical analyses (see Appendix~A) suggests that query prediction could be a promising way to alleviate the issue of weakly correlated passages for context-supervised pre-training. Thus, this paper focuses on exploring query prediction techniques to improve context-supervised pre-training methods for dense retrieval. Our proposed method, termed query-as-context pre-training, assumes that a query derived from a passage (using a generative model like T5) is more likely to be a relevant context to the passage. In contrast to the previous context-supervised methods that create a training pair using two randomly selected passages from a document, the query-as-context method generates a training pair by combining a passage with a predicted query, as illustrated in Figure~\ref{fig:fig2}.

There are several advantages to using the query-as-context setting. Firstly, the query is more likely to be related to the passage because it is generated from the passage. Additionally, the use of passage-query pairs in supervised downstream retrieval training is consistent with using passage-query pairs in pre-training, which helps to bridge the gap between the two processes. Finally, since the passage-query pairs are generally shorter than the previously used passage-passage pairs, it speeds up the pre-training process and reduces the training overhead.

To verify the effectiveness of our proposed query-as-context pre-training, we conduct experiments on large-scale web search benchmarks: MS-MARCO Passage Ranking (Nguyen et al., 2016), TREC Deep Learning (DL) Track 2019 (Craswell et al., 2020a) and Track 2020 (Craswell et al., 2020b). We also evaluate query-as-context pre-trained models on the BEIR (Thakur et al., 2021) benchmark with a large set of out-of-domain datasets. Experimental results show that query-as-context achieves considerable gains over competing baselines.

Our contributions can be summarized as follows:
\begin{itemize}
\item We reveal the previously ignored issue of weakly correlated passage pairs during context-supervised pre-training.
\item We propose query-as-context pre-training, a simple yet effective pre-training technique to alleviate the issue above.
\item Experiments show that query-as-context pre-training brings considerable gains and meanwhile speeds up pre-training.
\end{itemize}

\begin{figure}[t]
\centering
\fbox{\parbox{0.95\linewidth}{\centering IMAGE NOT PROVIDED}}
\caption{A comparison of context-supervised pre-training and query-as-context pre-training.}
\label{fig:fig2}
\end{figure}

\section{Preliminary: Context-supervised Pre-training}
In this section, we begin by providing an overview of the pre-training corpus. Subsequently, we describe the masked language modeling task, which serves as a foundational task of pre-training. Finally, we present two representative contrastive and generative context-supervised pre-training methods, on which our proposed query-as-context will be applied.

\paragraph{Pre-training Corpus}
Given a set of documents, we randomly extract pairs of passages from each document, which forms a training corpus as follows:
\begin{equation}
{ {x_0,y_0}, \ldots, {x_n,y_n} }
\label{eq:corpus}
\end{equation}
where ${x_a, y_a}$ is a pair of passages from the same document.

\paragraph{Masked Language Modeling (MLM)}
Formally, given a passage $x$ with $n$ tokens, a special token [CLS] is added to the beginning of the passage, resulting in
\begin{equation}
x: {x_0, x_1, \ldots, x_n}
\label{eq:mlm_input}
\end{equation}
where $x_0$ represents the [CLS] token. Then, a certain percentage of positions are randomly selected as ``mask positions'' ($\textit{mask-pos}$) and are replaced with a special token [MASK] or a random token.
The masked passage is then passed through a text encoder, which commonly consists of $L$ layers of transformer blocks.
For the $l$-th transformer layer in the encoder, its outputs are the hidden states of the layer
\begin{equation}
h_l : {h^l_0, h^l_1, \ldots, h^l_n}.
\label{eq:hidden}
\end{equation}
The output of the last layer is then used to calculate the MLM's target loss
\begin{equation}
\mathcal{L}*{\text{mlm}} := \sum*{i \in \textit{mask-pos}} \mathrm{CE}!\left(\phi(h^L_i), x_i\right),
\label{eq:mlm_loss}
\end{equation}
where $\mathrm{CE}$ is short for cross entropy function and $\phi$ is a projection of the corresponding hidden states of $x_i$ to a vocabulary distribution.

\subsection{coCondenser}
coCondenser (Gao and Callan, 2021b) is a representative contrastive context-supervised method. For coCondenser, two passages from a document are considered relevant and form a positive pair, while two passages from different documents are considered as irrelevant and form a negative pair. These pairs constitute mini-batches for contrastive learning. A common approach for generating an embedding representation of a passage is to use the hidden states of the [CLS] position in the last layer of the encoder, i.e., $h^L_0$. Thus, the embedding representations of passages $x$ and $y$ are $h^L_0(x)$ and $h^L_0(y)$, simplified as $h_x$ and $h_y$. Then, for a mini-batch $B$, the contrastive learning objective w.r.t.\ $x$ is formulated as:
\begin{equation}
\mathcal{L}*{\text{co}} := - \log \frac{\exp(\mathrm{sim}(h_x,h_y)/\tau)}{\sum*{h' \in B}\exp(\mathrm{sim}(h_x,h')/\tau)},
\label{eq:coco}
\end{equation}
where $\tau$ is a temperature hyper-parameter and $\mathrm{sim}(\cdot,\cdot)$ is the dot product similarity function.

An additional auxiliary decoder is also appended to the encoder, which consists of $L^{f}$ layers of transformers. The auxiliary decoder takes the concatenation of the [CLS] representation from the $L$-th layer, i.e., $h^L_0$, and the token representations from the encoder's $M$-th (e.g.\ 6-th) layer, i.e., ${h^M_1,\ldots,h^M_n}$, as inputs. Similar to MLM, the output of the auxiliary decoder's last layer is then used to perform an auxiliary MLM pre-training:
\begin{equation}
\mathcal{L}^{f}*{\text{mlm}} := \sum*{i \in \textit{mask-pos}} \mathrm{CE}!\left(\phi(h^{f}*i), x_i\right).
\label{eq:auxmlm}
\end{equation}
Finally, the total loss of coCondenser is:
\begin{equation}
\mathcal{L} := \mathcal{L}*{\text{mlm}} + \mathcal{L}^{f}*{\text{mlm}} + \mathcal{L}*{\text{co}}.
\label{eq:coco_total}
\end{equation}
For more details, please refer to (Gao and Callan, 2021b).

\subsection{CoT-MAE}
CoT-MAE (Wu et al., 2022) is a representative generative context-supervised method that uses an asymmetric encoder-decoder structure, with a deep encoder of $L$ layers and a shallow decoder of $N$ layers. It performs MLM training on both the encoder and the decoder simultaneously. For a pair of passages ${x,y}$, suppose $x$ is fed into the encoder side and $y$ is fed into the decoder side.

On the encoder side, $x$ is reconstructed using only the unmasked tokens in the passage, similar to BERT's MLM process, but with a higher mask rate (e.g.\ 30%).
On the decoder side, $y$ is reconstructed using both its unmasked tokens and the contextual passage $x$. The decoder takes the sentence embedding of $x$, i.e., $h_x$, and the word representations of masked $y$ as input, which are concatenated as:
\begin{equation}
d^{0} : {h_x, a_1, \ldots, a_n}.
\label{eq:cot_input}
\end{equation}
The concatenation $d^{0}$ is then passed through the $N$ layers of Transformer blocks, and the hidden states of $k$ layer is formulated as:
\begin{equation}
d^{k} : {d^{k}*0, d^{k}*1, \ldots, d^{k}*n}.
\label{eq:cot_hidden}
\end{equation}
The outputs of the last layer in decoder are then used for LM pre-training, with the loss defined as:
\begin{equation}
\mathcal{L}*{\text{ctx-mlm}} := \sum*{i \in \textit{mask-pos}} \mathrm{CE}!\left(\phi(d^{N}*i), y_i\right).
\label{eq:cot_loss}
\end{equation}
The subscript $\textit{ctx}$ denotes the process is context-supervised. Then, we add the losses from both the encoder and the decoder to get the final loss:
\begin{equation}
\mathcal{L} := \mathcal{L}*{\text{mlm}} + \mathcal{L}*{\text{ctx-mlm}}.
\label{eq:cot_total}
\end{equation}
For more details, please refer to (Wu et al., 2022).

\section{Query-as-context Pre-training}
In this section, we first introduce the details of query-as-context pre-training, and then introduce the fine-tuning process of the pre-trained models on the retrieval tasks.

\subsection{Pre-training}
Pre-training is conducted on a large scale of documents without annotations. For each document $D$, we extract a set of passages with a maximum length, ${x_0, x_1, \ldots}$. Following (Nogueira and Lin, 2019), for each passage $x_i$, we use a fine-tuned T5 model for generating queries. We apply nucleus sampling with \texttt{top-p=0.95} and \texttt{topk=25} to produce multiple queries for promoting diversity.

Specially, each passage $x_i$ will be fed into the fine-tuned T5 model, and generate $C$ candidate queries, ${q_i^{c}}*{c=1}^{C}$. During training, we will randomly select one of the candidate queries to use as the context for the passage:
\begin{equation}
y_i := \mathrm{sample}\left({q_i^{c}}*{c=1}^{C}\right).
\label{eq:sample}
\end{equation}
The passage and sampled query form a training pair ${x_i, y_i}$, which can be used to replace the original pair used in Equation~\ref{eq:corpus}. Specifically, the passage-query pair are directly used for contrastive pre-training of coCondenser. For CoT-MAE, we feed the passage into the encoder, and query into the decoder for generative pre-training. Model implementations for coCondenser and CoT-MAE have been introduced in Section~2.1 and 2.2.

\subsection{Fine-tuning}
We fine-tune on the downstream retrieval tasks to verify the effectiveness of pre-training. Following (Gao and Callan, 2021b; Wu et al., 2022), the fine-tuning process on MS-MARCO is based on a two-stage pipeline with hard negative mining (Gao et al., 2022b), as depicted in Figure~\ref{fig:fig3}. This process involves two stages of training, bi-encoder retriever 1 and bi-encoder retriever 2, which are both initialized with the query-as-context pre-trained models. The retrievers are trained with contrastive learning on the manually annotated passage-query pairs. For a manually annotated passage-query pair $(p^+, q^+)$, the representations of the passage and the query form a positive example $(h_{p^+}, h_{q^+})$. When training retriever 1, for query $q^+$, the negative samples ${p^-}$ include in-batch negative passages and BM25 mined hard negative passages. When training retriever 2, hard negatives are also mined using a well-trained retriever 1 and combined with the other negative passages to create the negative samples ${p^-}$. Both stages are optimized using the InfoNCE loss.

\begin{equation}
\mathcal{L}^{n} := -\log \frac{\exp(\mathrm{sim}(h_{p^+}, h_{q^+})/\tau)}{\sum_{p \in {p^+,p^-}}\exp(\mathrm{sim}(h_{p}, h_{q^+})/\tau)},
\label{eq:infonce}
\end{equation}
where $\tau$ is a temperature hyper-parameter fixed to 1 and $\mathrm{sim}(\cdot,\cdot)$ is dot product similarity function.

Following (Thakur et al., 2021), we train the retriever with MS-MARCO negatives\footnote{\url{[https://sbert.net/datasets/msmarco-hard-negatives.jsonl.gz}}](https://sbert.net/datasets/msmarco-hard-negatives.jsonl.gz}}) for the out-of-domain evaluation on BEIR benchmarks.

\begin{figure}[t]
\centering
\fbox{\parbox{0.95\linewidth}{\centering IMAGE NOT PROVIDED}}
\caption{Illustration of the fine-tuning pipeline. The query-as-context pre-trained model is used to initialize the dual-encoder retrievers.}
\label{fig:fig3}
\end{figure}

\section{Experiment}
In this section, we provide details on the pre-training and fine-tuning processes. Then we present the experimental results.

\subsection{Pre-training}
\paragraph{Query-as-context Dataset}
Following (Gao and Callan, 2021b; Wu et al., 2022), the pre-training dataset is collected from the MS-MARCO passages corpus, which contains 3.2 million documents. We use NLTK to split each document into sentences, and group these sentences into passages of no more than 144 consecutive tokens. For each passage, we generate candidate queries via a public T5 model\footnote{\url{[https://huggingface.co/doc2query/all-with-prefix-t5-base-v1}}](https://huggingface.co/doc2query/all-with-prefix-t5-base-v1}}). During pre-training, we select a batch of passages at each step and randomly choose a candidate query as context for each passage to form a relevant pair.

\paragraph{Model Implementation}
Following (Wu et al., 2022), the encoder for CoT-MAE is initialized with a pre-trained 12-layer BERT-base model, while the decoder is initialized from scratch. We pre-train the model using the AdamW optimizer for a maximum of 50k steps, with a learning rate of $4\times 10^{-4}$, a batch size of 16k, and a linear schedule with a warmup ratio of 0.1. We use 16 Tesla V100 GPUs to train the model for 60 hours, and then discard the decoder, leaving only the encoder for fine-tuning.

Following (Gao and Callan, 2021b), the encoder for coCondenser is initialized from a pre-trained 12-layer Condenser (Gao and Callan, 2021a) model. The training is conducted on 8 Tesla V100 GPUs for 120{,}000 steps over 90 hours using the AdamW optimizer with a learning rate of $10^{-4}$, a global batch size of 2k, and a weight decay of 0.01. Once the pre-training is finished, the Condenser head is discarded, resulting in a model with the same architecture as BERT$_{\text{base}}$ for fine-tuning.

\subsection{Fine-tuning}
\paragraph{Datasets and Evaluation}
We fine-tune the pre-trained coCondenser and CoT-MAE on MS-MARCO passage ranking (Nguyen et al., 2016), TREC Deep Learning (DL) Track 2019 (Craswell et al., 2020a) and 2020 (Craswell et al., 2020b) tasks for evaluation.

MS-MARCO (Nguyen et al., 2016) is a benchmark dataset that contains real user queries collected from Bing search and web pages, and includes approximately 8.8 million passages in total. The training set consists of around 500{,}000 annotated query-document pairs, while the dev set contains 6{,}980 annotated queries. Since the test set is not publicly available, the dev set is used for evaluation following previous work (Gao and Callan, 2021b; Wu et al., 2022). We evaluate our performance on MS-MARCO using MRR@10, Recall@50, and Recall@1K.

TREC Deep Learning (DL) (Craswell et al., 2020a,b) tracks provide test sets with more elaborate annotations to evaluate the real capacity of ranking models. We evaluate the 2019 and 2020 test sets. The 2019 test set contains 43 annotated queries and the 2020 test set contains 54 annotated queries. We evaluate our performance on TREC with NDCG@10.

\paragraph{Implementation}
We reuse a widely adopted evaluation pipeline (Gao and Callan, 2021b; Wu et al., 2022; Gao et al., 2022b), with a common random seed (42) to support reproducibility. Note that, as we focus on improving the pre-training technique, we do NOT use any enhanced methods, such as distillation from a strong re-ranker (Ren et al., 2021b; Santhanam et al., 2021) or multi-vector representation (Khattab and Zaharia, 2020), which can lead to further improvements. The fine-tuning is only trained on the MS-MARCO dataset and evaluated on the dev set and TREC DL 2019/2020 test sets. It's trained on 8 Tesla V100 GPUs using the AdamW optimizer with a learning rate of $2\times 10^{-5}$, a global batch size of 64, and a weight decay of 0.01. The passage length is also set to 144, the negative depth is set to 200 and the number of negative passages for one query in the fine-tuning iteration is 15.

\subsection{Baselines}
Our baseline methods include the sparse retrieval method and the dense retrieval method, as shown in Table~\ref{tab:main}. Results of sparse retrieval baselines are mainly from (Qu et al., 2020), including BM25, docT5query (Nogueira and Lin, 2019), DeepCT (Dai and Callan, 2019) and GAR (Mao et al., 2020). Results of dense retrieval baselines are mainly from (Gao and Callan, 2021b; Liu and Shao, 2022; Ren et al., 2021b; Ma et al., 2022), including ANCE (Xiong et al., 2020), SEED (Lu et al., 2021), TAS-B (Hofst"{a}tter et al., 2021), RetroMAE (Liu and Shao, 2022), SimLM (Wang et al., 2022) and etc. We compare the query-as-context performances with their baselines on both retriever 1 and retriever 2.

\subsection{Main Results}
As shown in Table~\ref{tab:main}, the results demonstrate that query-as-context pre-training leads to improved performance.

\paragraph{coCondenser}
When reproducing coCondenser, the pre-training steps extend to 120k steps. The main evaluation metric, MRR@10 on the MS-MARCO passage ranking dataset, of retriever 2 improves by 0.6pp compared to the original paper (Gao and Callan, 2021b). When query-as-context pre-training is used, there is a further improvement of 0.6pp on MRR@10. On both TREC DL 19 and 20 test sets, there are improvements of 2pp on DL 19 and 3.4pp on DL 20. In addition, query-as-context pre-training also improves the MRR@10 and R@50 scores of retriever 1.

\paragraph{CoT-MAE}
When reproducing CoT-MAE, for efficiency, we adopt a much larger batch size than in (Wu et al., 2022), which allows us to reduce the number of training steps from 1200k to 50k. This results in faster training, but somehow lower performance on the MS-MARCO MRR@10 metric compared to the original paper. However, when query-as-context pre-training is applied, there is an obvious improvement of 1.4pp on MRR@10, reaching 40.2. Even compared to the 1200k model's performance in the original paper, we still achieve a non-trivial improvement of 0.8pp. To the best of our knowledge, this is the new state-of-the-art result for a single vector pre-trained (not a reranker-distilled) dense retriever. On both TREC DL 19 and 20 test sets, there are improvements of 0.8pp on DL 19 and 3pp on DL 20. In addition, query-as-context pre-training also improves the MRR@10, R@50, and R@1k scores of retriever 1.

Overall, the query-as-context pre-training approach is effective, improving both contrastive and generative context-supervised pre-training. This is due to two main reasons: (1) pre-trained models can provide better parameters initialization for both retriever 1 and retriever 2; (2) a better retriever 1 can be used to mine more effective hard negatives, which further improves the training of retriever 2.

\begin{table*}[t]
\centering
\small
\begin{tabular}{lcccccc}
\toprule
\multirow{2}{*}{Model} & \multicolumn{3}{c}{MS-MARCO} & \multicolumn{1}{c}{TREC DL 19} & \multicolumn{1}{c}{TREC DL 20} \
\cmidrule(lr){2-4}\cmidrule(lr){5-5}\cmidrule(lr){6-6}
& MRR@10 & R@50 & R@1k & NDCG@10 & NDCG@10 \
\midrule
\multicolumn{6}{l}{Sparse retrieval} \
BM25 & 18.7 & 59.2 & 85.7 & 51.2 & 47.7 \
DeepCT (Dai and Callan, 2019) & 24.2 & 69.0 & 91.0 & 57.2 & [ILLEGIBLE] \
docT5query (Nogueira and Lin, 2019) & 27.5 & 64.4 & 89.1 & 64.2 & [ILLEGIBLE] \
\midrule
\multicolumn{6}{l}{Dense retrieval} \
NPRINC (Lu et al., 2020) & 31.1 & 97.7 & [ILLEGIBLE] & 64.5 & 64.6 \
ANCE (Xiong et al., 2020) & 33.0 & 95.9 & [ILLEGIBLE] & 71.2 & 69.3 \
SEED (Lu et al., 2021) & 33.9 & 96.1 & [ILLEGIBLE] & 70.4 & [ILLEGIBLE] \
TAS-B (Hofst"{a}tter et al., 2021) & 34.0 & 97.5 & [ILLEGIBLE] & 82.9 & [ILLEGIBLE] \
COIL (Gao et al., 2021) & 35.5 & 96.3 & [ILLEGIBLE] & 84.1 & 67.8 \
ColBERT (Khattab and Zaharia, 2020) & 36.0 & 96.8 & [ILLEGIBLE] & [ILLEGIBLE] & [ILLEGIBLE] \
COSTA (Ma et al., 2022) & 36.6 & 97.3 & [ILLEGIBLE] & [ILLEGIBLE] & [ILLEGIBLE] \
Condenser (Gao and Callan, 2021a) & 36.6 & 97.4 & [ILLEGIBLE] & [ILLEGIBLE] & [ILLEGIBLE] \
RocketQA (Qu et al., 2020) & 37.0 & 97.9 & [ILLEGIBLE] & 69.8 & [ILLEGIBLE] \
PAIR (Ren et al., 2021a) & 37.9 & 98.2 & [ILLEGIBLE] & 71.4 & [ILLEGIBLE] \
SimLM (Wang et al., 2022) & 39.1 & 98.6 & [ILLEGIBLE] & 68.1 & [ILLEGIBLE] \
RetroMAE (Liu and Shao, 2022) & 39.3 & 98.5 & [ILLEGIBLE] & 70.5 & 69.7 \
LED (Zhang et al., 2022a) & 39.6 & 98.3 & [ILLEGIBLE] & 70.6 & 67.9 \
\midrule
coCondenser (Gao and Callan, 2021b) & 38.2 & 86.5 & 98.4 & 71.1 & 68.4 \
coCondenser (120K) -- retriever 1$^\dagger$ & 37.0 & 86.0 & 98.5 & 68.2 & 68.1 \
w/ query-as-context (120K) -- retriever 1 & 37.4 & 87.3 & 98.6 & 68.8 & 69.2 \
coCondenser (120K) -- retriever 2$^\dagger$ & 38.8 & 87.8 & 98.8 & 71.1 & 68.4 \
w/ query-as-context (120K) -- retriever 2 & 39.4 & 88.6 & 99.0 & 73.1 & 71.8 \
\midrule
CoT-MAE (Wu et al., 2022) & 39.4 & 87.0 & 98.7 & 70.9 & 69.7 \
CoT-MAE (50K) -- retriever 1$^\dagger$ & 37.2 & 85.7 & 98.7 & 65.7 & 66.5 \
w/ query-as-context (50K) -- retriever 1 & 38.6 & 87.7 & 98.8 & 67.7 & 67.8 \
CoT-MAE (50K) -- retriever 2$^\dagger$ & 38.8 & 87.3 & 98.8 & 70.7 & 69.7 \
w/ query-as-context (50K) -- retriever 2 & 40.2 & 88.8 & [ILLEGIBLE] & 71.5 & 72.7 \
\bottomrule
\end{tabular}
\caption{Main results on MS-MARCO passage ranking and TREC DL datasets. $^\dagger$ denotes our reproduction using publicly available codes. The score that is better in comparison is marked in bold.}
\label{tab:main}
\end{table*}

\subsection{Out-of-domain Evaluation}
We evaluate the out-of-domain performance of query-as-context pre-trained models on the zero-shot benchmark BEIR (Thakur et al., 2021). BEIR benchmark contains 9 different open-domain information retrieval tasks from 18 different datasets. We evaluate the models on the 14 publicly available datasets\footnote{The current state-of-the-art models on the BEIR benchmark reach higher scores as they are pre-trained on the WIKI dataset. Due to the high cost of pre-training, we directly evaluate the models pre-trained on the MS-MARCO dataset and leave the exploration on the WIKI dataset in future work.}. As shown in Table~\ref{tab:beir}, both the coCondenser and the CoT-MAE results show non-trivial improvements on most datasets when using query-as-context pre-training. Specifically, using query-as-context pre-training improves the performance of the coCondenser model on 9 different datasets. The improvement in CoT-MAE is more significant, with notable gains observed on 13 datasets.

\begin{table}[t]
\centering
\small
\begin{tabular}{lcccc}
\toprule
Dataset & \multicolumn{1}{c}{coCondenser} & \multicolumn{1}{c}{[ILLEGIBLE]} & \multicolumn{1}{c}{CoT-MAE} & \multicolumn{1}{c}{[ILLEGIBLE]} \
\midrule
trec-covid & 0.632 & 0.703 & 0.646 & 0.665 \
nfcorpus & 0.333 & 0.330 & 0.319 & 0.340 \
nq & 0.531 & 0.548 & 0.513 & 0.546 \
hotpotqa & 0.538 & 0.583 & 0.512 & 0.572 \
fiqa & 0.319 & 0.322 & 0.288 & 0.326 \
arguana & 0.389 & 0.447 & 0.312 & 0.416 \
webis-touche2020 & 0.213 & 0.204 & 0.202 & 0.212 \
cqadupstack & 0.310 & 0.341 & 0.312 & 0.337 \
quora & 0.866 & 0.864 & 0.781 & 0.859 \
dbpedia-entity & 0.373 & 0.386 & 0.355 & 0.406 \
scidocs & 0.133 & 0.145 & 0.132 & 0.151 \
fever & 0.728 & 0.664 & 0.107 & 0.688 \
climate-fever & 0.204 & 0.199 & 0.173 & 0.220 \
scifact & 0.599 & 0.648 & 0.591 & 0.642 \
\midrule
Average & 0.441 & 0.456 & 0.417 & 0.456 \
\bottomrule
\end{tabular}
\caption{Out-of-domain evaluation on BEIR benchmark. The score that is better in comparison is marked in bold.}
\label{tab:beir}
\end{table}

\section{Analyses}
In this section, we examine the efficiency advantage and analyze the impact of different settings on query-as-context pre-training.

\subsection{Impact of Generated Query Number}
During pre-training, using multiple candidate queries leads to better diversity as each passage is paired with a different candidate query in each epoch. Therefore, we explore the effect of the number of generated queries. As shown in Table~\ref{tab:qnum}, for coCondenser, increasing the number of queries from 1 to 5 slightly improves performance on the MS-MARCO dataset and leads to a good improvement on the TREC DL 19 and 20 test sets. For CoT-MAE, using 5 queries lead to an increase on the MS-MARCO dataset and TREC DL20 test set, while a slight performance decrease in the TREC DL 19 test set. However, further increasing the number of candidate queries will generally bring about a decline in performance. A proper number of queries retains their correlation to the passages, thus yielding higher performance in query-as-context pre-training.

\begin{table*}[t]
\centering
\small
\begin{tabular}{lcccccccccc}
\toprule
\multirow{2}{*}{Model} & \multirow{2}{*}{Query Number} &
\multicolumn{2}{c}{Retriever-1} &
\multicolumn{3}{c}{MS-MARCO} &
\multicolumn{2}{c}{Retriever-2} &
\multicolumn{1}{c}{TREC DL 19} &
\multicolumn{1}{c}{TREC DL 20} \
\cmidrule(lr){3-4}\cmidrule(lr){5-7}\cmidrule(lr){8-9}\cmidrule(lr){10-10}\cmidrule(lr){11-11}
& & MRR@10 & R@50 & R@1k & MRR@10 & R@50 & R@1k & [ILLEGIBLE] & NDCG@10 & NDCG@10 \
\midrule
coCondenser & 1  & 37.7 & 87.6 & 98.7 & 39.3 & 88.5 & 98.9 & 72.3 & 71.1 & 70.8 \
coCondenser & 5  & 37.4 & 87.3 & 98.8 & 39.4 & 88.6 & 99.0 & 73.1 & 71.8 & 72.7 \
coCondenser & 10 & 37.6 & 87.0 & 98.8 & 39.1 & 88.5 & 98.9 & 71.1 & 70.9 & 71.7 \
coCondenser & 20 & 37.7 & 87.2 & 98.8 & 39.4 & 88.3 & 99.0 & 71.5 & 70.3 & 69.9 \
\midrule
CoT-MAE & 1  & 38.3 & 87.4 & 98.7 & 39.9 & 88.7 & 98.8 & 70.8 & 71.7 & 72.5 \
CoT-MAE & 5  & 38.6 & 87.7 & 98.8 & 40.2 & 88.8 & [ILLEGIBLE] & 72.7 & 71.5 & 71.5 \
CoT-MAE & 10 & 38.5 & 87.2 & 98.8 & 39.7 & 88.7 & [ILLEGIBLE] & 71.7 & 72.5 & 72.2 \
CoT-MAE & 20 & 38.3 & 87.5 & 98.8 & 39.7 & 88.5 & [ILLEGIBLE] & 69.9 & 72.2 & [ILLEGIBLE] \
\bottomrule
\end{tabular}
\caption{Impact of the number of generated queries. The score that is better in comparison is marked in bold.}
\label{tab:qnum}
\end{table*}

\subsection{Impact of Mixed Context}
We further explore the effect of mixing the two kinds of contextual pairs, passage-query and passage-passage. In a training step, we randomly choose to use either the passage-query or passage-passage pair as input with the same probability. As shown in Table~\ref{tab:mixed}, mixing does not improve the effect for coCondenser and CoT-MAE, despite increasing the diversity of context. The decrease aligns with the human-annotated correlation results in Appendix~A. The passage-passage pairs have a higher proportion of low correlation pairs, so combining passage-query and passage-passage pairs will be less effective than using passage-query pairs alone. This also indicates that for pre-training tailored for intensive retrieval, the relevance of training pairs is more crucial than diversity.

\begin{table}[t]
\centering
\small
\begin{tabular}{lcccccc}
\toprule
Model & Mixed & \multicolumn{2}{c}{Retriever-1} & \multicolumn{2}{c}{Retriever-2} \
\cmidrule(lr){3-4}\cmidrule(lr){5-6}
& & MRR@10 & R@50 & MRR@10 & R@50 \
\midrule
coCondenser & X & 37.4 & 87.3 & 39.4 & 88.6 \
coCondenser & \checkmark & 37.4 & 86.7 & 39.4 & 88.1 \
CoT-MAE & X & 38.6 & 87.7 & 40.2 & 88.8 \
CoT-MAE & \checkmark & 36.9 & 85.6 & 38.4 & 87.1 \
\bottomrule
\end{tabular}
\caption{Effect of mixing passage-query and passage-passage pairs in pre-training.}
\label{tab:mixed}
\end{table}

\section{Related Works}
\paragraph{Dense Retrieval}
Different techniques have been developed to improve dense retrieval, both in fine-tuning and pre-training stages. In fine-tuning stage, attempts includes mining hard negatives (Xiong et al., 2020; Zhan et al., 2021), late interaction (Khattab and Zaharia, 2020), query clustering (Hofst"{a}tter et al., 2021), reranker distillation (Lin et al., 2021b; Santhanam et al., 2021), data augmentation (Qu et al., 2020) and jointly learning (Ren et al., 2021b; Zhang et al., 2022b, 2021). In pre-training stages, attempts are divided into two categories. One category focuses on improving the encoder using auxiliary self-supervised auto-encoding tasks (Lu et al., 2021; Gao and Callan, 2021a; Liu and Shao, 2022; Zhou et al., 2022). The other category proposes passage prediction tasks to resemble passage retrieval in pre-training (Chang et al., 2020; Gao and Callan, 2021b; Ma et al., 2022). The most related methods in this category are (Gao and Callan, 2021b) and (Wu et al., 2022). (Gao and Callan, 2021b) introduces a context-supervised contrastive pre-training process, with the hypothesis that passages from the same document are closer than those from different documents. (Wu et al., 2022) introduces a context-supervised generative masked auto-encoding task via the decoder-side reconstruction task assisted by contextual embedding. Our work is on the basis of these two methods.

\paragraph{Query Prediction}
Query Prediction is a technique originally introduced to the IR community to expand passages. It can significantly improve the performance of BM25 by generating additional queries and appending them to passages before building the inverted index (Nogueira and Lin, 2019). Query prediction has also been used to learn better sparse (Mallia et al., 2021) or dense (Li et al., 2022) representations for documents. In scenarios where data is scarce, query prediction can be used for domain adaptation by generating synthetic queries on target domains for model training (Ma et al., 2020). To reduce noise in the generated data, a cross-encoder can also be used for pseudo-labeling (Wang et al., 2021). The most related work to ours is (Li et al., 2022), which encodes each document with a set of generated pseudo-queries to obtain query-informed document representations. However, (Li et al., 2022) focuses on improving the fine-tuning process for dense retrieval, while we are working on the pre-training process.

\section{Conclusions}
In this work, we propose query-as-context pre-training, a simple yet effective technique to alleviate the previously ignored issue of weakly correlated pairs during context-supervised pre-training. Extensive experiments well validate its effectiveness and efficiency.

\section{Limitations}
A passage is more likely to have a high correlation with its corresponding generated query than another randomly selected passage from the same document. However, limited by the capabilities of the T5 model, there are still a large number of unrelated passage-query pairs. We believe that more powerful large language models have the potential to further alleviate this problem, which is left to our future research.

\begin{thebibliography}{99}

\bibitem{bai2020sparterm}
Yang Bai, Xiaoguang Li, Gang Wang, Chaoliang Zhang, [ILLEGIBLE], Jun Xu, Zhaowei Wang, Fangshan Wang, and Qun Liu. 2020.
Sparterm: Learning term-based sparse representation for fast text retrieval.
\textit{arXiv preprint arXiv:2010.00768}.

\bibitem{chang2020pretraining}
Wei-Cheng Chang, Felix X.~Yu, Yin-Wen Chang, Yiming Yang, and Sanjiv Kumar. 2020.
Pre-training tasks for embedding-based large-scale retrieval.
\textit{arXiv preprint arXiv:2002.03932}.

\bibitem{craswell2020trec2019}
Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, and Ellen M.~Voorhees. 2020a.
Overview of the TREC 2019 deep learning track.
\textit{arXiv preprint arXiv:2003.07820}.

\bibitem{craswell2020trec2020}
Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Fernando Campos, and Ellen M.~Voorhees. 2020b.
Overview of the TREC 2020 deep learning track.
\textit{ArXiv, abs/2003.07820}.  % [ILLEGIBLE formatting in source]

\bibitem{dai2019deepct}
Zhuyun Dai and Jamie Callan. 2019.
Context-aware sentence/passage term importance estimation for first stage retrieval.
\textit{arXiv preprint arXiv:1910.10687}.

\bibitem{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018.
BERT: Pre-training of deep bidirectional transformers for language understanding.
\textit{arXiv preprint arXiv:1810.04805}.

\bibitem{fan2021pretrainingir}
Yixing Fan, Xiaohui Xie, Yinqiong Cai, Jia Chen, Xinlu Ma, Xiangsheng Li, Ruqing Zhang, Jiafeng Guo, and Yiqun Liu. 2021.
Pre-training methods in information retrieval.
\textit{arXiv preprint arXiv:2111.13853}.

\bibitem{formal2021spladev2}
Thibault Formal, Carlos Lassance, Benjamin Piwowarski, and St'ephane Clinchant. 2021a.
SPLADE v2: Sparse lexical and expansion model for information retrieval.
\textit{arXiv preprint arXiv:2109.10086}.

\bibitem{formal2021splade}
Thibault Formal, Benjamin Piwowarski, and St'ephane Clinchant. 2021b.
SPLADE: Sparse lexical and expansion model for first stage ranking.
In \textit{Proceedings of the 44th International ACM SIGIR Conference on [MISSING]}, pages [MISSING].

\bibitem{gao2022cir}
Jianfeng Gao, Chenyan Xiong, Paul Bennett, and Nick Craswell. 2022a.
Neural approaches to conversational information retrieval.
\textit{arXiv preprint arXiv:2201.05176}.

\bibitem{gao2021condenser}
Luyu Gao and Jamie Callan. 2021a.
Condenser: a pre-training architecture for dense retrieval.
\textit{arXiv preprint arXiv:2104.08253}.

\bibitem{gao2021cocondenser}
Luyu Gao and Jamie Callan. 2021b.
Unsupervised corpus-aware language model pre-training for dense passage retrieval.
\textit{arXiv preprint arXiv:2108.05540}.

\bibitem{gao2022cocondenser}
Luyu Gao and Jamie Callan. 2022.
Unsupervised corpus-aware language model pre-training for dense passage retrieval.
In \textit{[ILLEGIBLE: Proceedings of [MISSING]]}, pages [MISSING].

\bibitem{gao2022tevatron}
Luyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan. 2022b.
Tevatron: An efficient and flexible toolkit for dense retrieval.
\textit{arXiv preprint arXiv:2203.05765} % [ILLEGIBLE in source: arXiv:.1907.11692 / 2203.01151]

\bibitem{gao2022semanticmodels}
Jiafeng Guo, Yinqiong Cai, Yixing Fan, Fei Sun, Ruqing Zhang, and Xueqi Cheng. 2022.
Semantic models for the first-stage retrieval: A comprehensive review.
\textit{ACM Transactions on Information Systems (TOIS)}, 40(4):142.

\bibitem{hofstaetter2021tasb}
Sebastian Hofst"{a}tter, Sheng-Chieh Lin, Jheng-Hong Yang, Jimmy Lin, and Allan Hanbury. 2021.
Efficiently teaching an effective dense retriever with balanced topic aware sampling.
In \textit{Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval}, pages 113--122.

\bibitem{karpukhin2020dpr}
Vladimir Karpukhin, Barlas O\u{g}uz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020.
Dense passage retrieval for open-domain question answering.
\textit{arXiv preprint arXiv:2004.04906}.

\bibitem{khattab2020colbert}
Omar Khattab and Matei Zaharia. 2020.
ColBERT: Efficient and effective passage search via contextualized late interaction over BERT.
In \textit{Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval}, pages [MISSING].

\bibitem{lee2020phrases}
Jinhyuk Lee, Mujeen Sung, Jaewoo Kang, and Danqi Chen. 2020.
Learning dense representations of phrases at scale.
\textit{arXiv preprint arXiv:2012.12624}.

\bibitem{li2022deepqueryinteractions}
Zehan Li, Nan Yang, Liang Wang, and Furu Wei. 2022.
Learning diverse document representations with deep query interactions for dense retrieval.
\textit{arXiv preprint arXiv:2208.04232}.

\bibitem{lin2021synthesis}
Jimmy Lin, Rodrigo Nogueira, and Andrew Yates. 2021a.
Pretrained transformers for text ranking: BERT and beyond.
\textit{Synthesis Lectures on Human Language Technologies}, 14(4):1--325.

\bibitem{lin2021inbatch}
Sheng-Chieh Lin, Jheng-Hong Yang, and Jimmy Lin. 2021b.
In-batch negatives for knowledge distillation with tightly-coupled teachers for dense retrieval.
In \textit{Proceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP-2021)}, pages 163--173.

\bibitem{liu2019roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.
RoBERTa: A robustly optimized BERT pretraining approach.
\textit{arXiv preprint arXiv:1907.11692}.

\bibitem{liu2022retromae}
Zheng Liu and Yingxia Shao. 2022.
RetroMAE: Pre-training retrieval-oriented transformers via masked auto-encoder.
\textit{arXiv preprint arXiv:2205.12035}.

\bibitem{lu2020nprinc}
Jing Lu, Gustavo Hern'andez Abrego, Ji Ma, Jianmo Ni, and Yinfei Yang. 2020.
Neural passage retrieval with improved negative contrast.
\textit{arXiv preprint arXiv:2010.12523}.

\bibitem{lu2021seed}
Shuqi Lu, Di He, Chenyan Xiong, Guolin Ke, Waleed Malik, Zhicheng Dou, Paul Bennett, Tie-Yan Liu, and Arnold Overwijk. 2021.
Less is more: pre-train a strong siamese encoder for dense text retrieval using a weak decoder.
In \textit{Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing}, pages [MISSING].

\bibitem{ma2020zeroshot}
Ji Ma, Ivan Korotkov, Yinfei Yang, Keith Hall, and Ryan McDonald. 2020.
Zero-shot neural passage retrieval via domain-targeted synthetic question generation.
\textit{arXiv preprint arXiv:2004.14503}.

\bibitem{ma2022costa}
Xinyu Ma, Jiafeng Guo, Ruqing Zhang, Yixing Fan, and Xueqi Cheng. 2022.
Pre-train a discriminative text encoder for dense retrieval via contrastive span prediction.
\textit{arXiv preprint arXiv:2204.10641}.

\bibitem{mallia2021impacts}
Antonio Mallia, Omar Khattab, Torsten Suel, and Nicola Tonellotto. 2021.
Learning passage impacts for inverted indexes.
In \textit{Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval}, pages 1723--1727.

\bibitem{mao2020gar}
Yuning Mao, Pengcheng He, Xiaodong Liu, Yelong Shen, Jianfeng Gao, Jiawei Han, and Weizhu Chen. 2020.
Generation-augmented retrieval for open-domain question answering.
\textit{arXiv preprint arXiv:2009.08553}.

\bibitem{nguyen2016msmarco}
Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. 2016.
MS MARCO: A human generated machine reading comprehension dataset.
In \textit{CoCo@ NIPS} [ILLEGIBLE].

\bibitem{nogueira2019doc2query}
Rodrigo Nogueira and Jimmy Lin. 2019.
From doc2query to docTTTTTquery.
Online preprint, 6. % [ILLEGIBLE]

\bibitem{qu2020rocketqa}
Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Wayne Xin Zhao, Daxiang Dong, Hua Wu, and Haifeng Wang. 2020.
RocketQA: An optimized training approach to dense passage retrieval for open-domain question answering.
\textit{arXiv preprint arXiv:2010.08191}.

\bibitem{ren2021pair}
Ruiyang Ren, Shangwen Lv, Yingqi Qu, Jing Liu, Wayne Xin Zhao, Qiaoqiao She, Hua Wu, Haifeng Wang, and Ji-Rong Wen. 2021a.
PAIR: Leveraging passage-centric similarity relation for improving dense passage retrieval.
\textit{arXiv preprint arXiv:[ILLEGIBLE]}.

\bibitem{ren2021rocketqav2}
Ruiyang Ren, Yingqi Qu, Jing Liu, Wayne Xin Zhao, Qiaoqiao She, Hua Wu, Haifeng Wang, and Ji-Rong Wen. 2021b.
RocketQAv2: A joint training method for dense passage retrieval and passage re-ranking.
\textit{arXiv:2110.07367}.

\bibitem{santhanam2021colbertv2}
Keshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, and Matei Zaharia. 2021.
ColBERTv2: Effective and efficient retrieval via lightweight late interaction.
\textit{arXiv:2112.01488}.

\bibitem{shen2022lexmae}
Tao Shen, Xiubo Geng, Chongyang Tao, Can Xu, Xiaolong Huang, Binxing Jiao, Linjun Yang, and Daxin Jiang. 2022.
LexMAE: Lexicon-bottlenecked pre-training for large-scale retrieval.
\textit{arXiv preprint arXiv:2208.14754}.

\bibitem{thakur2021beir}
Nandan Thakur, Nils Reimers, Andreas R"{u}ckl'{e}, Abhishek Srivastava, and Iryna Gurevych. 2021.
BEIR: A heterogeneous benchmark for zero-shot evaluation of information retrieval models.
\textit{arXiv preprint arXiv:2104.08663}.

\bibitem{wang2021gpl}
Kexin Wang, Nandan Thakur, Nils Reimers, and Iryna Gurevych. 2021.
GPL: Generative pseudo labeling for unsupervised domain adaptation of dense retrieval.
\textit{[MISSING]}.

\bibitem{wu2022cotmae}
Xing Wu, Guangyuan Ma, Meng Lin, Zijia Lin, Zhongyuan Wang, and Songlin Hu. 2022.
Contextual mask auto-encoder for dense passage retrieval.
\textit{arXiv preprint arXiv:2208.07670}.

\bibitem{wang2022simlm}
Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. 2022.
SimLM: Pre-training with representation bottleneck for dense passage retrieval.
\textit{arXiv preprint arXiv:2207.02578}.

\bibitem{xiong2020ance}
Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett, Junaid Ahmed, and Arnold Overwijk. 2020.
Approximate nearest neighbor negative contrastive learning for dense text retrieval.
\textit{arXiv:2007.00808}.

\bibitem{yu2021fewshot}
Shi Yu, Zhenghao Liu, Chenyan Xiong, Tao Feng, and Zhiyuan Liu. 2021.
Few-shot conversational [ILLEGIBLE].
\textit{[MISSING]}.

\bibitem{zhan2021hardneg}
Jingtao Zhan, Jiaxin Mao, Yiqun Liu, Jiafeng Guo, Min Zhang, and Shaoping Ma. 2021.
Optimizing dense retrieval model training with hard negatives.
In \textit{Proceedings of the 44th International ACM SIGIR Conference on [MISSING]}, pages [MISSING].

\bibitem{zhang2022led}
Kai Zhang, Chongyang Tao, Tao Shen, Can Xu, Xiubo Geng, Binxing Jiao, and Daxin Jiang. 2022a.
LED: Lexicon-enlightened dense retriever for large-scale retrieval.
\textit{CoRR}, abs/2208.13661.

\bibitem{zhang2022hlatr}
Yanzhao Zhang, Dingkun Long, Guangwei Xu, and Pengjun Xie. 2022b.
HLAtr: Enhance multi-stage text retrieval with hybrid list aware transformer reranking.
\textit{arXiv preprint arXiv:2205.10569}.

\bibitem{zhou2022master}
Kun Zhou, Xiao Liu, Yeyun Gong, Wayne Xin Zhao, Daxin Jiang, Nan Duan, and JiRong Wen. 2022.
MASTER: Multi-task pre-trained bottlenecked masked autoencoders are better dense retrievers.
\textit{arXiv preprint arXiv:2212.07841}.

\bibitem{zhu2021adaptive}
Yunchang Zhu, Liang Pang, Yanyan Lan, Huawei Shen, and Xueqi Cheng. 2021.
Adaptive information seeking for open-domain question answering.
\textit{arXiv preprint arXiv:2109.06747}.

\end{thebibliography}

\appendix
\section{Statistically Analysis of Weakly Correlated Passages}
We randomly select 200 documents from the MS-MARCO dataset and randomly select a passage from each document. Then we construct the contextual pairs in two ways:
\begin{enumerate}
\item Random passage-passage pair: Referring to coCondenser (Gao and Callan, 2022), we randomly select another passage within the same document as the context for the passage.
\item Generated passage-query pair: Referring to the out-of-shelve docT5query (Nogueira and Lin, 2019), we use query prediction technology to generate a query as the context for the passage.
\end{enumerate}
We asked the annotators to label whether the random contexts or generated queries are strongly related to the corresponding passages. We manually annotate the 200 passage-passage pairs and passage-query pairs as high-correlation or low-correlation respectively. To eliminate preference bias, we divide 6 annotators into two groups. One group annotates 100 passage-passage pairs and 100 passage-query pairs, while the other annotates the remaining pairs. The correlation of each pair is voted by the annotation results of three annotators. The statistical results are shown in Table~\ref{tab:corr}.

Only 35.5% of the passage-passage pairs are highly correlated, compared to 56.6% of the passage-query pairs. Therefore, we suggest that the generated query is a more relevant context than the randomly sampled passages. However, due to the limited ability of the base-sized T5 model, nearly half of the generated queries are still not quite exact or strongly correlate to the corresponding passage. We will further explore the potential ability to utilize large language models to generate more precise and semantic correlate queries for improving the performance boundaries of dense passage retrieval pre-training.

\begin{table}[t]
\centering
\small
\begin{tabular}{lcc}
\toprule
Pairs & Random passage-passage & Generated passage-query \
\midrule
Correlation rate & 35.5% & 56.5% \
\bottomrule
\end{tabular}
\caption{Correlation statistics of human annotation results of different contextual pairs, each with 200 pairs. The score that is better in comparison is marked in bold.}
\label{tab:corr}
\end{table}

\end{document}
=====END FILE=====

=====FILE: figures/README.txt=====
This project reconstructs figures from the source PDF using placeholders.
All figures are represented in main.tex with a boxed note "IMAGE NOT PROVIDED".
=====END FILE=====
