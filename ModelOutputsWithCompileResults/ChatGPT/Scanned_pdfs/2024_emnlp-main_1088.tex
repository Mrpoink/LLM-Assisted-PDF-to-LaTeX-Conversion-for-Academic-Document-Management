=====FILE: main.tex=====
\documentclass[11pt]{article}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage{microtype}
\usepackage{geometry}
\geometry{margin=1in}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{array}
\usepackage{url}
\usepackage{hyperref}
\usepackage{enumitem}

\title{Factuality of Large Language Models: A Survey}
\author{
Yuxia Wang$^{1}$, Minghan Wang$^{2}$, Muhammad Arslan Manzoor$^{1}$, Fei Liu$^{3}$,\
Georgi Georgieva$^{1}$, Rocktim Jyoti Das$^{1}$, Preslav Nakov$^{1}$\
$^{1}$MBZUAI, $^{2}$Monash University, $^{3}$Google, $^{4}$Sofia University\
\texttt{{yuxia.wang, preslav.nakov}@mbzuai.ac.ae}
}
\date{}

\begin{document}
\twocolumn
\maketitle

\begin{abstract}
Large language models (LLMs), especially
when instruction-tuned for chat, have become
part of our daily lives, freeing people from the
process of searching, extracting, and, integrat-
lng information from multiple sources by of-
fering a straightforward answer to a variety of
questions in a single place. Unfortunately, in
many cases, LLM responses are factually in-
correct, which limits their applicability in real-
world scenarios. As a result, research on evalu-
ating and improving the factuality of LLMs has
attracted a lot of attention recently. In this sur-
vey, we critically analyze existing work with
the aim to identify the major challenges and
their associated causes, pointing out to potential
solutions for improving the factuality of LLMs,
and analyzing the obstacles to automated factu-
ality evaluation for open-ended text generation.
We further offer an outlook on where future
research should go.
\end{abstract}

\section{Introduction}
Large language models (LLMs) have become an
integral part of our daily lives. When instruction-
tuned for chat, they have enabled digital assis-
tants that can free people from the need to search,
extract, and tntegrate information from multiple
sources by offering straightforward answers in a
single chat. While people naturally expect LLMs
to always present reliable information that is con-
sistent with real-world knowledge, LLMs tend to
fabricate ungrounded statements, resulting in mis-
information (Tonmoy et a1.,2024), which limits
their utility. Thus, assessing and improving the fac-
tuality of the text generated by LLMs has become
an emerging and crucial research area, aiming to
identify potential errors and to advance the devel-
opment of more reliable LLMs (Chen et a1.,2023).

To this end, researchers have collected multi-
ple datasets, introduced a variety of measures to
evaluate the factuality of LLMs, and proposed nu-
merous strategies leveraging extemal knowledge
through retrieval, self-reflection, and early refine-
ment in model generation to mitigate factual er-
rors (Tonmoy et a1.,2024). Numerous surveys (Ton-
moy et a1.,2024; Huang et a1.,2023a1 Wang et al.,
2023b) have explored factuality or hallucinations
in large language models across various modali-
ties. While they either lack in-depth discussion
or are too specific to grasp the fundamental chal-
lenges, promising solutions in factuality evaluation
and enhancement, and some ambiguous concepts
in LLM factuality. We summarized these surveys
in Table~\ref{tab:survey-comparison}.

Our survey aims to bridge this gap by provid-
ing an in-depth analysis of LLM factuality, with
an emphasis on recent studies to reflect the rapidly
evolving nature of the field. We offer a comprehen-
sive overview of different categorizations, evalua-
tion methods, and mitigation techniques for LLM
factuality in both language and vision modalities.
Additionally, we explore a novel research avenue
that seeks to improve LLM calibration. This in-
cludes making models aware of their knowledge
limitations and enhancing the reliability of their
output confidence.

\begin{table*}[t]
\centering
\small
\setlength{\tabcolsep}{6pt}
\begin{tabular}{@{}p{2.4cm}p{1.8cm}p{1.4cm}p{1.2cm}p{1.2cm}p{1.4cm}p{8.0cm}@{}}
\toprule
Survey & Date & Pages & Eval & Improve & Multimodal & Contributions and limitations \
\midrule
Our work & [MISSING] & [MISSING] & / & / & / & Discusses ambiguous concepts in LLM factuality, compaes and alalyzes evaluation and enhancement approaches from academic and practical perspectives, outlining major challenges and proflising avenues to explore. \
(Tonmoy et al., 2024) & 15-June-2024 & 49 & / & / & [MISSING] & Summrizes recent work in tems of nirigating LLM hatiucinatioDs, bur lacks com. parison between different approaches and discussions to identify open questions md challenges. \
(Gao et al., 2023b) & 08-lan-2024 & 44 & / & / & [MISSING] & Summmizes rhree RAG paradigms: naive, advanced, ud modulu RAG, with key elements Md evaluation methods for the thee major components in RAG (retriever, generator, and augmentation). \
(Huug et al,, 2023b) & 18-Dec-2023 & 11 & X & [MISSING] & [MISSING] & Analyzes the reasons for hallucinations, md presents a comprehensive oveniew of hallucination detection methods, benchmilks, and approaches ro miugate hallucinations. \
(wang et al., 2023b) & 09-Nov-2023 & 32 & / & / & [MISSING] & Deailed lierature review of factuality improvement and enhancement methods covering both rerieval augmentation and non-retrieval augmentation, missing discussion of majoi bottleneck issues in LLM factuality and promising directions to invesdgare. \
(Rawre et al.,2023b) & 18-Oct-2023 & 29 & X & [MISSING] & [MISSING] & Extensively elucidates the problem of hallucination across all major modalities of foun- dation models, including texr (general, multilingual, domain-specific LLMS), inage, video, and audio, However, inadequate coverage of approaches, in-depth caregorization and comparison between methods. \
(ZhME et il'2023c) & 18-Sept-2023 & [MISSING] & /Y & [MISSING] & [MISSING] & OrSanized by differenr kaining stages of LLMS, discusses potenrial sources of LLM hallucinations and in-depth review of recent sork on addressing the problem. \
(Guo et a1.,2022) & Feb-2022 & [MISSING] & X & [MISSING] & [MISSING] & Focused on the automated facFchecking pipeline \
\bottomrule
\end{tabular}
\caption{Comparison of different surveys on the factuality of LLMs. Eval: Evaluation; Improve: Improvement.}
\label{tab:survey-comparison}
\end{table*}

\section{Background}
Hallucination and factuality, while conceptually
distinct, often occur in similar contexts and are
sometimes used interchangeably, rendering them
intricately intertwined, posing a challenge in dis-
cerning their distinct boundaries, and causing a con-
siderable amount of misconception. In this section,
we seek to disambiguate and ref,ne our understand-
ing of these two closely aligned concepts, thereby
preventing misinterpretation and reducing potential
confusion. Additionally, we further include two
closely-related axes : relevance and trustworthiness
for LLM evaluation to illustrate their nuance in
relation to factuality.

\subsection{Hallucination vs.\ Factuality}
The concept of
hallucination in the context of traditional natural
language generation tasks is typically referred to
as the phenomenon in which the generated content
appears nonsensical or unfaithful to the provided
source content (Ji et al., 2023). One concrete ex-
ample is made-up information in an abstractive
summary with additional insights beyond the scope
of the original source document.

In the age of LLMs, the term hallucination has
been reimagined, encompassing any deviation from
factual reality or the inclusion of fabricated ele-
ments within generated texts (Tonmoy et aI.,2024;
Rawte et al., 2023b). (Zhane et al., 2023 c) defi ne
hallucination as the characteristic of LLMs to gen-
erate content that diverges from the user input, con-
tradicts previously generated context, or mis-aligns
with established world knowledge. (Huang et al.,
2023b) merge the input- and context-conflicting
types of hallucinations and further take logical in-
consistency into account to formfaithfulness hal-
lucination. Another category is factuality halluci-
nation, referring to the discrepancy between gener-
ated content and verifiable real-world facts, mani-
festing as (1) factual inconsistency and (2) factual
fabrication.

Factuality, on the other hand, is concerned with
a model's ability to learn, acquire, and utilize fac-
tual knowledge. (Wang et aL,2023b) characterize
factuality issues as the probability of LLMs pro-
ducing content inconsistent with established facts.
It is important to note that hallucination content
may not always involve factual missteps. Though
a piece of generated text may exhibit divergence
from the initial prompt's specifics, it falls into hal-
lucinations, not necessarily a factual issue if the
content is accurate.

It is crucial to distinguish between factual er-
rors and instances of hallucination. The former
involves inaccurate information whereas the latter
can present unanticipated and yet factually substan-
tiated content (Wang et a1.,2023b).

Summary: Factuality is the ability of LLMs to
generate content consistent with factual informa-
tion and world knowledge. Although both halluci-
nations and factuality may impact the credibitrity
of LLMs in the context of content generation, they
present distinct challenges. Hallucinations occur
when LLMs produce baseless or untruthful content,
not grounded in the given source. In contrast, factu-
ality errors arise when the model fails to accurately
learn and utilize factual knowledge. It is possible
for a model to be factually correct yet still produce
hallucinations by generating content that is either
off-topic or more detailed than what is requested.

\subsection{Trustworthiness/Reliability vs.\ Factuality}
In
the context of LLMs, factuality (Wang eta1.,Z023b)
refers to a modei's capability of generating con-
tents of factual information, grouneded in reliable
sources (e.g., dictionaries, Wikipedia or textbooks),
with commonsense, world and domain-specific
knowledge taken into account. In contrast, ,,trust-
wofthiness" (Sun et al.,2024) extends beyond mere
factual accuracy and is measured on eight dimen-
sions: truthfulness, safety, fairness, robustness, pri-
vacy, ethics, transparency, and accountability.

\section{EvaluatingFactuality}
Evaluating LLM factuality on open-ended gener-
ations presents a non-trivial challenge, discerning
the degree to which a generated textual statement
aligns with objective reality. Studies employ vari-

\subsection{Datasets and Metrics}
While (Zhang et al., 2023c) outlined tasks and mea-
sures for hallucination evaluation, there is no com-
parative analysis of existing datasets to assess var-
ious aspects in regards to model factuality (e.g..
knowledge grounding, fast-changing facts, snow-
balling hallucinations, robustness to false premises,
and uncertainty awareness). We categori ze the
datasets in the format of discrimination or gener-
ation, and highlights the challenges in automatic
evaluation for long-form open-ended generations.
Current benchmarks largely assess the factuality
in LLMs based on two capabilities: proficiency in
distinguishing factual accuracy in a context and
ability to generate factually sound content.

The former typically comes in the form of a
multi-choice question, with the expected response
being a label of one of A, B, C, and D. For instance,
HotpotQA, StrategyQA, MMLU. This form of eval-
uation has been widely used to measure the gen-
eral knowledge proficiency and factual accuracy
of LLMs, largely thanks to its automation-friendly
nature. Under this evaluation formulation, model
responses are easily parsed and compared with gold
standard labels, enabling the calculation of accu-
racy or F1 scores against established benchmarks.

Precisely assessing the factuality of free-form
LLM outputs remains a significant challenge due
to the inherent limitations of automatic methods in
the face of open-ended generation and the absence
of definitive gold standard responses within an ex-
pansive output space. To make automatic evalua-
tion feasible, many studies constrain the generation
space to (1) YesA{o; (2) short-form phrase; and (3)
a list of entities through controlling the categories
of questions and generation length.

Perhaps the most demanding, yet inherently re-
alistic scenario is free-form long text generation,
such as biography generation. For this, the most
commonly used and reliable methods rely on hu-
man experts following specific guidelines, and auto-
matic fact-checkers based on retrieved information,
such as FactScore, Factool and Factcheck-GPT, to
facilitate efficient and consistent evaluation.

These automatic fact-checkers generally first de-
compose a document into a set of atomic claims,
and then verify one by one whether the claim is
true or false based on the retrieved evidence, ei-
ther from offline Wikipedia or online Web pages.
The percentage of true claims over all statements
in a document is used to reflect the factual sta-
tus of a response (refer to FactScore). The aver-
aged Factscore over a dataset is in turn used to as-
sess a model's factuality accuracy, However, there
is no guarantee that automatic fact-checkers are
100% accwate in their verification process. (Wang
eta1.,2023c) showthateventhestate-of-the-artver-
ifieq equipped with GPT-4 and supporting evidence
retrieved with Google search, can only achieve
an F1 score of 0.63 in identifying false claims
and Fl=0.53 using PerplexityAl (compared with
human-annotated labels for claims: true or false).

\begin{table*}[t]
\centering
\small
\setlength{\tabcolsep}{6pt}
\begin{tabular}{@{}p{1.0cm}p{3.8cm}p{3.4cm}p{1.2cm}p{1.2cm}p{6.8cm}p{0.9cm}@{}}
\toprule
Type & Dataset (Reference) & Topic & Size & ER% & Evaluation and Metrics used in Original Paper & Freq \
\midrule
I & Factscore-Bio (Min and et al., 2023) & Biography & 549 & 42.6 & Human annotation md automated fact-checkers & 4 \
I & Factcheck-GPT (Wa\g et al., 2023c) & Open-ended questions & 94 & 64.9 & Humm annotation & 1 \
I & FacTool-QA (Chern et a.,2023) & Knowledge-based QA & 50 & 54.0 & Human mnotation md automated facrcheckers & 2 \
I & FE bV - W K (Cher et. il., 2023) & Knowledge-based QA & 184 & 46.2 & Human unotation, Accuracy md Fl score & 1 \
I & HaluEval (Li and, et al.,2023a) & Open-ended questions & 5000 & t2.3 & Human annotation, AUROC + LLMjudge + PARENT & 3 \
I & FreshQA (Y! et a1.,2023\ ) & Open-ended questions & 499 & ut: & Human annotation & 2 \
I & SelfAware (Yin et 0J.,2023b) & Open-ended questions & 3369 & [MISSING] & Evaluate the LLM awaeness of unknown bv Fl-score & 1 \
\midrule
II & Snowball (Zhmg et al., 2023b) & YesNo question & 1500 & 9,1 & Exact match + Accuracv,/Fl-score & [MISSING] \
\midrule
III & Wki-cateBory & Multispan QA & 55 & [MISSING] & - Precision/recall@5 & [MISSING] \
III & (Dhuliawala et al., 2023) & NMe some [Mexicon flms] & 428 & [MISSING] & - Exact match + F1 score & [MISSING] \
III & (Dhuliawala et al,, 2023) & Shon'tem Answer & [MISSING] & [MISSING] & [MISSING] & [MISSING] \
\midrule
IV & TruthfulQA (Lin et al., 2022) & False belief or misconception & 817 & [MISSING] & - Accuracy & 5 \
IV & HotpotQA (Yang and et al., 201 8) & Multi-step reasoning & 113k & [MISSING] & - Exact match + F1 score & 11 \
IV & StrategyQA (Geva et al., 2021) & Multi-step reasoning & 2780 & [MISSING] & - RecaU@10 & 3 \
IV & MMLU (Hendrycks et aI.,2021) & Knowledge & 15700 & [MISSING] & - Accuracy & 4 \
\bottomrule
\end{tabular}
\caption{Four types of datasets used to evaluate LLM factuality. I: open-ended generation; II: Yes/No answer; III: short-term or list of entities answer; IV: A, B, C, D multiple Choice QA. Labeled datasets under type I are mostly generated by ChatGPT, and FactScore-Bio (ChatGPT, InstGPT and PerplexityAl). ER: Human-annotated Error Rate. Freq: usage frequency as evaluation set in our first 50 references.}
\label{tab:datasets}
\end{table*}

Summary: We categorize datasets that evaluate
LLM factuality into four types, depending on the
answer space and the difficulty degree on which
accurate automatic quantiflcation can be performed
(see Table~\ref{tab:datasets}). They are: (I) open-domain, free-
form, long-terrn responses (FactScore: the percent-
age of the correct claims verified by human or au-
tomated fact-checker); (II) YesA{o answer w/wt ex-
planation (extract Yes/I.{o, metrics for binary clas-
sification); (III) short-form answer (Exact match
the answer with gold labels and calculate accuracy)
or the listing answer (recall@K); and (IV) multi-
choice QA (metrics for multi-class classification).

\subsection{Other Metrics}
In addition to evaluating the methods discussed
above, (Lee et a1.,2022) quantified the hallucina-
tions using two metrics, both requiring document-
level groundtruth: (1) hallucinated named entities
error measures the percentage of named entities in
the generations that do not appear in the ground-
truth document; (2) entailment ratio eval:uates the
number of generations that can be entailed by the
ground-truth reference, over all generations.

(Rawte et a7.,2023a) deflned the hallucination
vulnerability index (HVI), which takes a spectrum
of factors into account, to evaluate and rank LLMs.

Some factuality measurement tasks, such as
claim extraction and evidence retrieval are non-
trivial to automate. (Rawte et a1.,2023a) curated
publicly available LLM hallucination mitigation
benchmark, where LLM generations are scored
by humans when automated extemal knowledge re-
trieval fails to resolve a claim clearly. While widely
used for factuality evaluation, this hybrid approach
may suffer from human annotation bias.

\section{Improving Factuality}
Improving the factuality of an LLM often requires
updating its internal knowledge, editing fake, out-
dated and biased elements, thereby making its out-
put reflect a revised collection of facts, maximiz-
ing the probability of P(truthlprompt). One op-
tion is to adopt gradient-based methods to update
model parameters to encourage desired model out-
put. This includes pre-training, supervised fine-
tuning and RLXF. We can also explore injecting a
new fact into LLMs or overwriting the false knowl-
edge stored in LLM memory by in-context learning
(ICL). When models store factually correct knowl-
edge but produce errors, they can in some cases
rectify them through self-reasoning, reflection, and
multi-agent debates.

We discuss these methods throughout the life-
cycle of an LLM, ranging from pre-training, to
inference, to post-processing. Another important
element is retrieval augmentation, which enhances
the generation capabilities of LLMs by anchoring
them in external knowledge that may not be stored
or contradict the information in LLM parametric
memory. It can be incorporated at various stages
throughout model training and the subsequent in-
ference process (Gao et al., 2023b), and is therefore
not discussed individually.

\subsection{Pre-training}
LLMs store a vast amount of world knowledge
in their parameters through the process of pre-
training. The quality of the pre-training data plays
a crucial role and misinformation could potentially
cause LLMs to generate false responses, motivat-
ing the utilization of high-quality textual corpora.
However, the prohibitively massive amount of pre-
training data, typically consisting of trillions of
tokens, renders manual flltering and editing imprac-
tically laborious. To this end, automated filtering
methods have been proposed. For instance, (Brown
et a1.,2020) introduce a method to only focus on a
small portion of the CommonCrawl dataset that ex-
hibits similarity to high-quality reference corpora.
(Touvron et al., 2023) propo se to enhance factual
robustness of mixed corpora by up-sampling docu-
ments from the most reliable sources, thereby am-
plifying knowledge accuracy and mitigating hal-
lucinations. During the pre-training phase of phi-
1.5, (Li and et a1.,2023b) synthesize "textbook-
like" data, consists of and rich in high-quality com-
monsense reasoning and world knowledge. While
careful co{pus curation remains the comerstone of
pre-training for enhanced factuality, the task be-
comes increasingly challenging with the expansion
of dataset scale and the growing demand for linguis-
tic diversity. It is therefore crucial to develop novel
strategies that guarantee the consistency of factual
knowledge across diverse cultural landscapes.

(Borgeaud et a1.,2021) propose RETRO, a re-
trieval augmented pre-training approach. An auto-
regressive LLM is trained from scratch with a re-
trieval module that is practically scalable to large-
scale pre-training by retrieving billions oftokens.
RETRO shows better accuracy and is less prone to
hallucinate compared to GPT (Wang et al.,2O23a).
While limitations lie in that RETRO performance
could be compromised if the retrieval database con-
tains inaccurate, biased or outdated information.
-25ya additional computation is required for the
pre-training of LLMs with retrieval.

\subsection{Tlrning and RLXF}
Continued domain-specific SFT has shown to be ef-
fective for enhancing factuality, particularly in the
absence of such knowledge during pre-training. For
instance, (Elaraby eta1.,2023) enhance the factual
accuracy of LLMs through knowledge injection
(KI). Knowledge, in the form of entity summaries
or entity triplets, is incorporated through SFT by
either intermediate tuning, i.e. first on knowledge
and then on instruction data; or combined tuning,
i.e. on the mixture of both. While some improve-
ments are exhibited, the method alone can be insuf-
ficient to fully mitigate factual errors.

For general-purpose LLMs, SFT is typically em-
ployed to improve the instruction-following capa-
bilities as opposed to factual knowledge which is
mostly learned in pre-training. However, this pro-
cess may inadvertently reveal areas of knowledge
not covered in the pre-training, causing the risk of
behavior cloning, where a model feigns understand-
ing and responds with hallucinations to questions
it has little knowledge of (Torabi et al., 2018). R-
tuning (Zhangetal.,2023a) is proposed to address
this issue with two pivotal steps: first, assessing
the knowledge gap between the model's paramet-
ric knowledge and the instruction tuning data, and
second, creating a refusal-aware dataset for SFT. It
enables LLMs to abstain from answering queries
beyond their parametric knowledge scope. On the
other hand, Belnfo (Razumovskaia et a1.,2023)
improve factual alignment through the form of be-
havioral fine-tuning. The creation ofthe behavioral
tuning dataset emphasizes two goals: selectivity
(choosing correct information from the knowledge
source) and response adequacy (informing the user
when no relevant information is available or asking
for clariflcation). Both methods effectively control
LLMs on non-parametric questions but require ex-
tra effort in dataset curation and might hinder the
models' retention of parametric knowledge.

Sycophancy (Sharma et al., 2023), another
source of factuality errors, often arises from mis-
alignments during SFT and RLHF(Ouyang et al.,
2022), This is partially attributed to human annota-
tors' tendency to award higher scores to responses
they like rather than those that are factually accu-
rate. (Wei et a1.,2023) explore the correlation of
sycophancy with model scaling and instruction tun-
ing. They propose a synthetic-data intervention
method, using various NLP tasks to teach models
that truthfulness is independent of user opinions.
However, one limitation is that the generalizability
of their approach remains unclear for varied prompt
formats and diverse user opinions.

(Tian et a1.,2023) utilize direct preference op-
timization (DPO) (Rafailov et a1.,2023) with the
feedback of factuality score either from automatic
fact-checkers or LLMs predictive confidence. In-
domain evaluation shows promising results on bi-
ographies and medical queries, but generalization
performance across domains and unseen domains
is under-explored. (Kdksal et al., 2023) propose
hallucination-augmented recitations (HAR). It en-
courages the model to attribute to the contexts
rather than its parametric knowledge, by tuning the
model on the counterfactual dataset created lever-
aging LLM hallucinations. This approach offers a
novel way to enhance LLM attribution and ground-
ing in open-book QA. However, challenges lie in
refining counterfactual generation for consistency
and expanding its application to broader contexts.

RetrievalAugmentation Incorporatingretrieval
mechanisms during fine-tuning has been shown to
enhance the LLM factuality on downstream tasks,
particularly in open-domain QA. DPR (Karpukhin
et al., 2020) reflnes a dual-encoder framework,
consisting of two BERT models. It employs a
contrastive loss to align the hidden representa-
tions of questions and their corresponding answers,
obtained through the respective encoder models.
RAG (Lewis et al., 2020) and FiD (Izacard and
Grave, 2020) study a flne-tuning recipe for retrieval-
augmented generation models, focusing on open-
domain QA tasks. WebGPT (Nakano et a1.,2021)
fine-tunes GPT-3 (Brown et a1.,2020) by RLHF,
providing questions with factually correct long-
form reference generation. The implementation
in a text-based web-browsing environment allows
the model to search and navigate the web.

\subsection{Inference}
We categorize approaches to improve factuality
during inference into two: (1) optimizing decod-
ing strategies to strengthen model factuality; and
(2) empowering LLM leamed ability by either in-
context learning (ICL) or self-reasoning.

\subsubsection{Decoding Strategy}
Sampling from the top subword candidates with
a cumulative probability of p, known as nucleus
sampling (top-p) (Holtzman et a1.,2020), sees a
decrease in factuality performance compared to
greedy decoding, despite higher diversity. This is
likely due to its over-encouragement of random-
ness. Building on the hypothesis that sampling
randomness may damage factuality when gener-
ating the latter part of a sentence than the begin-
ning, (Lee et a1.,2022) introduce factual-nucleus
sampling, which dynamically reduces the nucleus-
p value as generation progresses to limit diversity
and improve factuality, modulating factual integrity
and textual diversity.

Apart from randomness, some errors arise when
knowledge conflicts, where context contradicts in-
formation present in the model's prior knowledge.
Context-aware decoding (CAD) (Shi et a1.,2023)
prioritizes current context over prior knowledge,
and employs contrastive ensemble logits, adjust-
ing the weight of the probability distribution when
predicting the next token with or without context.
Despite the factuality boost, CAD is a better fit
for tasks involving knowledge conflicts and heavily
reliant on high-quality context.

In contrast, DoLa (Chuang et a1., 2023) takes
into account both upper and lower (earlier) lay-
ers, as opposed to only the flnal (mature) layer.
This method dynamically selects intermediate lay-
ers at each decoding step, in which an appropriate
premature layer contains less factual information
with maximum divergence among the subset of the
early layers, This method effectively harnesses the
distinct contributions of each layer to factual gen-
erations. However, DoLa increases the decoding
time by 1.0lx to 1.08x and does not utilize exter-
nal knowledge, which limits its ability to correct
misinformation learned during training.

\subsubsection{ICL and Self-reasoning}
In context learning (ICL) allows an LLM to lever-
age and learn from demonstration examples in its
context to perform a particular task without the
need to update model parameters. (Zheng et a1.,
2023) present that it is possible to perform knowl-
edge editing via ICL through facts included in cur
demonstration examples, thereby correcting fake,
or outdated facts. The objective of demonstration
examples is to teach LLMs how to: (1) identify and
copy an answer; (2) generalize using in-context
facts; (3) ignore irrelevant facts in context.

While it is rather easy for LLMs to copy answers
from contexts, changing predictions of questions
related to the new facts accordingly, and keeping
the original predictions if the question is irrelevant
to the modified facts, remains tough.

Another line of research leverages the self-
reasoning capability of LLMs. (Du et a1.,2023)
improve LLM factuality through multi-agent de-
bate. This approach flrst instantiates a number of
agents and then makes them debate over answers re-
turned by other agents until a consensus is reached.
One interesting finding is that more agents and
longer debates tend to lead to better results. This
approach is orthogonal and can be applied in ad-
dition to many other generation methods, such as
complex prompting strategy (e.g., CoT (Wei et al.,
2022), ReAct (Yao et a1.,2023), Reflexion (Shinn
et a1.,2023)) and retrieval augmentation.

Take-away: Zheng et al. (2023) evaluate rhe
effectiveness of knowledge editing on subject-
relation-object triplets, an unrealistic setting com-
pared to open-ended free-form text assessment.
Previous methods (Mitchell et al., 2021; Meng
et a1.,2022) use finetuning over texts containing
specific text to improve factuality. The relationship
between SFT and ICL may also been an interesting
avenue to explore. More specifically, we seek an-
swers to two research questions: (1) What types of
facts and to what extent can facts be edited effec-
tively, learned by LLMs through ICL? (Z) Would
SFT do a better job at learning from examples that
are difficult for ICL? More broadly, what is the
best way to insert new facts or edit false knowledge
stored in LLMs. The community may also benefit
from an in-depth comparative analysis of the effec-
tiveness of improving factuality between SFT and
ICL (perhaps also RLXF).

Retrieval Augmentation can be applied before,
during, and after model generation.
one commonly used option is to apply re_
trieval augmentation prior to response genera-
tion. For questions requiring up-to-date world
knowledge to answer, (Vu et a1.,2023) augment
LLM prompts with web-retrieved information and
demonstrate the effectiveness on improving ac-
curacy on FreshQA. where ChatGPT and GpT-4
struggle due to their lack of up-to-date information.
(Gao et a1.,2023a) place all relevant paragraphs in
the context and encourage the model to cite sup-
porting evidence, instructing LLMs to understand
retrieved documents and generate correct citations,
thereby improving reliability and factuality.

Pre-generation retrieval augmentation is bene-
ficial as the generation process is conditioned on
the retrieval results, implicitly constraining the out-
put space. While improving factual accuracy, this
comes at the cost of spontaneous and creative re-
sponses, largely limiting the capabilities of LLMs.

An alternative method is to veriff and rectify fac-
tual errors after the model generates all content.
However, LLMs have been shown to be susceptible
to hallucination snowballing (Zhang et a1.,2023b),
a common issue where a model attempts to make
its response consistent with previously generated
content even ifit is factually incorrect.

Striking a balance between preserving cre-
ative elements and avoiding enor propagation,
EVER (Kang et a1.,2023) and "a stitch in time
saves nine" (Varshney etaJ.,2023) actively detect
and correct factual errors during generation sen-
tence by sentence. The former leverages retrieved
evidence for verification, and the latter incorporates
the probability of dominant concepts in detection.
Their findings suggest that timely correcting errors
during generation can prevent snowballing and fir-
ther improve factuality. Nonetheless, the primary
concern for this iterative process of generate-verify-
correct in real-time systems is latency, making it
difficult to meet the high+hroughput and respon-
siveness demand (Kang et a1.,2023).

\subsection{Automatic Fact Checkers}
An automatic fact-checking framework typically
consists of three components: claim processor, re-
triever, and verifier as shown in Figure~\ref{fig:fact-checker-framework}, though
the implementation of verification pipelines may
differ. For example, FACTOR (Muhlgay et al.,
2023) and Factscore (Min and et a1.,2023) only
detect falsehoods without correction. While RA.RR
depends on web-retrieved information (Gao et al.,
2022), and CoVe (Dhuliawala et a1.,2023) only re-
lies on LLM parametric knowledge (Dhuliawala
et d.., 2023) to perform both detection and cor-
rection, albeit at a coarse granularity, editing the
entire document. Compared to fine-grained ver-
ification over claims, it is unable to spot false
spans precisely and tends to result in poor preser-
vation of the original input. FacTbol (Chern et al.,
2023) and Foctcheck-GPZ (Wang et a1.,2023c)
edit atomic claims. While the former breaks a doc-
ument down to independent checkworthy claims
with three steps : decomposition, decontextualiza-
tion and checkworthiness identification, the latter
employs GPT-4 to extract verifiable claims directly.
Evaluating the effectiveness of fact-checkers re-
mains challenging, making the improvement of
such systems a dif0cult task.

Engineering and Practical Considerations Au-
tomatic fact-checking involve tasks of extracting
atomic check-worthy claims, collecting evidence
either by leveraging the knowledge stored in the
model parameters or retrieved extemally, and veri-
fication. While straightforward to implement, this
pipeline may be susceptible to error propagation.
Major bottleneck lies in the absence of automatic
evaluation measures to assess the quality of inter-
mediate steps, in particular, the claim processor
and evidence retriever as there is no gold standard.

The input to a claim processor is a document
and the expected output is a list of atomic check-
worthy claims or atomic verifiable facts. There is
no consensus on the granularity of "atomic claims",
making consistent decomposition difficult. Addi-
tionally, the concept of check-worthy and verifiable
claims are subjective. Consequently, the definition
of an atomic check-worthy claim remains a highly
debatable topic. This naturally leads to different
"gold" human-annotated atomic claims annotated
following various guidelines and distinct imple-
mentation approaches to decompose a document.

Given a document, even if assuming a ground-
truth list of atomic claims, it is an open question
how to assess the quality of automatically derived
decomposition results. (Wang et al., 2023c) assess
the agreement in the number of claims between
ground truth and predictions, followed by exam-
ining the semantic similarity between two claims
at the same index when the claim count aligns.
Entailment ratio presented in Section 3.2 is also
applicable (Lee et a1.,2022).

While it is much simpler when the evidence is
constrained (e.g., to Wikipedia documents as is the
case for FEVER (Thorne et al., 2018)), accurate
retrieval of evidence from the Internet and subse-
quently quantifying the quality of such retrieval
results remain challenging. Similar to the assess-
ment of atomic claims, goldJabeled evidence is

unavailable and infeasible to obtain in the expan-
sive open search space.

The only step where we can confidently evaluate
its quality is the accuracy of verification, a simple
binary true/false label given a document/claim. In
conclusion, perhaps the most significant hurdle for
the development and improvement of automatic
fact-checkers lies in the automated assessment and
quantification of the quality at intermediate stages.

\begin{figure}[t]
\centering
\fbox{\parbox{0.95\columnwidth}{\centering IMAGE NOT PROVIDED}}
\caption{Fact-checker framework: claim processor, retriever, and verifler, with optional step of summarizing and explaining in gray.}
\label{fig:fact-checker-framework}
\end{figure}

\section{Factuality of Multimodal LLMs}
Factuality or hallucination in Multimodal Large
Language Models refers to the phenomenon of gen-
erated responses being inconsistent with the image
content. Current research on multimodal factuality
can be further categoized into three types:
\begin{enumerate}[leftmargin=*]
\item Existence Factualityi incorrectly claiming the
existence of certain objects in the image.
\item Attribute Factuality'. describing the attributes
of certain objects in a wrong way, e.g. identi-
fying the colour of a car incorrectly.
\item Relationship Factuality: false descriptions of
relationships between objects, such as relative
positions and interactions.
\end{enumerate}

\subsection*{Evaluation}
CHAIR (Rohrbach et al., 2018) is
the first benchmark for assessing the accuracy of
object existence within captions, focusing on a pre-
defined set of objects in the COCO dataset (Lin
et a1.,2014). However, this approach can be mis-
leading since the COCO dataset is frequently used
in training sets, providing a limited perspective
when used as the sole basis for evaluation, In con-
trast, POPE (Li et al., 2023) evaluates object hallu-
cination with multiple binary choice prompts, both
positive and negative, querying if a specific object
exists in the image. More recently, (Li et a1.,2023)
proposed GPT4 -As s isted Visual Instruction Ev alu-
ation (GAVIE) to evaluate the visual hallucination
Additionally, (Gunjal et al., 2023) demonstrated
the use of human evaluation to avoid inaccuracies
and systematic biases.

\subsection*{Mitigation}
The methods for improving factuality
in MLLMs can be broadly categoizedinto the cat-
egories: finetuning-based method, inference time
correction and representation leaming.

Fine-tuning methods such as LRV-
Instruction (Liu et a7., 2023) and LLaVA-
RLHF (Sun et a1.,2023) follow an intuitive and
straightforward solution of collecting specialized
data such as positive and negative instructions
or human preference pairs. This data is used for
finetuning the model, thus resulting in models with
fewer hallucinated responses. Whereas inference
time approaches mitigate factuality by correcting
output generation. Woodpecker (Yin et a1.,2023a)
and LURE (Zhou et al., 2023) use specialized
models to rectify model generation. There are
other works such as HallE-Switch (Zhai et al.,
2023), VCD (Leng eta1.,2023), and HACL (Jiang
et al., 2023) that analyse and improve feature
representation to improve factuality.

\section{Challenges and Future Directions}
We first identify three major challenges for improv-
ing the factuality of LLMs, and then we point to
several promising directions for future work.

Language models learn a language distribution,
not facts. The training objective of language
modeling is to maximize the probability of a sen-
tence, as opposed to that of a factual statement.
While capable of generating seemingly coherent
and fluent outputs upon convergence, models are
not guaranteed to always return a factual response.

Automatic evaluation of the factual accuracy
of open-ended generations remains challenging.
Existing studies on factuality enhancement use dif-
ferent benchmarks and evaluation measures, mak-
ing fair comparisons difficult, which motivates the
need for a unified automated evaluation framework
that uses the same collection of datasets and met-
rics. Current approaches rely on either human eval-
uation or results of automated fact-checkers such
as FactScore and FacTool (Min and et a1.,2023;
Chem et a1.,2023), However, automatically quan-
tifying the quality of automated fact-checkers is
itself an open question, resulting in a chicken and
egg situation.

Latency and multi-hop reasoning could be the
bottleneck of RAG systems. Retrievers serve as
the core component in RAG systems, and the ef-
fectiveness of RAGs is largely influenced by the
quality (coverage and relevance) of the retrieved
documents. Latency and difficulties in gathering
the most pertinent evidence are the primary chal-
lenges in retrieval. While this is partly due to the
inability of ranking algorithms to retrieve such doc-
uments, ceftain facts require information gathered
from various sources and multi-hop reasoning.

\subsection*{Potential Futare Directions}
Mitigation in infer-
ence: We observe that models can often generate
a correct answer in multiple trials even if some
attempts are wrong (Tian et a1.,2023). This moti-
vates us to ask how to provide an anchor that can
guide LLM decoding to the factually correct path?
Iteratively detecting, correcting, and generating
during generation has been demonstrated to be ef-
fective to mitigate hallucinations. If simply cor-
recting the first one or two sentences, how much
improvements can we expect for subsequent gen-
erations? Can factually correct and relevant sen-
tences, phrases or concepts serve as anchors?

Development of better retrieval algorithms: In-
tegrating Retrieval-Augmented Generation (RAG)
into Large Language Models (LLMs) is challeng-
ing due to the prevalence of unreliable information,
such as fake news, on the internet. This compro-
mises the accuracy of the knowledge retrieved, re-
sulting in LLMs generating responses based on in-
correct input, Consequently, future research should
focus on improving retrieval techniques to enhance
the factuality of LlM-generated responses.

Improving the fficiency and the accuracy of
automate d fac t - che ckers : The key breakthrough
in effectively evaluating the factual accuracy of
LLMs lies in establishing accurate and efficient
fact-checkers. This requires improvement of the
quality of the evidence used for making veracity
decisions. Moreover, many recent methods rely on
the factuality of stronger models such as GPT-4 for
claim verification. Not only is this computationally
expensive, but it also tends to be highly sensitive to
minor prompt changes and LLM updates. A small
task-speciflc and well fine-tuned NLI model can be
a more viable, robust, and cost-efficient option.

\section{Conclusion}
We presented an overview on the factuality of
LLMs, surveying a number of studies covering
topics such as evaluation and improvement meth-
ods (applicable at various stages: pre-training, SFT,
inference and post-processing) along with their re-
spective challenges. We also identified three major
issues and pointed out to promising future research
directions.

\section*{Limitations}
Despite conducting an extensive literature review
to encompass all existing research on the factual-
ity of LLMs, some studies may have been omitted
due to the rapidly evolving nature of this research
area. We endeavored to include all pertinent stud-
ies and references wherever feasible. This survey
only briefly touches upon the factuality issues as-
sociated with vision language models. However,
there is room for a more in-depth exploration of
mitigation techniques speciflc to vision-language
models. Additionally, comprehensive discussions
are also necessary for language models that incor-
porate other modalities, such as video and speech.

\begin{thebibliography}{99}

\bibitem{} Sebastian Borgeaud, Arthur Mensch, and Jordan Hoff-
mann et al. 2021. Improving language models by
retrieving from trillions of tokens. In ICML.

\bibitem{} Tom B. Brown, Benjamin Mann, Nick Ryder, and
Melanie Subbiah et al. 2020. Language models are
few-shot learners. In Neurl PS 2020.

\bibitem{} Shiqi Chen, Yiran Zhao, Jinghan Zhang, and, et at.2023.
Felm: Benchmarking factuality evaluation of large
language models. arXiv preprint arXiv : 2 3 I 0.0074 l .

\bibitem{} I-Chun Chern, Steffi Chem, and Shiqi Chen et al.2023.
Factool: Factuality detection in generative AI - A
tool augmented framework for multi-task and multi-
domain scenarios. Co RR, abs/2307 .13528.

\bibitem{} Yung-Sung Chuang, Yujia Xie, and Hongyin Luo et al.
2023. Dola: Decoding by contrasting layers im-
proves factuality in large language models. CoRR,
abs/2309.03883.

\bibitem{} Shehzaad Dhuliawala, Moj taba Komeili, and et al. 2023.
Chain-of-verifi cation reduces hallucination in large
language models. arXiv p rep rint arXiv : 2 3 09. I I 49 5 .

\bibitem{} Yilun Du, Shuang Li, Antonio Torralba, Joshua B.
Tenenbaum, and Igor Mordatch. 2023. Improving
factuality and reasoning in language models through
multiagent debate. C o RR, abs12305.14325.

\bibitem{} Mohamed Elaraby, Mengyin Lu, and Jacob Dunn et al.
2023. Halo: Estimation and reduction of hallucina-
tions in open-source weak large language models.
CoRR, abs12308.11764.

\bibitem{} Luyu Gao, Zhuyun Dai, and Panupong et al. Pa-
supat. 2022. Attributed text generation via
post-hoc research and revision. arXiv preprint
arXiv:2210.08726.

\bibitem{} Tianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen.
2023a. Enabling large language models to generate
text with citations. In EMNLP, pages 6465-6488.

\bibitem{} Yunfan Gao, Yun Xiong, and et al. 2023b. Retrieval-
augmented generation for large language models: A
survey. CoRR, abs/2312.10997 .

\bibitem{} Mor Geva, Daniel Khashabi, and et al. 2021. Did aris-
totle use a laptop? A question answering benchmark
with implicit reasoning strategies. TACL,9:346-361 .

\bibitem{} Anish Gunjal, Jihan Yn, and Erhan Bas. 2023. De-
tecting and preventing hallucinations in large vision
language models. In AAAI Conference on Artificial
Intelligence.

\bibitem{} Zhijiang Guo, Michael Schlichtkrull, and Andreas Vla-
chos.2022. A survey on automated fact-checking.
TACL, 10178-206.

\bibitem{} Dan Hendrycks, Collin Burns, and et aL.202L. Mea-
suring massive multitask language understanding. In
ICLR 2O2I .

\bibitem{} Junyi Li and Xiaoxue Cheng et aL.2023a. Halueval: A
large-scale hallucination evaluation benchmark for
large language models. CoRR, abs12305.1174i.

\bibitem{} Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang,
Wayne Xin Zhao, and Ji rong Wen. 2023. Evalu-
ating object hallucination in large vision-language
models. In Conference on Empirical Methods in
Natural Longuage P roc e s s in g.

\bibitem{} Yuanzhi Li and S6bastien Bubeck et al. 2023b. Text-
books are all you need II: phi-1.5 technical report.
CoRR, abs/2309.05463.

\bibitem{} Stephanie Lin, Jacob Hilton, and Owain Evans.2022.
Truthfulqa: Measuring how models mimic human
falsehoods. In ACL, pages 3214-3252.

\bibitem{} Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James
Hays, Pietro Perona, Deva Ramanan, Piotr Doll6r,
and C. Lawrence Zitnick.2014. Microsoft coco:
Common objects in context. ln European Conference
on Computer Vsion.

\bibitem{} Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, yaser
Yacoob, and Lijuan Wan1.2023. Mitigating hal-
lucination in large multi-modal models via robust
instruction tuning.

\bibitem{} Kevin Meng, David Bau, Alex Andonian, and yonatan
Belinkov. 2022. Locating and editing factual asso-
ciations in gpt. In Netral Information processing
Systems.

\bibitem{} Sewon Min and Kalpesh Krishna et a|.2023. Factscore:
Fine-grained atomic evaluation of factual precision
in long form text generation. CoRR, abs/2305.14251 .

\bibitem{} Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea
Finn, and Christopher D. Manning. 2021. Fast model
editing at scale. ArXiv, absl2ll0. 1 1309.

\bibitem{} Dor Muhlgay, Ori Ram, and Inbal Magar et aL.2023.
Generating benchmarks for factuality evaluation of
language models. CoRR, abs12307.06908.

\bibitem{} Reiichiro Nakano, Jacob Hilton, and et al. 2021. We-
bgpt: Browser-assisted question-answering with hu-
man feedback . ArXiv, abs12112.09332.

\bibitem{} Long Ouyang, Jeff Wu, and Xu Jiang et aL.2022. Train-
ing language models to follow instructions with hu-
man feedback . ArXiv, abs/2203 .02155 .

\bibitem{} Rafael Rafailov, Archit Sharma, and et al. 2023. Direct
preference optimization: Your language model is
secretly a reward model. CoRR, abs/2305.18290.

\bibitem{} Vipula Rawte, Swagata Chakraborty, and Agnibh et al.
Pathak. 2023a. The troubling emergence of halluci-
nation in large language models - an extensive defi-
nition, quantification, and prescriptive remediations.
In EMNLP 2023, pages 2541-2513.

\bibitem{} Vipula Rawte, Amit P. Sheth, and Amitava Das. 2023b.
A survey ofhallucination in large foundation models.
CoRR, abs12309.05922.

\bibitem{} Evgeniia Razumovskaia, Ivan Vulic, and
Pavle Markovic et a1. 2023. Dial beinfo for
faithfulness: Improving factuality of information-
seeking dialogue via behavioural fine-tuning. CoRR,
abs12311.09800.

\bibitem{} Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns,
Trevor Darrell, and Kate Saenko. 2018. Object hal-
lucination in image captioning. In Conference on
Empirical Methods in Natural Language processing.

\bibitem{} Mrinank Sharma, Meg Tong, Tomasz Korbak, David Du-
venaud, and Amanda Askell et aL.2023. Towards un_
derstanding sycophancy in language models. CoRR,
abs/2310.13548.

\bibitem{} Weijia Shi, Xiaochuang Han, and et al.2023. Trusting
your evidence: Hallucinate less with context-aware
decoding. arXiv prep rint arXiv : 23 05. I 47 3 9.

\bibitem{} Noah Shinn, Federico Cassano, and Gopinath etaL.2023.
Reflexion: Language agents with verbal reinforce-
ment learning . In NeuralPS.

\bibitem{} Lichao Sun, Yue Huang, and Haoran Wang et aL.2024.
Trustllm: Trustworthiness in large language models.
A rX iv, absl 240 1 .0556 I .

\bibitem{} Boxin Wang, Wei Ping, and et al. 2023a. Shall we pre_
train autoregressive language mociels with retrieval?
a comprehensive study. In EMNLp.

\bibitem{} Cunxiang Wang, Xiaoze Liu, and et al. 2023b. Sur_
vey on factuality in large language models: Knowl-
edge, retrieval and domain-specificity. ArXiv,
abs12310,07521.

\bibitem{} Yuxia Wang, Revanth Gangi Reddy, and et a|.2023c.
Factcheck-gpt: End-to-end fine-grained document-
level fact-checking and correction of LLM output.
CoRR, abs12311.09000.

\bibitem{} Jason Wei, Xuezhi Wang, and Dale et aL.2022. Chain_
of-thought prompting elicits reasoning in large lan_
guage models. In NeurIpS 2022.

\bibitem{} Jerry W Wei, Da Huang, and yifeng L\ et a|.2023.
Simple synthetic data reduces sycophancy in large
language models. Co RR, abs/2308.0395g.

\bibitem{} Zhilin Yang and Peng Qi et al. 201g. Hotpotqa: A
dataset for diverse, explainable multi-hopquestion
answering. In EMNLp 20 I 8, pages 2369_23g0.

\bibitem{} Shunyu Yao, Jeffrey Zhao, andDian et al. 2023. React:
Synergizing reasoning and acting in language models.
ln ICLR.

\bibitem{} Shukang Yin, Chaoyou Fu, and et al. 2023a. Wood_
pecker: Hallucination correction for multimodal
large language models. CoRR, abs12310.16045.

\bibitem{} Zhangyte Yin, Qiushi Sun, and eipeng Guo et al. 2023b.
Do large language models know what they don,t
know? In ACL,pages 8653-8665.

\bibitem{} Bohan Zhai, Str.lia Yang, Xiangchen Zhao, Chenfeng
Xu, Sheng Shen, Dongdi Zhao, Kurt Keutzer, Man_
ling Li, Tan Yan, and Xiangun Fan. 2023. Halle-
switch: Rethinking and controlling object existence
hallucinations in large vision language models for
detailed caption. ArXiv, abs/23 l0.Ol71 9.

\bibitem{} Hanning Zhang, Shizhe Diao, and et al. 2OZ3a. R-
tuning: Teaching large language models to refuse
unknown questions. CoRR, abs/2311.0967 j .

\bibitem{} Muru Zhang, Ofir Press, and et al. 2OZ3b. How lan-
guage model hallucinations can snowball . CoRR,
abs/2305.13534.

\bibitem{} Yue Zhang, Yafu Li, and et al. 2023c. Siren,s song
in the AI ocean: A survey on hallucination in large
language models. CoRR, abs/230901219.

\bibitem{} CeZheng, Lei Li, and et al. 2023. Can we edit factual
knowledge by in-context learning? In EMNLp,pages
48624876.

\bibitem{} Yi11ng Zhou, Chenhang Cui, Jaehong yoon, Linjun
Zhang, Zhun Deng, Chelsea Finn, Mohit Bansal, and
Huaxiu Yao.2023. Analyzing and mitigating object
hallucination in large visionJanguage models. ArXlv,
abs12310.00754.

\bibitem{} Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu,
Chunyuan Li, Yikang Shen, Chuang Gan, Liangyan
Gui, Yu-Xiong Wang, Yming Yang, Kurt Keutzer,
and Trevor Darrell. 2023. Aligning large multi-
modal models with factually augmented rlhf . ArXiv,
abs/2309.14525.

\bibitem{} James Thorne, Andreas Vlachos, and et al. 2018.
FEVER: a large-scale dataset for fact extraction and
VERification. In NAACL, pages 809-819.

\bibitem{} Katherine Tian, Eric Mitchell, and et al. 2023. Fine-
tuning language models for factuality. arXiv preprint
arXiv:231 1.08401 .

\bibitem{} S. M. Towhidul Islam Tonmoy, S. M. Mehedi Zaman,
and et al. 2024. Acomprehensive survey ofhallucina-
tion mitigation techniques in large language models.
CoRR, abs12401.01313.

\bibitem{} Faraz Torabi, Garrett Warnell, and Peter Stone. 2018.
Behavioral cloning from observation. In IJCAI,
pages 4950-4957. ijcai.org.

\bibitem{} Hugo Touvron, Louis Martin, and et a|.2023. Llama2:
Open foundation and fine-tuned chat models. CoRR,
abs/2307,09288.

\bibitem{} Neeraj Varshney, Wenlin Yao, and et air.2023. A stitch
in time saves nine: Detecting and mitigating halluci-
nations of llms by validating low-confidence genera-
tion. CoRR. abs/2307.03987.

\bibitem{} Tu Vu, Mohit Iyyer, and et al. 2023. Freshllms: Re-
freshing large language models with search engine
augmentation . arXiv p rep r int arXiv : 2 3 I 0. 0 3 2 1 4.

\end{thebibliography}

\end{document}
=====END FILE=====
