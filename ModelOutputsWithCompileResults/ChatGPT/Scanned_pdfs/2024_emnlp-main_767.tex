=====FILE: main.tex=====
\documentclass[11pt,twocolumn]{article}

\usepackage[margin=1in]{geometry}
\usepackage{times}
\usepackage{latexsym}
\usepackage{microtype}

\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
\usepackage{amssymb}
\usepackage{url}
\usepackage[hidelinks]{hyperref}

\setlength{\columnsep}{0.25in}

\title{Related Work and Citation Text Generation: A Survey}

\author{
Xiangci Li\textsuperscript{1,2,*}\quad Jessica Ouyang\textsuperscript{1}\
\textsuperscript{1}University of Texas at Dallas,\ \textsuperscript{2}Amazon Web Services\
\texttt{[lixiangci8@gmail.com](mailto:lixiangci8@gmail.com),\ \ [jessica.ouyang@utdallas.edu](mailto:jessica.ouyang@utdallas.edu)}
}

\date{}

\begin{document}
\maketitle

\begin{abstract}
To convince readers of the novelty of their re-
search paper, authors must perform a litera-
ture review and compose a coherent story that
connects and relates prior works to the cur-
rent work. This challenging nature ofliterature
review writing makes automatic related work
generation (RWG) academically and compu-
tationally interesting, and also makes it an ex-
cellent test bed for examining the capability of
SOTA nafural language processing (NLP) mod-
els. Since the initial proposal of the RWG task,
its popularity has waxed and waned, following
the capabilities of mainstream NLP approaches.
In this work, we survey the zoo of RWG his-
torical works, summarizing the key approaches
and task deflnitions and discussing the ongoing
challenges of RWG.
\end{abstract}

\footnotetext[1]{\url{[https://www.semanticscholar.org/faq/what-are-research-feeds}}](https://www.semanticscholar.org/faq/what-are-research-feeds}})
\footnotetext[2]{* Work performed before the author joined AWS.}

\section{Introduction}
Academic research is an exploratory activity to
solve problems that have never been resolved be-
fore. Each academic research paper must sit at
the frontier of the field and present novelties that
have not been addressed by prior work; to convince
readers of the novelty of the current work, the au-
thors must perform a literature review to compare
their work with the prior work. In natural language
processing (NLP), a short literature review is usu-
ally conducted under the ``Related Work'' section
(RWS). Writing an RWS is non-trivial; it is insuffl-
cient to simply concatenate generic summaries of
prior works. Instead, composing a coherent story
that connects each related work and the current (cit-
ing) work, reflecting the author's understanding of
their field, is preferred (Li and Ouyang, 2024).

The challenging nature of RWS writing makes
automatic related work generation (RWG) an aca-
demically and computationally interesting prob-
lem. RWG is a complex task that involves multiple
NLP subtasks, such as retrieval-augmented gen-
eration, long document understanding, and query-
focused multi-document summarization. Moreover,
since most NLP papers have an RWS and NLP re-
searchers are natural domain experts for evaluating
these RWS, the RWG task is an excellent test bed
for examining the capability of SOTA NLP models.

RWG also fills a practical need. Due to the rapid
pace of research publications, including pre-prints
that have not yet been peer-reviewed, keeping up to
date with the latest work in a research area is very
time-consuming. Even with daily feed tools, like
the Semantic Scholar Research Feed\textsuperscript{1}, researchers
still have to curate, read, and digest all the new
papers in their feed. Thus, there is a need for con-
cise, automatically generated literature reviews that
regularly summarize the papers in a user's feed.

Since Hoang and Kan (2010) initially proposed
the task, the popularity of RWG has waxed and
waned, following the capabilities of mainstream
NLP approaches: from rule-based to extractive
summarization, then to abstractive summarization
on the sentence level, and finally to abstractive
section-level RWG. Currently there is a surge of
renewed interest in RWG due to the recent success
of large language models (LLMs). In this work, we
survey the zoo of RWG historical works.

We find that, surprisingly, most RWG works are
not directly comparable because they vary drasti-
cally in task definition and simplifying assumptions
(Section 2), as well as using different input features
and representations (Section 3). There is no stan-
dard benchmark dataset for RWG (Section 4), as
most works apply custom preprocessing to extract
RWS or individual citations, reflecting differences
in their task definitions. Further, many works do
not release their models or generated outputs, so
it is often impossible for later works to compare
against earlier approaches (Section 5). Finally, we
discuss ethical concerns related to RWG, such as
plagiarism and non-factual statements, and the po-
tential consequences of fully automatic RWG on
the human process of scientific thinking and writ-
ing (Section 6.3).

\begin{table*}[t]
\centering
\small
\setlength{\tabcolsep}{4pt}
\begin{tabular}{@{}lcccccccccc@{}}
\toprule
& \multicolumn{3}{c}{Output Unit} & \multicolumn{2}{c}{Cited Paper Input} & \multicolumn{2}{c}{Citation Order/Grouping} & \multicolumn{2}{c}{Availability}\
\cmidrule(lr){2-4}\cmidrule(lr){5-6}\cmidrule(lr){7-8}\cmidrule(lr){9-10}
& Sent. & Para. & Sect. & Excerpts & Full Text & Given & Predicted & Code & Data\
\midrule
\multicolumn{10}{l}{\textbf{Extractive}}\
Hoang and Kan (2010) &  & $\checkmark$ &  &  & $\checkmark$ & $\checkmark$ &  &  & $\checkmark$\
Hu and Wan (2014) &  & $\checkmark$ &  & $\checkmark$ &  & [ILLEGIBLE] & $\checkmark$ &  &  \
Wang et al. (2018) &  & $\checkmark$ &  &  & $\checkmark$ & $\checkmark$ &  &  & $\checkmark$\
Chen and Zhuge (2019) &  & $\checkmark$ &  & $\checkmark$ &  & $\checkmark$ &  &  &  \
Wang et al. (2019) &  & $\checkmark$ &  &  & $\checkmark$ & $\checkmark$ &  &  & $\checkmark$\
Deng et al. (2021) &  & $\checkmark$ &  & $\checkmark$ & $\checkmark$ &  & $\checkmark$ &  &  \
\midrule
\multicolumn{10}{l}{\textbf{Abstractive (citation)}}\
AbuRa'ed et al. (2020) & $\checkmark$ &  &  & $\checkmark$ &  &  &  & $\checkmark$ & $\checkmark$\
Xing et al. (2020) & $\checkmark$ &  &  & $\checkmark$ &  &  &  &  & $\checkmark$\
Ge et al. (2021) & $\checkmark$ &  &  & $\checkmark$ &  &  &  &  & $\checkmark$\
Luu et al. (2021) & $\checkmark$ &  &  & $\checkmark$ &  &  &  &  & $\checkmark$\
Jung et al. (2022) & $\checkmark$\textsuperscript{*} &  &  & $\checkmark$ &  &  &  & $\checkmark$ & $\checkmark$\
Li et al. (2022) & $\checkmark$\textsuperscript{*} &  &  & $\checkmark$ &  &  &  & $\checkmark$ & $\checkmark$\
Gu and Hahnloser (2023) & $\checkmark$ &  &  & $\checkmark$ &  &  &  & $\checkmark$ & $\checkmark$\
Li et al. (2023) & $\checkmark$\textsuperscript{*} &  &  &  & $\checkmark$\textsuperscript{\dag} &  &  & $\checkmark$ & $\checkmark$\
Mandal et al. (2024) & $\checkmark$\textsuperscript{*} &  &  & $\checkmark$ &  &  &  & $\checkmark$ & $\checkmark$\
\midrule
\multicolumn{10}{l}{\textbf{Abstractive (section)}}\
Chen et al. (2021) &  & $\checkmark$ &  & $\checkmark$ &  & $\checkmark$ &  &  & $\checkmark$\
Chen et al. (2022) &  & $\checkmark$ &  & $\checkmark$ &  & $\checkmark$ &  &  & $\checkmark$\
Liu et al. (2023) &  & $\checkmark$ &  & $\checkmark$ &  & $\checkmark$ &  &  & $\checkmark$\
Li and Ouyang (2024) &  & $\checkmark$ &  & $\checkmark$ & $\checkmark$\textsuperscript{\dag} &  & $\checkmark$ & $\checkmark$\textsuperscript{\ddag} & $\checkmark$\
Martin-Boyle et al. (2024) &  & $\checkmark$ &  & $\checkmark$ &  &  & $\checkmark$\textsuperscript{**} & $\checkmark$\textsuperscript{\ddag} & $\checkmark$\
\bottomrule
\end{tabular}
\caption{Comparison of the task definitions of extractive and both single-citation and full-section abstractive approaches to related work generation.
\textsuperscript{*} indicates works that allow multi-sentence citations.
\textsuperscript{\dag} indicates works that extract snippets/features from the cited paper full text.
\textsuperscript{**} indicates works that use human editing to improve predicted citation groupings.
\textsuperscript{\ddag} indicates works that provide large language model prompts.}
\label{tab:task_definitions}
\end{table*}

\section{Task Definition}
The task definition for RWG has varied as the
SOTA text summarization approach has evolved
over time. Even where the overall approach is
similar (e.g. extractive or abstractive approaches)
different assumptions are made with respect to the
availability of system inputs and the unit at which
an RWS is generated (Table~\ref{tab:task_definitions}).

\subsection{Extractive Related Work Generation}
Hoang and Kan (2010) defined RWG as generat-
ing the RWS of a target paper given the rest of the
target paper and all cited papers. This focus on ex-
tracting and concatenating salient sentences from
the cited papers to form an RWS was used by most
subsequent extractive RWG approaches (Hu and
Wan, 2014; Wang et al., 2018; Deng et al., 2021).
One key variant is that of Chen and Zhuge (2019);
Wang et al. (2019), who also extracted sentences from
other works that also referenced the cited papers.

Otherwise, the main difference among extractive
approaches is in how they order the extracted sen-
tences: Hoang and Kan (2010); Wang et al. (2018);
Chen and Zhuge (2019); Wang et al. (2019) as-
sumed the correct ordering as input (either via a
human-constructed topic tree or the ground truth
ordering of the target RWS), while Hu and Wan
(2014); Deng et al. (2021) used topic modeling
and a sentence reordering module, respectively, to
predict an ordering.

\subsection{Abstractive Related Work Generation}
With the advent of neural language models, two
different versions of the abstractive RWG task have
been proposed: generating single citation texts ver-
sus paragraphs or full RWS.

\subsubsection{Citation Text Generation}
Early neural language models, such as the Pointer-
Generator (See et al., 2017) and early pretrained
Transformers (Vaswani et al., 2017), were capa-
ble of fluent abstractive summarization but had
severe input length restrictions. Because scientific
research papers are very long documents, a new
version of the RWG task arose: generating individ-
ual citation texts. The system input now needed
to include only one or a few cited papers, and to
further shorten the system input, researchers no
longer used full cited paper texts. Instead, these
works used short input features such as the cited
paper's title and abstract.

Several types of citation text generation works
have been proposed. We categorize them as:
(1) citation sentence generation and (2) citation
span generation.

\subsubsection{Section-level Related Work Generation}
As neural language models improved their ability to
encode long inputs and generate long outputs, RWG
works began to target generating full RWS sections.
These works typically assume that the list of cited
papers and a (possibly predicted) ordering/grouping
is available, and also assume that some excerpts
from the cited papers (often their abstracts) can be
used as input.

\begin{table*}[t]
\centering
\small
\setlength{\tabcolsep}{3pt}
\begin{tabular}{@{}p{2.6cm}p{2.1cm}p{1.1cm}p{1.6cm}p{1.1cm}p{1.8cm}p{1.6cm}p{1.6cm}@{}}
\toprule
Work & Approach & Output Unit & Citation Order & Domain & Cited Paper Input & Target Paper Input & Outputs \
\midrule
Hoang and Kan (2010) & Extractive & Paragraph & Given & NLP & All cited papers, excluding those whose abstracts do not contain any of the topic tree terms & Target paper full text & RWS \
Hu and Wan (2014) & Extractive & Paragraph & Predicted & NLP & Cited papers and citation contexts & Target paper full text & RWS \
Wang et al. (2018) & Extractive & Paragraph & Given & NLP & Cited papers and citation contexts & Target paper full text & RWS \
Chen and Zhuge (2019) & Extractive & Paragraph & Given & Biomedical & Cited papers and related papers' citation contexts & Target paper full text & RWS \
Wang et al. (2019) & Extractive & Paragraph & Given & CS & Cited papers and citation contexts & Target paper full text & RWS \
Deng et al. (2021) & Extractive & Paragraph & Predicted & Biomedical & Cited papers and citation contexts & Target paper full text & RWS \
\midrule
AbuRa'ed et al. (2020) & Citation sentence generation & Sentence & [ILLEGIBLE] & Biomedical & Cited papers abstracts & Target paper abstract & Citation sentences \
Xing et al. (2020) & Citation sentence generation & Sentence & [ILLEGIBLE] & NLP & Citation contexts, cited papers' abstracts & [MISSING] & Citation sentences \
Ge et al. (2021) & Citation sentence generation & Sentence & [ILLEGIBLE] & Biomedical & Cited paper's abstract and the citing paper's sentence context & Citing paper's context & Citing sentences \
Luu et al. (2021) & Citation sentence generation & Sentence & [ILLEGIBLE] & CS & Two papers, relationship graph, and label & [MISSING] & Citation sentences \
Jung et al. (2022) & Citation sentence generation & Sentence & [ILLEGIBLE] & CS & Citing context, cited paper's abstract, citation intent & [MISSING] & Citation sentences \
Li et al. (2022) & Citation span generation & Sentence & [ILLEGIBLE] & NLP & Citing context, cited paper's abstract & [MISSING] & Citation spans \
Gu and Hahnloser (2023) & Citation sentence generation & Sentence & [ILLEGIBLE] & CS & Citing context, cited paper's abstract, citation intent & [MISSING] & Citation sentences \
Li et al. (2023) & Citation span generation & Sentence & [ILLEGIBLE] & NLP & Cited papers' abstracts, CTS & [MISSING] & Citation spans \
Mandal et al. (2024) & Citation sentence generation & Sentence & [ILLEGIBLE] & CS & Citing context, CTS & [MISSING] & Citation sentences \
\midrule
Chen et al. (2021) & Section-level generation & Paragraph & Given & NLP & Cited papers' abstracts & Target paper abstract & RWS \
Chen et al. (2022) & Section-level generation & Paragraph & Given & NLP & Cited papers' abstracts & Target paper abstract & RWS \
Liu et al. (2023) & Section-level generation & Paragraph & Given & NLP & Cited papers' abstracts & Target paper abstract & RWS \
Li and Ouyang (2024) & Section-level generation & Paragraph & Predicted & NLP & Cited papers full texts/abstracts, citation function descriptions & Target paper abstract & RWS \
Martin-Boyle et al. (2024) & Section-level generation & Paragraph & Predicted & CS & Cited papers' abstracts & Target paper abstract & RWS \
\bottomrule
\end{tabular}
\caption{A summary of RWG works, showing (approximate) task aspects. Cells marked as [ILLEGIBLE] or [MISSING] reflect unreadable or unavailable content.}
\label{tab:task_aspects}
\end{table*}

\section{Approaches}
In this section, we describe the most commonly
used RWG input features and summarize the main
RWG modeling approaches.

\subsection{Input Features}
Existing RWG works use a variety of input features
from the target paper, cited papers, and citation
contexts. The most commonly used features are the
cited paper abstracts and citation contexts (Table~\ref{tab:input_features}).

\begin{table}[t]
\centering
\small
\setlength{\tabcolsep}{3pt}
\begin{tabular}{@{}lcccc@{}}
\toprule
Input Feature & Extractive & CTG & RWSG & Total \
\midrule
Target paper full text & 6 & 0 & 0 & 6 \
Target paper abstract & 0 & 1 & 5 & 6 \
Cited paper full text & 2 & 0 & 1 & 3 \
Cited paper abstract & 3 & 8 & 5 & 16 \
Citation context & 5 & 8 & 0 & 13 \
Citation intent/function label & 0 & 2 & 1 & 3 \
Paper relationship graph & 0 & 1 & 0 & 1 \
Paper relationship summary & 0 & 0 & 1 & 1 \
\bottomrule
\end{tabular}
\caption{A summary of the input features used by RWG works. Extractive includes Hoang and Kan (2010); Hu and Wan (2014); Wang et al. (2018); Chen and Zhuge (2019); Wang et al. (2019); Deng et al. (2021). CTG includes AbuRa'ed et al. (2020); Xing et al. (2020); Ge et al. (2021); Luu et al. (2021); Jung et al. (2022); Li et al. (2022); Li et al. (2023); Gu and Hahnloser (2023). RWSG includes Chen et al. (2021); Chen et al. (2022); Liu et al. (2023); Li and Ouyang (2024); Martin-Boyle et al. (2024).}
\label{tab:input_features}
\end{table}

\subsection{Approach Categories}
\subsubsection{Extractive Approaches}
Most extractive RWG works use sentence extraction
methods based on topic modeling, ranking, or graph
methods. Hoang and Kan (2010) use a heuristic
approach to select and order sentences based on a
topic tree. Hu and Wan (2014) use PLSA for topic
modeling, SVR for sentence importance scoring, and
global optimization for sentence selection. Wang et al.
(2018) proposed a custom neural sequence-to-sequence
model with a random walk over a heterogeneous
bibliography graph. Chen and Zhuge (2019) model
paper relationships via a graph and select sentence
nodes that cover the minimum Steiner tree of the graph.
Wang et al. (2019) leverage both topic model and
cited text spans. Deng et al. (2021) use BERT-based
sentence extraction and reordering.

\subsubsection{Citation Text Generation Approaches}
\paragraph{Citation sentence generation.}
AbuRa'ed et al. (2020) applied PTGen and Transformer
models. Xing et al. (2020) used manual and automatic
annotation of citation sentences with PTGEN-Cross
based on a cross-attention mechanism. Ge et al. (2021)
used a citation network as auxiliary input and citation
function and salient sentences in cited papers as auxiliary
output in a multi-task learning setup. Luu et al. (2021)
proposed SciGPT2 with IE-extracted term lists and
ranking based on entity matching. Jung et al. (2022)
used BART/T5-based citation sentence generation with
citation intents. Gu and Hahnloser (2023) used
fine-tuned GPT-Neo and Galactica with Proximal
Policy Optimization. Mandal et al. (2024) used citation
context along with citation spans as the generation target.

\paragraph{Citation span generation.}
Li et al. (2022) proposed LED-based citation span
generation. Li et al. (2023) used RAG & LED with
ROUGE-based CTS retrieval.

\subsubsection{Section-level Abstractive Approaches}
Chen et al. (2021) used a Transformer-based hierarchical
encoder with a relationship modeling module. Chen et al.
(2022) improved over Chen et al. (2021) by encoding the
target paper's abstract. Liu et al. (2023) proposed a custom
Causal Intervention Module (CaM) inserted between
Transformer blocks. Li and Ouyang (2024) used GPT-3.5
for feature generation (e.g. faceted summary, relationship
and usage of citations) and GPT-4 based RWG. Martin-Boyle
et al. (2024) used GPT-4 with human-in-the-loop.

\subsection{Applying Citation Analysis}
Citation analysis is a related area ofresearch study-
ing the properties of citations in scientific writing.
Several studies have proposed taxonomies such as
citation function (Garfield et al., 1965; Teufel et al.,
2006; Dong and Schf,fer, 201 l; Jurgens et al., 201 8;
Tuarob et al., 20 19 ; Zhao et al., 20 79), citation in-
tent (Cohan etal.,2019; Lauscher eta1.,2021), and
citation sentiment (Athar, 2011; Athar and Teufel,
2012; Ravi et al., 2018), and such labels have been
used to improve RWG performance.

Ge et al. (2021) used citation function predic-
tion as an auxiliary training objective. Jung et al.
(2022); Gu and Hahnloser (2023) used citation in-
tents to perform controllable citation text genera-
tion. Inspired by the observation of Lauscher et al.
(2022) that simple citation label sets struggle to
represent ambiguous, real-world citations, Li and
Ouyang (2024) used LlM-generated, natural lan-
guage descriptions of function of a cited paper in
other, similar works that also cited it.

Other work has studied the discourse properties
and organization ofcitations. Jaidka et al. (2010,
2011); Khoo et al. (2011); Jaidka et al. (2013b,a)
classified literature reviews into integrative (sum-
marizing individual cited papers) and descriptive
(focusing on high-level ideas from multiple papers)
writing styles. Li et al. (2022) proposed a more
fine-grained taxonomy at the citation level, label-
ing citations as dominant (the main focus of their
sentence) or reference (tangential to the rest oftheir
sentence),
Li and Ouyang (2024) used this taxonomy to an-
alyzethe writing style of LlM-generated RWS and
observed a strong correlation between the propor-
tion of reference-type citations and human prefer-
ence scores, concluding that human readers prefer
integrative RWS supported by reference-type cita-
tions. Similarly, Martin-Boyle et al. (2024) found
that both human-written and human-assisted, LLM-
generated RWS had significantly more cited papers
per sentence than pure machine-generated RWS.
put. Li and Ouyang (2024) extended this idea to
section-level RWG by proposing to use a human-
written short summary of the main ideas of the
target RWS. Also for section-level RWG, Martin
Boyle et al. (2024) introduced a human-in-the-loop
component where the user edited an predicted cited
paper grouping before the generation step,

\section{Datasets}
Despite the twenty published works on RWG, there
is no standard benchmark dataset for the task. As
we discussed in Section 2, most RWG works de-
fine their own version of the task; they also create
their own datasets, adapted to their particular task
definition. In this section, we describe the most
commonly used sources of scientific articles (Table
\ref{tab:dataset_sources}) and summarize how RWG works have built on
these sources. The details of each work's datasets
can be found in Appendix Table~\ref{tab:appendix_table6}.

\begin{table}[t]
\centering
\small
\setlength{\tabcolsep}{3pt}
\begin{tabular}{@{}lcc@{}}
\toprule
Source & Domain & Notes \
\midrule
ACL Anthology Network (AAN) & NLP & ACL papers, citation network \
S2ORC & Multi-domain & Large corpus with metadata \
CORWA (partition of S2ORC) & NLP & Citation-oriented RWG annotations \
Delve & Multi-domain & Scholarly search and analysis \
\bottomrule
\end{tabular}
\caption{Common sources of scientific articles used in RWG datasets (Table content partly [ILLEGIBLE] in source).}
\label{tab:dataset_sources}
\end{table}

\section{Evaluation}
The evaluation methods in RWG can be categorized
into automatic evaluation using metrics such as ROUGE
and human evaluation of aspects such as fluency,
coherence, and relevance. Many works use human
evaluation due to poor correlation between automated
metrics and human judgments.

\section{Discussion and Future Work}
\subsection{Common Limitations and Suggestions for Future Work}
We find several limitations common to existing
work on RWG for future work to consider.

\paragraph{Citation ordering and organization.}
Out of twenty surveyed RWG works, only four attempt
to predict the correct ordering and/or grouping of
citations into paragraphs (Hu and Wan, 2074;Deng
et aL.,2027: Liu et al., 2023; Martin-Boyle et al.,
2024); an additional two papers acknowledge the
citation ordering and grouping problem but assume
a human-provided ordering is available (Hoang and
Kan, 2010) or use a chronological ordering heuris-
tic (Li and Ouyang,2024). Yet Li and Ouyang
(2024) observed that human readers noticed and
disliked errors in citation grouping, such as when
chronologically adjacent cited papers about differ-
ent topics were placed in the same paragraph, and
Martin-Boyle et al. (2024) also found that citation
grouping errors affected perceived quality.

\paragraph{Retrieval-augmented related work generation.}
Existing RWG works assume the list of cited pa-
pers is available as input, but this assumption is
unrealistic, as evidenced by the existence of ``miss-
ing related work'' in many papers.

\paragraph{Multi-paper citations.}
Where existing works have often explicitly ex-
cluded multi-paper citations, future works should
explicitly target them. Similarly, the distinction be-
tween the reference-style citations (Li et al., 2022),
which are more like extreme summarization, and
the dominanr-style citations that current models
tend to produce, should be accounted for; future
works can use different models for these two very
different citation styles.

\subsection{Ethical Concerns}
First, factual consistency in summarization is an active
research area (Cao et a1.,2018; Goodrich et al.,
2019; Falke et a1.,2019; Kryscinski et al., 2019).
Thus, it is possible for an absrractive RWG system
to output plagiarized or hallucinated text, which
should be of concern to any user who wishes to use
such a system to write an RWS.

Second, the use of RWG to write an RWS for a
paper one intends to submit for publication raises
questions of academic dishonesty. Is it ethical for a
researcher to put an automatically generated RWS
in a submitted manuscript? Does this mean the
researcher is claiming to have written that RWS,
as they presumably wrote the rest of the paper?
Do the answers to these questions change if the
researcher has edited the automatically generated
RWS? As with many concerns relating to the use
of powerful modem LLMs, these questions are
very new, and there is as yet no consensus among
the scientiflc community on how to answer them.
While automatically generated RWS as currently
easy to recognize, we nonetheless urge caution on
the part of RWG researchers and users.

Third, RWG is a challenging task even for hu-
mans; in many doctoral programs, writing a formal
literature review is part of their candidacy qual-
ifying exams (Knopf, 2006). Thus, the process
of writing an RWS may be considered an impor-
tant process for researchers where they must read
broadly and think deeply about how their contribu-
tions fit into the bigger picture of their field. Some
RWG works have argued that writing an RWS is
arduous and time-consuming, and so RWG should
save researchers from having to do it, but we argue
this position ignores the value of RWS writing as
a learning and thinking experience. We urge RWG
researchers to consider human-in-the-loop frame-
works, following Gu and Hahnloser (2023); Li and
Ouyang (2024); Martin-Boyle et al. (2024).

\section{Limitations of this Survey}
There is currently a surge of interest in RWG, so
new papers are being published that may not be
included in this survey.
Due to the length limit, we are not able to give
a detailed discussion of each work's methodology
and implementation. We include cheat sheets in
Appendix A to summarize the surveyed works from
various perspectives. We also do not compare the
specific performance scores of the surveyed works
because they are generally not directly comparable.
As with any survey paper, the opinions and in-
telpretations are ours and may not reflect what the
authors of the surveyed papers believe about their
own work.

\begin{thebibliography}{99}

\bibitem{ref1}
Ahmed AbuRa'ed, Horacio Saggion, Alexander Shvets,
and Alex Bravo.2020. Automatic related work sec-
tion generation: experiments in scientific document
abstracting. S ci ent ome trics, 1 25 (3 ) : 3 I 59-3 I 8 5.

\bibitem{ref2}
Uchenna Akujuobi and Xianglian g Zhang. 20 I 7. Delve:
a dataset-driven scholarly search and analysis system.
ACM S I GKD D Explorations New sl ette r, 19 (2):36-
46.

\bibitem{ref3}
Awais Athar.20l1. Sentiment analysis of citations us-
ing sentence structure-based features. In Proceedings
of theACL20ll student session, pages 8l-87.

\bibitem{ref4}
Awais Athar and Simone Teufel. 2012. Context-
enhanced citation sentiment detection. In Proceecl-
ings of the 20 l2 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Longuage Technologies, pages 597-
601, Montr6al, Canada. Association for Computa-
tional Linguistics.

\bibitem{ref5}
Iz Beltagy, Matthew E. Peters, and Arman Cohan,
2020. Longformer: The long-document transformer,
arXiv:2004.05150.

\bibitem{ref6}
Ziqiang Cao, Furu Wei, Wenjie Li, and Sujian Li. 2018.
Faithful to the original: Fact aware neural abstractive
summarization. In Proceedings of the AAAI Confer-
ence on Artificial Intelligence, volume 32.

\bibitem{ref7}
Jingqiang Chen and Hai Zhuge. 2019. Automatic gener-
ation of related work through summarizing citations.
Concurrency and Computation: Practice and Experi-
ence, 31(3):e4261 .

\bibitem{ref8}
Xiuying Chen, Hind Alamro, Mingzhe Li, Shen Gao,
Rui Yan, Xin Gao, and Xiangliang Zhang. 2022,
Target-aware abstractive related work generation with
contrastive learning. In Proceedings of the 45th In-
ternational ACM SIGIR Conference on Research and
Development in Information Retrieval, pages 373-
383.

\bibitem{ref9}
Xiuying Chen, Hind Alamro, Mingzhe Li, Shen Gao, Xi-
angliang Zhang, Dongyan Zhao, and Rui Yan. 2021.
Capturing relations between scientific papers: An
abstractive model for related work section generation.
In Proceedings of the 59th Annual Meeting of the
Association for Computational Linguistics and the
llth International Joint Confer-
ence on Natural Lan-
guage Processing (Volume l: Long Papers), pages
6068-607 7, Online. Association for Computational
Linguistics.

\bibitem{ref10}
Arman Cohan, Waleed Ammar, Madeleine Van Zuylen,
and Field Cady. 2019. Structural scaffolds for ci-
tation intent classification in scientific publications.
arXiv p rep rint arXiv : I 904.0 I 6 0 8.

\bibitem{ref11}
Zekun Deng, Zixin Zerg, Weiye Gu, Jiawen Ji, and
Bolin Hua. 2021. Afiomatic related work section
generation by sentence extraction and reordering.

\bibitem{ref12}
Cailing Dong and Ulrich Sch[fer.2011. Ensemble-style
self-training on citation classification. In Proceed-
ings of 5th international joint conference on natural
language processing, pages 623-631.

\bibitem{ref13}
Gtines Erkan and Dragomir R Radev. 2004. Lexrank:
Graph-based lexical centrality as salience in text sum-
marization. Jourual of artificial intelligence research,
22:457479.

\bibitem{ref14}
Tobias Falke, Leonardo F. R. Ribeiro, Prasetya Ajie
Utama, Ido Dagan, and Iryna Gurevych. 2019. Rank-
ing generated summaries by correctness: An interest-
ing but challenging application for natural language
inference. In Proceedings of the 57th Annual Meet-
ing of the Association for Computational Linguistics,
pages 2214-2220, Florence, Italy. Association for
Computational Linguistics.

\bibitem{ref15}
Eugene Garfield et al. 1965. Can citation indexing be
automated. ln Statistical association methods for
me c hanize d d o c ument ation, symp o s ium pro c e e din g s,
volume 269, pages 189-192. Washington,

\bibitem{ref16}
Yubin Ge, Ly Dinh, Xiaofeng Liu, Jinsong Su, Ziyao
Lu, Ante Wang, and Jana Diesner.2021 . BACO:
A background knowledge- and content-based frame-
work for citing sentence generation. In Proceedings
of the 59th Anntnl Meeting of the Association for
Computational Linguistics and the I lth International
Joint Conference on Natural ktnguage Processing
(Volume l: Long Papers), pages 146G1478, Online.
Association for Computational Linguistics.

\bibitem{ref17}
Ben Goodrich, Vinay Rao, Peter J. Liu, and Mohammad
Saleh. 2019. Assessing the factual accuracy of gener-
ated text. In Proceedings of the 25th ACM SIGKDD
Int e rnat ional C onfe re nc e o n Kn ow I e d g e D is c ov e ry
& Data Mining, KDD'19, page 166-175, New York,
NY USA. Association for Computing Machinery.

\bibitem{ref18}
Max Grusky, Mor Naaman, and Yoav Artzi. 2018.
Newsroom: A dataset of 1.3 million summaries with
diverse extractive strategies. ln Proceedings of the
2018 Conference of the North American Chapter of
the As s ociation for Computational Linguistic s : H u-
man l,ttnguage Technologies, Volume I (Long Pa-
p e rs ), pages 7 08-7 19, New Orleans, Louisiana. As-
sociation for Computational Linguistics.

\bibitem{ref19}
Nianlong Gu and Richard H. R. Hahnloser.2023. Con-
trollable citation sentence generation with language
models.

\bibitem{ref20}
Cong Duy Vu Hoang and Min-Yen Kan. 2010. Towards
automated related work summarization. In Coling
2010: Posters, pages 421435, Beijing, China. Col-
ing 2010 Organizing Committee.

\bibitem{ref21}
Yue Hu and Xiaojun Wan. 2014. Automatic genera-
tion ofrelated work sections in scientiflc papers: an
optimization approach. In Proceedings of the 2014
Conference on Empirical Methods in Nattrral Lan-
guage Processing (EMNLP), pages 1624-1633.

\bibitem{ref22}
Kokil Jaidka, Christopher Khoo, and Jin-Cheon Na.
2010. Imitating human literature review writing: An
approach to multi-document summarization. In In-
ternational Conference on Asian Digital Libraries,
pages 116_1 19. Springer.

\bibitem{ref23}
Kokil Jaidka, Christopher Khoo, and Jin-Cheon Na.
2013a. Deconstructing human literature reviews-a
framework for multi-document summarization. In
proceedings of the l4th European workshop on natu-
ral language generation, pages 125-135.

\bibitem{ref24}
Kokil Jaidka, Christopher SG Khoo, and Jin-Cheon Na.
2013b. Literature review writing: how information
is selected and transformed. ln Aslib Proceedings.
Emerald Group Publishing Limited.

\bibitem{ref25}
Kokil Jaidka Jaidka, Christopher Khoo Khoo, and
Jin-Cheon Na Na. 2011. Literature review writ-
ing: a study of information selection from cited pa-
perslkokil jaidka, christopher khoo and jin-cheon na.

\bibitem{ref26}
Shin g-Yun Jung, Ting-Han Lin, Chia-Hung Liao, Shyan-
Ming Yuan, and Chuen-Tsai Sun. 2022. Intent-
controllable citation text generation. Mathematics,
10( 10):1763.

\bibitem{ref27}
David Jurgens, Srijan Kumar, Raine Hoover, Dan Mc-
Farland, and Dan Jurafsky. 2018. Measuring the
evolution of a scientific field through citation frames.
Transactions of the Association for Computational
Li n gu i sr ic s, 6:39 I 406.

\bibitem{ref28}
Christopher SG Khoo, Jin-Cheon Na, and Kokil Jaidka.
201 l. Analysis of the macrolevel discourse structure
of literature reviews. Online Information Review.

\bibitem{ref29}
Judith L Klavans, Min-yen Kan, and Kathleen McKe-
own. 2001. Domain-specific informative and indica-
tive summarization for information rctrieval. Pro-
ceedings of the Document Understanding Workshop.

\bibitem{ref30}
Jeffrey W Knopf. 2006. Doing a literature review. PS
Politic al Scienc e & Politic s, 39 (l):127 -132.

\bibitem{ref31}
Wojciech Kryscinski, Nitish Shirish Keskar, Bryan Mc-
Cann, Caiming Xiong, and Richard Socher. 2019.
Neural text summarization: A critical evaluation. In
Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the 9th
Intemational Joint Conference on Natural Language
P ro c e s s in g ( EMN LP - I J C N LP.), pages 540-5 5 1, Hong
Kong, China. Association for Computational Linguis-
tics.

\bibitem{ref32}
Anne Lauscher, Brandon Ko, Bailey Kuehl, Sophie
Johnson, Arman Cohan, David Jurgens, and Kyle
Lo. 2022. MultiCite: Modeling realistic citations
requires moving beyond the single-sentence single-
label setting. In Proceedings ofthe 2022 Conference
of the North American Chapter of the Association for
C omputational Lingui s tic s : Human lnnguage Te ch-
nologies, pages 1875-1889, Seattle, United States.
Association for Computational Linguistics.

\bibitem{ref33}
Anne Lauscher, Brandon Ko, Bailey Kuhl, Sophie John-
son, David Jurgens, Arman Cohan, and Kyle Lo.
2021. Multicite: Modeling realistic citations requires
moving beyond the single-sentence single-label set-
ting. arXiv preprint arXiv :2 I 07.004 I 4.

\bibitem{ref34}
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio
Petroni, Vladimir Karpukhin, Naman Goyal, Hein-
rich Kiittler, Mike Lewis, Wen-tau Yih, Tim Rock-
tiischel, et al. 2020. Retrieval-augmented generation
for knowledge-intensive nlp tasks. Advances in Neu-
r al I nfo rmati o n P ro c e s s in g Sy st ems, 33 :9 459 -9 47 4.

\bibitem{ref35}
Xiangci Li, YiHui Lee, and Jessica Oryang. 2023.
Cited text spans for citation text generation. arXiv
p re p rint arXiv : 2 3 09. 06 3 6 5 .

\bibitem{ref36}
Xiangci Li, Biswadip Mandal, and Jessica Ouyang.
2022. CORWA: A citation-oriented related work
annotation dataset. In Proceedings of the 2022 Con-
ference of the North American Chapter of the As-
sociation for Computational Linguistics: Human
Lttn g ua g e Te c hn o I o g i e s, pages 5 426-5440, S eattle,
United States. Association for Computational Lin-
guistics.

\bibitem{ref37}
Xiangci Li and Jessica Ouyang. 2024. Explaining re-
lationships among research papers. arXiv preprint
arXiv:2402.13426.

\bibitem{ref38}
Chin-Yew Lin.2004. Rouge: A package for automatic
evaluation of summaries. In Text summarization
branches out, pages 7 4-81.

\bibitem{ref39}
Jiachang Liu, Qi Zhang, Chongyang Shi, Usman
Naseem, Shoujin Wang, Liang Hu, and Ivor Tsang.
2023. Causal intervention for abstractive related
work generation. In Findings of the Association
for C omputational Linguistics : EMNLP 202 3, pages
2148-2159, Singapore. Association for Computa-
tional Linguistics.

\bibitem{ref40}
Yang Liu and Mirella Lapata. 2019. Text summariza-
tion with pretrained encoders. ln Proceedings of
the 2019 Conference on Empirical Methods in Natu-
ral Language Processing and the 9th International
loint Conference on Natural l,onguage Processing
(EMNLP-IJCNLP), pages 3730-3740, Hong Kong,
China. Association for Computational Linguistics.

\bibitem{ref41}
Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kin-
ney, and Daniel Weld. 2020. S2ORC: The semantic
scholar open research corpus. In Proceedings of the
58th Annual Meeting of the Association for Compu-
tational Linguistics, pages 49694983, Online. Asso-
ciation for Computational Linguistics.

\bibitem{ref42}
Kelvin Luu, Xinyi Wu, Rik Koncel-Kedziorski, Kyle
Lo, Isabel Cachola, and Noah A. Smith. 2021. Ex-
plaining relationships between scientiflc documents.
ln Proceedings of the 59th Annual Meeting of the
Association for Computational Linguistics and the
I lth International Joint Conference on Natural Lan-
guage Processing (Volume l: Long Papers),pages
2130-2144, Online. Association for Computational
Linguistics.

\bibitem{ref43}
Biswadip Mandal, Xiangci Li, and Jessica Ouyang.
2024. Contextualizing generated citation texts, arXiv
preprint arXiv : 2402. I 80 54.

\bibitem{ref44}
Anna Martin-Boyle, Aahan Tyagi, Marti A Hearst, and
Dongyeop Kang.2024. Shallow synthesis of knowl-
edge in gpt-generated texts: A case study in au-
tomatic related work composition. arXiv preprint
arXiv:2402.12255.

\bibitem{ref45}
Rui Meng, Khushboo Thaker, Lei Zhang, Yue Dong,
Xingdi Yuan, Tong Wang, and Daqing He. 2021.
Bringing structure into summaries: a faceted sum-
marization dataset for long scientific documents. In
Proceedings of the 59th Annual Meeting of the Asso-
ciation for Computcttional Linguistics and the I ltlt
International Joint Conference on Natural Language
Processing (Volume 2: Short Papers), pages 1080-
1089, Online. Association for Computational Linguis-
tics.

\bibitem{ref46}
Rada Mihalcea and Paul Tarau. 2004. Textrank: Bring-
ing order into text. ln Proceedings ofthe 2004 con-
ference on empirical methods in natural language
processing, pages 404-41 1.

\bibitem{ref47}
Shashi Narayan, Shay B. Cohen, and Mirella Lapata.
2018. Don't give me the details, just the summary!
Topic-aware convolutional neural networks for ex-
treme summarization. In Proceedings of the 2018
Conference on Empirical Methods in Natural lrtn-
guage Processlng, Brussels, Belgium.

\bibitem{ref48}
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation . In Proceedings of the
40th annual meeting of the Association for Computa-
tional Linguistics, pages 3 1 1-3 I 8.

\bibitem{ref49}
Dragomir R Radev, Hongyan Jing, Malgorzata StyS, and
Daniel Tam. 2004. Centroid-based summarizarion
of multiple documents. Information Processing &
M anagement, 40(6):9 I 9-938.

\bibitem{ref50}
Dragomir R Radev, Pradeep Muthukrishnan, Vahed
Qazvinian, and Amjad Abu-Jbara. 2013. The acl
anthology network corpus. Langtrage Resources and
Ev aluatio n, 47 (4):9 1 9-9 44.

\bibitem{ref51}
Kumar Ravi, Srirangaraj Setlur, Vadlamani Ravi, and
Venu Govindaraju. 20 1 8. Article citation sentimenr
analysis using deep learning. In 2018 IEEE 17th
Inte rnational Conferenc e on Co gnitiv e Informatic s
& Cognitive Computing (lCCt'o CC.), pages 78-85.
IEEE.

\bibitem{ref52}
Abigail See, Peter J. Liu, and Christopher D. Manning.
201'7. Get to the point: Summarization with pointer-
generator networks. In Proceedings ofthe 55th An-
rutal Meeting of the Associationfor Computational
Linguistics (Volume l: Long Papers), pages 1073-
1083, Vancouver, Canada. Association for Computa-
tional Linguistics.

\bibitem{ref53}
Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela,
and Jason Weston. 2021. Retrieval augmentation
reduces hallucination in conversation. In Findings
of the Association for Computational Linguistics:
EMNLP 202 1, pages 3784-3803, Punta Cana, Do-
minican Republic. Association for Computational
Linguistics.

\bibitem{ref54}
Simone Teufel, Advaith Siddharthan, and Dan Tidhar.
2006. Automatic classification of citation function.
In Proceedings of the 2006 conference on empirical
methods in natural language processlng, pages 103-
110.

\bibitem{ref55}
Suppawong Tuarob, Sung Woo Kang, Poom Wet-
tayakorn, Chanatip Pomprasit, Tanakitti Sachati,
Saeed-Ul Hassan, and Peter Haddawy. 2019. Au-
tomatic classification of algorithm citation functions
in scientific literature. IEEE Transactions on Knowl-
edge and Data Engineering, 32(10): 188 l-l 896.

\bibitem{ref56}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, f-ukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in neural information pro-
cessing systems, pages 5998-6008.

\bibitem{ref57}
Petar Velickovic, Guillem Cucurull, Arantxa Casanova,
Adriana Romero, Pietro Lio', and Yoshua Ben-
gio. 2018. Graph attention networks. ArXiv,
abs/1710.10903.

\bibitem{ref58}
Pancheng Wang, Shasha Li, Haifang Zhou, Jintao Tang,
and Ting Wang. 2019. Toc-rwg: Explore the com-
bination of topic model and citation information for
automatic related work generation. IEEE Access,
8:13043-13055.

\bibitem{ref59}
Yongzhen Wang, Xiaozhong Liu, ancl Zheng Gao. 20 1 8.
Neural related work summarization with a joint
context-driven attention mechanism. In Proceed-
ings of the 2018 Conference on Empirical Methods
in N at ural lnn gua g e P roc e s s in g, pages 17 7 6-17 86,
Brussels, Belgium. Association for Computational
Linguistics.

\bibitem{ref60}
Mark Wasson. 1998. Using leading text for news sum-
maries: Evaluation results and implications for com-
mercial summarization applications . In 36th Annual
Meeting of the Association for Computational Lin-
guistics and lTth International Conference on Com-
putational Linguistics, Volume 2, pages 1364-1368.

\bibitem{ref61}
Xinyu Xing, Xiaosheng Fan, and Xiaojun Wan. 2020.
Automatic generation of citation texts in scholarly
papers: A pilot study. In Proceedings of the 58th
Annual Meeting of the Association for Computational
Lin gttistic s, pages 6 1 8 1-6 1 90.

\bibitem{ref62}
Michihiro Yasunaga, Jungo Kasai, Rui Zhang, Alexan-
der R Fabbri, Irene Li, Dan Friedman, and
Dragomir R Radev. 2019. Scisummnet: A large
annotated corpus and content-impact models for scr-
entific paper summarization with citation networks.
In Proceedings of the AAAI Conference on Artificial
Intelligence, volume 33, pages 7386-7393.

\bibitem{ref63}
HeZhao, Zhunchen Luo, Chong Feng, Anqing Zheng,
and Xiaopeng Lfi.2019 . A context-based framework
for modeling the role and function of on-line resource
citations in scientific literature. In Proceedings of
the 2019 Conference on Empirical Methods in Nattr-
ral Language Processing and the 9th International
Joint Conference on Natural Language Processing
( EMNLP -IJCNLP), pages 5206-5215.

\end{thebibliography}

\appendix
\section{Appendix}

\subsection{Cheat Sheets for Prior Works}

\begin{table*}[t]
\centering
\small
\setlength{\tabcolsep}{4pt}
\begin{tabular}{@{}l p{13cm}@{}}
\toprule
Prior Work & Approaches \
\midrule
Hoang and Kan (2010) & Heuristic approach to generate general and specific content separately given a topic tree \
Hu and Wan (2014) & PLSA for topic modeling, SVR for sentence importance score, and global optimization for sentence selection \
Wang et al. (2018) & Custom neural seq2seq model (CNN, LSTM, attention), random walk for encoding heterogeneous bibliography graph \
Chen and Zhuge (2019) & Considering papers co-cite the cited papers; Representing graph for relationship modeling of papers, then finding sentence nodes that cover the minimum Steiner tree of the graph \
Wang et al. (2019) & Leveraging both topic model and cited text spans \
Deng et al. (2021) & BERT-based sentence extraction & reordering \
AbuRa'ed et al. (2020) & Applying PTGen and Transformer \
Xing et al. (2020) & Manual annotation + automatic annotation of citation sentences; PTGEN-Cross based on cross-attention mechanism \
Ge et al. (2021) & Citation network as auxiliary input; citation function & salient sentences in cited papers as auxiliary output; multi-task learning \
Luu et al. (2021) & SciGPT2; IE-Extracted Term Lists; ranking based on entity matching \
Li et al. (2022) & LED-based citation span generation \
Jung et al. (2022) & BART/T5-based citation sentence generation with citation intents \
Li et al. (2023) & RAG & LED; ROUGE-based CTS retrieval \
Gu and Hahnloser (2023) & Fine-tuned GPT-Neo & Galactica with Proximal Policy Optimization \
Mandal et al. (2024) & Using citation context along with citation spans as generation target \
Chen et al. (2021) & Transformer-based hierarchical encoder; relationship modeling module \
Chen et al. (2022) & Improved over Chen et al. (2021) by encoding target paper's abstract \
Liu et al. (2023) & Proposed a custom Causal Intervention Module (CaM) inserted between Transformer blocks \
Li and Ouyang (2024) & GPT-3.5 for feature generation, e.g.\ faceted summary, relationship & usage of citations; GPT-4 based RWG \
Martin-Boyle et al. (2024) & GPT-4 with human-in-the-loop \
\bottomrule
\end{tabular}
\caption{A summary of the approaches of the prior works.}
\label{tab:table7}
\end{table*}

\begin{table*}[t]
\centering
\small
\setlength{\tabcolsep}{4pt}
\begin{tabular}{@{}l p{4.4cm} p{3.6cm} p{3.6cm}@{}}
\toprule
Prior Work & Baselines & Automatic & Human Evaluation \
\midrule
Hoang and Kan (2010) & LEAD, MEAD & ROUGE recall (1, 2, S4, SU4) & Correctness, novelty, fluency, usefulness \
Hu and Wan (2014) & MEAD, LexRank & ROUGE F1 (1, 2, SU4) & Correctness, readability, usefulness \
Wang et al. (2018) & Luhn, MMR, LexRank, SumBasic, NltkSum, Pointer Network & ROUGE F1 (1, 2, L) & Compliance to target paper, intuitiveness, usefulness \
Chen and Zhuge (2019) & MEAD, LexRank, RoWoS & ROUGE F1 (1, 2) & N/A \
Wang et al. (2019) & LexRank, SumBasic, JS-Gen, TopicSum & ROUGE recall & F1 (1, 2, SU4) & N/A \
Deng et al. (2021) & MEAD & ROUGE precision, recall, F1 (1, 2, L) & informativeness, fluency, succinctness \
\bottomrule
\end{tabular}
\caption{A summary of the evaluation methods of the extractive related work generation works.}
\label{tab:table8}
\end{table*}

\begin{table*}[t]
\centering
\small
\setlength{\tabcolsep}{4pt}
\begin{tabular}{@{}l p{4.4cm} p{3.6cm} p{3.6cm}@{}}
\toprule
Prior Work & Baselines & Automatic & Human Evaluation \
\midrule
AbuRa'ed et al. (2020) & MEAD, TextRank, SUMMA, SEQ\textsuperscript{3} & ROUGE precision, recall, F1 (1, 2, L, SU4) & N/A \
Xing et al. (2020) & RandomSen, MaxSimSen, EXT-Oracle, COPY-CIT, PTGEN & ROUGE F1 (1, 2, L) & Readability, Content, Coherence, Overall \
Ge et al. (2021) & LexRank, TextRank, EXT-Oracle, PTGEN, PTGEN-Cross & ROUGE F1 (1, 2, L) & Fluency, relevance, coherence, overall \
Luu et al. (2021) & N/A & BLEU, ROUGE-L & Correct, Specific \
Li et al. (2022) & Citation sentence generation & ROUGE F1 (1, 2, L) & Fluency, coherence, relevance, overall \
Jung et al. (2022) & EXT-Oracle, ablations & ROUGE F1 (1, 2, L), SciBERTScore, citation intent accuracy & Correct, specific, plausible, intent \
Li et al. (2023) & Citation span generation based on cited abstracts, & human-annotated CTS & BLEU, ROUGE-F1-L, METEOR, QuestEval, ANLI & Fluency, coherence, relevance, overall \
Gu and Hahnloser (2023) & BART-base & -large, GPT-Neo 125M & 1.3B, Galactica 125M & 1.3B &6.7B, LLaMA-7B ablations, GPT-3.5-turbo & ROUGE F1 (1, 2, L), Intent alignment score, keyword recall, fluency score & Intent alignment, keyword recall, fluency & similarity to the ground truth \
Mandal et al. (2024) & Ablations & N/A & Fluency, coherence, relevance, overall \
\midrule
Chen et al. (2021) & LEAD, TextRank, BertSumEXT, MGSum-ext, PTGen+Cov, TransformerABS, BertSumAbs, MGSum-abs, GS & ROUGE F1 (1, 2, L) & QA, informativeness, coherence, succinctness \
Chen et al. (2022) & LEAD, LexRank, NES, BertSumEXT, MGSum EMS, RRG, BertSumAbs & ROUGE F1 (1, 2, L, SU) & QA, informativeness, coherence, succinctness \
Liu et al. (2023) & TexRank, BertSumEXT, MGSum-ext & -abs, TransformerABS, RRG, BertSumAbs, GS, T5-base, BART-base, Longformer, NG-abs, TAG & ROUGE F1 (1, 2, L) & QA, informativeness, coherence, succinctness \
Li and Ouyang (2024) & Ablations & ROUGE F1 (1, 2, L) & Fluency, coherence, relevance (cited, target), factuality, usefulness, writing, overall, # of errors \
Martin-Boyle et al. (2024) & Human & Human-in-the-loop & # of edges, average node degree, density, cluster coefficient & Qualitative analysis \
\bottomrule
\end{tabular}
\caption{A summary of the evaluation methods of the abstractive related work generation works.}
\label{tab:table9}
\end{table*}

\begin{table*}[t]
\centering
\small
\setlength{\tabcolsep}{4pt}
\begin{tabular}{@{}p{2.7cm} p{7.4cm} p{5.0cm}@{}}
\toprule
Perspective & Definition & Used By \
\midrule
Fluency, Readability & Does the summary's exposition flow well, in terms of syntax as well as discourse? &
Hoang and Kan (2010); Hu and Wan (2014); Deng et al. (2021); Xing et al. (2020); Ge et al. (2021); Chen et al. (2021, 2022); Li et al. (2022, 2023); Gu and Hahnloser (2023); Mandal et al. (2024); Li and Ouyang (2024) \
Correctness & Is the summary content relevant to (express the factual relationship with) the hierarchical topics/cited papers given? &
Hoang and Kan (2010); Hu and Wan (2014); Luu et al. (2021); Jung et al. (2022) \
Novelty & Does the summary introduce novel information that is significant in comparison with the human created summary? &
Hoang and Kan (2010) \
Usefulness & Is the summary useful in supporting the researchers to quickly grasp the related works given hierarchical topics? &
Hoang and Kan (2010); Hu and Wan (2014); Wang et al. (2018); Li and Ouyang (2024) \
Content, Relevance & Whether the citation text is relevant to the cited paper's abstract &
Wang et al. (2018); Xing et al. (2020); Ge et al. (2021); Li et al. (2022, 2023); Gu and Hahnloser (2023); Mandal et al. (2024); Li and Ouyang (2024) \
Coherence & Whether the citation text is coherent with the citing paper's context &
Xing et al. (2020); Ge et al. (2021); Chen et al. (2021, 2022); Li et al. (2022, 2023); Liu et al. (2023); Mandal et al. (2024); Li and Ouyang (2024) \
Informativeness & Does the related work convey important facts about the topic question? &
Deng et al. (2021); Chen et al. (2021, 2022); Liu et al. (2023) \
Succinctness & Does the related work avoid repetition? &
Deng et al. (2021); Chen et al. (2021); Liu et al. (2023) \
Overall & Overall quality &
Xing et al. (2020); Ge et al. (2021); Li et al. (2022, 2023); Mandal et al. (2024); Li and Ouyang (2024) \
Intuitiveness & How intuitive is the related work section for readers to grasp the key content? &
Wang et al. (2018) \
QA & Retain the key information? &
Chen et al. (2021, 2022); Liu et al. (2023) \
Specific & Whether the explanation describes a specific relationship between the two works &
Luu et al. (2021); Jung et al. (2022) \
Factuality, # of errors & Does the output contain factual errors? &
Li and Ouyang (2024) \
Plausible, writing & Writing style of citation text / RWS &
Jung et al. (2022); Li and Ouyang (2024) \
Qualitative analysis & Descriptive case study &
Li and Ouyang (2024); Martin-Boyle et al. (2024) \
Intent alignment & Whether the output aligns with the input intent. &
Jung et al. (2022); Gu and Hahnloser (2023) \
Keyword recall & Whether the output contains the input key words. &
Gu and Hahnloser (2023) \
\bottomrule
\end{tabular}
\caption{A summary of the perspectives for human evaluation.}
\label{tab:table10}
\end{table*}

\begin{table*}[t]
\centering
\small
\setlength{\tabcolsep}{4pt}
\begin{tabular}{@{}p{3.5cm}p{13cm}@{}}
\toprule
Work & Dataset and preprocessing details \
\midrule
Hoang and Kan (2010) & AAN; limited to 250 papers; topic trees and evaluation data built from SIGIR 2002 papers. \
Hu and Wan (2014) & ACL Anthology; 200 papers; citations extracted by patterns; filtered to those with $\le$ 10 citations. \
Wang et al. (2018) & ACL Anthology; 1,000 papers; extracted citation contexts and built heterogeneous bibliography graph. \
Chen and Zhuge (2019) & Biomedical (PubMed) corpus; co-citation based extraction; [ILLEGIBLE]. \
Wang et al. (2019) & CS papers; extracted citations and related contexts; [ILLEGIBLE]. \
Deng et al. (2021) & Biomedical; sentence extraction and reordering dataset; [ILLEGIBLE]. \
AbuRa'ed et al. (2020) & Scientific document abstracting dataset; [ILLEGIBLE]. \
Xing et al. (2020) & Annotated citation sentences; [ILLEGIBLE]. \
Ge et al. (2021) & BACO dataset; [ILLEGIBLE]. \
Luu et al. (2021) & Explaining relationships between scientific documents; [ILLEGIBLE]. \
Jung et al. (2022) & Citation intent dataset; [ILLEGIBLE]. \
Li et al. (2022) & CORWA dataset; citation-oriented annotations; [ILLEGIBLE]. \
Gu and Hahnloser (2023) & Controllable citation generation dataset; [ILLEGIBLE]. \
Li et al. (2023) & Cited text spans (CTS) retrieval dataset; [ILLEGIBLE]. \
Mandal et al. (2024) & Contextualizing generated citation texts dataset; [ILLEGIBLE]. \
Chen et al. (2021) & Section-level RWG dataset; [ILLEGIBLE]. \
Chen et al. (2022) & Improved dataset; [ILLEGIBLE]. \
Liu et al. (2023) & Abstractive related work generation dataset; [ILLEGIBLE]. \
Li and Ouyang (2024) & LLM-generated feature prompts; [ILLEGIBLE]. \
Martin-Boyle et al. (2024) & Human-in-the-loop RWG study; [ILLEGIBLE]. \
\bottomrule
\end{tabular}
\caption{Appendix Table 6: Dataset details for each surveyed work.}
\label{tab:appendix_table6}
\end{table*}

\end{document}
=====END FILE=====

=====FILE: figures/README.txt=====
This LaTeX project does not include extracted figure image assets from the PDF.
Where figures were present, they should be represented using LaTeX figure environments
with a boxed note "IMAGE NOT PROVIDED". This paper's appendix content in the provided
PDF consisted of tables (Table 7--Table 10) rather than separate figure images.
=====END FILE=====
