=====FILE: main.tex=====
% 
\documentclass[11pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage{times}
\usepackage{microtype}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{array}
\usepackage{url}

\title{MTA4DPR: Multi-Teaching-Assistants Based Iterative Knowledge Distillation for Dense Passage Retrieval}

\author{
Qixi Lu$^{1,2}$, Endong Xun$^{1}$, Gongbo Tang$^{2}$\thanks{Corresponding author}\
$^{1}$Beijing Advanced Innovation Center for Language Resources,\
Beijing Language and Culture University, China\
$^{2}$School of Information Science, Beijing Language and Culture University, China\
\texttt{[lqxaixxh@gmail.com](mailto:lqxaixxh@gmail.com), {edxun, gongbo.tang}@blcu.edu.cn}
}

\date{}

\begin{document}
\maketitle

\begin{abstract}
Although Dense Passage Retrieval (DPR) models have achieved significantly enhanced performance, their widespread application is still hindered by the demanding inference efficiency and high deployment costs. Knowledge distillation is an efficient method to compress models, which transfers knowledge from strong teacher models to weak student models. Previous studies have proved the effectiveness of knowledge distillation in DPR. However, there often remains a significant performance gap between the teacher and the distilled student. To narrow this performance gap, we propose MTA4DPR, a Multi-Teaching-Assistants based iterative knowledge distillation method for Dense Passage Retrieval, which transfers knowledge from the teacher to the student with the help of multiple assistants in an iterative manner; with each iteration, the student learns from more performant assistants and more difficult data. The experimental results show that our 66M student model achieves the state-of-the-art performance among models with same parameters on multiple datasets, and is very competitive when compared with larger, even LLM-based, DPR models.
\end{abstract}

\section{Introduction}
Although PLM/LLM-based Dense Passage Retrieval (DPR) models \cite{karpukhin2020dense,qin2024llmrankers} have superior performance, those models’ inference efficiency and deployment costs are still cumbering their wide applications. To obtain an efficient and effective DPR model, researchers are paying more attention to knowledge distillation. Previous studies \cite{zeng2022curriculum,sun2024lead,lu2022erniesearch} have proved the effectiveness of knowledge distillation in DPR. However, the performance gap between the teacher and the distilled student often remains significant, especially when the teacher is a very good one.

In this paper, we hypothesize that incorporating assistants into knowledge distillation can help improve students’ performance, just as teaching assistants in universities can assist students in learning course content. In addition, inspired by curriculum learning \cite{bengio2009curriculum}, we also believe that multiple iterations can further narrow the gap between the teacher and the student since the latter is capable of learning from more challenging data and more effective assistants as the iterations go on. Therefore, we introduce MTA4DPR, a multi-teaching-assistants based iterative distillation method. Specifically, MTA4DPR transfers knowledge from the teacher to the student with the help of multiple assistants iteratively. For each iteration, we first use off-the-shelf teacher/assistant DPR models to generate datasets for training and evaluation. Then, we use a fusion module to generate a series of fused assistants. After that, we train the student to learn from the teacher with the help of the best assistant selected among all fused and original assistants by our selection module, as illustrated in Figure~\ref{fig:mta4dpr-framework}. At the end of each iteration, we evaluate the student’s performance and replace the worst-performing assistant with it if it outperforms any existing assistants. What’s more, we also incorporate data that the student predicted incorrectly in the previous iteration into the newly constructed dataset, by which the difficulty of each iteration’s dataset is increased. In this way, as the training iterates, the student can learn from more performant assistants and more difficult data.

The experimental results on MS MARCO, TREC DL 2019 and 2020 and Natural Questions show the effectiveness of our method. Our 66M student model achieves the state-of-the-art performance among models with same parameters on multiple datasets, and is competitive when compared with larger, even LLM-based, DPR models.

To summarize, our main contributions are:
\begin{enumerate}
\item We propose a novel distillation method MTA4DPR, which improves the student’s retrieval performance with the help of assistant models.
\item The experimental results show the effectiveness of our proposed method, achieving very competitive results even when compared with larger, even LLM-based, DPR models.
\item Not constrained by model structures and tasks, MTA4DPR is orthogonal to existing distillation methods and can be combined with other distillation pipelines to further improve the performance.
\end{enumerate}

\begin{figure}[t]
\centering
\fbox{\parbox{0.95\linewidth}{\centering IMAGE NOT PROVIDED}}
\caption{MTA4DPR Framework. MTA4DPR transfers knowledge from the teacher to the student with the help of the best assistant. The Fusion Module is used to generate fused assistants from the original assistants, and the Selection Module is used to select the best assistant among all original and fused assistants. The dotted arrows indicate that the corresponding procedures are not involved in the backpropagation of the training.}
\label{fig:mta4dpr-framework}
\end{figure}

\section{Related Work}
\subsection{Dense Retrieval}
Despite its wide applications, sparse retrieval, such as BM25, can not thoroughly solve the lexical mismatch problem, although query/document expansion \cite{nogueira2019doc2query,formal2021splade} and term-weighting \cite{lin2021few,gao2021coil} have been proposed to help mitigate the problem. For this reason, dense retrievers, especially those built upon PLMs or LLMs, have received more and more attention. They map both passages and queries into dense vectors, the relevance between which can be computed by dot products. Recently, a large number of methods have been proposed to improve dense retrievers’ performance, including negative sampling \cite{xiong2020annncl}, knowledge distillation \cite{zeng2022curriculum,sun2024lead,lin2023prod} and joint optimization of retrievers and rankers \cite{ren2021rocketqav2}.

\subsection{Knowledge Distillation}
Knowledge Distillation transfers knowledge from the teacher to the student, allowing the latter to have good performance with high efficiency. To achieve this goal, students are forced to learn knowledge representations provided by teachers, including response-based knowledge \cite{hinton2015distill,beyer2022patient}, intermediate knowledge \cite{adriana2015fitnets,chen2018darkrank,heo2019activation} and relation-based knowledge \cite{peng2019correlation,huang2022stronger,yang2022crossimage}.

Recently, more and more studies focus on multi-teacher distillation, which can draw diverse knowledge from multiple teacher models, improving the student model’s performance \cite{wu2021oneteacher,son2021denselyguided,lin2023prod}. Mirzadeh et al.\ (2020) proposes TAKD, a multi-step knowledge distillation method to bridge the gap between the teacher and the student, in which a larger teacher model distills a smaller teacher model and the latter distills a much smaller student model. Yuan et al.\ (2021) proposes a reinforced method to combine multiple teacher models’ prediction to get the final knowledge, which is used to distill the student model. In all the above studies, researchers tend to treat all teachers equally, combining their predictions using various strategies to train the student model. We argue that treating all teachers equally might be suboptimal given their varying performance.

Different from previous studies, in MTA4DPR, the best-performing model is considered as the primary teacher and involved in the entire training process, while the remaining models serve as assistants, only one of which participates in each training batch. This concept can be analogized to university students learning from a professor with the help of multiple assistants, only one of which is selected for each topic based on their speciality. Furthermore, we experiment with iteratively replacing underperforming assistants with better-performing student models, which further improves the performance of the final student model.

\section{Methodology}
\subsection{Preliminary}
\subsubsection{Task Description}
Assume we have a training set
[
D={(q_i, P_i, S_i)}*{i=1}^{n}
]
where $q_i$ is the query, $P_i$ consists of a positive passage $p_i^{+}$ and $k$ hard negatives $P_i^{-}={p*{i,j}^{-}}*{j=1}^{k}$ (passages that are difficult to distinguish from the positive passage) and
$S_i={S*{i,1}, S_{i,2}, \ldots, S_{i,d}, \ldots}$ consists of relevance scores computed by the teacher/assistants and
$S_{i,d}={S^{j}*{i,d}}*{j=1}^{k+1}$ denotes scores calculated by the $d$-th model, our target is to train a DPR model that retrieves the positive passage $p_i^{+}$ for the query $q_i$.

\subsubsection{Dual-Encoders and Cross-Encoders}
Depending on how queries and passages are encoded, we categorize DPR models into dual-encoders and cross-encoders.

Dual-encoders \cite{karpukhin2020dense} map query $q_i$ and passage $p_j$ into dense vectors, and the relevance between $q_i$ and $p_j$ is computed by the dot product of their representations:
\begin{equation}
S_{DE}(q_i,p_j)=E_{DE}(q_i)^{T}\cdot E_{DE}(p_j)
\label{eq:de}
\end{equation}
where $E_{DE}(\cdot)$ is the dense vector, and $S_{DE}(q_i,p_j)$ represents the relevance score of $q_i$ and $p_j$.

Cross-encoders \cite{kenton2019bert} concatenate $q_i$ and $p_j$ as the input to PLMs/LLMs. The relevance between $q_i$ and $p_j$ is calculated by the representation of [CLS] in the final layer with a projection layer $W$:
\begin{equation}
S_{CE}(q_i,p_j)=W^{T}\cdot E_{CE}([\mathrm{CLS}];q_i;[\mathrm{SEP}];p_j)
\label{eq:ce}
\end{equation}
where $[;]$ is the concatenation operation, and $S_{CE}(q_i,p_j)$ is the similarity of $q_i$ and $p_j$.

In practice, we use contrastive loss, which encourages $\langle q_i,p_i^{+}\rangle$ to be closer together and $\langle q_i,p_i^{-}\rangle$ to be further apart, to train DPR models:
\begin{equation}
L_{CL}=-\log \frac{e^{S(q_i,p_i^{+})}}{e^{S(q_i,p_i^{+})}+\sum_{p_{i,j}^{-}\in P_i^{-}} e^{S(q_i,p_{i,j}^{-})}}
\label{eq:cl}
\end{equation}

\subsubsection{Knowledge Distillation for DPR}
Recent studies have successfully applied knowledge distillation to training more compact DPR models. A common approach is to use a teacher model to compute relevance scores $S$ for $\langle q,p\rangle$ pairs, which are then used as the training data for knowledge distillation. To distill the soft labels (scores) from teachers to students, KL divergence $L_{KL}(\mathrm{tea},\mathrm{stu})$ is used as the loss function:
\begin{equation}
\tilde{S}^{j}*{\mathrm{tea},i}=\frac{e^{S*{\mathrm{tea}}(q_i,p_j)}}{\sum_{p'\in P_i} e^{S_{\mathrm{tea}}(q_i,p')}}
\label{eq:tea}
\end{equation}
\begin{equation}
\tilde{S}^{j}*{\mathrm{stu},i}=\frac{e^{S*{\mathrm{stu}}(q_i,p_j)}}{\sum_{p'\in P_i} e^{S_{\mathrm{stu}}(q_i,p')}}
\label{eq:stu}
\end{equation}
\begin{equation}
L_{KL}(\mathrm{tea},\mathrm{stu})=-KL(\tilde{S}*{\mathrm{tea},i}\Vert \tilde{S}*{\mathrm{stu},i})
\label{eq:kl}
\end{equation}
where $\tilde{S}*{\mathrm{tea},i},\tilde{S}*{\mathrm{stu},i}\in \mathbb{R}^{|P_i|}$ denote the probability distributions over candidate passages $P_i$, and $\tilde{S}^{j}*{\mathrm{tea},i}, \tilde{S}^{j}*{\mathrm{stu},i}$ denote the $j$-th element of $\tilde{S}*{\mathrm{tea},i}, \tilde{S}*{\mathrm{stu},i}$. For convenience, we use $L_{KL}(\mathrm{tea},\mathrm{stu})$, $L_{KL}(\mathrm{ta},\mathrm{stu})$, $L_{KL}(\mathrm{tea},\mathrm{ta})$ to represent the KL divergence between teachers and students, assistants and students, and teachers and assistants.

\subsection{The MTA4DPR Framework}
MTA4DPR transfers knowledge from the teacher DPR model to the student with the help of $m$ ($m\ge 1$) assistant models. For each iteration, we first use these models to generate training and evaluation datasets (Section~3.2.1) which become increasingly difficult as the iterations go on; then, we select the best assistant for each training batch (Section~3.2.3) and train the student model using the teacher together with the selected assistant (Section~3.2.4). The training of one iteration is shown in Figure~\ref{fig:mta4dpr-framework}.

\subsubsection{Data Preparation}
At the start of each iteration, we use the teacher and assistants to generate the corresponding datasets.

\paragraph{Retrieve top-$k$ passages}
We first use each of the $m$ assistants to retrieve the top-$k$ most relevant passages (except the positive passage(s)) for each query $q$. Then, we merge all retrieved passages together and collect scores from each assistant model for each $\langle q,p\rangle$ pair. In this way, query $q$ has one or more positive(s) and $d$ negatives ($k\le d\le mk$) each of which has $m$ scores computed by the aforementioned $m$ assistant models.

\paragraph{Re-rank using RRF scores}
From the previous step, we have $d$ negatives for each query $q_i$, and then we sort these passages in the descending order based on the scores assigned by each assistant, resulting in a set of rankings $R$, each ranking $r$ being a permutation on $p_1,\ldots,p_{|d|}$. Then, we use RRF \cite{cormack2009rrf}, Reciprocal Rank Fusion, to re-rank these $d$ passages, taking the top-$k$ passages with the highest scores as the final hard negatives $P_i^{-}$ for query $q_i$:
\begin{equation}
\mathrm{RRF\ score}(p)=\sum_{r\in R}\frac{1}{c+r(p)}
\label{eq:rrf}
\end{equation}
where $c=60$ following Cormack et al.\ (2009), and $r(p)$ denotes the position of $p$ in ranking $r$.

Finally, we use the teacher to calculate the relevance score for each $\langle q_i,p_j\rangle$ pair where $p_j\in P_i$.
By performing the above operations on all training queries, we obtain the base dataset for the current iteration, from which we extract 1% as the evaluation dataset $D_{\mathrm{eval}}$, leaving the rest as the training dataset $D_{\mathrm{train}}$.

In addition, inspired by Lin et al.\ (2023), we collect the queries for which the teacher can predict the positive as top-1 while the student from the previous iteration can not predict correctly. These queries with the positive passage and the top-$k$ hard negative passages predicted by the student will be added to the generated dataset.

\subsubsection{Fusion Strategy}
Inspired by ensemble learning \cite{mienye2020ensemble}, which enhances predictive performance by leveraging the collective strengths of diverse models, we propose a simple yet efficient fusion strategy to combine knowledge of multiple assistants:
\begin{equation}
S_i=\frac{1}{K}\sum_{k=1}^{K} S_{i,k}
\label{eq:fuse}
\end{equation}
where $S_{i,k}$ is the score distribution between $q_i$ and $P_i$ computed by the $k$-th assistant models.

Specifically, say we have $S_{i,A}$, $S_{i,B}$ and $S_{i,C}\in \mathbb{R}^{|P_i|}$ respectively computed by assistants $A$, $B$ and $C$; by just taking the average of $S_{i,A}$ and $S_{i,B}$, $S_{i,A}$ and $S_{i,C}$, $S_{i,B}$ and $S_{i,C}$, and all three assistants, we can obtain four different new score distributions, i.e.\ $\frac{S_{i,A}+S_{i,B}}{2}$, $\frac{S_{i,A}+S_{i,C}}{2}$, $\frac{S_{i,B}+S_{i,C}}{2}$ and $\frac{S_{i,A}+S_{i,B}+S_{i,C}}{3}$. All these fused score distributions are considered as knowledge contributed by certain fused assistants in MTA4DPR, and are involved in the selection method for assistants.

\subsubsection{Assistant Selection}
To select the best assistant for each training batch, we investigate three heuristic selection strategies:

\paragraph{KL Divergence}
KL divergence measures the similarity between two distributions. The higher the similarity, the smaller the KL divergence. We calculate the KL divergence between the score distributions of the teacher model and each assistant, and consider the assistant that achieves the minimum KL divergence as the best teaching assistant.

\paragraph{Spearman’s Footrule}
Spearman’s Footrule measures the absolute distance between two sorted lists, similar to edit distance. It is suitable for comparing the similarity between two permutations, with smaller values indicating more similar permutations. We calculate the Spearman’s Footrule distances between the teacher and each assistant, and consider the assistant that has the minimum distance with the teacher as the best.

\paragraph{Rank Biased Overlap}
Rank Biased Overlap (RBO) compares the overlap of two ranked lists at increasing depths. Unlike Spearman’s Footrule, it assigns different weights to different depths, with top-1 having the highest weight. The value of RBO ranges from 0 to 1, and larger values indicate more similar sorted lists. We calculate the RBO measures between the teacher and each assistant, and consider the assistant that has the maximum RBO value as the best assistant.

Please note that since this computation process is only for selecting the best assistant, it does not participate in the gradient backpropagation.

\subsubsection{The Student Model Optimization}
For each training batch, we first use the selection method described in Section~3.2.3 to select the best assistant model. Then, we use $L_{CL}$, $L_{KL}$ to optimize the student model which is also a dual-encoder:
\begin{equation}
L_{\mathrm{total}}=\alpha L_{CL}+\beta L_{KL}(\mathrm{tea},\mathrm{stu})+\gamma L_{KL}(\mathrm{ta},\mathrm{stu})
\label{eq:total}
\end{equation}
where $\alpha$, $\beta$, $\gamma$ are hyper-parameters, $L_{CL}$ is the contrastive loss of the student model (see more in Eq.~\eqref{eq:cl}). We also calculate the KL divergence $L_{KL}(\mathrm{ta},\mathrm{stu})$, $L_{KL}(\mathrm{tea},\mathrm{stu})$ as part of the loss during training, forcing the student to learn the score distributions of the best assistant and the teacher.

At the end of each iteration, we evaluate the student’s performance on the evaluation dataset, replace the worst-performing assistant with the student if it outperforms any of the existing assistants, and then regenerate the training/evaluation dataset. We repeat all the above operations, from generating datasets to optimizing the student model, until the training ends. The entire training process is introduced in Algorithm~1 in Appendix~\ref{app:algo}.

\section{Experiments and Analysis}
\subsection{Experimental Settings}
We conduct experiments on four retrieval datasets: MS MARCO passage, TREC DL 2019, TREC DL 2020 \cite{craswell2020trec2020,craswell2020trec2019} and Natural Questions (NQ) \cite{kwiatkowski2019nq} datasets. We use the averaged [CLS] representations of the student model’s last three layers to represent each query/passage, and dot product to compute the similarity between the query and passage. Following previous studies, we report MRR@10, Recall@50 and Recall@1k on MS MARCO dev set, and nDCG@10 on TREC DL 2019 and 2020; and we choose Recall@5, Recall@20 and Recall@100 as the evaluation metrics for Natural Questions.

\paragraph{Baselines}
To make a comprehensive comparison, we compare MTA4DPR with three groups of baselines: sparse retrieval models and dense retrieval models with/without knowledge distillation. Specifically, sparse retrieval models include BM25 \cite{robertson2009bm25}, DeepCT \cite{dai2019deepct}, GAR \cite{mao2021gar}, docT5query \cite{nogueira2019doc2query}, COIL-full \cite{gao2021coil}, UniCOIL \cite{lin2021few} and SPLADE-max \cite{formal2021splade}; dense retrieval models without knowledge distillation include DPR \cite{karpukhin2020dense}, ANCE \cite{xiong2020annncl}, Condenser \cite{gao2021condenser}, XTR-base \cite{lee2024xtr}, CotMAE \cite{wu2023cotmae}, GTR-XXL \cite{ni2022gtr} and RepLLaMA-7B \cite{ma2024repllama}; dense retrieval models with knowledge distillation include RocketQAv1 \cite{qu2021rocketqa}, PAIR \cite{ren2021pair}, RocketQAv2 \cite{ren2021rocketqav2}, ERNIE-Search \cite{lu2022erniesearch}, SimLM \cite{wang2023simlm}, RetroMAE \cite{xiao2022retromae}, LEAD \cite{sun2024lead}, CL-DRD \cite{zeng2022curriculum} and PROD \cite{lin2023prod}.

\paragraph{Model Initialization}
For MS MARCO, to balance the trade-off between efficiency and effectiveness, we choose dual-encoders as the assistants and the cross-encoder as the teacher. Specifically, we set CotMAE, SimLM-distilled, RetroMAE and M2DPR \cite{lu2024m2dpr} as assistants, since they are the most performant off-the-shelf dense retrievers to our knowledge. Their MRR@10 on MS MARCO dev set are 39.4, 41.1, 41.6 and 42.0, respectively. SimLM-reranker, a well performant cross-encoder, is considered as the teacher model with 43.7 MRR@10. Besides, to validate the effectiveness on NQ dataset, we simply use RocketQAv1 and PAIR as the assistants, and ERNIE-search as the teacher model with Recall@20 82.7, 83.5 and 85.3 on NQ test set. The student DPR models are initialized with the SimLM-base model.

\paragraph{Training Details}
For MS MARCO, we set the iterations to 3, as our experiments show that the performance improvement becomes marginal beyond the 3rd iteration. For each iteration, we use 1 Tesla A100 80G GPU to train our student model for 20,000 steps using AdamW optimizer with learning rate of $3\times 10^{-5}$. Each query in the training set has several positive passages and $k=100$ hard negatives. Each training batch has 64 queries, each of which has 1 positive passage and 34 hard negatives randomly sampled from the training set. The weight decay is set to 0.01. The max query length is 32, and the max passage length is 144. To balance each term of the final loss, $\alpha$, $\beta$ and $\gamma$ are set to 0.2, 1, 15. For NQ, we reuse the same settings as those on MS MARCO with a few exceptions. The training steps for each iteration is set to 10,000 steps, and the max passage length is 192.

\subsection{Main Results}
The results comparing MTA4DPR with multiple baselines on the MS MARCO, TREC DL 19 and 20 and NQ datasets are shown in Table~\ref{tab:main-msmarco} and Table~\ref{tab:main-nq}. From the tables, we can observe that the 66M student model trained by MTA4DPR achieves MRR@10 41.1 on MS MARCO, nDCG@10 71.2 on TREC DL 19, nDCG@10 71.1 on TREC DL 20 and Recall@20 83.6 on NQ, which outperforms most 66M distilled student models, and is competitive when compared with larger DPR models (the 110M ones), even with the LLM-based models.

In addition, we have the following observations:
\begin{enumerate}
\item RepLLaMA-7B achieves MRR@10 41.2 on MS MARCO, nDCG@10 74.3 and 72.1 on TREC DL 19 and 20, far surpassing most baselines without knowledge distillation, which means that, without knowledge distillation, the larger the model, the better the retrieval performance.
\item 110M DPR models trained with knowledge distillation, such as SimLM (MRR@10 41.1 on MS MARCO dev) and ERNIE-Search (Recall@20 85.3 on NQ test), can achieve better retrieval performance when compared with the models with the same or even much bigger sizes without knowledge distillation, from which we can see that knowledge distillation can effectively transfer knowledge from large teacher DPR models to small student models.
\item RepLLaMA-7B performs about nDCG@10 2.0 better than 66M DPR models on DL 20 which is mainly used to test models’ ability to capture fine-grained semantics. This implies that, in capturing fine-grained semantics, large DPR models are much better than small models, which motivates us to further optimize small models’ ability to capture fine-grained semantic.
\end{enumerate}

\begin{table}[t]
\centering
\small
\setlength{\tabcolsep}{4pt}
\begin{tabular}{l r r r r r}
\toprule
Model & #Params & MRR@10 & R@50 & R@1k & DL19 nDCG@10 \
\midrule
\multicolumn{6}{l}{Sparse Retrieval} \
BM25 & - & 18.7 & 59.2 & 85.7 & 49.7 \
DeepCT & 110M & 24.3 & 69.0 & 91.0 & 55.0 \
docT5query & - & 27.2 & 75.6 & 94.7 & 64.2 \
COIL-full & 110M & 35.5 & - & 96.3 & 70.4 \
UniCOIL & 110M & 35.2 & 80.7 & 95.8 & - \
SPLADE-max & 110M & 34.0 & - & 96.5 & 68.4 \
\midrule
\multicolumn{6}{l}{Dense Retrieval without KD} \
XTR-base & 110M & 37.4 & - & 98.0 & - \
CotMAE & 110M & 39.4 & 87.0 & 98.7 & - \
GTR-XXL & 4.8B & 38.8 & - & 99.0 & - \
RepLLaMA-7B & 7B & 41.2 & - & 99.4 & 74.3 \
\midrule
\multicolumn{6}{l}{Dense Retrieval with KD} \
RocketQAv2 & 110M & 38.8 & 86.2 & 98.1 & - \
SimLM & 110M & 41.1 & 87.8 & 98.7 & 71.4 \
RetroMAE & 110M & 41.6 & 88.6 & 98.8 & - \
LEAD & 66M & 37.8 & - & 97.4 & 70.4 \
CL-DRD & 66M & 38.2 & - & - & 72.5 \
PROD & 66M & 39.3 & 87.0 & 98.4 & 73.3 \
MTA4DPR & 66M & 41.1 & 88.4 & 98.7 & 71.2 \
\bottomrule
\end{tabular}

\vspace{4pt}
\small
\begin{tabular}{l r r}
\toprule
Model & DL20 nDCG@10 & Notes \
\midrule
BM25 & 48.7 & \
DeepCT & 55.6 & \
docT5query & 61.9 & \
COIL-full & - & \
UniCOIL & - & \
SPLADE-max & - & \
XTR-base & - & \
CotMAE & 70.4 & \
GTR-XXL & - & \
RepLLaMA-7B & 72.1 & \
RocketQAv2 & - & \
SimLM & 69.7 & \
RetroMAE & - & \
LEAD & 68.9 & \
CL-DRD & 68.7 & \
PROD & - & \
MTA4DPR & 71.1 & \
\bottomrule
\end{tabular}
\caption{Main results on MS MARCO and DL 19 and 20 datasets. The best scores are marked in bold, and the second places are underlined. `KD'' denotes knowledge distillation, and `#Params'' represents the number of model parameters. Please note that, by SimLM, we mean SimLM-distilled, not SimLM-reranker or SimLM-base.}
\label{tab:main-msmarco}
\end{table}

\begin{table}[t]
\centering
\small
\begin{tabular}{l r r r r}
\toprule
Model & #Params & R@5 & R@20 & R@100 \
\midrule
BM25 & - & - & 59.1 & 73.7 \
GAR & - & 60.9 & 74.4 & 85.3 \
DPR & 110M & - & 78.4 & 85.4 \
ANCE & 110M & - & 81.9 & 87.5 \
Condenser & 110M & - & 83.2 & 88.4 \
RocketQAv1 & 110M & 74.0 & 82.7 & 88.5 \
PAIR & 110M & 74.9 & 83.5 & 89.1 \
ERNIE-Search & 110M & 77.0 & 85.3 & 89.7 \
MTA4DPR & 66M & 74.5 & 83.6 & 88.3 \
\bottomrule
\end{tabular}
\caption{Main results on NQ. ``#Params'' represents the number of model parameters.}
\label{tab:main-nq}
\end{table}

\subsection{Ablation Study}
To validate the effectiveness of each module of our method, we conduct the ablation study. All ablation results come from 3-iteration training, except for ``w/o iterations'' in which we deliberately disabled the iteration to show its effectiveness.

The results in Table~\ref{tab:ablation} demonstrate the effectiveness of our model. We can see that removing any module will decrease the final performance, with the removal of the teaching assistants resulting in the most significant performance drop. Additionally, we also have the following observations.:
\begin{enumerate}
\item Without teaching assistants, the student model’s performance drops to MRR@10 39.9 on MS MARCO and Recall@20 82.2 on NQ, which indicates that using teaching assistants can help students better learn the knowledge from teacher/assistant models.
\item The performance also drops to MRR@10 40.8 on MS MARCO and Recall@20 83.4 on NQ without fusion strategy. Through further analysis, we find that the KL divergence between fused score distributions and the teacher’s score distribution tends to be smaller than that of original assistants, which means students can learn more useful information from fused assistants than the original assistants.
\item Finally, without training iterations, the performance of the student model drops to MRR@10 40.1 on MS MARCO and Recall@20 82.7 on NQ. This indicates that our iterative training method which enable students to learn from better teacher/assistants and more difficult data at each iteration improves the student’s performance.
\end{enumerate}

\begin{table}[t]
\centering
\small
\begin{tabular}{l r r r r r r}
\toprule
& \multicolumn{3}{c}{MS MARCO} & \multicolumn{3}{c}{NQ} \
\cmidrule(lr){2-4}\cmidrule(lr){5-7}
& MRR@10 & R@50 & R@1k & R@5 & R@20 & R@100 \
\midrule
MTA4DPR & 41.1 & 88.4 & 98.7 & 74.5 & 83.6 & 88.3 \
-w/o assistants & 39.9 & 86.8 & 98.5 & 71.4 & 82.2 & 87.3 \
-w/o fusion & 40.8 & 87.7 & 98.7 & 73.2 & 83.4 & 88.2 \
-w/o iterations & 40.1 & 87.1 & 98.6 & 71.9 & 82.7 & 87.5 \
\bottomrule
\end{tabular}
\caption{Ablation results on MS MARCO and NQ.}
\label{tab:ablation}
\end{table}

\subsection{Analysis}
We further analyze our proposed method from the following perspectives, i.e.\ the performance of the student model at each iteration, the assistant selection methods, student models’ scale, assistant models’ performance, the assistants selected, the complexity of the training process and the computational costs of the student models.

\subsubsection{Multi-iteration Retrieval Performance}
We report the retrieval performance of our 66M DPR model in each iteration, as shown in Table~\ref{tab:multiiter}. As expected, as the number of iterations increases, the performance also improves, from MRR@10 40.1 to 41.1 on MS MARCO and from Recall@20 82.7 to 83.6 on NQ. This indicates that to some extent, better assistant models combined with more difficult data will further improve the performance of the student model.

\begin{table}[t]
\centering
\small
\begin{tabular}{l r r r r r r}
\toprule
& \multicolumn{3}{c}{MS MARCO} & \multicolumn{3}{c}{NQ} \
\cmidrule(lr){2-4}\cmidrule(lr){5-7}
& MRR@10 & R@50 & R@1k & R@5 & R@20 & R@100 \
\midrule
1-th iteration & 40.1 & 87.1 & 98.6 & 71.9 & 82.7 & 87.5 \
2-th iteration & 40.7 & 87.9 & 98.7 & 73.1 & 83.3 & 88.0 \
3-th iteration & 41.1 & 88.4 & 98.7 & 74.5 & 83.6 & 88.3 \
\bottomrule
\end{tabular}
\caption{Multi-iteration Retrieval Performance on MS MARCO and NQ.}
\label{tab:multiiter}
\end{table}

\subsubsection{The impact of selection methods}
We compare multiple methods to select the best assistant, as described in Section~3.2.3. Table~\ref{tab:select} shows the results of MTA4DPR models using different selection methods. Compared with a random assistant, using KL, Spearman’s Footrule, and RBO selection methods can further improve retrieval performance, indicating that the teaching assistants selected by these three methods are more beneficial to the distillation process. Among these three methods, we chose KL selection method which obtains the best performance for the other experiments.

\begin{table}[t]
\centering
\small
\begin{tabular}{l r r r r r r}
\toprule
& \multicolumn{3}{c}{MS MARCO} & \multicolumn{3}{c}{NQ} \
\cmidrule(lr){2-4}\cmidrule(lr){5-7}
& MRR@10 & R@50 & R@1k & R@5 & R@20 & R@100 \
\midrule
Random & 40.5 & 87.6 & 98.6 & 72.8 & 82.4 & 87.5 \
SF & 40.8 & 87.7 & 98.7 & 74.3 & 83.1 & 87.9 \
RBO & 40.9 & 87.9 & 98.8 & 74.1 & 83.0 & 88.1 \
KL & 41.1 & 88.4 & 98.7 & 74.5 & 83.6 & 88.3 \
\bottomrule
\end{tabular}
\caption{Performance of MTA4DPR models with different selection methods on MS MARCO and NQ. ``SF'' denotes Spearman’s Footrule.}
\label{tab:select}
\end{table}

\subsubsection{The impact of the number of layers and the embedding sizes of student models}
We use the proposed method to distill student DPR models with different number of layers and embedding sizes. As shown in Table~\ref{tab:sizes}, we can see that:
\begin{enumerate}
\item MTA4DPR can improve the retrieval performance of the student models with different number of layers and embedding sizes; and as the number of layers and the number of embedding size increase, the performance improves.
\item It is worth noting that our 33M DPR model is almost equivalent to the existing 110M DPR models on Recall@1k on MS MARCO. Due to the fact that retrievers are often used in the first stage of retrieve-rerank pipeline in practical scenarios, a 33M DPR model can be used to reduce query time.
\item Finally, we also find that the 12-layer 384-dimensional models outperform the 3-layer 768-dimensional models, despite having fewer parameters. We speculate that this might be due to the 12-layer models’ ability to capture more complex text interactions owing to its greater depth. We will investigate this further in future work.
\end{enumerate}

\begin{table}[t]
\centering
\small
\begin{tabular}{r r r r r r}
\toprule
#Layers & #Emb & #Params & MRR@10 & R@50 & R@1k \
\midrule
6 & 384 & 17M & 36.0 (↑ 1.1) & 81.6 (↑ 1.3) & 96.3 (↑ 0.3) \
12 & 384 & 33M & 40.1 (↑ 0.8) & 87.2 (↑ 0.7) & 98.4 (↑ 0.0) \
3 & 768 & 45M & 39.4 (↑ 0.9) & 86.5 (↑ 1.1) & 98.4 (↑ 0.1) \
6 & 768 & 66M & 41.1 (↑ 1.2) & 88.4 (↑ 1.6) & 98.7 (↑ 0.2) \
12 & 768 & 110M & 41.8 (↑ 0.7) & 88.6 (↑ 0.8) & 98.8 (↑ 0.1) \
\bottomrule
\end{tabular}
\caption{Results of MTA4DPR models with different sizes on MS MARCO. `\#Layers'' denotes the number of layers of the model, and `#Emb'' denotes the embedding size of the model. `\#Params denotes the number of model parameters. `↑'' denotes the improvement compared with traditional knowledge distillation methods.}
\label{tab:sizes}
\end{table}

\subsubsection{The impact of the performance of assistant models}
We wonder how the performance of the assistants affects the distillation process. To this end, we conducted five groups of experiments, i.e.\ No assistant, Single-assistant distillation, Double-assistant distillation, Triple-assistant distillation and Quadruple-assistant distillation. No assistant involved distillation using only the teacher model without any assistants. Single-assistant distillation experiments are done using just one assistant and one teacher for distillation. Double-assistant distillation utilized one teacher and two assistants along with a fusion strategy for distillation, and so on.

The results are listed in Table~\ref{tab:assistcombo}. From the table, we have the following observations:
\begin{enumerate}
\item Compared to not using assistants, even the result of using the weakest assistant model is better than the no-assistant way. For example, using only CotMAE can increase the value of MRR@10 from 39.9 to 40.2 on MS MARCO dev set. This strongly proves the effectiveness of using assistant models.
\item R&M is better than other double-assistant combinations, S&R&M is better than other triple-assistant combinations. This implies that the better the performance of assistants, the better the performance of the distilled student model.
\end{enumerate}

\begin{table}[t]
\centering
\small
\setlength{\tabcolsep}{3pt}
\begin{tabular}{l l r r r r r}
\toprule
Method & Assistant Models & MRR@10 & R@50 & R@1k & DL19 nDCG@10 & DL20 nDCG@10 \
\midrule
No assistant & / & 39.9 & 86.8 & 98.5 & 69.2 & 67.7 \
\midrule
\multicolumn{7}{l}{Single-assistant distillation} \
& C & 40.2 & 87.3 & 98.5 & 69.8 & 68.1 \
& S & 40.4 & 87.3 & 98.5 & 70.0 & 68.9 \
& R & 40.6 & 87.7 & 98.7 & 70.0 & 69.7 \
& M & 40.6 & 87.6 & 98.8 & 70.2 & 69.3 \
\midrule
\multicolumn{7}{l}{Double-assistant distillation} \
& C&S & 40.4 & 87.3 & 98.7 & 69.6 & 68.6 \
& C&R & 40.6 & 87.3 & 98.7 & 69.6 & 69.7 \
& C&M & 40.5 & 87.6 & 98.7 & 70.0 & 69.9 \
& S&R & 40.7 & 87.3 & 98.7 & 69.2 & 69.3 \
& S&M & 40.6 & 87.7 & 98.7 & 70.1 & 69.0 \
& R&M & 40.8 & 87.8 & 98.8 & 70.3 & 69.9 \
\midrule
\multicolumn{7}{l}{Triple-assistant distillation} \
& C&S&R & 40.7 & 87.6 & 98.7 & 70.8 & 70.3 \
& C&S&M & 40.8 & 87.7 & 98.7 & 70.1 & 69.0 \
& C&R&M & 40.9 & 88.0 & 98.8 & 70.3 & 69.9 \
& S&R&M & 41.0 & 88.0 & 98.8 & 70.6 & 70.7 \
\midrule
Quadruple-assistant distillation & C&S&R&M & 41.1 & 88.4 & 98.7 & 71.2 & 71.1 \
\bottomrule
\end{tabular}
\caption{Results of distilled DPR models with different assistants combinations on MS MARCO dev set and DL 19 and 20 datasets. `C'', `S'', `R'' and `M'' represent CotMAE, SimLM, RetroMAE and M2DPR, respectively. ``C&S'' denotes the fusion result of CotMAE and SimLM.}
\label{tab:assistcombo}
\end{table}

\subsubsection{The composition of the best assistant}
We explore which assistant is selected as the best one in each batch during the whole training procedure. The composition of the best teaching assistants selected on MS MARCO is shown in Figure~\ref{fig:assistcomp}. From the figure, we can see that the fusion result of RetroMAE and M2DPR is chosen for nearly 50% of the time, which confirms once again the effectiveness of the fusion strategy.

\begin{figure}[t]
\centering
\fbox{\parbox{0.95\linewidth}{\centering IMAGE NOT PROVIDED}}
\caption{The composition of the best teaching assistants selected on MS MARCO. `R'' denotes RetroMAE, `S'' denotes SimLM, `M'' denotes M2DPR and `R&M'' denotes the fusion result of RetroMAE and M2DPR.}
\label{fig:assistcomp}
\end{figure}

\subsubsection{The complexity of the training process}
The time consumption of our method can be divided into two parts: model training and data construction. The time taken to train a 6-layer 768-dimensional student model is shown in Table~\ref{tab:complexity}. Since the teachers/assistants are not actually involved in the training process but only provide query-passage pair scores, which can be obtained during data construction, the training time of our method is only about 25 minutes longer than that of the traditional knowledge distillation, primarily due to the selection of the best teaching assistant for each batch. For the data construction, we require approximately 4.7 more hours compared to the traditional knowledge distillation method. The additional time is mainly spent on scoring unseen query-passage pairs using both the teacher and assistants models, which will be used for the next iteration. While time-consuming, this process provides us a more difficult dataset, which can further improve the performance of the student model.

\begin{table}[t]
\centering
\small
\begin{tabular}{l r r}
\toprule
Model & Training Time & Data Construction Time \
\midrule
MTA4DPR & 7.53 hours & 12.9 hours \
Traditional KD & 7.12 hours & 8.2 hours \
\bottomrule
\end{tabular}
\caption{The complexity of the training process.}
\label{tab:complexity}
\end{table}

\subsubsection{The computational costs of MTA4DPR}
We also conduct more experiments to further validate the efficiency and the computational costs of the student model distilled by our proposed method under three different settings, as shown in Table~\ref{tab:costs}. From the table, we can see that: reducing the embedding size is more efficient than reducing model layers in terms of the model size (decreased from 110M to 33M) and index size (decreased from 25.2G to 12.8G); while reducing model layers provide more improvement in terms of the model encoding time (decreased from 304.30s to 163.23s with the 512 batch size, and from 135.82s to 87.86s with the 1024 batch size).

\begin{table}[t]
\centering
\small
\begin{tabular}{r r l r r r}
\toprule
#Layers & #Emb & Index Size & #Params & Encoding Time (bs=512) & Encoding Time (bs=1024) \
\midrule
6 & 768 & 25.2G & 66M & 163.23s & 87.86s \
12 & 384 & 12.8G & 33M & 297.06s & 131.67s \
12 & 768 & 25.2G & 110M & 304.30s & 135.82s \
\bottomrule
\end{tabular}
\caption{The computational costs of student DPR models with different sizes. `Encoding Time'' is the time taken to encode the whole MS MARCO corpus. `#Emb'' denotes the embedding size of the model. Please note that this metric is pure GPU computation time and doesn’t include the time for data loading or other operations. ``bs'' denotes the batch size.}
\label{tab:costs}
\end{table}

\section{Conclusion}
In this paper, we propose MTA4DPR, an iterative multi-assistant distillation method for DPR. It distills the student with the help of the teaching assistants in an iterative manner, with each iteration creating more difficult datasets and more performant assistants. The experimental results on MS MARCO, TREC DL 2019 and 2020 and Natural Questions show the effectiveness of our method. Our 66M DPR model can achieve the state-of-the-art performance among models with same parameters on multiple datasets and is very competitive when compared with larger, even LLM-based, DPR models. MTA4DPR confirms that the iterative distillation with multiple assistants can improve the distillation performance. Since it is orthogonal to existing distillation methods, other distillation pipelines can be combined with MTA4DPR to further improve their performance.

In addition, MTA4DPR is not constrained by model structures and tasks, and can be broadly applicable other fields than DPR, including text classification, question answering and text summarization, etc.

\section*{Limitations}
We consider the following four points as the limitations of this work:

First, due to flexibility and scalability considerations, we only distill the score distributions provided by teacher/assistants, while ignoring information provided by intermediate layers of teacher/assistant models which can be beneficial to further improve the student models’ performance.

Second, at the first training iteration, our method requires multiple off-the-shelf DPR models, but when there are not enough available models, we need to train teacher/assistant DPR models from scratch, which may increase the training costs.

Third, for the sake of the training phase’s simplicity and efficiency, we only use heuristic strategies when generating fused scores and selecting the best teaching assistant. To further improve student performance, we can design more complex and effective generation and selection methods.

Finally, in the future, we can continue to explore the impact of the number and performance of teaching assistants on the final retrieval result of student models, and find out how to determine what kind of teaching assistant is good.

\section*{Acknowledgements}
This work is supported by National Natural Science Foundation of China (NSFC) (No. 62076038).

\section*{Ethics Statement}
The MTA4DPR method mainly aims at retrieving the most relevant passages for a given query in an effective and efficient manner. And the experiments are based on the MS MARCO, TREC DL 2019 and 2020 and Natural Questions datasets, which is unlikely to include harmful content.

\section*{Licenses}
All SimLM models are under license MIT. RetroMAE is under license artistic-2.0. CotMAE and M2DPR is not under any licenses. MS MARCO passage dataset, TREC DL 2019 and 2020 and Natural Questions datasets also don’t extend any license and allows for academic usage.

\section*{References}
\begin{thebibliography}{99}

\bibitem{adriana2015fitnets}
Romero Adriana, Ballas Nicolas, K Samira Ebrahimi, Chassang Antoine, Gatta Carlo, and Bengio Yoshua.
\newblock 2015.
\newblock Fitnets: Hints for thin deep nets.
\newblock Proc.\ ICLR, 2(3):1.

\bibitem{bengio2009curriculum}
Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston.
\newblock 2009.
\newblock Curriculum learning.
\newblock In Proceedings of the 26th annual international conference on machine learning, pages 41--48.

\bibitem{beyer2022patient}
Lucas Beyer, Xiaohua Zhai, Amélie Royer, Larisa Markeeva, Rohan Anil, and Alexander Kolesnikov.
\newblock 2022.
\newblock Knowledge distillation: A good teacher is patient and consistent.
\newblock In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10925--10934.

\bibitem{chen2018darkrank}
Yuntao Chen, Naiyan Wang, and Zhaoxiang Zhang.
\newblock 2018.
\newblock Darkrank: Accelerating deep metric learning via cross sample similarities transfer.
\newblock In Proceedings of the AAAI conference on artificial intelligence, volume 32.

\bibitem{cormack2009rrf}
Gordon V Cormack, Charles LA Clarke, and Stefan Buettcher.
\newblock 2009.
\newblock Reciprocal rank fusion outperforms condorcet and individual rank learning methods.
\newblock In Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval, pages 758--759.

\bibitem{craswell2020trec2020}
Nick Craswell, Bhaskar Mitra, Emine Yilmaz, and Daniel Campos.
\newblock 2020a.
\newblock Overview of the trec 2020 deep learning track.
\newblock Text REtrieval Conference, Text REtrieval Conference.

\bibitem{craswell2020trec2019}
Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, and Ellen M Voorhees.
\newblock 2020b.
\newblock Overview of the trec 2019 deep learning track.
\newblock arXiv preprint arXiv:2003.07820.

\bibitem{dai2019deepct}
Zhuyun Dai and Jamie Callan.
\newblock 2019.
\newblock Deeper text understanding for ir with contextual neural language modeling.
\newblock In Proceedings of the 42nd international ACM SIGIR conference on research and development in information retrieval, pages 985--988.

\bibitem{formal2021splade}
Thibault Formal, Benjamin Piwowarski, and Stéphane Clinchant.
\newblock 2021.
\newblock Splade: Sparse lexical and expansion model for first stage ranking.
\newblock In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 2288--2292.

\bibitem{gao2021condenser}
Luyu Gao and Jamie Callan.
\newblock 2021a.
\newblock Condenser: a pretraining architecture for dense retrieval.
\newblock In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 981--993.

\bibitem{gao2021ready}
Luyu Gao and Jamie Callan.
\newblock 2021b.
\newblock Is your language model ready for dense representation fine-tuning?
\newblock CoRR, abs/2104.08253.

\bibitem{gao2021coil}
Luyu Gao, Zhuyun Dai, and Jamie Callan.
\newblock 2021.
\newblock Coil: Revisit exact lexical match in information retrieval with contextualized inverted list.
\newblock In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3030--3042.

\bibitem{heo2019activation}
Byeongho Heo, Minsik Lee, Sangdoo Yun, and Jin Young Choi.
\newblock 2019.
\newblock Knowledge transfer via distillation of activation boundaries formed by hidden neurons.
\newblock In Proceedings of the AAAI conference on artificial intelligence, volume 33, pages 3779--3787.

\bibitem{hinton2015distill}
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.
\newblock 2015.
\newblock Distilling the knowledge in a neural network.
\newblock arXiv preprint arXiv:1503.02531.

\bibitem{huang2022stronger}
Tao Huang, Shan You, Fei Wang, Chen Qian, and Chang Xu.
\newblock 2022.
\newblock Knowledge distillation from a stronger teacher.
\newblock Advances in Neural Information Processing Systems, 35:33716--33727.

\bibitem{karpukhin2020dense}
Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih.
\newblock 2020.
\newblock Dense passage retrieval for open-domain question answering.
\newblock In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6769--6781.

\bibitem{kenton2019bert}
Jacob Devlin, Ming-Wei Chang, Kenton and Lee Kristina Toutanova.
\newblock 2019.
\newblock Bert: Pre-training of deep bidirectional transformers for language understanding.
\newblock In Proceedings of naacL-HLT, volume 1, page 2. Minneapolis, Minnesota.

\bibitem{kwiatkowski2019nq}
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov.
\newblock 2019.
\newblock Natural questions: A benchmark for question answering research.
\newblock Transactions of the Association for Computational Linguistics, 7:452--466.

\bibitem{lee2024xtr}
Jinhyuk Lee, Zhuyun Dai, Sai Meher Karthik Duddu, Tao Lei, Iftekhar Naim, Ming-Wei Chang, and Vincent Zhao.
\newblock 2024.
\newblock Rethinking the role of token retrieval in multi-vector retrieval.
\newblock Advances in Neural Information Processing Systems, 36.

\bibitem{lin2021few}
Jimmy Lin and Xueguang Ma.
\newblock 2021.
\newblock A few brief notes on deepimpact, coil, and a conceptual framework for information retrieval techniques.
\newblock arXiv preprint arXiv:2106.14807.

\bibitem{lin2023prod}
Zhenghao Lin, Yeyun Gong, Xiao Liu, Hang Zhang, Chen Lin, Anlei Dong, Jian Jiao, Jingwen Lu, Daxin Jiang, Rangan Majumder, et al.
\newblock 2023.
\newblock Prod: Progressive distillation for dense retrieval.
\newblock In Proceedings of the ACM Web Conference 2023, pages 3299--3308.

\bibitem{lu2024m2dpr}
Qixi Lu.
\newblock 2024.
\newblock M2DPR: A multi-task multi-view representation learning framework for dense passage retrieval.
\newblock In NAACL Student Research Workshop 2024.

\bibitem{lu2022erniesearch}
Yuxiang Lu, Yiding Liu, Jiaxiang Liu, Yunsheng Shi, Zhengjie Huang, Shikun Feng Yu Sun, Hao Tian, Hua Wu, Shuaiqiang Wang, Dawei Yin, et al.
\newblock 2022.
\newblock Ernie-search: Bridging cross-encoder with dual-encoder via self on-the-fly distillation for dense passage retrieval.
\newblock arXiv preprint arXiv:2205.09153.

\bibitem{ma2024repllama}
Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin.
\newblock 2024.
\newblock Fine-tuning llama for multi-stage text retrieval.
\newblock In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 2421--2425.

\bibitem{mao2021gar}
Yuning Mao, Pengcheng He, Xiaodong Liu, Yelong Shen, Jianfeng Gao, Jiawei Han, and Weizhu Chen.
\newblock 2021.
\newblock Generation-augmented retrieval for open-domain question answering.
\newblock In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4089--4100.

\bibitem{mienye2020ensemble}
Ibomoiye Domor Mienye, Yanxia Sun, and Zenghui Wang.
\newblock 2020.
\newblock Improved predictive sparse decomposition method with densenet for prediction of lung cancer.
\newblock Int.\ J.\ Comput, 1:533--541.

\bibitem{mirzadeh2020takd}
Seyed Iman Mirzadeh, Mehrdad Farajtabar, Ang Li, Nir Levine, Akihiro Matsukawa, and Hassan Ghasemzadeh.
\newblock 2020.
\newblock Improved knowledge distillation via teacher assistant.
\newblock In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 5191--5198.

\bibitem{ni2022gtr}
Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernandez Abrego, Ji Ma, Vincent Zhao, Yi Luan, Keith Hall, Ming-Wei Chang, et al.
\newblock 2022.
\newblock Large dual encoders are generalizable retrievers.
\newblock In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 9844--9855.

\bibitem{nogueira2019doc2query}
Rodrigo Nogueira, Jimmy Lin, and AI Epistemic.
\newblock 2019.
\newblock From doc2query to doctttttquery.
\newblock Online preprint, 6:2.

\bibitem{peng2019correlation}
Baoyun Peng, Xiao Jin, Jiaheng Liu, Dongsheng Li, Yichao Wu, Yu Liu, Shunfeng Zhou, and Zhaoning Zhang.
\newblock 2019.
\newblock Correlation congruence for knowledge distillation.
\newblock In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 5007--5016.

\bibitem{qin2024llmrankers}
Zhen Qin, Rolf Jagerman, Kai Hui, Honglei Zhuang, Junru Wu, Le Yan, Jiaming Shen, Tianqi Liu, Jialu Liu, Donald Metzler, et al.
\newblock 2024.
\newblock Large language models are effective text rankers with pairwise ranking prompting.
\newblock In Findings of the Association for Computational Linguistics: NAACL 2024, pages 1504--1518.

\bibitem{qu2021rocketqa}
Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Wayne Xin Zhao, Daxiang Dong, Hua Wu, and Haifeng Wang.
\newblock 2021.
\newblock Rocketqa: An optimized training approach to dense passage retrieval for open-domain question answering.
\newblock In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5835--5847.

\bibitem{ren2021pair}
Ruiyang Ren, Shangwen Lv, Yingqi Qu, Jing Liu, Wayne Xin Zhao, Qiaoqiao She, Hua Wu, Haifeng Wang, and Ji-Rong Wen.
\newblock 2021a.
\newblock Pair: Leveraging passage-centric similarity relation for improving dense passage retrieval.
\newblock In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 2173--2183.

\bibitem{ren2021rocketqav2}
Ruiyang Ren, Yingqi Qu, Jing Liu, Wayne Xin Zhao, Qiaoqiao She, Hua Wu, Haifeng Wang, and Ji-Rong Wen.
\newblock 2021b.
\newblock Rocketqav2: A joint training method for dense passage retrieval and passage re-ranking.
\newblock In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2825--2835.

\bibitem{robertson2009bm25}
Stephen Robertson, Hugo Zaragoza, et al.
\newblock 2009.
\newblock The probabilistic relevance framework: Bm25 and beyond.
\newblock Foundations and Trends\textsuperscript{\tiny\textregistered} in Information Retrieval, 3(4):333--389.

\bibitem{son2021denselyguided}
Wonchul Son, Jaemin Na, Junyong Choi, and Wonjun Hwang.
\newblock 2021.
\newblock Densely guided knowledge distillation using multiple teacher assistants.
\newblock In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9395--9404.

\bibitem{sun2024lead}
Hao Sun, Xiao Liu, Yeyun Gong, Anlei Dong, Jingwen Lu, Yan Zhang, Linjun Yang, Rangan Majumder, and Nan Duan.
\newblock 2024.
\newblock Lead: liberal feature-based distillation for dense retrieval.
\newblock In Proceedings of the 17th ACM International Conference on Web Search and Data Mining, pages 655--664.

\bibitem{wang2023simlm}
Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei.
\newblock 2023.
\newblock SimLM: Pre-training with representation bottleneck for dense passage retrieval.
\newblock In Proceedings of the 61th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2244--2258, Toronto, Canada. Association for Computational Linguistics.

\bibitem{wu2021oneteacher}
Chuhan Wu, Fangzhao Wu, and Yongfeng Huang.
\newblock 2021.
\newblock One teacher is enough? pre-trained language model distillation from multiple teachers.
\newblock In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4408--4413.

\bibitem{wu2023cotmae}
Xing Wu, Guangyuan Ma, Meng Lin, Zijia Lin, Zhongyuan Wang, and Songlin Hu.
\newblock 2023.
\newblock Contextual masked auto-encoder for dense passage retrieval.
\newblock In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 4738--4746.

\bibitem{xiao2022retromae}
Shitao Xiao, Zheng Liu, Yingxia Shao, and Zhao Cao.
\newblock 2022.
\newblock Retromae: Pre-training retrieval-oriented language models via masked auto-encoder.
\newblock In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 538--548.

\bibitem{xiong2020annncl}
Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul N Bennett, Junaid Ahmed, and Arnold Overwijk.
\newblock [ILLEGIBLE].
\newblock Approximate nearest neighbor negative contrastive learning for dense text retrieval.
\newblock In International Conference on Learning Representations.

\bibitem{yang2022crossimage}
Chuanguang Yang, Helong Zhou, Zhulin An, Xue Jiang, Yongjun Xu, and Qian Zhang.
\newblock 2022.
\newblock Cross-image relational knowledge distillation for semantic segmentation.
\newblock In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12319--12328.

\bibitem{yuan2021reinforced}
Fei Yuan, Linjun Shou, Jian Pei, Wutao Lin, Ming Gong, Yan Fu, and Daxin Jiang.
\newblock 2021.
\newblock Reinforced multi-teacher selection for knowledge distillation.
\newblock In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 14284--14291.

\bibitem{zeng2022curriculum}
Hansi Zeng, Hamed Zamani, and Vishwa Vinay.
\newblock 2022.
\newblock Curriculum learning for dense retrieval distillation.
\newblock In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 1979--1983.

\end{thebibliography}

\appendix
\section{Algorithm 1}
\label{app:algo}

\begin{figure}[t]
\centering
\fbox{\begin{minipage}{0.95\linewidth}
\textbf{Algorithm 1 MTA4DPR Training Process}

\medskip
\textbf{Input:} $T$: the teacher model; $TA$: the assistant models; $M_{\theta}$: the student model; $Q$: the query set; $P$: the passage set; max_iter: maximum number of training iterations; max_steps: maximum number of training steps; $\eta$: Learning rate;

\textbf{Output:} $M_{\theta}$

\medskip
\begin{tabbing}
\hspace{1.5em}=\hspace{2.5em}=\kill
1:> $i \leftarrow 0$ \
2:> \textbf{while} $i < $ max_iter \textbf{do} \
3:>> $D_{\mathrm{train}}, D_{\mathrm{eval}} \leftarrow \mathrm{GenDataset}(T, TA, Q, P)$ \
4:>> \textbf{repeat} \
5:>>> $id^{best}*{TA} \leftarrow \mathrm{TASelect}(D*{\mathrm{train}})$ \
6:>>> $\theta \leftarrow \theta - \eta \nabla_{\theta} L_{\mathrm{total}}(D_{\mathrm{train}}, M_{\theta}, id^{best}*{TA})$ \
7:>> \textbf{until} max_steps reached \
8:>> $outperformed_{TA} \leftarrow \mathrm{Compare}(M*{\theta}, TA, D_{\mathrm{eval}})$ \
9:>> \textbf{if} $outperformed_{TA}$ \textbf{then} \
10:>>> remove $Worst(TA)$ \
11:>>> add $M_{\theta}$ into $TA$ \
12:>> \textbf{end if} \
13:>> $i \leftarrow i + 1$ \
14:> \textbf{end while}
\end{tabbing}
\end{minipage}}
\caption{Algorithm 1: MTA4DPR Training Process}
\end{figure}

\end{document}
=====END FILE=====

=====FILE: figures/README.txt=====
This LaTeX project was generated from a PDF. Images are not included.
All figures have been replaced with placeholders reading: IMAGE NOT PROVIDED.
=====END FILE=====
