\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Contemporary NLP Paradigm with language tasks formalized as datasets for which models produce predictions. Recent LLMs break down this compartmentalization (dashed lines), impacting all stages of the cycle. We argue that establishing trust requires rethinking every facet of this framework, as formalization and evaluation become increasingly difficult.}}{3}{figure.1}\protected@file@percent }
\newlabel{fig:contemporary-paradigm}{{1}{3}{Contemporary NLP Paradigm with language tasks formalized as datasets for which models produce predictions. Recent LLMs break down this compartmentalization (dashed lines), impacting all stages of the cycle. We argue that establishing trust requires rethinking every facet of this framework, as formalization and evaluation become increasingly difficult}{figure.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Desiderata for Trustworthy LLMs}{3}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{D1. Knowledge about Model Input.}{3}{section*.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{D2. Knowledge about Model Behaviour.}{4}{section*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{D3. Knowledge of Evaluation Protocols.}{4}{section*.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{D4. Knowledge of Data Origin.}{4}{section*.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}What Can We Do to Gain Trust Now and in Future?}{5}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Explain Skills Required versus Skills Employed}{5}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Facilitate Representative and Comparable Qualitative Analysis}{6}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Be Explicit about Data Provenance}{6}{subsection.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Trustworthiness and User Trust}{7}{section.4}\protected@file@percent }
\bibcite{adi2017fine}{1}
\bibcite{amini2023probing}{2}
\bibcite{ansell2022composable}{3}
\bibcite{atanasova2023faithfulness}{4}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusions}{8}{section.5}\protected@file@percent }
\bibcite{aytekin2022neural}{5}
\bibcite{bach2022systematic}{6}
\bibcite{baum2017two}{7}
\bibcite{bowman2022dangers}{8}
\bibcite{bubeck2023sparks}{9}
\bibcite{chiesurin2023dangers}{10}
\bibcite{choenni2023languages}{11}
\bibcite{chung2022scaling}{12}
\bibcite{clarke2023hmc}{13}
\bibcite{conneau2020unsupervised}{14}
\bibcite{conneau2018what}{15}
\bibcite{debruyn202220q}{16}
\bibcite{decao2021editing}{17}
\bibcite{dettmers2023qlora}{18}
\bibcite{dziri2022origin}{19}
\bibcite{eisenstein2022informativeness}{20}
\bibcite{ethayarajh2022understanding}{21}
\bibcite{french1999catastrophic}{22}
\bibcite{frosst2017distilling}{23}
\bibcite{gira2022debiasing}{24}
\bibcite{glasmachers2017limits}{25}
\bibcite{gonzalez2021explanations}{26}
\bibcite{hays1979applications}{27}
\bibcite{hedderich2022label}{28}
\bibcite{ilharco2023editing}{29}
\bibcite{lauscher2022socioprobe}{30}
\bibcite{li2022calibration}{31}
\bibcite{li2022commonsense}{32}
\bibcite{li2023chain}{33}
\bibcite{liang2022holistic}{34}
\bibcite{mallen2023when}{35}
\bibcite{manning2014stanford}{36}
\bibcite{mccloskey1989catastrophic}{37}
\bibcite{mccoy2019right}{38}
\bibcite{mialon2023augmented}{39}
\bibcite{miaschi2020linguistic}{40}
\bibcite{miaschi2021what}{41}
\bibcite{mikolov2013linguistic}{42}
\bibcite{openai2023gpt4}{43}
\bibcite{ouyang2022training}{44}
\bibcite{paranjape2023art}{45}
\bibcite{peskoff2023credible}{46}
\bibcite{piktus2023roots}{47}
\bibcite{ponti2023combining}{48}
\bibcite{popper1934logik}{49}
\bibcite{pruthi2020estimating}{50}
\bibcite{raffel2020exploring}{51}
\bibcite{raji2021ai}{52}
\bibcite{ribeiro2020beyond}{53}
\bibcite{rodriguez2021evaluation}{54}
\bibcite{rogers2023closed}{55}
\bibcite{ruffinelli2020teach}{56}
\bibcite{sanh2022multitask}{57}
\bibcite{sarti2021looks}{58}
\bibcite{scao2022bloom}{59}
\bibcite{schick2023toolformer}{60}
\bibcite{schlangen2021targeting}{61}
\bibcite{schramowski2020making}{62}
\bibcite{shen2023hugginggpt}{63}
\bibcite{shumailov2023curse}{64}
\bibcite{smithrenner2020no}{65}
\bibcite{swayamdipta2020dataset}{66}
\bibcite{taori2023alpaca}{67}
\bibcite{tedeschi2023meaning}{68}
\bibcite{touvron2023llama_a}{69}
\bibcite{touvron2023llama_b}{70}
\bibcite{veselovsky2023artificial}{71}
\bibcite{vulic2023probing}{72}
\bibcite{vulic2020probing}{73}
\bibcite{wang2018glue}{74}
\bibcite{wang2022finding}{75}
\bibcite{wang2023selfconsistency}{76}
\bibcite{wang2023camels}{77}
\bibcite{wang2022supernatural}{78}
\bibcite{wei2022finetuned}{79}
\bibcite{wei2022emergent}{80}
\bibcite{wei2022cot}{81}
\bibcite{weller2023according}{82}
\gdef \@abspage@last{16}
