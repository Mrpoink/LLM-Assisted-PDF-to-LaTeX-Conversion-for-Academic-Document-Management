\documentclass[conference]{IEEEtran}
\usepackage{amsmath, amssymb, graphicx, algorithm, algorithmic, booktabs, cite}

\title{%%%PLACEHOLDER: TITLE_0001%%%}
\author{
\IEEEauthorblockN{%%%PLACEHOLDER: AUTHOR_0001%%%
\and
%%%PLACEHOLDER: AUTHOR_0002%%%
\and
%%%PLACEHOLDER: AUTHOR_0003%%%
\and
%%%PLACEHOLDER: AUTHOR_0004%%%}
\IEEEauthorblockA{Department of Data Science \& AI, Monash University\\
Email: firstname.lastname@monash.edu}
}

\begin{document}
\maketitle

\begin{abstract}
Large Multimodal Models (LMMs) have
achieved great success recently, demonstrating
a strong capability to understand multimodal
information and to interact with human users.
Despite the progress made, the challenge of
detecting high-risk interactions in multimodal
settings, and in particular in speech modal-
ity, remains largely unexplored. Conventional
research on risk for speech modality primar-
ily emphasises the content (e.g., what is cap-
tured as transcription). However, in speech-
based interactions, paralinguistic cues in audio
can significantly alter the intended meaning
behind utterances. In this work, we propose
a speech-specific risk taxonomy, covering 8
risk categories under hostility (malicious sar-
casm and threats), malicious imitation (age,
gender, ethnicity), and stereotypical biases (age,
gender, ethnicity). Based on the taxonomy,
we create a small-scale dataset for evaluating
current LMMs capability in detecting these
categories of risk. We observe even the lat-
est models remain ineffective to detect vari-
ous paralinguistic-specific risks in speech (e.g.,
Gemini 1.5 Pro is performing only slightly
above random baseline).1 Warning: this pa-
per contains biased and offensive examples.
\end{abstract}


\section{Introduction}
Large language models (LLMs) (Touvron et al.,
2023a; Chiang et al., 2023; Anil et al., 2023) have
showcased superior ability to in-context learning
and robust zero-shot performance across various
downstream natural language tasks (Xie et al.,
2021; Brown et al., 2020; Wei et al., 2022). Build-
ing on the foundation established by LLMs, Large
Multimodal Models (LMMs) (Chu et al., 2023;
Reid et al., 2024; Tang et al., 2024; Hu et al.,
2024) equipped with multimodal encoders extend
the scope beyond mere text, and facilitate interac-
tions centred on visual and auditory inputs. This
evolution marks a significant leap towards more
comprehensive and versatile AI systems.

Although LMMs show the capability to pro-
cess and interact in a wide-range of multimodal
forms, they still embody several challenges asso-
ciated with safety and risks. Investigating these
potential issues in LMMs requires both a modality-
specific definition of risk, and suitable benchmarks.
While there is a dedicated body of work in the
text domain to probe various aspects of LLMs
beyond downstream performance, such categori-
cal investigations are missing for other modalities
such as speech. For instance, existing risk detec-
tion protocols for speech modality (Yousefi and
Emmanouilidou, 2021; Rana and Jha, 2022; Nada
et al., 2023; Reid et al., 2022; Ghosh et al., 2021)
only focus on the content aspect (i.e., what could be
captured by speech transcription), and neglect risks
induced by paralinguistic cues, the unique feature
of speech. To highlight this further, consider how
various interpretations of the transcript ``I feel so
good'' arises depending on the utterance form (e.g.,
varying tones, and emotions such as angry, sad, de-
pressed, or imitation of a specific gender, age or
ethnicity) in audio speech.

In this work, we move towards addressing this
gap for speech modality by introducing a proto-
col to evaluate the capability of LMMs in detect-
ing the risks induced specifically by paralinguis-
tic cues. To our knowledge, our work is the first
to explore the risk awareness at the paralinguistic
level. We propose a speech taxonomy, covering
3 main categories: hostility, malicious imitation,
and stereotypical biases, and further expand them
into 8 corresponding sub-categories, which em-
phasise the implicit and subtle risks induced by
paralinguistic cues in speech. Figure 1 provides
a high-level overview of risk categories consid-
ered in this work (§3). We then manually create a
high-quality set of seed transcriptions for 4 of the
sub-categories (hostile-sarcasm, and gender, age,
ethnicity stereotypical biases; 10-15 examples per
each sub-category). The seed set has been con-
trolled to not leak the category of risk through the
transcript alone. The seed sets are then expanded
further by leveraging GPT-4. All samples (262
samples) were further filtered by three human an-
notators to maintain quality, resulting in 180 final
transcriptions. Three human annotators are all from
our co-authors of this paper (2 faculty members and
1 PhD student, all with expertises in NLP). The
annotators work independently and do not have
access to each other’s annotation results during
the process to avoid undesired biases. To convert
these transcripts into audio, we used advanced text-
to-speech (TTS) systems, Audiobox (Vyas et al.,
2023) and Google TTS,2
to generate various syn-
thetic speeches with paralinguistic cues, resulting
in 1,800 speech instances.

In experiments, we evaluate 5 most recent
speech-supported LMMs, Qwen-Audio-Chat (Chu
et al., 2023), SALMONN-7B/13B (Tang et al.,
2024), WavLLM (Hu et al., 2024), and Gemini-1.5-
Pro (Reid et al., 2024), under various prompting
strategies. Notably, Gemini 1.5 Pro performs very
similar to random baseline (50\%), while WavLLM
performs worse that random guessing. Among the
other two models, Qwen-Audio-Chat has a more
stable success pattern under various prompting
strategies, while SALMONN-7/13B do the best un-
der certain prompting configurations. We attribute
these differences in performance to different selec-
tion and adaptation of audio encoders. Among the
risk categories, the one that seems the most diffi-
cult is Age Stereotypical Bias where even the best
configuration’s result is only slightly above random
baseline (54\%). For Gender and Ethnicity Stereo-
typical Biases the best result gets above 60\%, and
for Malicious Sarcasm it goes further into (70\%).

To the best of our knowledge our paper presents
the first speech-specific risk taxonomy, focused ex-
clusively on risks associated with paralinguistic
aspects of audio. We hope our taxonomy, bench-
mark, and evaluation protocol to encourage further
investigation of risk in speech modality, and guide
LMM developers towards more holistic evaluation
and safeguarding across modalities.


\section{Related Work}
The research on LLMs has shown increased focus
on safety and responsibility, leading to significant
advancements in benchmarking these models’ abil-
ity to handle and respond to harmful content in
text modality. Notable contributions in this area
include the three-level hierarchical risk taxonomy
introduced by Do-Not-Answer (Wang et al., 2023),
which created a dataset containing 939 prompts that
model should not respond to. SafetyBench (Zhang
et al., 2023b) explored 7 distinct safety categories
across the multiple choice questions, while CVal-
ues (Xu et al., 2023) established the first Chinese
safety benchmark for evaluating the capability of
LLMs. Goat-bench (Khanna et al., 2024) evaluated
LMMs in detecting implicit social abuse in memes.
Although many research efforts focus on mitigating
the generation of harmful content, OR-Bench (Cui
et al., 2024) presented 10 common rejection cat-
egories including 8k seemingly toxic prompts to
benchmark the over-refusal of LLMs.

On conventional toxic speech detection task, the
research has mostly focused on the content aspect.
DeToxy-B (Ghosh et al., 2021) is proposed as a
large-scale dataset for speech toxicity classifica-
tion. Rana and Jha (2022) combined emotion by
using multimodal learning to detect hate speech,
and Reid et al. (2022) presented sensing toxic-
ity from in-game communications. While content-
focused line of research was relevant for a while,
the transcription generated by the recent highly
capable Automatic Speech Recognition (ASR) sys-
tems such as Whisper (Radford et al., 2023) could
merge this line of research into text-based safety re-
search (e.g., through a cascaded design of ASR and
LLM). However, this type of cascaded approach
also excludes the paralinguistic cues in audio as the
focus remains on the transcription of ASR.

While early works in Speech-based LLMs
shown minimal real progress in speech understand-
ing (Su et al., 2023; Zhang et al., 2023a; Zhao et al.,
2023), recent works through alignment of represen-
tation spaces between speech encoder’s output and
text-based LLM’s input (either with full end-to-end
training, or partial training of adaptors) have shown
promising progress (Chu et al., 2023; Reid et al.,
2024; Tang et al., 2024; Hu et al., 2024). These
models, now matured enough, exhibit high compe-
tence in understanding speech (Lin et al., 2024a,b;
Ma et al., 2023; Xue et al., 2023). Building on this
context, our research aims to evaluate the capability
of LMMs to detect risks initiated by paralinguis-
tic cues, addressing a critical gap in the current
understanding of speech-specific risks.


\section{Speech-Specific Risk Taxonomy}
Our speech taxonomy is as shown in Figure 1.
To delineate the risks associated with paralinguis-
tic cues, we establish 3 primary categories of
risk speech. In contrast to conventional risk con-
cerns centred on the speech content, we empha-
sise the significance of paralinguistic cues, in-
cluding tone, emotion, and speaker information.
Subsequently, we identify 8 corresponding sub-
categories in which ostensibly low-risk speech con-
tent may be transformed into delivery, manifested
in an implicit and subtle manner, due to the influ-
ence of corresponding paralinguistic cues.

\subsection{Hostility}
This category includes risks covering malicious
sarcasm and threats. Hostility in communication
typically conveys aggression, disparagement, and
the intent to harm, significantly increasing psycho-
logical pressure and violating principles of respect
and politeness. Emotion and tone serve as paralin-
guistic cues that induce hostility, transforming os-
tensibly low-risk content into risky speech, altering
the perceived intent of the words spoken.

\subsubsection{Malicious Sarcasm}
We distinguish risky sar-
casm and jokes based on the scenarios and the
deliveries. Our considered sarcasm often arises in
workplace and teamwork, where speakers express
strong anger and mockery. In these scenarios, sar-
casm is perceived as particularly aggressive and can
have detrimental effects on mental health, leading
to stress and anxiety among colleagues (Colston,
1997; Toplak and Katz, 2000; Katz et al., 2004;
Zhu and Wang, 2020).

\subsubsection{Threats}
They represent a severe form of aggres-
sive communication. In our definition, it is implic-
itly delivered by the speaker’s emotion and tone,
which creates a fear atmosphere and conveys im-
plication to harm. The presence of threats within
communication significantly harms the psycholog-
ical health of others, and often escalate conflicts,
leading to toxic environment.

\subsection{Malicious Imitation}
This category encompasses risky communication
that involve the deliberate mimicry of voice charac-
teristics associated with gender, age, and ethnicity.
Such imitations, in the form of ridiculing and of-
fending, aim to propagate and reinforce stereotypes,
discrimination, or bias, leading to undermining the
dignity of individuals and psychological trauma.
The paralinguistic cues here are the comparison
between the speaker’s original voice and the exag-
gerated change of voice characteristics.

\subsubsection{Gender}
Gender-based imitation possibly involves
exaggerating the feminine voice coupled with im-
plicit stereotypes, aiming to demean and undermine
the female group.

\subsubsection{Age}
Age-based imitation often targets the elderly.
The imitative voice coupled with specific content
depict them as a weak and old-fashioned group who
is out of touch, which can reinforce stereotypes and
exacerbate ageist.

\subsubsection{Ethnicity}
Ethnicity-based imitation targets ac-
cents of groups with different cultural background.
This form of imitation often perpetuates racial and
ethnic stereotypes, deepening cultural divides and
exacerbating tensions in multicultural settings.

\subsection{Stereotypical Biases}
This category focuses on the risks associated with
conversations that exhibits implicit stereotypes
based on gender, age, and ethnicity. Stereotypi-
cal biases in communication often implicitly mani-
fests through responses that may appear neutral but
are loaded with underlying discriminatory attitudes.
We characterise the paralinguistic cues harbouring
risks in this category to include the gender, age,
and ethnicity of the first and second speakers.

\subsubsection{Gender}
In cases of gender-based stereotypical
bias, responses may implicitly convey stereotypical
beliefs about abilities, roles, or behaviours associ-
ated with the female group. The content may be
neutral, but the paralinguistics cues may harbour
risks offensive to others. We consider risky interac-
tions that contain a female and a male speaker.

\subsubsection{Age}
Stereotypical Bias against the elderly is exhib-
ited in conversations that reflect age-related stereo-
types. Responses to the elderly individuals may
assume incompetence, resistance to change, or be-
ing out of touch. We consider risky interactions
that contain an elderly and a young speaker.

\subsubsection{Ethnicity}
In the case of ethnicity stereotypical
bias, responses may reflect stereotypes to a group,
biases to their ability, or discrimination to cultural
practices. It reinforces ethnic stereotypes and can
hinder the equal treatment of individuals from di-
verse cultural backgrounds. We consider risky in-
teractions in this category that contain an accented
speaker and a native speaker.


\section{Data Collection and Curation}
We curate our speech dataset for evaluation by
(i) manually creating samples as seeds for each
speech sub-category based on the corresponding
risk description, (ii) leveraging seed instances to
prompt GPT-4 to expand the sample set, and (iii) us-
ing advanced TTS systems, Audiobox and Google
TTS, to generate synthetic speech for 4 risk sub-
categories according to their specific paralinguistic
descriptions (see Figure 2). Due to the safeguards
and limitation of existing TTS system, we generate
synthetic speech for these risk sub-categories: mali-
cious sarcasm, age, gender, and ethnicity stereotyp-
ical biases. Table 1 provides our dataset statistics,
and we report the average speech lengths in Ap-
pendix F.

More specifically, each sample in our dataset is
a quadraple $(x, z, s, y)$ where (i) $x$ is the textual
content (created by human or GPT4), (ii) $z$ is the
description of paralingustic cues covering emotion,
tone, gender, age, and ethnicity, (iii) $s$ is the auto-
matically generated speech $s = \text{TTS}(x, z)$ based
on Audiobox (Vyas et al., 2023) or Google TTS, and
(iv) $y$ is the label in $\{\text{low-risk, malicious sar-
casm, age, gender, ethnicity stereotypical biases}\}$.

Creating a speech dataset entirely through hu-
man effort presents significant challenges, primar-
ily due to its high costs, extensive time require-
ments, and the difficulty of finding individuals ca-
pable of accurately acting specific speech descrip-
tions. These challenges often make the process
inefficient and impractical, which lead us to lever-
age GPT-4 and advanced TTS systems for speech
rendering, allowing to create diverse and scalable
datasets at a fraction of the cost and time. However,
we still need to bypass the safeguard restricting
us to obtain safety-related data. The rest of this
section outlines how to address these challenges.

\subsection{Text Samples}
\textbf{Seeds.} We first manually create 20 sample pairs
of $(x, z)$ for each risk sub-category label $y$. These
samples are quality controlled and filtered by 3
expert annotators based on these criteria: (i) the
content $x$ is ostensibly low-risk, and (ii) when com-
bined with paralinguistic $z$, it is mapped to the risk
label $y$ (including the 4 risk labels plus the low-
risk label). A sample is removed if at least two
annotators find it low quality.

\textbf{GPT-4 Generation.} Manually creating samples
is a time-consuming and costly process. Capitalis-
ing on the wide knowledge of GPT-4, we leverage
the human-curated samples as seed templates, and
prompt GPT-4 to generate more samples. Normally,
we may describe a risk sub-category and include
human-curated samples, and request GPT-4 to gen-
eralise them to more scenarios. However, GPT-4
tends to refuse responding to such requests due to
its safeguards. We thus employ a strategy analo-
gous to Wang et al. (2023) to overcome this issue,
as explained below.

Specifically, in this OpenAI API, there is a chain
of messages tagged as user and assistant alternat-
ing. In the first user role’s message, we define a
risk sub-category and request GPT-4 to produce
samples. In the next assistant role’s message, we
fabricate a response where we put our seed samples
here to simulate that GPT-4 has responded to our
first request. In the final user role’s message, we
request GPT-4 generate additional 30 samples. We
feed this conversation history including the above
3 messages into GPT-4, and GPT-4 successfully
respond to our last request and provide additional
30 samples. These samples are annotated and fil-
tered by human annotators, serving as seeds for
iterative generation. We mix human-generated and
GPT-4-generated samples as the text sample set
where each sample has a risk version and a low-
risk version by keeping the same $x$ and modifying
$z$.

\subsection{Synthesising Speech}
\textbf{Sarcasm \& Age Stereotypical Bias.} For each
$(x, z)$ in these categories, we generate 5 high-risk
speech and 5 low-risk speech using Audiobox. We
provide detailed speech descriptions for generation
in Table 8 of Appendix C. The low-risk versions
are generated from the modified paralinguistic de-
scription $z'$, as described in the following.

\begin{itemize}
    \item For malicious sarcasm, We describe $z$ as ``speaking with angry emotion, and a mocking tone'', and
    $z'$ as ``speaking with happy and excited emotions''.
    \item For age stereotypical bias, we distinguish be-
    tween risk speech and low-risk speech based on the
    age of the first speaker. We describe $z$ as ``the first
    speaker is an elderly person, the second person is
    a young person'', and the corresponding $z'$ is ``the
    first speaker is a young person, the second person
    is also a young person''. We first generate 5 speech
    of the second-speaker for each sample, and then
    generate 10 speech of the first-speaker, including 5
    risk version and 5 low-risk version, based on $z$ and
    $z'$. We finally manually cut the long silence and
    noise in collected speech, and concatenate speech
    waves of the first and the second speakers with 0.8
    seconds silence in between.
\end{itemize}

\textbf{Gender, Ethnicity Stereotypical Biases.} We
utilise Google TTS service to generate synthetic
speech for risk categories: gender stereotypical
bias and ethnicity stereotypical bias. To distin-
guish the risk and low-risk speech, we control the
gender and ethnicity of the first speaker.
\begin{itemize}
    \item For gender stereotypical bias, We describe $z$ as
    ``the first speaker is a woman, the second person
    is a man'', and the corresponding $z'$ is ``the first
    speaker is man, the second person is also a man''.
    we randomly select 5 female and 5 male voices
    from the en-US language list to serve as the first
    speaker, and an additional 5 male voices as the
    second speaker. We then create conversations by
    pairing each of the 5 female first-speakers with
    the 5 male second-speakers to constitute the risk
    speech samples. Similarly, pairing each of the 5
    male first-speakers with the 5 male second-speakers
    generates the low-risk speech samples. All speech
    waves are concatenated with 0.8 seconds of silence
    in between.
    \item For ethnicity stereotypical bias, a similar strategy
    is employed. We describe $z$ as ``the first speaker
    is a person with accent and diverse ethnicity back-
    grounds, the second speaker is a person with Amer-
    ican native accent'', and the corresponding $z'$ is
    ``the first speaker is a person with American na-
    tive accent, the second speaker is also a person
    with American native accent''. However, due to the
    limitation of Google TTS providing only Indian-
    accented voices, we are restricted to using Indian
    voices as the first speaker. Specifically, we select
    5 voices each from the en-IN and en-US language
    lists to serve as the first speaker, with an additional
    set of 5 voices chosen from the en-US list as the
    second speaker. These selections are then system-
    atically paired and concatenated into conversations
    following the same protocol used for the gender-
    based pairings.
\end{itemize}

All generated speeches are quality checked by
the annotators. Especially, for the Audiobox-
generated speech (for Malicious Sarcasm, Age),
although we feed voice requirements to Audiobox,
some of the generated speeches still do not follow
the requirements. Therefore, filtering by annotators
is necessary, to collect speech that strictly meets
our requirements and to ensure that humans can
easily distinguish the sarcasm, gender, ethnicity,
and age in the generated speeches.


\section{Experiments}
We evaluate the capabilities of LMMs in detect-
ing the risk induced by paralinguistic cues under 4
risk sub-categories: malicious sarcasm, and stereo-
typical biases for gender, age, and ethnicity. We
first describe our evaluation prompts and model
selection (§6.1). Next, we report the results on
5 LMMs via a diverse set of evaluation prompts
(§6.2). Lastly, we analyse the capabilities of LMMs
and patterns observed (§6.3).

\subsection{Experimental Settings}
\textbf{Prompts.} Our basic prompts for evaluation are
Yes/No (Y/N) questions (e.g., Is there any indi-
cation of sarcasm in the speaker’s delivery in the
audio?), and Multi-choice (MC) questions (e.g.,
What is the speaker implying? Choose the most
appropriate response.; A. Compliments, support
and gratitude; B. Sarcasm). When we evaluate
LMMs in the multi-choice setting, we reverse the
option positions and conduct inferences twice, and
we report the averaged results. We also try Chain-
of-thought (CoT) style which allows us to investi-
gate whether step-by-step reasoning could improve
LMMs’ detection capability by appending Let’s
think step-by-step (Kojima et al., 2022) to the start
of both Y/N and MC prompts. This is denoted as
CoT + Y/N, or CoT + MC. Additionally, to increase
LMM’s chance of success, we also try appending
more revealing (Pre-task) questions in the Y/N and
MC prompts by asking the LMM to first predict
a relevant paralinguistc cue in the audio before at-
tempting to answer the Y/N or MC questions (e.g.,
Please recognize the speaker’s sentiment, and ...).
This is denoted as Pre-task + Y/N, or Pre-task +
MC. We provide detailed prompts for each risk
sub-categories in Table 9 of Appendix D.

\textbf{Models.} We evaluate 5 recent LMMs with
instruction-following and speech understanding ca-
pabilities. Qwen-Audio-Chat (Chu et al., 2023)
is an instruction following version of Qwen-
Audio (Chu et al., 2023) with a Whisper au-
dio encoder and QwenLM (Bai et al., 2023).
SALMONN-7/13B (Tang et al., 2024) is a Whis-
per and BEATs (Chen et al., 2023) dual audio en-
coders and VicunaLLM (Chiang et al., 2023). We
evaluate both 7B and 13B variants. WavLLM (Hu
et al., 2024), is the latest LMM achieving state-
of-the-art on universal speech benchmarks and is
equipped with Whisper and WavLM (Chen et al.,
2022) dual encoders and LLaMA-2 (Touvron et al.,
2023b). Gemini-1.5-Pro (Reid et al., 2024) is a
widely used recent proprietary LMM with native
multi-modal capabilities. We used the API access
for Gemini-1.5-pro. In all evaluations, we set the
temperature as 0 and switched off sampling for
reproducibility of experimental results. Accuracy
and macro-averaged F1 score are used as metrics.

\subsection{Main Results}
We report evaluation results in Table 2 (F1 exhibits
similar pattern - see Table 6 of §A). We show the
average performance among LMMs for each task,
and the weighted average performance by the num-
ber of task samples for each combination between
LMM and prompt across 4 risk sub-categories. Our
findings are summarised along various axes.

\textbf{Prompting Styles.} Do Y/N and MC exhibit a sys-
tematic difference in performance? Do CoT and
Pre-task query improve the results? Do models
show high degree of sensitivity to prompting style?
Is there a preferred mode of prompting?
We observe that, on most of sub-categories, MC
is a more effective prompting strategy. Especially,
SALMONN reacts with severe misalignment and
biases on Y/N, but it achieves the best performance
when it is switched to MC. CoT, as a common
strategy to promote logical thinking of LLMs, does
not show its impact on LMM for combining mul-
timodal cues. In contrast, the adoption of Pre-task
activates most of models to achieve a better result
on various sub-categories. It suggests the implicit
signal from paralinguistic cues help models inte-
grating multimodal cues. These observations leads
to Pre-task + MC as the best prompting strategy.

\textbf{Models.} Is there a model outperforming the rest
on all risk sub-categories? Is there a specific pre-
training protocol or choice of encoder-LLM that
has a clear advantage? Are there models that per-
form near random baseline?
We don’t conclude there is a model outperform-
ing the rest on all sub-categories, however, results
exhibit two patterns that models follow. Qwen-
Audio-Chat achieves the best overall performance
across 4 sub-categories and also achieves competi-
tive performance on each sub-category. Its average
performance across 6 prompting strategies outper-
form other models on 2 sub-categories, demon-
strating its stabilility and robustness to prompts.
Gemini-1.5-Pro follows the similar pattern, which
suggests a overall stable and robust performance
across different prompting stragegies and achieve
the best average F1 score on 3 sub-categories. How-
ever, SALMONN-7B/13B demonstrate an opposite
pattern where they show outstanding risk detec-
tion ability on 3 sub-categories of stereotypical bi-
ases and achieve the best performance, respectively.
But they exhibit vulnerable to prompts, especially,
SALMONN-7B could not make a reaction under
Y/N even though effective Pre-task strategy slightly
mitigates this, and SALMONN-13B are not able
to maintains the consistent performance across dif-
ferent prompts under the same sub-category (e.g.,
62.58 vs. 34.84 under gender stereotypical bias).
Meanwhile, WavLLM fails to detect any risk, and
show severe misalignment and biases across all sub-
categories. By observing these two patterns and the
pre-training protocol of LLMs, we attribute them
to the different states of audio encoders. Specif-
ically, audio encoders in Qwen-Audio-Chat and
Gemini-1.5-Pro are fine-tuned in pre-training stage
leading them to effectively extract features from
inputs and generate more stable and consistent em-
beddings, exhibiting robustness to prompts. How-
ever, frozen audio encoders coupled with adapter
in SALMONN and WavLLM are more likely to
be vulnerable to the change of inputs and prompts,
and the dual encoders settings mixed with irrele-
vant non-speech feature limit its ability to generate
more stable and consistent embeddings.

\textbf{Difficulty of Sub-categories.} Are there risk sub-
categories that are much harder for models to de-
tect and why? Is there any patterns in the misclas-
sified instances?
Most of models perform near or over 60\% of
accuracy on detection of malicious sarcasm where
its paralinguistic cue is sentiment displayed as emo-
tion and speaking tone in utterances. Emotion
recognition as a basic speech task is included in the
pre-training stage of most models, resulting in mod-
els’ ability to recognise and reason with it. How-
ever, detection in stereotypical biases produce 2
more complex difficulties for models to overcome:
(i) recognise the number of speakers, and (ii) recog-
nise the voice features of the first speaker. Most
of models lack of training to solve these issues,
leading to a overall performance below 60\% of ac-
curacy. We analyse these difficulties, and include
GPT-4 evaluation as performance ceiling assuming
these difficulties are overcome.

In the misclassified instances, a significant pat-
tern is that models respond mainly based on the
content of speech. A common response is ``The au-
dio content is [...]. Therefore, there is no hint of sar-
casm/biases''. For the conversation sub-categories,
based on the filtering process mentioned in the
above, humans can easily distinguish the voices
of two speakers and we also add fixed silence be-
tween the utterances of two speakers. This is a
critical finding which underscores the absence of
safeguards in multimodal LLMs beyond the speech
content.


\section{Conclusion}
We presented a speech-specific risk taxonomy
where paralinguistic cues in speech can transform
low-risk textual content into high-risk speech. We
created a high quality synthetic speech dataset un-
der human annotation and filtering. We observed
that even the most recent large multimodal mod-
els (such as Gemini 1.5 pro) perform near random
baseline, with some of the recent speechLLMs scor-
ing even worse than random guesses.


\section{Limitations}
We expect to extend our evaluation experiments
to all risk types in our taxonomy, however, the
existing safeguards of TTS system prevents the
generation of such synthetic data. Our results pro-
vide insights on the ethnicity sub-category, but our
data generation pipeline is bounded by the cover-
age of existing TTS and audio generators, we plan
to further extend into other ethnicity in the future
work. Our ongoing plan is to hire human speakers
for collecting real data. Additionally, all LMMs
are evaluated on our synthetic dataset, and human-
generated speech could potentially introduce other
artefacts, making this task even more challenging.
We provided certain conjectures to explain evalua-
tion results and the capabilities of LMMs, but this
initial attempt requires further analyse in separate
works.


```latex
\section{Ethics Statement}
This research aims to open an avenue for system-
atically evaluating the capabilities of Large Mul-
timodal Models in detecting risk associated with
speech modality. The nature of this data is inher-
ently sensitive. To ensure our data (and its future
extensions) access facilitates progress towards safe-
guarding and does not contribute to harmful de-
signs, we will place the data access behind a request
form, demanding researchers to provide detailed
affiliation and intention of use, under a strict term
of use. Additionally, we have adhered to the us-
age policy of Audiobox and Google TTS, and did
not generate speech containing any explicit toxic
content.
```


\begin{thebibliography}{99}
Rohan Anil, Andrew M Dai, Orhan Firat, Melvin John-
son, Dmitry Lepikhin, Alexandre Passos, Siamak
Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng
Chen, et al. 2023. Palm 2 technical report. arXiv
preprint arXiv:2305.10403.

Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,
Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei
Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin,
Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu,
Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren,
Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong
Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang
Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian
Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi
Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang,
Yichang Zhang, Zhenru Zhang, Chang Zhou, Jin-
gren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023.
Qwen technical report. CoRR, abs/2309.16609.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. Advances in neural information processing
systems, 33:1877–1901.

Sanyuan Chen, Chengyi Wang, Zhengyang Chen,
Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki
Kanda, Takuya Yoshioka, Xiong Xiao, Jian Wu, Long
Zhou, Shuo Ren, Yanmin Qian, Yao Qian, Jian Wu,
Michael Zeng, Xiangzhan Yu, and Furu Wei. 2022.
Wavlm: Large-scale self-supervised pre-training for
full stack speech processing. IEEE J. Sel. Top. Signal
Process., 16(6):1505–1518.

Sanyuan Chen, Yu Wu, Chengyi Wang, Shujie Liu,
Daniel Tompkins, Zhuo Chen, Wanxiang Che, Xi-
angzhan Yu, and Furu Wei. 2023. Beats: Audio pre-
training with acoustic tokenizers. In International
Conference on Machine Learning, ICML 2023, 23-29
July 2023, Honolulu, Hawaii, USA, volume 202 of
Proceedings of Machine Learning Research, pages
5178–5193. PMLR.

[ILLEGIBLE]

\end{thebibliography}


\appendix
\section{Experimental Results}
%%%PLACEHOLDER: TAB_0004%%%

\section{Examples for Sub-categories}
%%%PLACEHOLDER: TAB_0005%%%

\section{Description of Speech Generation from Audiobox}
%%%PLACEHOLDER: TAB_0006%%%

\section{Prompting Strategies}
%%%PLACEHOLDER: TAB_0007%%%

\section{Computational Hardware and API}
We conduct all our evaluation experiments and
analysis on 4×A100 GPUs. No fine-tuning was
done and the experiments only involved inference.
For Gemini 1.5 Pro we used gemini-1.5-pro API,
and for GPT-4 we used gpt-4-turbo API. Tem-
perature was set to 0 and sampling at decoding was
switched off.

\section{The Average Speech Lengths}
%%%PLACEHOLDER: TAB_0008%%%


\end{document}
