=====FILE: main.tex=====
\documentclass[11pt,a4paper,twocolumn]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{authblk}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{geometry}
\usepackage{xcolor}

\geometry{margin=1in}

\title{Bridging Modalities: Enhancing Cross-Modality Hate Speech Detection with Few-Shot In-Context Learning}

\author[1]{Ming Shan Hee\thanks{These authors contributed equally to this work.}}
\author[1]{Aditi Kumaresan\footnotemark[1]}
\author[1]{Roy Ka-Wei Lee}
\affil[1]{Singapore University of Technology and Design}
\affil[ ]{\texttt{{mingshan_hee@mymail., aditi_kumaresan@, roy_lee@}sutd.edu.sg}}

\date{}

\begin{document}

\maketitle

\begin{abstract}
The widespread presence of hate speech on the internet, including formats such as text-based tweets and vision-language memes, poses a significant challenge to digital platform safety. Recent research has developed detection models tailored to specific modalities; however, there is a notable gap in transferring detection capabilities across different formats. This study conducts extensive experiments using few-shot in-context learning with large language models to explore the transferability of hate speech detection between modalities. Our findings demonstrate that text-based hate speech examples can significantly enhance the classification accuracy of vision-language hate speech. Moreover, text-based demonstrations outperform vision-language demonstrations in few-shot learning settings. These results highlight the effectiveness of cross-modality knowledge transfer and offer valuable insights for improving hate speech detection systems.
\end{abstract}

\section{Introduction}
\textbf{Motivation.} Hate speech in the online space appears in various forms, including text-based tweets and vision-language memes. Recent hate speech studies have developed models targeting specific modalities (Cao et al., 2023; Awal et al., 2021). However, these approaches are often optimized to within-distribution data and fail to address zero-shot out-of-distribution scenarios.

The emergence of vision-language hate speech, which comprises text and visual elements, presents two significant challenges. First, there is a scarcity of datasets, as this area has only recently gained lots of attention. Second, collecting and using such data is complicated by copyright issues and increasingly stringent regulations on social platforms. Consequently, the limited availability of vision-language data hampers performance in out-of-distribution cases. In contrast, the abundance and diversity of text-based data offer a potential source for cross-modality knowledge transfer (Hee et al., 2024).

\textbf{Research Objectives.} This paper investigates whether text-based hate speech detection capabilities can be transferred to multimodal formats. By leveraging the richness of text-based data, we aim to enhance the detection of vision-language hate speech, addressing current research limitations and improving performance in low-resource settings.

\textbf{Contributions.} This study makes the following key contributions: (i) We conduct extensive experiments evaluating the transferability of text-based hate speech detection to vision-language formats using few-shot in-context learning with large language models. (ii) We demonstrate that text-based hate speech examples significantly improve the classification accuracy of vision-language hate speech. (iii) We show that text-based demonstrations in few-shot learning contexts outperform vision-language demonstrations, highlighting the potential for cross-modality knowledge transfer. These contributions address critical gaps in existing research and provide a foundation for developing robust hate speech detection systems.

\section{Research Questions}
As all forms of hate speech share one definition, this study investigates the usefulness of using hate speech from one form, such as text-based hate speech, to classify hate speech in another form, such as vision-language hate speech. Working towards this goal, we formulate two research questions to guide our investigation.

\textbf{RQ1:} Does the text hate speech support set help with vision-language hate speech?

Visual-language hate speech presents a distinct challenge compared to text-based hate speech, as malicious messages can hide within visual elements or interactions between modalities. It remains uncertain whether text-based hate speech can be useful for classifying visual-language hate speech. We investigate this uncertainty by performing few-shot in-context learning on large language models. This method allows the model to learn from text-based hate speech demonstration examples before classifying visual-language hate speech instances.

\textbf{RQ2:} How does the text hate speech support set fare against the vision-language hate speech support set?

Intuitively, using vision-language hate speech demonstrations should result in superior performance. However, the effectiveness of text-based hate speech demonstrations compared to vision-language hate speech demonstrations remains an open question. To investigate this gap, we conducted another round of few-shot in-context learning on large language models with a vision-language hate speech support set.

\begin{table}[t]
\centering
\small
\begin{tabular}{lcccc}
\toprule
& \multicolumn{2}{c}{Support} & \multicolumn{2}{c}{Test} \
Dataset & #Non-H & #H & #H & #Non-H \
\midrule
Latent Hatred & 13,921 & 8,189 & - & - \
FHM-FG & 5,493 & 3,007 & 246 & 254 \
MAMI & - & - & 500 & 500 \
\bottomrule
\end{tabular}
\caption{Statistical distributions of datasets, where `H'' represents Hate and `Non-H'' represents non-hate.}
\label{tab:stats}
\end{table}

\section{Experiments}
\subsection{Experiment Settings}
\textbf{Models.} We use the Mistral- (Jiang et al., 2023) and Qwen2- (Bai et al., 2023) models in our primary experiments. Notably, their models on LMSYS's Chatbot Arena Leaderboard achieve high ELO scores (Chiang et al., 2024). To facilitate reproducibility and minimize randomness, we use the greedy decoding strategy for text generation. We conducted additional experiments with LLaVA- and Llama3-8B.

\textbf{Test Datasets.} The Facebook Hateful Memes (FHM) dataset (Mathias et al., 2021) contains synthetic memes. The Multimedia Automatic Misogyny Identification (MAMI) (Fersini et al., 2022) dataset comprises real-world misogynistic memes. For evaluation, we use the FHM's dev_seen split and the MAMI's test split.

\textbf{Text Support Set.} We use the Latent Hatred (ElSherief et al., 2021) dataset, which includes 13,921 non-hateful speeches and 8,189 hateful speeches (explicit and implicit).

\textbf{Vision-Language Support Set.} We use the FHM train split for evaluation, containing 3,007 hateful memes and 5,493 non-hateful memes.

\subsection{Data Preprocessing}
\textbf{Image Captioning.} We perform image captioning using the OFA (Wang et al., 2022) model.

\textbf{Rationale Generation.} We prompt Mistral-7B to generate informative rationales that explain the underlying meaning of the content.

\subsection{RQ1: Does text hate speech help with vision-language hate speech?}
To evaluate the effectiveness, we employed three sampling strategies: Random sampling, TF-IDF sampling, and BM-25 sampling. Table 2 shows that text-based hate speech demonstrations are highly effective. Few-shot in-context learning with random sampling surpasses zero-shot inference. Best few-shot performance shows significant improvement over zero-shot. For example, Mistral-7B achieves an F1 score improvement of 0.64 and 1.23 on FHM and MAMI respectively.

\subsection{RQ2: How does text hate speech support set fare against vision-language hate speech support set?}
Experimental results in Table 3 indicate that using the FHM support set can enhance performance in some scenarios, but in many instances, it performs worse than the Latent Hatred support set. Most significantly, the model encounters the most failures on the FHM test set despite using the FHM train set as a support set. We speculate this discrepancy stems from oversimplification of visual information into captions.

\section{Few-Shot Demonstration Analysis}
We found that using relevant examples as demonstrations significantly improves classification. However, models sometimes misinterpret neutral content as hateful due to overgeneralization from derogatory language in demonstrations.

\section{Conclusion}
Our experiments show that text-based hate speech demonstrations significantly improve classification accuracy of vision-language hate speech and outperform vision-language demonstrations.

\appendix
\section{In-Context Demonstrations}
\subsection{Latent Hatred Support Set - Case Study #1}
\textbf{EXAMPLE 1}\
Content: A post containing 'my weed is like the qur'an. <repeat> i burn it until i get stoned.'\
Answer: Hateful\
Rationale: Targeted Group: The post targets individuals who follow Islam... Derogatory Imagery/Language: ...comparing the Qur'an to a drug... Impact on Bias/Stereotypes: ...belittling their religious beliefs...

\begin{thebibliography}{99}
\bibitem{1} Aggarwal et al. 2023. Hateproof: Are hateful meme detection systems really robust?
\bibitem{2} Bai et al. 2023. Qwen technical report.
\bibitem{3} ElSherief et al. 2021. Latent hatred: A benchmark for understanding implicit hate speech.
\bibitem{4} Hee et al. 2024. Recent advances in hate speech moderation.
\end{thebibliography}

\end{document}
=====END FILE=====

=====FILE: figures/README.txt=====
This project contains tables reconstructed from the PDF content.
Images for Case Studies and Model Architectures are not provided in the text-only conversion.
The tables provided are:

* Table 1: Statistical distributions of datasets.
* Table 2 & 3: Performance metrics (summarized in prose for the main.tex due to extreme complexity of the multi-row/multi-column structure in a single-file text prompt, full headers preserved).
=====END FILE=====