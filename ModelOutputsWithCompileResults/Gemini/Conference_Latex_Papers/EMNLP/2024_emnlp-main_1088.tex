=====FILE: main.tex=====
\documentclass[twocolumn]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{url}
\usepackage{geometry}
\usepackage{tabularx}
\usepackage{caption}
\usepackage{float}

% Page geometry to match standard conference papers
\geometry{a4paper, margin=1in}

\title{Factuality of Large Language Models: A Survey}

\author{
Yuxia Wang$^{1}^{1}^{3}^{1}^{1}$ \
$^{1}$MBZUAI, $^{2}$Monash University, $^{3}$Google, $^{4}$Sofia University \
\texttt{{yuxia.wang, preslav.nakov}@mbzuai.ac.ae}
}

\date{}

\begin{document}

\maketitle

\begin{abstract}
Large language models (LLMs), especially when instruction-tuned for chat, have become part of our daily lives, freeing people from the process of searching, extracting, and integrating information from multiple sources by offering a straightforward answer to a variety of questions in a single place. Unfortunately, in many cases, LLM responses are factually incorrect, which limits their applicability in real-world scenarios. As a result, research on evaluating and improving the factuality of LLMs has attracted a lot of attention recently. In this survey, we critically analyze existing work with the aim to identify the major challenges and their associated causes, pointing out to potential solutions for improving the factuality of LLMs, and analyzing the obstacles to automated factuality evaluation for open-ended text generation. We further offer an outlook on where future research should go.
\end{abstract}

\section{Introduction}

Large language models (LLMs) have become an integral part of our daily lives. When instruction-tuned for chat, they have enabled digital assistants that can free people from the need to search, extract, and integrate information from multiple sources by offering straightforward answers in a single chat. While people naturally expect LLMs to always present reliable information that is consistent with real-world knowledge, LLMs tend to fabricate ungrounded statements, resulting in misinformation (Tonmoy et al., 2024), which limits their utility.

Thus, assessing and improving the factuality of the text generated by LLMs has become an emerging and crucial research area, aiming to identify potential errors and to advance the development of more reliable LLMs (Chen et al., 2023). To this end, researchers have collected multiple datasets, introduced a variety of measures to evaluate the factuality of LLMs, and proposed numerous strategies leveraging external knowledge through retrieval, self-reflection, and early refinement in model generation to mitigate factual errors (Tonmoy et al., 2024).

Numerous surveys (Tonmoy et al., 2024; Huang et al., 2023a; Wang et al., 2023b) have explored factuality or hallucinations in large language models across various modalities. While they either lack in-depth discussion or are too specific to grasp the fundamental challenges, promising solutions in factuality evaluation and enhancement, and some ambiguous concepts in LLM factuality. We summarized these surveys in Table \ref{tab:surveys}.

Our survey aims to bridge this gap by providing an in-depth analysis of LLM factuality, with an emphasis on recent studies to reflect the rapidly evolving nature of the field. We offer a comprehensive overview of different categorizations, evaluation methods, and mitigation techniques for LLM factuality in both language and vision modalities. Additionally, we explore a novel research avenue that seeks to improve LLM calibration. This includes making models aware of their knowledge limitations and enhancing the reliability of their output confidence.

\section{Background}

Hallucination and factuality, while conceptually distinct, often occur in similar contexts and are sometimes used interchangeably, rendering them intricately intertwined, posing a challenge in discerning their distinct boundaries, and causing a considerable amount of misconception. In this section, we seek to disambiguate and refine our understanding of these two closely aligned concepts, thereby preventing misinterpretation and reducing potential confusion. Additionally, we further include two closely-related axes: relevance and trustworthiness for LLM evaluation to illustrate their nuance in relation to factuality.

\begin{table*}[t]
\centering
\caption{Comparison of different surveys on the factuality of LLMs. Eval: Evaluation; Improve: Improvement.}
\label{tab:surveys}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l c c c c c p{8cm}}
\toprule
\textbf{Survey} & \textbf{Date} & \textbf{Pages} & \textbf{Eval} & \textbf{Improve} & \textbf{Multimodal} & \textbf{Contributions and limitations} \
\midrule
Our work & 15-June-2024 & 9 & X & X & X & Discusses ambiguous concepts in LLM factuality, compares and analyzes evaluation and enhancement approaches from academic and practical perspectives, outlining major challenges and promising avenues to explore. \
(Tonmoy et al., 2024) & 08-Jan-2024 & 19 & \checkmark & X & \checkmark & Summarizes recent work in terms of mitigating LLM hallucinations, but lacks comparison between different approaches and discussions to identify open questions and challenges. \
(Gao et al., 2023b) & 18-Dec-2023 & 26 & \checkmark & X & X & Summarizes three RAG paradigms: naïve, advanced, and modular RAG, with key elements and evaluation methods for the three major components in RAG (retriever, generator, and augmentation). \
(Huang et al., 2023b) & 09-Nov-2023 & 49 & \checkmark & X & X & Analyzes the reasons for hallucinations, and presents a comprehensive overview of hallucination detection methods, benchmarks, and approaches to mitigate hallucinations. \
(Wang et al., 2023b) & 18-Oct-2023 & 44 & X & \checkmark & X & Detailed literature review of factuality improvement and enhancement methods covering both retrieval augmentation and non-retrieval augmentation, missing discussion of major bottleneck issues in LLM factuality and promising directions to investigate. \
(Rawte et al., 2023b) & 18-Sept-2023 & 11 & \checkmark & X & \checkmark & Extensively elucidates the problem of hallucination across all major modalities of foundation models, including text, image, video, and audio. However, inadequate coverage of approaches, in-depth categorization and comparison between methods. \
(Zhang et al., 2023c) & 03-Sept-2023 & 32 & \checkmark & X & X & Organized by different training stages of LLMs, discusses potential sources of LLM hallucinations and in-depth review of recent work on addressing the problem. \
(Guo et al., 2022) & Feb-2022 & 19 & X & X & X & Focused on the automated fact-checking pipeline. \
\bottomrule
\end{tabular}%
}
\end{table*}

\paragraph{Hallucination vs. Factuality}
The concept of hallucination in the context of traditional natural language generation tasks is typically referred to as the phenomenon in which the generated content appears nonsensical or unfaithful to the provided source content (Ji et al., 2023). One concrete example is made-up information in an abstractive summary with additional insights beyond the scope of the original source document.

In the age of LLMs, the term hallucination has been reimagined, encompassing any deviation from factual reality or the inclusion of fabricated elements within generated texts (Tonmoy et al., 2024; Rawte et al., 2023b). (Zhang et al., 2023c) define hallucination as the characteristic of LLMs to generate content that diverges from the user input, contradicts previously generated context, or mis-aligns with established world knowledge. (Huang et al., 2023b) merge the input- and context-conflicting types of hallucinations and further take logical inconsistency into account to form faithfulness hallucination. Another category is factuality hallucination, referring to the discrepancy between generated content and verifiable real-world facts, manifesting as (1) factual inconsistency and (2) factual fabrication.

Factuality, on the other hand, is concerned with a model's ability to learn, acquire, and utilize factual knowledge. (Wang et al., 2023b) characterize factuality issues as the probability of LLMs producing content inconsistent with established facts. It is important to note that hallucination content may not always involve factual missteps. Though a piece of generated text may exhibit divergence from the initial prompt's specifics, it falls into hallucinations, not necessarily a factual issue if the content is accurate.

It is crucial to distinguish between factual errors and instances of hallucination. The former involves inaccurate information whereas the latter can present unanticipated and yet factually substantiated content (Wang et al., 2023b).

\textbf{Summary:} Factuality is the ability of LLMs to generate content consistent with factual information and world knowledge. Although both hallucinations and factuality may impact the credibility of LLMs in the context of content generation, they present distinct challenges. Hallucinations occur when LLMs produce baseless or untruthful content, not grounded in the given source. In contrast, factuality errors arise when the model fails to accurately learn and utilize factual knowledge. It is possible for a model to be factually correct yet still produce hallucinations by generating content that is either off-topic or more detailed than what is requested.

\paragraph{Trustworthiness/Reliability vs. Factuality}
In the context of LLMs, factuality (Wang et al., 2023b) refers to a model's capability of generating contents of factual information, grounded in reliable sources (e.g., dictionaries, Wikipedia or textbooks), with commonsense, world and domain-specific knowledge taken into account. In contrast, "trustworthiness" (Sun et al., 2024) extends beyond mere factual accuracy and is measured on eight dimensions: truthfulness, safety, fairness, robustness, privacy, ethics, transparency, and accountability.

\section{Evaluating Factuality}

Evaluating LLM factuality on open-ended generations presents a non-trivial challenge, discerning the degree to which a generated textual statement aligns with objective reality. Studies employ various benchmarks, evaluation strategies and metrics to achieve this goal.

\subsection{Datasets and Metrics}

While (Zhang et al., 2023c) outlined tasks and measures for hallucination evaluation, there is no comparative analysis of existing datasets to assess various aspects in regards to model factuality (e.g., knowledge grounding, fast-changing facts, snowballing hallucinations, robustness to false premises, and uncertainty awareness). We categorize the datasets in the format of discrimination or generation, and highlights the challenges in automatic evaluation for long-form open-ended generations.

Current benchmarks largely assess the factuality in LLMs based on two capabilities: proficiency in distinguishing factual accuracy in a context and ability to generate factually sound content. The former typically comes in the form of a multi-choice question, with the expected response being a label of one of A, B, C, and D. For instance, HotpotQA, StrategyQA, MMLU. This form of evaluation has been widely used to measure the general knowledge proficiency and factual accuracy of LLMs, largely thanks to its automation-friendly nature. Under this evaluation formulation, model responses are easily parsed and compared with gold standard labels, enabling the calculation of accuracy or F1 scores against established benchmarks.

Precisely assessing the factuality of free-form LLM outputs remains a significant challenge due to the inherent limitations of automatic methods in the face of open-ended generation and the absence of definitive gold standard responses within an expansive output space. To make automatic evaluation feasible, many studies constrain the generation space to (1) Yes/No; (2) short-form phrase; and (3) a list of entities through controlling the categories of questions and generation length.

Perhaps the most demanding, yet inherently realistic scenario is free-form long text generation, such as biography generation. For this, the most commonly used and reliable methods rely on human experts following specific guidelines, and automatic fact-checkers based on retrieved information, such as FactScore, Factool and Factcheck-GPT, to facilitate efficient and consistent evaluation. These automatic fact-checkers generally first decompose a document into a set of atomic claims, and then verify one by one whether the claim is true or false based on the retrieved evidence, either from offline Wikipedia or online Web pages. The percentage of true claims over all statements in a document is used to reflect the factual status of a response (refer to FactScore). The averaged Factscore over a dataset is in turn used to assess a model's factuality accuracy. However, there is no guarantee that automatic fact-checkers are 100% accurate in their verification process. (Wang et al., 2023c) show that even the state-of-the-art verifier, equipped with GPT-4 and supporting evidence retrieved with Google search, can only achieve an F1 score of 0.63 in identifying false claims and  using PerplexityAI (compared with human-annotated labels for claims: true or false).

\textbf{Summary:} We categorize datasets that evaluate LLM factuality into four types, depending on the answer space and the difficulty degree on which accurate automatic quantification can be performed (see Table \ref{tab:datasets}). They are: (I) open-domain, free-form, long-term responses (FactScore: the percentage of the correct claims verified by human or automated fact-checker); (II) Yes/No answer w/wt explanation (extract Yes/No, metrics for binary classification); (III) short-form answer (Exact match the answer with gold labels and calculate accuracy) or the listing answer (recall@K); and (IV) multi-choice QA (metrics for multi-class classification).

\begin{table*}[t]
\centering
\caption{Four types of datasets used to evaluate LLM factuality. I: open-ended generation; II: Yes/No answer; III: short-term or list of entities answer; IV: A, B, C, D multiple Choice QA. Labeled datasets under type I are mostly generated by ChatGPT, and FactScore-Bio (ChatGPT, InstGPT and PerplexityAI). ER: Human-annotated Error Rate. Freq: usage frequency as evaluation set in our first 50 references.}
\label{tab:datasets}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l l l l c p{6cm} c}
\toprule
\textbf{Type} & \textbf{Dataset} & \textbf{Topic} & \textbf{Size} & \textbf{ER} & \textbf{Evaluation and Metrics} & \textbf{Freq} \
\midrule
I & FactScore-Bio (Min et al., 2023) & Biography & 549 & 42.6 & Human annotation and automated fact-checkers & 14 \
& FactcheckGPT (Wang et al., 2023c) & Open-ended questions & 94 & 64.9 & Human annotation & 1 \
& FacTool-QA (Chern et al., 2023) & Knowledge-based QA & 50 & 54.0 & Human annotation and automated fact-checkers & 2 \
& FELM-WK (Chen et al., 2023) & Knowledge & 184 & 46.2 & Human annotation, Accuracy and F1 score & 1 \
& HaluEval (Li et al., 2023a) & Knowledge-based QA & 5000 & 12.3 & Human annotation, AUROC + LLM judge + PARENT & 3 \
& FreshQA (Vu et al., 2023) & Open-ended questions & 499 & 68.0 & Human annotation & 2 \
& SelfAware (Yin et al., 2023b) & Open-ended questions & 3369 & - & Evaluate the LLM awareness of unknown by F1-score & - \
\midrule
II & Snowball (Zhang et al., 2023b) & Yes/No question & 1500 & 9.4 & Exact match + Accuracy/F1-score & 1 \
\midrule
III & Wiki-category List (Dhuliawala et al., 2023) & Name some Mexican films & 55 & - & Precision/recall@5 & - \
& Multispan QA (Dhuliawala et al., 2023) & Short-term Answer & 428 & - & Exact match + F1 score & - \
\midrule
IV & TruthfulQA (Lin et al., 2022) & False belief or misconception & 817 & - & Accuracy & 5 \
& HotpotQA (Yang et al., 2018) & Multi-step reasoning & 113k & - & Exact match + F1 score & 11 \
& Strategy QA (Geva et al., 2021) & Multi-step reasoning & 2780 & - & Recall@10 & 3 \
& MMLU (Hendrycks et al., 2021) & Knowledge & 15700 & - & Accuracy & 4 \
\bottomrule
\end{tabular}%
}
\end{table*}

\subsection{Other Metrics}

In addition to evaluating the methods discussed above, (Lee et al., 2022) quantified the hallucinations using two metrics, both requiring document-level ground-truth: (1) hallucinated named entities error measures the percentage of named entities in the generations that do not appear in the ground-truth document; (2) entailment ratio evaluates the number of generations that can be entailed by the ground-truth reference, over all generations.

(Rawte et al., 2023a) defined the hallucination vulnerability index (HVI), which takes a spectrum of factors into account, to evaluate and rank LLMs. Some factuality measurement tasks, such as claim extraction and evidence retrieval are non-trivial to automate. (Rawte et al., 2023a) curated publicly available LLM hallucination mitigation benchmark, where LLM generations are scored by humans when automated external knowledge retrieval fails to resolve a claim clearly. While widely used for factuality evaluation, this hybrid approach may suffer from human annotation bias.

\section{Improving Factuality}

Improving the factuality of an LLM often requires updating its internal knowledge, editing fake, outdated and biased elements, thereby making its output reflect a revised collection of facts, maximizing the probability of P(truth prompt). One option is to adopt gradient-based methods to update model parameters to encourage desired model output. This includes pre-training, supervised fine-tuning and RLXF. We can also explore injecting a new fact into LLMs or overwriting the false knowledge stored in LLM memory by in-context learning (ICL).

When models store factually correct knowledge but produce errors, they can in some cases rectify them through self-reasoning, reflection, and multi-agent debates. We discuss these methods throughout the lifecycle of an LLM, ranging from pre-training, to inference, to post-processing. Another important element is retrieval augmentation, which enhances the generation capabilities of LLMs by anchoring them in external knowledge that may not be stored or contradict the information in LLM parametric memory. It can be incorporated at various stages throughout model training and the subsequent inference process (Gao et al., 2023b), and is therefore not discussed individually.

\subsection{Pre-training}

LLMs store a vast amount of world knowledge in their parameters through the process of pre-training. The quality of the pre-training data plays a crucial role and misinformation could potentially cause LLMs to generate false responses, motivating the utilization of high-quality textual corpora. However, the prohibitively massive amount of pre-training data, typically consisting of trillions of tokens, renders manual filtering and editing impractically laborious. To this end, automated filtering methods have been proposed. For instance, (Brown et al., 2020) introduce a method to only focus on a small portion of the CommonCrawl dataset that exhibits similarity to high-quality reference corpora.

(Touvron et al., 2023) propose to enhance factual robustness of mixed corpora by up-sampling documents from the most reliable sources, thereby amplifying knowledge accuracy and mitigating hallucinations. During the pre-training phase of phi-1.5, (Li and et al., 2023b) synthesize "textbook-like" data, consists of and rich in high-quality commonsense reasoning and world knowledge. While careful corpus curation remains the cornerstone of pre-training for enhanced factuality, the task becomes increasingly challenging with the expansion of dataset scale and the growing demand for linguistic diversity. It is therefore crucial to develop novel strategies that guarantee the consistency of factual knowledge across diverse cultural landscapes.

(Borgeaud et al., 2021) propose RETRO, a retrieval augmented pre-training approach. An auto-regressive LLM is trained from scratch with a retrieval module that is practically scalable to large-scale pre-training by retrieving billions of tokens. RETRO shows better accuracy and is less prone to hallucinate compared to GPT (Wang et al., 2023a). While limitations lie in that RETRO performance could be compromised if the retrieval database contains inaccurate, biased or outdated information.  additional computation is required for the pre-training of LLMs with retrieval.

\subsection{Tuning and RLXF}

Continued domain-specific SFT has shown to be effective for enhancing factuality, particularly in the absence of such knowledge during pre-training. For instance, (Elaraby et al., 2023) enhance the factual accuracy of LLMs through knowledge injection (KI). Knowledge, in the form of entity summaries or entity triplets, is incorporated through SFT by either intermediate tuning, i.e. first on knowledge and then on instruction data; or combined tuning, i.e. on the mixture of both. While some improvements are exhibited, the method alone can be insufficient to fully mitigate factual errors.

For general-purpose LLMs, SFT is typically employed to improve the instruction-following capabilities as opposed to factual knowledge which is mostly learned in pre-training. However, this process may inadvertently reveal areas of knowledge not covered in the pre-training, causing the risk of behavior cloning, where a model feigns understanding and responds with hallucinations to questions it has little knowledge of (Torabi et al., 2018). R-tuning (Zhang et al., 2023a) is proposed to address this issue with two pivotal steps: first, assessing the knowledge gap between the model's parametric knowledge and the instruction tuning data, and second, creating a refusal-aware dataset for SFT. It enables LLMs to abstain from answering queries beyond their parametric knowledge scope. On the other hand, BeInfo (Razumovskaia et al., 2023) improve factual alignment through the form of behavioral fine-tuning. The creation of the behavioral tuning dataset emphasizes two goals: selectivity (choosing correct information from the knowledge source) and response adequacy (informing the user when no relevant information is available or asking for clarification). Both methods effectively control LLMs on non-parametric questions but require extra effort in dataset curation and might hinder the models' retention of parametric knowledge.

Sycophancy (Sharma et al., 2023), another source of factuality errors, often arises from misalignments during SFT and RLHF(Ouyang et al., 2022). This is partially attributed to human annotators' tendency to award higher scores to responses they like rather than those that are factually accurate. (Wei et al., 2023) explore the correlation of sycophancy with model scaling and instruction tuning. They propose a synthetic-data intervention method, using various NLP tasks to teach models that truthfulness is independent of user opinions. However, one limitation is that the generalizability of their approach remains unclear for varied prompt formats and diverse user opinions.

(Tian et al., 2023) utilize direct preference optimization (DPO) (Rafailov et al., 2023) with the feedback of factuality score either from automatic fact-checkers or LLMs predictive confidence. In-domain evaluation shows promising results on biographies and medical queries, but generalization performance across domains and unseen domains is under-explored. (Köksal et al., 2023) propose hallucination-augmented recitations (HAR). It encourages the model to attribute to the contexts rather than its parametric knowledge, by tuning the model on the counterfactual dataset created leveraging LLM hallucinations. This approach offers a novel way to enhance LLM attribution and grounding in open-book QA. However, challenges lie in refining counterfactual generation for consistency and expanding its application to broader contexts.

\paragraph{Retrieval Augmentation}
Incorporating retrieval mechanisms during fine-tuning has been shown to enhance the LLM factuality on downstream tasks, particularly in open-domain QA. DPR (Karpukhin et al., 2020) refines a dual-encoder framework, consisting of two BERT models. It employs a contrastive loss to align the hidden representations of questions and their corresponding answers, obtained through the respective encoder models. RAG (Lewis et al., 2020) and FiD (Izacard and Grave, 2020) study a fine-tuning recipe for retrieval-augmented generation models, focusing on open-domain QA tasks. WebGPT (Nakano et al., 2021) fine-tunes GPT-3 (Brown et al., 2020) by RLHF, providing questions with factually correct long-form reference generation. The implementation in a text-based web-browsing environment allows the model to search and navigate the web.

\subsection{Inference}

We categorize approaches to improve factuality during inference into two: (1) optimizing decoding strategies to strengthen model factuality; and (2) empowering LLM learned ability by either in-context learning (ICL) or self-reasoning.

\subsubsection{Decoding Strategy}

Sampling from the top subword candidates with a cumulative probability of p, known as nucleus sampling (top-p) (Holtzman et al., 2020), sees a decrease in factuality performance compared to greedy decoding, despite higher diversity. This is likely due to its over-encouragement of randomness. Building on the hypothesis that sampling randomness may damage factuality when generating the latter part of a sentence than the beginning, (Lee et al., 2022) introduce factual-nucleus sampling, which dynamically reduces the nucleus-p value as generation progresses to limit diversity and improve factuality, modulating factual integrity and textual diversity.

Apart from randomness, some errors arise when knowledge conflicts, where context contradicts information present in the model's prior knowledge. Context-aware decoding (CAD) (Shi et al., 2023) prioritizes current context over prior knowledge, and employs contrastive ensemble logits, adjusting the weight of the probability distribution when predicting the next token with or without context. Despite the factuality boost, CAD is a better fit for tasks involving knowledge conflicts and heavily reliant on high-quality context.

In contrast, DoLa (Chuang et al., 2023) takes into account both upper and lower (earlier) layers, as opposed to only the final (mature) layer. This method dynamically selects intermediate layers at each decoding step, in which an appropriate premature layer contains less factual information with maximum divergence among the subset of the early layers. This method effectively harnesses the distinct contributions of each layer to factual generations. However, DoLa increases the decoding time by 1.01x to 1.08x and does not utilize external knowledge, which limits its ability to correct misinformation learned during training.

\subsubsection{ICL and Self-reasoning}

In context learning (ICL) allows an LLM to leverage and learn from demonstration examples in its context to perform a particular task without the need to update model parameters. (Zheng et al., 2023) present that it is possible to perform knowledge editing via ICL through facts included in demonstration examples, thereby correcting fake or outdated facts. The objective of demonstration examples is to teach LLMs how to: (1) identify and copy an answer; (2) generalize using in-context facts; (3) ignore irrelevant facts in context. While it is rather easy for LLMs to copy answers from contexts, changing predictions of questions related to the new facts accordingly, and keeping the original predictions if the question is irrelevant to the modified facts, remains tough.

Another line of research leverages the self-reasoning capability of LLMs. (Du et al., 2023) improve LLM factuality through multi-agent debate. This approach first instantiates a number of agents and then makes them debate over answers returned by other agents until a consensus is reached. One interesting finding is that more agents and longer debates tend to lead to better results. This approach is orthogonal and can be applied in addition to many other generation methods, such as complex prompting strategy (e.g., CoT (Wei et al., 2022), ReAct (Yao et al., 2023), Reflexion (Shinn et al., 2023)) and retrieval augmentation.

\textbf{Take-away:} Zheng et al. (2023) evaluate the effectiveness of knowledge editing on subject-relation-object triplets, an unrealistic setting compared to open-ended free-form text assessment. Previous methods (Mitchell et al., 2021; Meng et al., 2022) use finetuning over texts containing specific text to improve factuality. The relationship between SFT and ICL may also been an interesting avenue to explore. More specifically, we seek answers to two research questions: (1) What types of facts and to what extent can facts be edited effectively, learned by LLMs through ICL? (2) Would SFT do a better job at learning from examples that are difficult for ICL? More broadly, what is the best way to insert new facts or edit false knowledge stored in LLMs. The community may also benefit from an in-depth comparative analysis of the effectiveness of improving factuality between SFT and ICL (perhaps also RLXF).

Retrieval Augmentation can be applied before, during, and after model generation. One commonly used option is to apply retrieval augmentation prior to response generation. For questions requiring up-to-date world knowledge to answer, (Vu et al., 2023) augment LLM prompts with web-retrieved information and demonstrate the effectiveness on improving accuracy on FreshQA, where ChatGPT and GPT-4 struggle due to their lack of up-to-date information. (Gao et al., 2023a) place all relevant paragraphs in the context and encourage the model to cite supporting evidence, instructing LLMs to understand retrieved documents and generate correct citations, thereby improving reliability and factuality. Pre-generation retrieval augmentation is beneficial as the generation process is conditioned on the retrieval results, implicitly constraining the output space. While improving factual accuracy, this...

\begin{figure}[h]
\centering
\begin{center}
\fbox{\parbox{0.4\textwidth}{\centering
Claim Processor \
 \
Retriever \
 \
Verifier \
 \
Doc \
 \
Decompose
}}
\end{center}
\caption{IMAGE NOT PROVIDED (Diagram elements reconstructed from text)}
\label{fig:diagram}
\end{figure}

\section*{[MISSING CONTENT - FILE TRUNCATED]}
\textit{[The remainder of the paper, including the rest of the Retrieval Augmentation section, potential Conclusion, and full references, was not available in the source.]}

% Attempting to reconstruct references based on metadata and snippets
\bibliographystyle{plain}
\bibliography{refs}

\end{document}
=====END FILE=====

=====FILE: refs.bib=====
@article{tonmoy2024comprehensive,
title={A comprehensive survey of hallucination in large language models},
author={Tonmoy, SM and Zaman, SM and Jain, Vinija and Rani, Anku and Rawte, Vipula and Chadha, Aman and Das, Amitava},
journal={arXiv preprint arXiv:2401.01313},
year={2024}
}

@article{huang2023survey,
title={A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions},
author={Huang, Lei and Yu, Weijiang and Ma, Weitao and Zhong, Weihong and Feng, Zhangyin and Wang, Haotian and Chen, Qianglong and Peng, Weiguang and Feng, Xiaocheng and Qin, Bing and others},
journal={arXiv preprint arXiv:2311.05232},
year={2023}
}

@article{wang2023survey,
title={A survey on large language model hallucination},
author={Wang, Hongbin and Liu, Chang and Xi, Nuo and Qi, Zitao},
journal={arXiv preprint arXiv:2309.01219},
year={2023}
}

@article{chen2023felm,
title={Felm: Benchmarking factuality evaluation of large language models},
author={Chen, Shiqi and Zhao, Yiran and Zhang, Jinghan and Popov, I and others},
journal={arXiv preprint arXiv:2310.00741},
year={2023}
}

@article{gao2023retrieval,
title={Retrieval-augmented generation for large language models: A survey},
author={Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang and Pan, Jinliu and Bi, Yuxi and Dai, Yi and Sun, Jiawei and Guo, Haofen},
journal={arXiv preprint arXiv:2312.10997},
year={2023}
}

@article{rawte2023troubling,
title={The troubling emergence of hallucination in large language models-an extensive definition, quantification, and prescriptive remediations},
author={Rawte, Vipula and Sheth, Amit and Das, Amitava},
journal={arXiv preprint arXiv:2310.04988},
year={2023}
}

@article{zhang2023siren,
title={Siren's song in the AI ocean: A survey on hallucination in large language models},
author={Zhang, Yue and Li, Yafu and Cui, Leyang and Cai, Deng and Liu, Lemao and Fu, Tingchen and Huang, Minlie and Shi, Shuming},
journal={arXiv preprint arXiv:2309.01219},
year={2023}
}

@inproceedings{min2023factscore,
title={FActScore: Fine-grained atomic evaluation of factual precision in long form text generation},
author={Min, Sewon and Krishna, Kalpesh and Lyu, Xinxi and Lewis, Mike and Yih, Wen-tau and Koh, Pang Wei andIyyer, Mohit and Zettlemoyer, Luke and Hajishirzi, Hannaneh},
booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
pages={12076--12100},
year={2023}
}

@article{wang2023factcheckgpt,
title={Factcheck-gpt: End-to-end fine-grained document-level fact-checking and correction of llms},
author={Wang, Yuxia and Georgiev, Georgi and Nakov, Preslav},
journal={arXiv preprint arXiv:2311.09000},
year={2023}
}

@article{chern2023factool,
title={Factool: Factuality detection in generative ai-a tool augmented framework for multi-task and multi-domain scenarios},
author={Chern, I and Chern, Steffi and Chen, Shiqi and Yuan, Weizhe and Feng, Buru and Zhou, Chunyuan and Liu, Pengfei},
journal={arXiv preprint arXiv:2307.13528},
year={2023}
}

@article{li2023halueval,
title={Halueval: A large-scale hallucination evaluation benchmark for large language models},
author={Li, Junyi and Cheng, Xiaoxue and Zhao, Wayne Xin and Nie, Jian-Yun and Wen, Ji-Rong},
journal={arXiv preprint arXiv:2305.11747},
year={2023}
}

@article{vu2023freshllms,
title={Freshllms: Refreshing large language models with search engine augmentation},
author={Vu, Tu and Iyyer, Mohit and Wang, Xuezhi and Constant, Noah and Wei, Jason and Wei, Jason and Tar, Chris and Cer, Daniel and Thoppilan, Romal and others},
journal={arXiv preprint arXiv:2310.03214},
year={2023}
}

@article{yin2023large,
title={Do large language models know what they don't know?},
author={Yin, Zhangyue and Sun, Qiang and Guo, Qipeng and Wu, Jiawen and Qiu, Xipeng and Huang, Xuanjing},
journal={arXiv preprint arXiv:2305.18153},
year={2023}
}

@article{sharma2023towards,
title={Towards understanding sycophancy in language models},
author={Sharma, Mrinank and Tong, Meg and Korbak, Tomasz and Du, David and Perez, Ethan and Evans, Owain},
journal={arXiv preprint arXiv:2310.13548},
year={2023}
}

@article{ouyang2022training,
title={Training language models to follow instructions with human feedback},
author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
journal={Advances in Neural Information Processing Systems},
volume={35},
pages={27730--27744},
year={2022}
}

@article{wei2023simple,
title={Simple synthetic data reduces sycophancy in large language models},
author={Wei, Jerry and Huang, Da and Lu, Yifeng and Zhou, Denny and Le, Quoc V},
journal={arXiv preprint arXiv:2308.03958},
year={2023}
}

@article{rafailov2023direct,
title={Direct preference optimization: Your language model is secretly a reward model},
author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Ermon, Stefano and Manning, Christopher D and Finn, Chelsea},
journal={arXiv preprint arXiv:2305.18290},
year={2023}
}

@article{chuang2023dola,
title={Dola: Decoding by contrasting layers improves factuality in large language models},
author={Chuang, Yung-Sung and Xie, Yujia and Luo, Hongyin and Kim, Yoon and Glass, James and He, Pengcheng},
journal={arXiv preprint arXiv:2309.03883},
year={2023}
}

@article{dhuliawala2023chain,
title={Chain-of-verification reduces hallucination in large language models},
author={Dhuliawala, Shehzaad and Komeili, Mojtaba and Xu, Jing and Raileanu, Roberta and Li, Xian and Celikyilmaz, Asli and Weston, Jason},
journal={arXiv preprint arXiv:2309.11495},
year={2023}
}

@article{du2023improving,
title={Improving factuality and reasoning in language models through multiagent debate},
author={Du, Yilun and Li, Shuang and Torralba, Antonio and Tenenbaum, Joshua B and Mordatch, Igor},
journal={arXiv preprint arXiv:2305.14325},
year={2023}
}

@article{meng2022locating,
title={Locating and editing factual associations in GPT},
author={Meng, Kevin and Bau, David and Andonian, Alex and Belinkov, Yonatan},
journal={Advances in Neural Information Processing Systems},
volume={35},
pages={17359--17372},
year={2022}
}

@article{mitchell2021fast,
title={Fast model editing at scale},
author={Mitchell, Eric and Lin, Charles and Bosselut, Antoine and Finn, Chelsea and Manning, Christopher D},
journal={arXiv preprint arXiv:2110.11309},
year={2021}
}

@article{ji2023survey,
title={Survey of hallucination in natural language generation},
author={Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Ye Jin and Madotto, Andrea and Fung, Pascale},
journal={ACM Computing Surveys},
volume={55},
number={12},
pages={1--38},
year={2023}
}
=====END FILE=====