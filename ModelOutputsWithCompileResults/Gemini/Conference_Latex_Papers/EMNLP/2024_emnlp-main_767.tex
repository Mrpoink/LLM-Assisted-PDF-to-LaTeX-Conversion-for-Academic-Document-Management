=====FILE: main.tex=====
\documentclass[10pt, twocolumn, letterpaper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\geometry{margin=1in}
\usepackage{times}
\usepackage{authblk}
\usepackage{hyperref}
\usepackage{natbib}

% Title setup
\title{Related Work and Citation Text Generation: A Survey}

\author[1,2]{Xiangci Li}
\author[1]{Jessica Ouyang}
\affil[1]{University of Texas at Dallas}
\affil[2]{Amazon Web Services}
\affil[ ]{\texttt{lixiangci8@gmail.com, jessica.ouyang@utdallas.edu}}

\date{}

\begin{document}

\maketitle

\begin{abstract}
To convince readers of the novelty of their research paper, authors must perform a literature review and compose a coherent story that connects and relates prior works to the current work. This challenging nature of literature review writing makes automatic related work generation (RWG) academically and computationally interesting, and also makes it an excellent test bed for examining the capability of SOTA natural language processing (NLP) models. Since the initial proposal of the RWG task, its popularity has waxed and waned, following the capabilities of mainstream NLP approaches. In this work, we survey the zoo of RWG historical works, summarizing the key approaches and task definitions and discussing the ongoing challenges of RWG.
\end{abstract}

\section{Introduction}

Academic research is an exploratory activity to solve problems that have never been resolved before. Each academic research paper must sit at the frontier of the field and present novelties that have not been addressed by prior work; to convince readers of the novelty of the current work, the authors must perform a literature review to compare their work with the prior work. In natural language processing (NLP), a short literature review is usually conducted under the "Rel... [MISSING]

\vspace{2cm}
\begin{center}
\fbox{\parbox{0.8\linewidth}{\centering \textbf{[CONTENT MISSING]} \ The content between the Introduction and the References (Pages 2--10) is not available in the provided source material.}}
\end{center}
\vspace{2cm}

\section*{References}
% Manually reconstructing the bibliography from the provided snippet (Page 11)

\begin{description}
\itemsep0.5em

```
\item[] Ben Goodrich, Vinay Rao, Peter J. Liu, and Mohammad Saleh. 2019. Assessing the factual accuracy of generated text. In \textit{Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining}, KDD '19, page 166-175, New York, NY, USA. Association for Computing Machinery.

\item[] Max Grusky, Mor Naaman, and Yoav Artzi. 2018. Newsroom: A dataset of 1.3 million summaries with diverse extractive strategies. In \textit{Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, Volume 1 (Long Papers), pages 708-719, New Orleans, Louisiana. Association for Computational Linguistics.

\item[] Nianlong Gu and Richard H. R. Hahnloser. 2023. Controllable citation sentence generation with language models.

\item[] Cong Duy Vu Hoang and Min-Yen Kan. 2010. Towards automated related work summarization. In \textit{Coling 2010: Posters}, pages 427-435, Beijing, China. Coling 2010 Organizing Committee.

\item[] Yue Hu and Xiaojun Wan. 2014. Automatic generation of related work sections in scientific papers: an optimization approach. In \textit{Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)}, pages 1624-1633.

```

\end{description}

\end{document}
=====END FILE=====