=====FILE: main.tex=====
\documentclass[10pt, a4paper, twocolumn]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\geometry{left=2cm, right=2cm, top=2.5cm, bottom=2.5cm}
\usepackage{times}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{natbib}
\usepackage{float}
\usepackage{url}

% Title and Author formatting
\title{\textbf{To Word Senses and Beyond: Inducing Concepts\with Contextualized Language Models}}
\author{Bastien Liétard \and Pascal Denis \and Mikaela Keller \
University of Lille, Inria, CNRS, Centrale Lille, \
UMR 9189 - CRISTAL, F-59000 Lille, France \
\texttt{first_name.last_name@inria.fr}}
\date{}

\begin{document}

\maketitle

\begin{abstract}
Polysemy and synonymy are two crucial inter-related facets of lexical ambiguity. While both phenomena are widely documented in lexical resources and have been studied extensively in NLP, leading to dedicated systems, they are often being considered independently in practical problems. While many tasks dealing with polysemy (e.g. Word Sense Disambiguation or Induction) highlight the role of word's senses, the study of synonymy is rooted in the study of concepts, i.e. meanings shared across the lexicon. In this paper, we introduce Concept Induction, the unsupervised task of learning a soft clustering among words that defines a set of concepts directly from data. This task generalizes Word Sense Induction. We propose a bi-level approach to Concept Induction that leverages both a local lemma-centric view and a global cross-lexicon view to induce concepts. We evaluate the obtained clustering on SemCor's annotated data and obtain good performance (BCubed  above 0.60). We find that the local and the global levels are mutually beneficial to induce concepts and also senses in our setting. Finally, we create static embeddings representing our induced concepts and use them on the Word-in-Context task, obtaining competitive performance with the State-of-the-Art.
\end{abstract}

\section{Introduction}

A crucial challenge in understanding natural language comes from the fact that the mapping between word forms and lexical meanings is many-to-many, due to polysemy (i.e., the multiplicity of meanings for a given form)\footnote{In this paper, we take polysemy in its most comprehensive definition, also including homonymy.} and synonymy (i.e., the multiplicity of forms for expressing a given meaning). Both polysemy and synonymy have been thoroughly studied in NLP, but mostly as independent problems, giving rise to dedicated systems.

Thus, Word Sense Disambiguation (WSD) aims at correctly mapping word occurrences to one of its senses \citep{raganato2017}, while Word Sense Induction (WSI), its unsupervised counterpart, aims at clustering word occurrences into latent senses directly from data \citep{manandhar2010,jurgens2013}. More recently, researchers have proposed the task of Word-in-Context (WiC), which consists in classifying pairs of word occurrences depending on whether they realize the same sense or not \citep{pilehvar2019}. All these works take a word centric view, which aims at identifying or characterizing the different senses of a given word, where these senses are bound to a word.

Another line of work, which takes a broader lexicon-wide perspective, is concerned with identifying synonyms, which are equivalence classes over different words that point to the same concept \citep{zhang2021,ghanem2023}, where concepts are semantic entities that are not bound to a word. In WordNet \citep{miller1995,fellbaum1998}, concepts are called synsets, defined as sets of synonyms. However, outside of lexical resources, synonymy and polysemy are usually considered as independent problems in the NLP literature. Yet, these two views are complementary. In lexicology, they correspond to two perspectives on the word-meaning mapping: semasiology and onomasiology. The former is the word-to-meanings view, where one can observe polysemy by looking at the different meanings a given word has. The latter is the meaning-to-words view, in which one can study synonymy by looking at the inventory of words that speakers use to express the same meaning.

In this paper, we propose a new task, called Concept Induction, that directly aims at learning concepts in an unsupervised manner from raw text. More precisely, this task aims at learning a soft clustering over a target lexicon (i.e., a set of words), in such a way that each cluster corresponds to a (latent) concept. Thus, this task both addresses polysemy (since polysemous words should appear in multiple clusters) and synonymy (since synonymous words should appear in the same cluster(s)).

Inducing concepts can be interesting for many external applications, like building lexical resources for low-resources languages \citep{velasco2023}, and can bring a different perspective in computational studies of meaning, moving the usual word-centric focus to a more meaning-centric state.

Our approach to Concept Induction relies on word occurrences for a target lexicon, represented as word embeddings derived from a Contextualized Language Model (in this case, BERT Large \citep{devlin2019}), which are then grouped, using hard clustering algorithms, into concept denoting clusters. While these concept clusters could in principle be obtained directly from word occurrences, we propose a bi-level methodology that leverages both a local, lemma-centric clustering (i.e., operating on only specific word occurrences), and a global, cross-lexicon clustering (i.e., operating on all words occurrences). From this perspective, our approach generalizes, and in fact builds upon classical Word Sense Induction, in that word senses are learned jointly alongside with concepts. We hypothesize that an approach taking both complementary resolutions in account will lead to improved Concept Induction and Word Sense Induction, i.e. that the two objectives can be mutually beneficial.

To validate our approach, we carried out experiments on the SemCor dataset, which provides a set of concepts (taking the form of WordNet synsets) related to word occurrences. We found that our bi-level clustering approach accurately learn concepts, achieving  scores above 0.60 on the task of Concept Induction compared to WordNet's synsets, outperforming competing approaches that use only local and global views. This demonstrates the benefits of our bi-level approach, and its ability to leverage both local and global views when inducing concepts. Interestingly, we show that the benefits go both ways: our proposed approach outperforms lemma-centric approaches when evaluated for WSI. Finally, we show that concept-aware static embeddings derived from our approach are also competitive with state-of-the-art approaches efficient on the Word-in-Context task, while using less training data. Through the new task of concept induction, we also contribute in a new way to the ongoing debate regarding the ability to align vector representations extracted from Contextualized Language Models to the semantic representations posited by (psycho-)linguists. In this vein, we conduct a qualitative evaluation of obtained clusters to ensure they indeed reflect concepts and gather synonyms. The source code we used for experiments is available at \url{[https://github.com/blietard/concept-induction](https://www.google.com/search?q=https://github.com/blietard/concept-induction)}.

\section{Related Work}

\subsection{Lexical resources for concepts}
Princeton's WordNet (PWN) \citep{miller1995,fellbaum1998} is a lexical database that has been the most widely used as a reference for most wordsense-related tasks for many years. In WordNet, the entry corresponding to a lemma has different wordsenses, each of them mapping to a synset. Synsets are WordNet's equivalents of our concepts. Lemmas whose wordsenses belong to the same synset are synonymous. WordNet 3.0 contains 117,659 synsets and is built from the work of psycholinguists and lexicographers, that not only describes synonymy but also other lexical relations such as hypernymy/hyponymy, antonymy, meronymy/holonymy, etc. But the amount of resources needed to create such lexical databases with human experts is considerable, making them a very rare and precious resource. They are not available for a large number of active languages, and even more rare for dead languages \citep{bizzoni2014,khan2022}.

\subsection{Word senses with Language Models}
With the recent development of neural Contextualized Language Models (CLM), several work use their hidden-layers to extract vector representations of word usages and retrieve word senses. These representations are fed to a classification (for WSD) or a clustering (in the case of WSI) algorithm to distinguish the word's senses \citep{scarlini2020,nair2020,saidi2023}. These embeddings-based approaches have applications in other fields: \citet{kutuzov2020} and \citet{martinc2020} use sense clusters found using CLM embeddings to study the change in meaning of words, and \citet{chronis2020} propose a many-Kmeans method to investigate semantic similarity and relatedness. Another line of work uses list of substitute tokens sampled from the CLM head to infer senses \citep{amrami2019,eyal2022} and are successful on WSI benchmarks like \citet{manandhar2010} and \citet{jurgens2013}.

\subsection{Structures of Meaning in CLM}
Recent research probes neural CLMs for alignments between representations from their latent spaces and semantic patterns and relations. Section 7.2 of \citet{haber2024} summarizes findings about polysemy in contextualized CLMS, showing that these models were able to detect polysemy and in some cases distinguish actual polysemy from homonymy. They report that representations from different senses may however overlap. \citet{hanna2021} shows that pretrained BERT embed knowledge of hypernymy but is limited to the more common hyponyms.

\citet{velasco2023} build on top of WSI techniques in an attempt to automatically construct a WordNet for Filipino, thus proposing a modeling of synonymy in this language. However, the evaluation of the synsets they obtained is limited by the lack of sense-annotated data for Filipino, and they could not evaluate the impact of their methodology on the two levels (senses and concepts).

Works like \citet{ethayarajh2019} and \citet{chronis2020} study the kind of information that was distributed across layers. The former concludes that syntactic and word-order information are distributed in the first layers while in deeper layers, representations are heavily influenced by contexts. The latter demonstrates, with a multi-prototypes embedding approach, that semantic similarity is best found in moderately late layers, while relatedness is best found in last layers.

\section{Concept Induction}

Our main motivation behind Concept Induction is to present a view of the mapping between words and their meaning(s).\footnote{This mapping is called patterns of lexification by François (2022); see also coexpression and synexpression in the terminology proposed by Haspelmath (2023).} This view is systemic, meaning that it should not be defined for individual words neither for individual concepts, but rather acknowledging these as a whole with interactions and relations. This extends beyond the primary objective of WSI, which defines word senses as pertaining to individual words only and does not explore relations between lemmas or concepts.

\subsection{Basic notions}
Consider a set of target words (or lemmas) and for each lemma, we have a set of occurrences of this word in a context (e.g. a sentence or a phrase). The set of target lemmas is referred to as the lexicon, while the corpus is the set of all occurrences. Our goal is to study the meaning of target words as they are used in the corpus.

In this study we call sense of a word its usage to refer to a concept. A polysemous word has multiple senses, each of them referring to a distinct concept. Two words are said to be synonyms for a given concept when each of them has one of their senses referring to this shared concept. Senses are defined "locally", i.e. bound to an individual word of the lexicon, as opposed to concepts which are defined "globally", i.e. across the whole lexicon. An occurrence of a word  realizes one of 's senses.

Consider the words "test" and "trial" and the following corpus: (A) the jury found them guilty in a fair trial. (B) candidates competed in a trial of skill. (C) the hero underwent a test of strength. The corpus is composed of two occurrences of "trial" and one occurrence of "test." In the corpus, "trial" is polysemous. Its first sense, illustrated in A, refers to a process of law. Its second sense, in B, refers to the concept of the act of undergoing testing. The sense of "test" in sentence C also corresponds to this concept: it's a case where "test" and "trials" are synonymous. Shifting the focus from senses to concepts, we will say that B and C instantiate the same concept, while A is an instance of a different concept.

\subsection{Task definition}
The goal of Concept Induction (CI) is to automatically learn a set of concepts directly from the data, i.e. learning a soft clustering  in the set of target words  that should correspond to the multiple concepts instantiated by occurrences of the corpus.  is a soft clustering because a word can be assigned to several clusters (when it is polysemous). Using a different perspective than WSI, the framework of Concept Induction provides a more complete view on meaning across the lexicon. Both WSI and CI capture polysemy, but CI also reveals synonymy across the lexicon. Like WSI, Concept Induction does not require a pre-defined set of concepts.

\subsection{Formal framework}
Let  be the lexicon. For all word  in , we denote  the -th occurrence of  in the corpus. We define  the set of  occurrences of . The corpus, denoted , is the union of all .

For a given word , the set  can be partitioned according to its different senses. We denote  the part of occurrences of  in the corpus corresponding to the -th sense of . We refer to these groups of occurrences as the sense clusters of . The set  forms a partition of  and we call  the set of all sense clusters of all words, i.e. .  is a "local" (lemma-centric) partition of the whole . The task of Word Sense Induction aims at learning the partition  given a corpus .

\begin{figure*}[t]
\centering
\fbox{\parbox{0.9\textwidth}{\centering
\vspace{1cm}
\textbf{[IMAGE NOT PROVIDED]} \
\textit{The original figure contains an illustration of the framework involving "trial" and "test" occurrences.}
\vspace{1cm}
}}
\caption{Illustration of our framework. The words "trial" is polysemous and has two senses corresponding to two different concepts, and is synonym with "test" for this second meaning.}
\label{fig:framework}
\end{figure*}

In this work, we aim at dividing the corpus into concepts instead of senses. We denote  the group of occurrences of words corresponding to the concept indexed by , and  the partition of  in  concept clusters. Unlike sense clusters of , a concept cluster  can gather occurrences of different words:  is a "global" partition.

Each occurrence  of a word  is associated to a sense cluster  and a concept cluster . We can say that a concept corresponding to  is instantiated by occurrence  through the sense corresponding to , or conversely that  uses the sense reflected in  to mean the concept described by concept cluster . All occurrences of sense cluster  appear in the same concept cluster .

In summary,  and  are partitions of  and are naturally constrained as follows:
\begin{enumerate}
\item By definition, a sense in  is associated to one and only one word .
\item An occurrence  realizes exactly one sense .
\item An occurrence  instantiates exactly one concept .
\item In a given sense  all occurrences are assigned to the same concept .
\item All  (i.e. same word) refer to distinct concepts.
\end{enumerate}

From the partition  on occurrences, one can derive  a clustering of the set of words  into concepts. To each concept cluster  we associate a cluster in  that contains all lemmas of  whose occurrences were assigned to . In , a polysemous word with  senses appears in  distinct clusters (one per sense), and synonyms appear in at least one common cluster (one per shared concept). We denote  the word-level soft-clustering and  the partition of occurrences that are learned on the data.

In Figure \ref{fig:framework} we illustrate this framework, using a corpus of occurrences of the words "test" and "trial". In this scenario,  and two concepts are instantiated: a process of law to determine someone's guilt and a challenge to evaluate a skill. The lemma "trial" exhibits two senses as it has occurrences corresponding to both concepts: "trial" is polysemous. The second concept is also instantiated by occurrences of "test", therefore "trial" and "test" show synonymy in this case. This toy example also follows all constraints formulated above.

\section{Methodology}

In this section we describe the methods we propose and evaluate for Concept Induction. We learn a clustering  drawing inspiration from the relations between , ,  and . In particular, the overall objective of our methodology consist in finding  (i.e. partition occurrences into concept clusters) to derive . Section 3.3 highlighted that there are two levels of partitions: a local level (senses) and a global one (concepts). The proposed approaches rely on both levels and the use of a Contextualized Language Model (CLM) to gather representations of occurrences influenced by the context.

\subsection{Proposed Bi-level Method}

\paragraph{Local (lemma-centric) clustering} Firstly, we propose to learn a word-sense partition for each target words individually. Using the CLM hidden layers, we extract a vector representation (the occurrence embedding) of every occurrence . We then learn a partition  of each  using a clustering algorithm on the embeddings. Each  describes the locally estimated sense clusters of word . Jointly considering these partitions for all , we obtain a partition  of the whole set of occurrences . This partition is local in the sense that each word has its occurrences clustered independently from other words.

\paragraph{Global (cross-lexicon) clustering} Once we have a local clustering , we turn from considering words independently to consider all words together. In this step, we learn a global clustering by merging local clusters of occurrences. To do so, we average embeddings of all occurrences in the same local cluster to get a single embedding representing each local cluster. Then we run a second clustering algorithm, this time using the averaged embeddings of local clusters. This global clustering defines a new partition  of the the corpus : when two local clusters  and  are merged into the same global cluster  (because their embeddings were clustered together), all their occurrences are assigned to global cluster . From this global occurrence partition  we can easily extract , a word-level soft-clustering of lemmas whose occurrences appear in the same .

This Bi-level method directly implements the system of constraints described in Section 3.3. Only constraint 5 is not enforced by design. Indeed, our local clusters being learned and not informed by an expert, the local clustering step may make errors, especially if the data for a given word are sparse. Allowing the global clustering to merge local clusters enables the correction of local clustering's recall errors using information from the global level. We also want to highlight that the proposed methodology is generic, in the sense that it is not tied to a specific choice of clustering algorithm.

\subsection{Local-only and Global-only}

\paragraph{Local-only} Sense-inducing systems (WSI approaches) that create only local clusters of occurrences for each word are said to be Local-only systems. We use them as baseline models that only produce word-level clusters of size 1 and do not reflect synonymy, but still learn polysemy.

\paragraph{Global-only} On the other hand, consider a system in which each occurrence is mapped to its own local cluster (i.e. no actual local clustering step), and the global step divides occurrences directly into global clusters. We refer to this kind of system as Global-only approaches. They allow to evaluate how useful the local clustering step is in the process: we hypothesize that the local step in Bi-level will reduce potential variance in occurrences by aggregating them, increasing Precision compared to Global-only.

\section{Experiments}

In this section, we evaluate the abilities of the proposed methods to induce concepts and compare the proposed bi-level approach to other methods. We investigate the advantages of the bi-level approach not only for the global viewpoint but also in the local setting.

\subsection{Settings}

\paragraph{Data} We choose to use the annotated part of the SemCor 3.0\footnote{\url{[http://web.eecs.umich.edu/~mihalcea/downloads.html](https://www.google.com/search?q=http://web.eecs.umich.edu/~mihalcea/downloads.html)#semcor}} corpus. This dataset contains occurrences for a wide number of words, and morpho-syntactic annotations provide their lemma and their Part-of-Speech tag. Among all lemmas having at least 10 annotated occurrences, we keep only nouns (excluding proper nouns) composed only of alphabetical characters with a minimum length or 3 letters.\footnote{For the sake of simplicity and clarity, this study is focused only on nouns. Indeed, other Parts-of-Speech induce extra difficulties. Verbs for instance required extra preprocessing steps and decisions (e.g. include or exclude gerundive uses, past participle employed like adjectives, etc.). Extension of experiments to other PoS is left to future work.} The resulting lexicon  contains 1,560 different lemmas, for which we gather a corpus  containing a total of 52,997 occurrences. SemCor is also semantically annotated, with each occurrence of a target lemma assigned to a synset in WordNet, that we consider to be the concept it refers to. We derive a reference partition of the occurrences  and a reference soft-clustering of the words  from annotations, for a total of 3,855 different concepts (WordNet's synsets) covered in . This set of concepts is the subset of WordNet corresponding to the textual data.

\paragraph{Evaluation of Concept Induction} We compare the learned word clustering  to the reference . We choose to use the BCubed metrics \citep{bagga1998}, obtaining Precision and Recall for the evaluated clustering compared to the reference, as well as an  score. To account for overlapping clusters, we use the Extended BCubed metrics proposed by \citet{amigo2009}, which has already been used as evaluation in SemEval2013 WSI task \citep{jurgens2013}. Using BCubed metrics, for a given evaluated clustering, low precision would mean that grouped lemmas should not have been clustered together because none of their occurrence annotations map to a shared concept according to annotations. Low recall means that the evaluated system fails to capture clusters of lemmas whose occurrences share a concept according to annotations. The number of common clusters between two words also impacts BCubed metrics: if two lemmas appear together in too many clusters compared to the reference clustering, precision is decreased; if the number of common clusters is too low, recall is decreased.

\paragraph{Development} To learn the clustering, candidate systems have access to the full set of occurrences-in-context\footnote{Sentences in which the lemma appears, paired with its position within them. If the lemma appears multiple times in the same sentence, we create several distinct occurrences, where only the position varies.} but not their annotations. To choose the appropriate set of hyperparameters, we create a Dev split of the annotations by randomly sampling 10% of concepts and revealing semantic annotations of the corresponding occurrences. We use them to evaluate Concept Induction for this small set of concepts, and choose the set of hyperparameters that scores best in BCubed .

\paragraph{Evaluation splits} In the final evaluation phase, we compute scores on all concepts / all occurrences, including the Dev split, as concepts in it are part of the whole subset of WordNet described by SemCor's annotations. In the full data, we found that 88% of the concepts were instantiated using only a single lemma. To better evaluate cases of synonymy, we also evaluate systems on a subset of the corpus, denoted "Synon", that contains only occurrences of concepts showing synonymy (the remaining 12% of concepts, instantiated through at least 2 distinct lemmas). Statistics are provided in Table 5 in Appendix B. Note that it only changes the set of concepts/lemmas for which the system is being evaluated, not the clustering's training data.

\subsection{Systems and baselines}

\paragraph{Clustering Algorithms} We try two different clustering algorithms relying on different paradigms: Kmeans (used in \citet{chronis2020}), a centroid-based algorithm with a fixed number of clusters, and Agglomerative clustering (used in \citet{saidi2023}; \citet{velasco2023}; dubbed "Agglo" for short), a deterministic hierarchical approach using a distance threshold to create a dynamic number of clusters instead of using a fixed one. Another difference between Kmeans and Agglo is that the former assumes that expected clusters are of nearly-spherical shape and balanced in number of points, while the latter does not make assumptions on the shape of data. Details of tested hyperaprameter values are provided in Appendix C.

\paragraph{Representations} Following \citet{chronis2020} and \citet{eyal2022}, we use BERT Large \citep{devlin2019}, a masked language model with 24 layers and 345M parameters. This allows for direct comparisons with these approaches. Also, BERT Large was found by \citet{haber2021} to allow for better grouping of sense interpretations than other LLMs.\footnote{We leave to further work the use of autoregressive and/or newer Language Models.} We average subwords' embeddings if needed. It is a common practice in previous work on semantic-related tasks to use the average of the last 4 layers to get embeddings; we decided to adopt the same "4 layers average pooling" strategy, but trying with different possible sets of layers (see Appendix C). Therefore, for a set of four layers, we average hidden states across the selected layers to get a single 1024-dimensional vector. We found that layers 14 to 17 obtained the best results on Dev for all methods (global/local-only and bilevel).

\paragraph{Sense-inducing systems} Comparison to Local-only systems will give a (strong) baseline just by inducing senses without aiming at concepts. We used the same clustering algorithms. We also implement the WSI method proposed by \citet{eyal2022}. It relies on a different paradigm, using the Language Model for substitution instead of word embeddings. From lists of substitutes, they build a graph of substitutes in which they find communities and then assign each occurrence to a community of substitutes to find the wordsenses. Because Local-only methods only induce senses, their hyperparameters are chosen to maximize a WSI objective on polysemous words of the dev split.

\paragraph{Baselines} We construct a candidate clustering  where each lemma has its own cluster. This baseline model is referred to as the "Lemmas" baseline. This is to evaluate the extent to which the information contained by the lemma alone can be used to induce concepts without any knowledge on word senses neither on context. As a second baseline, we create for each lemma as many singletons as the number of different concepts its occurrences are annotated with. All created clusters are of size 1: we account perfectly for polysemy but not at all for synonymy. This second baseline is dubbed "Oracle WSI".

\subsection{Concept Induction in SemCor}

In Table \ref{tab:ci_results} we display the Concept Induction scores  of proposed baselines and systems on the full SemCor data and on the Synon. split. On the full data, both the Lemmas and Oracle WSI baselines achieve very good performance because they have, by design, a perfect precision (they do not cluster lemmas at all and do not overestimate the number of clusters) and because 88% of concepts are instantiated with only a single lemma (thus their recall is still good). However, they are very limited on the Synon. split of the data, where concepts are instantiated with multiple lemmas.

\begin{table}[ht]
\centering
\small
\setlength{\tabcolsep}{3pt}
\begin{tabular}{lcccccc}
\toprule
& \multicolumn{3}{c}{\textbf{Full data}} & \multicolumn{3}{c}{\textbf{Synon.}} \
\textbf{Concept Induction} & \textbf{P} & \textbf{R} & \textbf{} & \textbf{P} & \textbf{R} & \textbf{} \
\midrule
\textbf{Baselines} & & & & & & \
Lemmas & 1.0 & .43 & .61 & 1.0 & .61 & .50 \
Oracle WSI & 1.0 & .75 & .86 & 1.0 & .39 & .56 \
\midrule
\textbf{Local-only Systems} & & & & & & \
Kmeans Local & .73 & .70 & .71 & .67 & .38 & .49 \
Agglo Local & .92 & .53 & .67 & .92 & .35 & .50 \
\citet{eyal2022} & .31 & .75 & .44 & .37 & .39 & .38 \
\midrule
\textbf{CI Systems} & & & & & & \
Kmeans Global & .48 & .65 & .56 & .68 & .54 & .60 \
Kmeans Bi-level & .70 & .59 & .64 & .82 & .59 & .47 \
Agglo Global & .60 & .61 & .60 & .82 & .50 & .62 \
Agglo Bi-level & .75 & .60 & .66 & .86 & .49 & .62 \
\bottomrule
\end{tabular}
\caption{Concept Induction BCubed Precision (P), Recall (R) and  the SemCor data averaged over 5 runs.}
\label{tab:ci_results}
\end{table}

The proposed Concept Induction systems reach scores ranging from .56 to .66 on the full data, half of them outperforming the Lemmas baseline, and from .59 to .62 on the Synon. split, outperforming all other systems. While still challenging, it exhibits that it is indeed possible to induce WordNet-based concepts in a corpus using LMs hidden layers vectors.

We also see that Kmeans-based approaches are consistently outperformed by Agglomerative methods. This indicates that the representational spaces in LM hidden layers are not organized in a nearly-spherical fashion as Kmeans algorithm assumes, but rather are populated less uniformly. This is reflected in precision and recall: Agglomerative systems reach a higher precision than Kmeans with similar recall.

Overall, results are in favor of Bi-level approaches over Global-only systems, with substantial improvements in  on the full data while obtaining (nearly) identical performance on concepts of multiple lemmas, and large increases in precision while the loss in recall is minimal. This demonstrates that considering the local (lemma-centric) perspective is beneficial to a global (cross-lexicon) view when inducing concepts. The local clustering, with the subsequent representation averaging, helps reducing variance in occurrences and therefore allow to reach higher levels of precision in the global clustering compared to Global-only. We would also like to emphasize that, while Global-only systems are more simple in design, their computational cost is usually higher than Bi-level ones, especially when the clustering algorithm's time complexity is quadratic with respect to the number of occurrences.

\subsection{Qualitative Analysis of Concepts Clusters}

We manually annotate word clusters (obtained from our best-performing approach, the Agglo Bi-level system) containing at least 2 lemmas according to the semantic similarity between lemmas. Distribution of cluster sizes (in number of lemmas) can be found in Appendix D. We distinguish four categories: synonyms when lemmas are cognitive synonyms (e.g. "necessity" and "need"), near-synonyms for lemmas close to be synonyms but showing slight difference in meaning (e.g. "duty" and "task", the former being stronger than the latter),\footnote{Notions of cognitive synonymy and near-synonymy are discussed by Stanojevic (2009).} related when lemmas show a topical (e.g. "dirt", "sand" and "mud") or lexical relations (e.g. antonyms like "man" and "woman") and invalid clusters when lemmas show no semantic relation (e.g. "child" and "idea").

\begin{table}[ht]
\centering
\small
\begin{tabular}{lcc}
\toprule
\textbf{Cluster size} & \textbf{2-3} & \textbf{4+} \
\midrule
Nb. of annotated clusters & 50 & 23 \
\midrule
\multicolumn{3}{l}{\textbf{Category (% of annotated clusters)}} \
Synonyms & 38 & 17 \
Near-synonyms & 24 & 35 \
Related & 26 & 48 \
Invalid & 8 & 0 \
\bottomrule
\end{tabular}
\caption{Qualitative manual evaluation of obtained word clusters of size .}
\label{tab:qualitative}
\end{table}

Proportions of these annotations are displayed in Table \ref{tab:qualitative} with respect to the cluster size, the number of lemmas in the cluster. For a given cluster size, if the number of clusters exceeds 50, we randomly sample 50 clusters to be annotated. Overall, the proportion of synonyms and near-synonyms is generally above 50% and less than 10% of clusters are invalid, indicating that most learned concepts are reliable and meaningful. We argue that the remaining related term clusters, while not synonyms, may still be interesting in less fine-grained studies. The portion of related clusters is in line with findings from previous work showing that BERT was also reflective of other lexical relations, such as hypernymy \citep{hanna2021}.

\subsection{Benefits at the Local Level}

We now turn back to the local level and assess whether the information brought at the global level helps distinguishing senses of individual words. Here we do not evaluate the word-level soft clustering, but the occurrence-level division of SemCor's data, considering each word independently. In other words, we evaluate WSI in SemCor using annotations as the reference sense clustering.

\paragraph{Evaluation of induced senses} For each word  we compare how its set of occurrences  is divided in  to how it is divided in the reference  provided by annotations using BCubed metrics, and we average scores obtained across . We display the WSI BCubed , as in previous WSI tasks like \citet{jurgens2013}. Following \citet{amrami2019}, we report  the Spearman correlation coefficient between the number of clusters a lemma is assigned to and its number of senses according to annotations, to ensure that the number of created senses actually scales with the actual degree of polysemy.

Note that, for CI systems, we evaluate the division of occurrences provided by the final clustering  (i.e. how occurrences are clustered after the global step and its potential merge operations). The quality of sense clusters induced by the local-step only is actually evaluated with Local-only systems.

\begin{table}[ht]
\centering
\small
\begin{tabular}{lcc}
\toprule
& \textbf{WSI } & \textbf{} \
\midrule
\textbf{Local-only Systems} & & \
Kmeans Local & .61 & NA\footnotemark \
Agglo Local & .77 & .04 \
\citet{eyal2022} & .46 & .51 \
\midrule
\textbf{CI Systems} & & \
Kmeans Global & .76 & .51 \
Kmeans Bi-level & .78 & .30 \
Agglo Global & .80 & .53 \
Agglo Bi-level & .80 & .46 \
\bottomrule
\end{tabular}
\caption{WSI BCubed  sense number correlation coefficient  on SemCor full data.}
\label{tab:wsi_results}
\end{table}
\footnotetext{Not computed for Kmeans because the number of cluster is constant.}

\paragraph{Local results} Results of this local evaluation are displayed in Table \ref{tab:wsi_results}. Let us recall that Local-only systems hyperparameters are chosen to maximize the WSI  the dev split, while those of CI systems maximize the Concept Induction . Nonetheless, one can observe that all CI systems outperform their Local-only counterparts, achieving higher WSI   even though their hyperparameters are not chosen to match the WSI itself. This indicates that the information brought at the global level by considering cross-lexicon relations may indeed help improving WSI, and benefits between local and global levels go both ways.

We explain the relatively poor performance of State-of-the-Art WSI system by the fact that we are in a particular setting, where the number of occurrences per lemma is relatively low in SemCor (30 per lemma on average) and so is the average number of occurrences per concept. Data sparsity is a favorable ground for word senses to be misrepresented. As such, methods meant to be applied on larger datasets like the one of \citet{eyal2022} may not work as well as expected. Our results show the limitations of these systems when the amount of training data is low and the interest of aiming at concepts to get senses. This scenario is motivated in areas where data are not available in large quantities and still require to induce senses. In the case of the study of Lexical Semantic Change (the evolution of word meanings over time), recent works perform WSI in diachronic corpora that are often unbalanced and small \citep{tahmasebi2021}.

\section{Extrinsic Evaluation with Concept-aware Embeddings}

In their work, \citet{eyal2022} derive sense-aware static embeddings from their WSI method, training them on the Wikipedia dataset and used them for the Word-in-Context (WiC) task. They achieve nearly-SotA results on the dataset proposed by \citet{pilehvar2019}, and report to be outperformed only by methods using external lexical knowledge and resources. We proceed to the same extrinsic evaluation of our work, constructing concept-aware embeddings using concept clusters of Concept Induction systems (Global-only and Bi-level Agglo). To obtain such embeddings, we average all vectors representating occurrences in SemCor contained each global cluster to get one vector per concept cluster.

The WiC task consists of determining whether two occurrences of a target lemma  correspond to the same sense. The WiC dataset's target words are nouns and verbs, but like in the rest of this paper, we restrict our scope to nouns. To solve the task, we use BERT Large to create representations of the two target occurrences. Each of them is assigned to a concept by finding the closest concept-aware using cosine distance. The decision depends on whether the two occurrences are mapped to the same concept (true) or to distinct ones (false).

\begin{table}[ht]
\centering
\small
\begin{tabular}{lc}
\toprule
\textbf{Model} & \textbf{Acc.} \
\midrule
\citet{eyal2022} (CBOW) & 59.3 \
\citet{eyal2022} (Skip-Grams) & 61.9 \
\midrule
Ours (Agglo global) & 58.8 \
Ours (Agglo bi-level) & 59.7 \
\bottomrule
\end{tabular}
\caption{Accuracy scores on the nouns of the WiC test dataset \citep{pilehvar2019}.}
\label{tab:wic_results}
\end{table}

Results are displayed in Table \ref{tab:wic_results}. Our concept-aware embeddings obtain very similar results to those of their sense-aware embeddings, with ours derived from our bi-level approach even outperforming their CBOW method. Interestingly, our embeddings were trained with far fewer resources than theirs, as we used 52,997 occurrences from the SemCor dataset while they used a dump of Wikipedia, gathering millions of occurrences. This emphasizes the value of concept-aware embeddings: the use of cross-lexicon information allows competitive results with fewer resources.

\section{Conclusion}

In this paper, we argued that, while word senses allow to investigate polysemy, concepts are a larger perspective that allows the study of polysemy as well as synonymy. We defined Concept Induction, the unsupervised task to learn a soft-clustering of words in a large lexicon, directly from their in-context occurrences in a corpus. Then, we proposed a formulation of this problem in terms of local (lemma-centric) and global (cross-lexicon) complementary views, and tested an approach that uses information from both levels using contextualized Language Models. On concept-annotated SemCor corpus, we found that this bi-level view was beneficial for Concept Induction, and even for Word Sense Induction with a low amount of training data. We validated the quality of obtained clusters with manual annotations, ensuring that clusters mostly correspond to actual synonyms and concepts. Finally, we showcased an external application of our methodology to create concept-aware embeddings that can be competitive to other methods on semantic tasks, such as Word-in-Context. Concept Induction opens the way for a different perspective on lexical semantics in NLP, and can be a basis for many studies of lexical meanings as it is expressive enough to reflect relations on both sides of the word-meaning mapping.

\section{Limitations}

The formal framework we defined uses terminology and notions from rather structuralist/relational assumptions of the language's lexical system (e.g. senses, discrete concepts, etc.). We made this choice based on how lexical databases like WordNet (and its derivatives), or other like the Historical Thesaurus of English for instance, are designed using the "word/sense/concept" structure. From a purely practical point of view, this choice makes sense as these resources would be the primary source for task data's annotations. Conceptually, senses are also a notion widely used in computational linguistics and we wanted to propose Concept Induction as a step "beyond" this conventional aspect and its related tasks. Future research may explore definitions/extensions of Concept Induction outside of this structuralist/relational framework, towards cognitive semantics for instance \citep{geeraerts2010}.

Evaluating Concept Induction is mainly limited by the low amount of suitable annotated corpora. Not only the data need to be annotated in concepts, but these annotations must cover a wide variety of lemmas for synonymy to be sufficiently represented in the corpus. Future work may find or create datasets meeting these requirements to evaluate Concept Induction outside of SemCor.

For now, the study is limited to nouns. Performances of benchmarked algorithms and systems may change with other Part-of-Speech tags. Our Bi-level method allows the global clustering to merge local clusters, leveraging lexicon-level information to be used to correct Word Sense Induction errors at the lemma-level. By its sequential nature, our method does not allow to split local clusters using global-level information, which could lead to better results. Further research directions include creating an iterative version of our methodology (alternating local and global clustering), or attempting to tackle both clustering objectives simultaneously with bi-level constrained clustering.

Our results about sense-induction at the local level showed that usual WSI methods may not be robust in our setting where there are few occurrences for some lemmas. We demonstrated that, in this setting, concept-inducing methods provided a better division in word senses. In many fields of linguistics, corpora are not very large and do not contain hundreds of occurrences for each word. Nonetheless, it is still uncertain if this observed advantage of CI systems would still hold on bigger datasets with many occurrences per lemma, a setting better-suited for usual WSI methods.

In this paper, we limited our study to Nouns, the morpho-syntactic class exhibiting the most prominent semantic features. We leave to further research the study of Concept Induction for Verbs, Adjectives, or the heterogeneous family of Adverbs.

\section{Ethical Considerations}

Our methodology uses pretrained Contextualized Language Models, which are know to encode and replicate social biases contained in their training data and sometimes amplify them. While we do not observe surface-level biases arising when manually annotating concept clusters, it is still an open question of how these social biases may influence or even change results when inducing concepts in SemCor.

\section*{Acknowledgements}

We gratefully thank the anonymous reviewers for their insightful comments. This research was funded by Inria Exploratory Action COMANCHE.

\begin{thebibliography}{99}

\bibitem[Amigó et al.(2009)]{amigo2009}
Enrique Amigó, Julio Gonzalo, Javier Artiles, and Felisa Verdejo. 2009.
\newblock A comparison of extrinsic clustering evaluation metrics based on formal constraints.
\newblock \emph{Information retrieval}, 12:461-486.

\bibitem[Amrami and Goldberg(2019)]{amrami2019}
Asaf Amrami and Yoav Goldberg. 2019.
\newblock Towards better substitution-based word sense induction.

\bibitem[Bagga and Baldwin(1998)]{bagga1998}
Amit Bagga and Breck Baldwin. 1998.
\newblock Entity-based cross-document coreferencing using the vector space model.
\newblock In \emph{36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics}, Volume 1, pages 79-85, Montreal, Quebec, Canada. Association for Computational Linguistics.

\bibitem[Bizzoni et al.(2014)]{bizzoni2014}
Yuri Bizzoni, Federico Boschetti, Harry Diakoff, Riccardo Del Gratta, Monica Monachini, and Gregory Crane. 2014.
\newblock The making of Ancient Greek WordNet.
\newblock In \emph{Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC'14)}, pages 1140-1147, Reykjavik, Iceland. European Language Resources Association (ELRA).

\bibitem[Chronis and Erk(2020)]{chronis2020}
Gabriella Chronis and Katrin Erk. 2020.
\newblock When is a bishop not like a rook? when it's like a rabbi! multi-prototype BERT embeddings for estimating semantic relationships.
\newblock In \emph{Proceedings of the 24th Conference on Computational Natural Language Learning}, pages 227-244, Online. Association for Computational Linguistics.

\bibitem[Devlin et al.(2019)]{devlin2019}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.
\newblock BERT: Pre-training of deep bidirectional transformers for language understanding.
\newblock In \emph{Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics.

\bibitem[Ethayarajh(2019)]{ethayarajh2019}
Kawin Ethayarajh. 2019.
\newblock How contextual are contextualized word representations? Comparing the geometry of BERT, ELMo, and GPT-2 embeddings.
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)}, pages 55-65, Hong Kong, China. Association for Computational Linguistics.

\bibitem[Eyal et al.(2022)]{eyal2022}
Matan Eyal, Shoval Sadde, Hillel Taub-Tabib, and Yoav Goldberg. 2022.
\newblock Large Scale Word Sense Induction. (Citation inferred based on context)

\bibitem[Fellbaum(1998)]{fellbaum1998}
Christiane Fellbaum. 1998.
\newblock WordNet: An Electronic Lexical Database.

\bibitem[Geeraerts(2010)]{geeraerts2010}
Dirk Geeraerts. 2010.
\newblock Theories of Lexical Semantics.

\bibitem[Ghanem et al.(2023)]{ghanem2023}
Ghanem et al. 2023. (Citation inferred based on context)

\bibitem[Haber and Poesio(2021)]{haber2021}
Haber and Poesio. 2021. (Citation inferred based on context)

\bibitem[Haber and Poesio(2024)]{haber2024}
Haber and Poesio. 2024. (Citation inferred based on context)

\bibitem[Hanna and Mareček(2021)]{hanna2021}
Michael Hanna and David Mareček. 2021.
\newblock Analyzing BERT's Knowledge of Hypernymy. (Citation inferred based on context)

\bibitem[Jurgens and Klapaftis(2013)]{jurgens2013}
David Jurgens and Ioannis Klapaftis. 2013.
\newblock SemEval-2013 Task 13: Word Sense Induction for Graded and Non-Graded Senses.

\bibitem[Khan et al.(2022)]{khan2022}
Khan et al. 2022. (Citation inferred based on context)

\bibitem[Kutuzov and Giulianelli(2020)]{kutuzov2020}
Andrey Kutuzov and Mario Giulianelli. 2020.
\newblock UiO-UvA at SemEval-2020 Task 1: Contextualised Embeddings for Lexical Semantic Change Detection.

\bibitem[Manandhar et al.(2010)]{manandhar2010}
Suresh Manandhar, Ioannis P. Klapaftis, Samuel Dillard, and Thamar Solorio. 2010.
\newblock SemEval-2010 Task 14: Word Sense Induction & Disambiguation.

\bibitem[Martinc et al.(2020)]{martinc2020}
Matej Martinc et al. 2020.
\newblock Leveraging Contextual Embeddings for Detecting Diachronic Semantic Shift.

\bibitem[Miller(1995)]{miller1995}
George A. Miller. 1995.
\newblock WordNet: A Lexical Database for English.

\bibitem[Nair et al.(2020)]{nair2020}
Sathish Nair et al. 2020. (Citation inferred based on context)

\bibitem[Pilehvar and Camacho-Collados(2019)]{pilehvar2019}
Mohammad Taher Pilehvar and Jose Camacho-Collados. 2019.
\newblock WiC: the Word-in-Context Dataset for Evaluating Context-Sensitive Meaning Representations.

\bibitem[Raganato et al.(2017)]{raganato2017}
Alessandro Raganato, Jose Camacho-Collados, and Roberto Navigli. 2017.
\newblock Word Sense Disambiguation: A Unified Evaluation Framework and Empirical Comparison.

\bibitem[Saidi and Jarray(2023)]{saidi2023}
Saidi and Jarray. 2023. (Citation inferred based on context)

\bibitem[Scarlini et al.(2020)]{scarlini2020}
Bianca Scarlini et al. 2020. (Citation inferred based on context)

\bibitem[Tahmasebi et al.(2021)]{tahmasebi2021}
Nina Tahmasebi et al. 2021. (Citation inferred based on context)

\bibitem[Velasco et al.(2023)]{velasco2023}
Velasco et al. 2023. (Citation inferred based on context)

\bibitem[Zhang et al.(2021)]{zhang2021}
Zhang et al. 2021. (Citation inferred based on context)

\end{thebibliography}

\end{document}
=====END FILE=====