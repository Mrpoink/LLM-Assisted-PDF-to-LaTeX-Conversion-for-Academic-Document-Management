ACK

=====FILE: main.tex=====
\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\usepackage{times}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{url}
\usepackage[round]{natbib}
\usepackage{hyperref}
\usepackage{float}

\geometry{left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm}

\title{Bridging Modalities: Enhancing Cross-Modality Hate Speech Detection\with Few-Shot In-Context Learning}

\author{
Ming Shan Hee\textsuperscript{1}*, Aditi Kumaresan\textsuperscript{1}*, Roy Ka-Wei Lee\textsuperscript{1} \
\textsuperscript{1}Singapore University of Technology and Design \
\texttt{{mingshan_hee, aditi_kumaresan, roy_lee}@sutd.edu.sg}
}

\date{}

\begin{document}

\maketitle

\begin{abstract}
\input{sections/00_abstract}
\end{abstract}

\section{Introduction}
\input{sections/01_introduction}

\section{Research Questions}
\input{sections/02_research_questions}

\section{Experiments}
\input{sections/03_experiments}

\section{Few-Shot Demonstration Analysis}
\input{sections/04_demonstration_analysis}

\section{Related Works}
\input{sections/05_related_works}

\section{Conclusion}
\input{sections/06_conclusion}

\section*{Acknowledgement}
\input{sections/07_acknowledgement}

\section*{Limitations}
\input{sections/08_limitations}

\section*{Ethical Considerations}
\input{sections/09_ethical}

\bibliographystyle{plainnat}
\bibliography{refs}

\appendix
\input{sections/10_appendix}

\end{document}
=====END FILE=====

=====FILE: sections/00_abstract.tex=====
The widespread presence of hate speech on the internet, including formats such as text-based tweets and vision-language memes, poses a significant challenge to digital platform safety. Recent research has developed detection models tailored to specific modalities; however, there is a notable gap in transferring detection capabilities across different formats. This study conducts extensive experiments using few-shot in-context learning with large language models to explore the transferability of hate speech detection between modalities. Our findings demonstrate that text-based hate speech examples can significantly enhance the classification accuracy of vision-language hate speech. Moreover, text-based demonstrations outperform vision-language demonstrations in few-shot learning settings. These results highlight the effectiveness of cross-modality knowledge transfer and offer valuable insights for improving hate speech detection systems\footnote{These authors contributed equally to this work. GitHub: \url{[https://github.com/Social-AI-Studio/Bridging-Modalities](https://github.com/Social-AI-Studio/Bridging-Modalities)}}.
=====END FILE=====

=====FILE: sections/01_introduction.tex=====
\paragraph{Motivation.} Hate speech in the online space appears in various forms, including text-based tweets and vision-language memes. Recent hate speech studies have developed models targeting specific modalities \citep{cao2023procap, awal2021angrybert}. However, these approaches are often optimized to within-distribution data and fail to address zero-shot out-of-distribution scenarios.

The emergence of vision-language hate speech, which comprises text and visual elements, presents two significant challenges. First, there is a scarcity of datasets, as this area has only recently gained lots of attention. Second, collecting and using such data is complicated by copyright issues and increasingly stringent regulations on social platforms. Consequently, the limited availability of vision-language data hampers performance in out-of-distribution cases. In contrast, the abundance and diversity of text-based data offer a potential source for cross-modality knowledge transfer \citep{hee2024recent}.

\paragraph{Research Objectives.} This paper investigates whether text-based hate speech detection capabilities can be transferred to multimodal formats. By leveraging the richness of text-based data, we aim to enhance the detection of vision-language hate speech, addressing current research limitations and improving performance in low-resource settings.

\paragraph{Contributions.} This study makes the following key contributions: (i) We conduct extensive experiments evaluating the transferability of text-based hate speech detection to vision-language formats using few-shot in-context learning with large language models. (ii) We demonstrate that text-based hate speech examples significantly improve the classification accuracy of vision-language hate speech. (iii) We show that text-based demonstrations in few-shot learning contexts outperform vision-language demonstrations, highlighting the potential for cross-modality knowledge transfer. These contributions address critical gaps in existing research and provide a foundation for developing robust hate speech detection systems.
=====END FILE=====

=====FILE: sections/02_research_questions.tex=====
As all forms of hate speech share one definition, this study investigates the usefulness of using hate speech from one form, such as text-based hate speech, to classify hate speech in another form, such as vision-language hate speech. Working towards this goal, we formulate two research questions to guide our investigation.

\paragraph{RQ1: Does the text hate speech support set help with vision-language hate speech?} Visual-language hate speech presents a distinct challenge compared to text-based hate speech, as malicious messages can hide within visual elements or interactions between modalities. It remains uncertain whether text-based hate speech can be useful for classifying visual-language hate speech. We investigate this uncertainty by performing few-shot in-context learning on large language models. This method allows the model to learn from text-based hate speech demonstration examples before classifying visual-language hate speech instances.

\paragraph{RQ2: How does the text hate speech support set fare against the vision-language hate speech support set?} Intuitively, using vision-language hate speech demonstrations should result in superior performance. However, the effectiveness of text-based hate speech demonstrations compared to vision-language hate speech demonstrations remains an open question. To investigate this gap, we conducted another round of few-shot in-context learning on large language models with a vision-language hate speech support set.
=====END FILE=====

=====FILE: sections/03_experiments.tex=====
\subsection{Experiment Settings}

\paragraph{Models.} We use the Mistral-7B \citep{jiang2023mistral} and Qwen2-7B \citep{bai2023qwen} models, both of which demonstrate strong performance across various benchmarks, in our primary experiments. Notably, their models on LMSYS's Chatbot Arena Leaderboard achieve high ELO scores \citep{chiang2024chatbot}. To facilitate reproducibility and minimize randomness, we use the greedy decoding strategy for text generation. We conducted additional experiments to support the findings in our paper further with two additional models: LLaVA-7B and LLaMA-8B. The results of these experiments are presented in Appendix I.

\paragraph{Test Datasets.} The Facebook Hateful Memes (FHM) dataset \citep{mathias2021fhm} contains synthetic memes categorized into five types of hate incitement: gender, racial, religious, nationality, and disability-based. The Multimedia Automatic Misogyny Identification (MAMI) \citep{fersini2022mami} dataset comprises real-world misogynistic memes classified into shaming, stereotype, objectification, and violence categories. Both datasets contain text overlay information, eliminating the need for an OCR model to extract text.

For evaluation, we use the FHM's \texttt{dev_seen} split, which includes 246 hateful memes and 254 non-hateful ones, and the MAMI's test split, consisting of 500 hateful and 500 non-hateful memes.

\paragraph{Text Support Set.} We use the Latent Hatred \citep{elsherief2021latent} dataset, which includes both explicit and implicit forms of hate speech, such as coded and indirect derogatory attacks. This dataset comprises 13,921 non-hateful speeches, 1,089 explicit hate speeches, and 7,100 implicit hate speeches.

\paragraph{Vision-Language Support Set.} We use the FHM train split for evaluation, containing 3,007 hateful memes and 5,493 non-hateful memes.

\begin{table}[h]
\centering
\begin{tabular}{lcccc}
\toprule
& \multicolumn{2}{c}{\textbf{Support}} & \multicolumn{2}{c}{\textbf{Test}} \
\textbf{Dataset} & \textbf{#Non-H} & \textbf{#H} & \textbf{#H} & \textbf{#Non-H} \
\midrule
Latent Hatred & 13,921 & 8,189 & - & - \
FHM & 3,007 & 5,493 & 246 & 254 \
MAMI & - & - & 500 & 500 \
\bottomrule
\end{tabular}
\caption{Statistical distributions of datasets, where "H" represents Hate and "Non-H" represents non-hate.}
\label{tab:datasets}
\end{table}

\subsection{Data Preprocessing}

\paragraph{Image Captioning.} To perform hateful meme classification with the large language models, we perform image captioning on the meme using the OFA \citep{wang2022ofa} model pre-trained on the MSCOCO \citep{lin2014microsoft} dataset.

\paragraph{Rationale Generation.} We prompt Mistral-7B to generate informative rationales that explain the underlying meaning of the content, providing additional context for the few-shot in-context learning. Specifically, the model generates rationales by using the content and ground truth labels (i.e., prompt + content  ground truth label explanation). For the Latent Hatred dataset, we use post information and labels, while for the FHM dataset, we use meme text, captions, and labels. To mitigate noise from varying rationale formulations, we instruct the model to consider both textual and visual elements, focusing on target groups, imagery, and the impact of tweet/meme bias perpetuation. More details can be found in Appendix F.

\subsection{RQ1: Does text hate speech help with vision-language hate speech?}

To evaluate the effectiveness of the few-shot in-context learning approach and the Latent Hatred support set, we employed three sampling strategies: Random sampling, TF-IDF sampling, and BM-25 sampling. The TF-IDF and BM-25 strategies leverage the text and caption information of the test record to identify similar examples from the support set, focusing on either the text or the generated rationale.

Table \ref{tab:results_latent} shows the comparison of zero-shot and few-shot in-context learning experiment results with Latent Hatred support set. The experimental results demonstrate that employing a few-shot in-context learning approach with text-based hate speech demonstrations is highly effective in classifying vision-language hate speech.

Firstly, while the random sampling strategy could retrieve more irrelevant demonstrations compared to other strategies, the few-shot in-context learning with random sampling surpasses the zero-shot inference performance on both models across two datasets in terms of F1 score. Secondly, the TF-IDF and BM-25 sampling strategies exceed the zero-shot inference performance on both models within the MAMI dataset. Conversely, within the FHM dataset, we observed several instances where some sampling strategies in the few-shot in-context learning scenario performed worse than zero-shot inference. However, these sampling strategies consistently outperformed zero-shot inference when run with 16-shots in-context learning.

Lastly, the best few-shot in-context learning performance within each dataset and each model shows significant improvement over zero-shot model performance. For example, the Mistral-7B model achieves an F1 score improvement of 0.64 and 1.23 on the FHM and MAMI datasets respectively.

\begin{table*}[t]
\centering
\small
\begin{tabular}{lclcccccc}
\toprule
& & & \multicolumn{3}{c}{\textbf{FHM}} & \multicolumn{3}{c}{\textbf{MAMI}} \
\textbf{Model} & \textbf{# Shots} & \textbf{Matching} & \textbf{Acc.} & \textbf{F1} & \textbf{#Inv} & \textbf{Acc.} & \textbf{F1} & \textbf{#Inv} \
\midrule
\multirow{9}{*}{Mistral-7B} & 0-shot & - & 0.614 & 0.594 & 0 & 0.619 & 0.568 & 0 \
\cmidrule{2-9}
& \multirow{5}{*}{4-shots} & Random & 0.618 & 0.613 & 0 & 0.655 & 0.636 & 0 \
& & TF-IDF Text & 0.634 & 0.634 & 0 & 0.653 & 0.649 & 0 \
& & TF-IDF Rationale & 0.618 & 0.618 & 0 & 0.662 & 0.658 & 0 \
& & BM-25 Text & 0.658 & 0.657 & 0 & 0.665 & 0.662 & 0 \
& & BM-25 Rationale & 0.598 & 0.596 & 0 & 0.676 & 0.671 & 0 \
\cmidrule{2-9}
& \multirow{3}{*}{8-shots} & Random & 0.620 & 0.611 & 0 & 0.634 & 0.602 & 0 \
& & TF-IDF Text & 0.642 & 0.641 & 0 & 0.665 & 0.658 & 0 \
& & TF-IDF Rationale & 0.626 & 0.625 & 0 & 0.657 & 0.649 & 0 \
& & BM-25 Text & 0.660 & 0.658 & 0 & 0.685 & 0.680 & 0 \
& & BM-25 Rationale & 0.612 & 0.608 & 0 & 0.669 & 0.661 & 0 \
\cmidrule{2-9}
& \multirow{3}{*}{16-shots} & Random & 0.618 & 0.610 & 0 & 0.642 & 0.611 & 0 \
& & TF-IDF Text & 0.644 & 0.644 & 0 & 0.675 & 0.668 & 0 \
& & TF-IDF Rationale & 0.632 & 0.631 & 0 & 0.632 & 0.631 & 0 \
& & BM-25 Text & 0.638 & 0.636 & 0 & 0.705 & 0.701 & 0 \
& & BM-25 Rationale & 0.614 & 0.611 & 0 & 0.665 & 0.659 & 0 \
\midrule
\multirow{9}{*}{Qwen2-7B} & 0-shot & - & 0.624 & 0.609 & 0 & 0.614 & 0.574 & 0 \
\cmidrule{2-9}
& \multirow{5}{*}{4-shots} & Random & 0.620 & 0.614 & 0 & 0.653 & 0.632 & 0 \
& & TF-IDF Text & 0.632 & 0.631 & 0 & 0.650 & 0.641 & 0 \
& & TF-IDF Rationale & 0.634 & 0.633 & 0 & 0.663 & 0.653 & 0 \
& & BM-25 Text & 0.644 & 0.642 & 0 & 0.672 & 0.664 & 0 \
& & BM-25 Rationale & 0.590 & 0.587 & 0 & 0.663 & 0.654 & 0 \
\cmidrule{2-9}
& \multirow{3}{*}{8-shots} & Random & 0.632 & 0.628 & 0 & 0.645 & 0.622 & 0 \
& & TF-IDF Text & 0.632 & 0.632 & 0 & 0.656 & 0.650 & 0 \
& & TF-IDF Rationale & 0.618 & 0.617 & 0 & 0.664 & 0.656 & 0 \
& & BM-25 Text & 0.654 & 0.653 & 0 & 0.679 & 0.674 & 0 \
& & BM-25 Rationale & 0.604 & 0.603 & 0 & 0.654 & 0.646 & 0 \
\cmidrule{2-9}
& \multirow{3}{*}{16-shots} & Random & 0.632 & 0.626 & 0 & 0.652 & 0.631 & 0 \
& & TF-IDF Text & 0.628 & 0.628 & 0 & 0.656 & 0.651 & 0 \
& & TF-IDF Rationale & 0.630 & 0.632 & 0 & 0.665 & 0.659 & 0 \
& & BM-25 Text & 0.632 & 0.631 & 0 & 0.678 & 0.674 & 0 \
& & BM-25 Rationale & 0.624 & 0.624 & 0 & 0.679 & 0.674 & 0 \
\bottomrule
\end{tabular}
\caption{Comparison of zero-shot and few-shot in-context learning with Latent Hatred support set. Inv: Invalids.}
\label{tab:results_latent}
\end{table}

\subsection{RQ2: How does text hate speech support set fare against vision-language hate speech support set?}

Table \ref{tab:results_fhm} shows the comparison of zero-shot and few-shot in-context learning experiment results with the FHM support set. The experimental results indicate that using the FHM support set can enhance model performance in some scenarios. However, it is noteworthy that in many instances, few-shot in-context learning performs worse than zero-shot model performance when compared against the Latent Hatred support set.

Most significantly, the model encounters the most failures on the FHM test set despite using the FHM train set as a support set. We also observed that the best model performance with the Latent Hatred support set surpasses the best model performance with the FHM support set across all instances. We speculate that this discrepancy may stem from the oversimplification of visual information into image captions and the broader topic coverage provided by the Latent Hatred dataset. Nevertheless, this suggests that text-based data can serve as a valuable resource for improving performance on multimodal tasks, particularly in low-resource settings.

\begin{table*}[t]
\centering
\small
\begin{tabular}{lclcccccc}
\toprule
& & & \multicolumn{3}{c}{\textbf{FHM}} & \multicolumn{3}{c}{\textbf{MAMI}} \
\textbf{Model} & \textbf{# Shots} & \textbf{Matching} & \textbf{Acc.} & \textbf{F1} & \textbf{#Inv} & \textbf{Acc.} & \textbf{F1} & \textbf{#Inv} \
\midrule
\multirow{9}{*}{Mistral-7B} & 0-shot & - & 0.614 & 0.594 & 0 & 0.619 & 0.568 & 0 \
\cmidrule{2-9}
& \multirow{5}{*}{4-shots} & Random & 0.622 & 0.617 & 0 & 0.656 & 0.642 & 0 \
& & Text + Cap. & 0.604 & 0.598 & 0 & 0.678 & 0.670 & 0 \
& & TF-IDF Rationale & 0.618 & 0.613 & 0 & 0.662 & 0.652 & 0 \
& & BM-25 Text + Cap. & 0.592 & 0.584 & 0 & 0.662 & 0.653 & 0 \
& & BM-25 Rationale & 0.620 & 0.617 & 0 & 0.667 & 0.659 & 0 \
\cmidrule{2-9}
& \multirow{3}{*}{8-shots} & Random & 0.624 & 0.615 & 0 & 0.652 & 0.632 & 0 \
& & Text + Cap. & 0.618 & 0.611 & 0 & 0.675 & 0.664 & 0 \
& & TF-IDF Rationale & 0.628 & 0.622 & 0 & 0.681 & 0.670 & 0 \
& & BM-25 Text + Cap. & 0.606 & 0.599 & 0 & 0.672 & 0.661 & 0 \
& & BM-25 Rationale & 0.628 & 0.624 & 0 & 0.674 & 0.666 & 0 \
\cmidrule{2-9}
& \multirow{3}{*}{16-shots} & Random & 0.620 & 0.614 & 0 & 0.668 & 0.651 & 0 \
& & Text + Cap. & 0.620 & 0.617 & 0 & 0.672 & 0.665 & 0 \
& & TF-IDF Rationale & 0.638 & 0.635 & 0 & 0.671 & 0.661 & 0 \
& & BM-25 Text + Cap. & 0.630 & 0.625 & 0 & 0.682 & 0.673 & 0 \
& & BM-25 Rationale & 0.634 & 0.633 & 0 & 0.687 & 0.680 & 0 \
\midrule
\multirow{9}{*}{Qwen2-7B} & 0-shot & - & 0.624 & 0.609 & 0 & 0.614 & 0.574 & 0 \
\cmidrule{2-9}
& \multirow{5}{*}{4-shots} & Random & 0.606 & 0.602 & 0 & 0.655 & 0.642 & 0 \
& & Text + Cap. & 0.620 & 0.620 & 0 & 0.659 & 0.657 & 0 \
& & TF-IDF Rationale & 0.636 & 0.636 & 0 & 0.650 & 0.646 & 0 \
& & BM-25 Text + Cap. & 0.616 & 0.616 & 0 & 0.676 & 0.674 & 0 \
& & BM-25 Rationale & 0.622 & 0.622 & 0 & 0.669 & 0.672 & 0 \
\cmidrule{2-9}
& \multirow{3}{*}{8-shots} & Random & 0.592 & 0.581 & 0 & 0.642 & 0.624 & 0 \
& & Text + Cap. & 0.606 & 0.604 & 0 & 0.648 & 0.645 & 0 \
& & TF-IDF Rationale & 0.620 & 0.613 & 0 & 0.649 & 0.644 & 0 \
& & BM-25 Text + Cap. & 0.619 & 0.623 & 0 & 0.665 & 0.662 & 0 \
& & BM-25 Rationale & 0.614 & 0.624 & 0 & 0.669 & 0.664 & 0 \
\cmidrule{2-9}
& \multirow{3}{*}{16-shots} & Random & 0.602 & 0.592 & 0 & 0.650 & 0.634 & 0 \
& & Text + Cap. & 0.610 & 0.610 & 0 & 0.649 & 0.648 & 0 \
& & TF-IDF Rationale & 0.604 & 0.604 & 0 & 0.656 & 0.653 & 0 \
& & BM-25 Text + Cap. & 0.610 & 0.610 & 0 & 0.654 & 0.653 & 0 \
& & BM-25 Rationale & 0.626 & 0.626 & 0 & 0.653 & 0.650 & 0 \
\bottomrule
\end{tabular}
\caption{Comparison of zero-shot and few-shot in-context learning with FHM support set.}
\label{tab:results_fhm}
\end{table*}
=====END FILE=====

=====FILE: sections/04_demonstration_analysis.tex=====
While including relevant few-shot in-context learning examples can improve model performance, the degree to which these examples benefit the model remains uncertain. To gain deeper insights, we examine the examples that got correctly classified and misclassified using the demonstration exemplars from the Latent Hatred support dataset. The detailed analysis and case study examples, along with their few-shot in-context demonstrations, can be found in Appendix G and H.

\paragraph{Latent Hatred's Support Set.} We found that using relevant examples as demonstrations significantly improves classification, as the additional context aids the model in evaluating similar content more effectively. This approach enhances the model's ability to generalize across diverse hate speech contexts and formats, thereby helping to reduce false negatives in edge cases. However, we also observed that models sometimes misinterpret neutral content as hateful. This misinterpretation may arise from exposure to demonstration examples that contain dismissive or derogatory language on sensitive topics. Consequently, these examples can lead to an overgeneralization of what qualifies as hateful, causing content that was correctly classified in a zero-shot setting to be misclassified. This issue is similar to the problem of oversensitivity to specific terms found in fine-tuned multimodal hate speech detection models \citep{cuo2022procap, hee2022explaining, rizzi2023investigating}.
=====END FILE=====

=====FILE: sections/05_related_works.tex=====
Numerous approaches have been proposed to tackle the online hate speech problem \citep{cao2023procap, lee2021disentangling, hee2023decoding, lin2024detecting}. While these approaches demonstrate impressive performance, they often require large amounts of data for fine-tuning, and the rapid evolution of hate speech can quickly render these models outdated. Furthermore, a recent study indicated that these models are vulnerable to adversarial attacks \citep{aggarwal2023hateproof}.

These challenges led to exploring few-shot hate speech detection approaches, where models learn using limited data \citep{meta2021fhm, awal2023model}. Mod-HATE trains specialized modules on related tasks and integrates the weighted module with large language models to enhance detection capabilities \citep{cao2024modularized}. Our approach contributes to this field by addressing the challenge of limited data availability, using the abundance and diversity of text-based hate speech as an alternative source for cross-modality knowledge transfer.
=====END FILE=====

=====FILE: sections/06_conclusion.tex=====
We investigated the possibility of cross-modality knowledge transfer using few-shot in-context learning with large language models. Our extensive experiments show that text-based hate speech demonstrations significantly improve the classification accuracy of vision-language hate speech, and using text-based demonstrations in few-shot in-context learning outperforms using vision-language demonstrations. For future works, we aim to extend our analysis to more datasets and explore other cross-modality knowledge transfer approaches such as cross-modality fine-tuning.
=====END FILE=====

=====FILE: sections/07_acknowledgement.tex=====
This research is supported by the Ministry of Education, Singapore, under its Academic Research Fund Tier 2 (Award ID: MOE-T2EP20222-0010). Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not reflect the views of the Ministry of Education, Singapore.
=====END FILE=====

=====FILE: sections/08_limitations.tex=====
There are several limitations in this research study.

\paragraph{Model Coverage and Model Size.} In this study, we evaluated and compared two large models containing 7B parameters. In the future, we aim to extend our analysis to other large models when more computational resources are available.

\paragraph{Large Language Model.} In this study, we evaluated few-shot in-context learning in large language models. The experiments are designed in this manner, so to ensure that there can be a fair comparison between the different support sets. We recognize that using a vision-language support set for few-shot in-context learning with a large vision-language model could achieve better performance. However, evaluation using large vision-language models would then be unfair to text support set for few-shot in-context learning.
=====END FILE=====

=====FILE: sections/09_ethical.tex=====
\paragraph{Impact of False Positives.} Developing a reliable and generalizable hate speech detection system is crucial, as false positives can significantly impact free speech and diminish user trust. Firstly, overly aggressive detection systems may mistakenly flag content that does not qualify as hate speech, thereby suppressing free speech and hindering meaningful discussions. Secondly, when users frequently encounter false positives, their confidence in the platform's moderation system may diminish. The reduced trust can result in decreased user engagement and a perception of bias within the platform.
=====END FILE=====

=====FILE: sections/10_appendix.tex=====
\section{Potential Risks}
This project seeks to counteract the dissemination of harmful memes, aiming to protect individuals from prejudice and discrimination based on race, religion and gender. However, we acknowledge the risk of malicious users reverse-engineering memes to evade detection by CMTL-RAG AI systems, which is strongly discouraged and condemned.

\section{Licenses and Usage}
\subsection{Models}
All of the LLMs used in this paper contain licenses permissive for academic and/or research use.
\begin{itemize}
\item Mistral-7B: Apache-2.0 License
\item Qwen2-7B: Apache-2.0 License
\item LLaVA-7B: Apache 2.0 License
\item LLaMA-8B: Llama 3.1 Community License
\end{itemize}

\subsection{Datasets}
All of the datasets used in this paper contain licenses permissive for academic and/or research use.
\begin{itemize}
\item Latent Hatred Dataset: MIT License
\item Hateful Memes Dataset: MIT License
\item Multimedia Automatic Misogyny Identification: Creative Commons License (CC BY-NC-SA 4.0)
\end{itemize}

\subsection{Anonymity}
[ILLEGIBLE/MISSING CONTENT]

\section{Computational Experiments}
[MISSING CONTENT]

\section{Few-Shot In-Context Learning}
In this approach, we retrieve relevant labelled examples from a 'support dataset' using similarity metrics such as TF-IDF or BM-25 for a given meme from the inference dataset. These examples are then provided as demonstrations in a few-shot prompt to enhance the model's understanding of the meme. Finally, we prompt the model to classify the meme, leveraging the augmented context for improved accuracy.

\section{Similarity Metrics}
\subsection{TF-IDF}
TFIDF is a statistical measure used to evaluate the importance of a word in a document relative to a collection of documents (corpus). By creating TF-IDF vectors for a 'support dataset', we can use cosine similarity to find the most similar records to a given inference record.

\subsection{BM25}
BM25 is an advanced version of the TF-IDF weighting scheme used in search engines. It incorporates term frequency saturation and document length normalization to improve retrieval performance. We generate vectors for each record in the 'support dataset' and use cosine similarity to identify records most similar to an inference record.

\section{Rationale Generation Details}
We use the Mistral-7B model, a state-of-the-art language model known for its capabilities in language understanding and generation. We implement a ten-shot prompting method to generate explanations for the hateful content.

\begin{quote}
\textbf{User:} Determine whether the following meme is hateful. Text: text Caption: {caption} \
\textbf{Assistant:} (label) \
\textbf{User:} Briefly provide an explanation, in no more than three points, for the meme being perceived as (label). Your explanation should address the targeted group, any derogatory imagery or language used, and the impact it has on perpetuating bias, stereotypes, prejudice, discrimination or inciting harm. \
\textbf{Assistant:} Answer: {rationale}
\end{quote}

The demonstration explanation follows a list format, where each list item addresses the targeted group, any derogatory imagery or language used, and the impact it has on perpetuating bias. Finally, to reiterate the classification of the post's hatefulness, the explanation [MISSING END OF SENTENCE].

\section{Case Study: Correct Classifications}
[MISSING CONTENT: Case Study examples not retrieved]

\section{Case Study: Misclassifications}
[MISSING CONTENT: Case Study examples not retrieved]

\section{Additional Experiments}
\subsection{LLaVA-7B}
\begin{table}[h]
\centering
\small
\begin{tabular}{lclcccccc}
\toprule
& & & \multicolumn{3}{c}{\textbf{FHM}} & \multicolumn{3}{c}{\textbf{MAMI}} \
\textbf{Model} & \textbf{# Shots} & \textbf{Matching} & \textbf{Acc.} & \textbf{F1} & \textbf{#Inv} & \textbf{Acc.} & \textbf{F1} & \textbf{#Inv} \
\midrule
LLaVA-7B & 0-shot & - & 0.512 & 0.509 & 27 & 0.553 & 0.533 & 30 \
\cmidrule{2-9}
& \multirow{5}{*}{4-shots} & Random & 0.592 & 0.588 & 0 & 0.581 & 0.575 & 0 \
& & TF-IDF Text+Cap & 0.590 & 0.576 & 0 & 0.585 & 0.606 & 0 \
& & TF-IDF Rationale & 0.594 & 0.581 & 0 & 0.590 & 0.613 & 0 \
& & BM-25 Text+Cap & 0.602 & 0.618 & 0 & 0.619 & 0.635 & 0 \
& & BM-25 Rationale & 0.559 & 0.590 & 0 & 0.593 & 0.594 & 0 \
\bottomrule
\end{tabular}
\caption{Additional results for LLaVA-7B.}
\label{tab:results_llava}
\end{table}
=====END FILE=====

=====FILE: refs.bib=====
@inproceedings{cao2023procap,
title={Pro-cap: Leveraging a frozen vision-language model for hateful meme detection},
author={Cao, Rui and Hee, Ming Shan and Kuek, Adriel and Chong, Wen-Haw and Lee, Roy Ka-Wei and Jiang, Jing},
booktitle={Proceedings of the 31st ACM International Conference on Multimedia},
pages={5244--5252},
year={2023}
}

@inproceedings{awal2021angrybert,
title={Angrybert: Joint learning target and emotion for hate speech detection},
author={Awal, Md Rabiul and Cao, Rui and Lee, Roy Ka-Wei and Mitrovi{'c}, Sandra},
booktitle={Pacific-Asia conference on knowledge discovery and data mining},
pages={701--713},
year={2021},
organization={Springer}
}

@article{hee2024recent,
title={Recent advances in hate speech moderation: Multimodality and the role of large models},
author={Hee, Ming Shan and Sharma, Shivam and Cao, Rui and Nandi, Palash and Nakov, Preslav and Chakraborty, Tanmoy and Lee, Roy Ka-Wei},
journal={arXiv preprint arXiv:2401.16727},
year={2024}
}

@inproceedings{mathias2021fhm,
title={The Hateful Memes Challenge: Detecting Hate Speech in Multimodal Memes},
author={Kiela, Douwe and Firooz, Hamed and Mohan, Aravind and Goswami, Vedanuj and Singh, Amanpreet and Ringshia, Pratik and Testuggine, Davide},
booktitle={Proceedings of NeurIPS},
year={2020}
}

@inproceedings{fersini2022mami,
title={SemEval-2022 Task 5: Multimedia Automatic Misogyny Identification},
author={Fersini, Elisabetta and Gasparini, Francesca and Corchs, Silvia and Rizzi, Giulia and Saibene, Aurora and Chulvi, Berta and Rosso, Paolo},
booktitle={Proceedings of the 16th International Workshop on Semantic Evaluation (SemEval-2022)},
pages={533--549},
year={2022}
}

@inproceedings{elsherief2021latent,
title={Latent Hatred: A Benchmark for Understanding Implicit Hate Speech},
author={ElSherief, Mai and Ziems, Caleb and Muchlinski, David and Anupindi, V and Seybolt, J and De Choudhury, Munmun and Yang, Diyi},
booktitle={Proceedings of EMNLP},
year={2021}
}

@article{jiang2023mistral,
title={Mistral 7B},
author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and De las Casas, Diego and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
journal={arXiv preprint arXiv:2310.06825},
year={2023}
}

@article{bai2023qwen,
title={Qwen technical report},
author={Bai, Jinze and Bai, Shuai and Chu, Yunfei and Cui, Zeyu and Dang, Kai and Deng, Xiaodong and Fan, Yang and Ge, Wenbin and Han, Yu and Huang, Fei and others},
journal={arXiv preprint arXiv:2309.16609},
year={2023}
}

@misc{chiang2024chatbot,
title={Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference},
author={Chiang, Wei-Lin and Zheng, Lianmin and Sheng, Ying and Angelopoulos, Anastasios Nikolas and Li, Tianle and Li, Dacheng and Zhang, Hao and Zhu, Banghua and Jordan, Michael and Gonzalez, Joseph E and Stoica, Ion},
year={2024},
publisher={arXiv}
}

@article{wang2022ofa,
title={OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework},
author={Wang, Peng and Yang, An and Men, Rui and Lin, Junyang and Bai, Shuai and Li, Zhikang and Ma, Jianxin and Zhou, Chang and Zhou, Jingren and Yang, Hongxia},
journal={CoRR},
volume={abs/2202.03052},
year={2022}
}

@inproceedings{lin2014microsoft,
title={Microsoft COCO: Common Objects in Context},
author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{'a}r, Piotr and Zitnick, C Lawrence},
booktitle={European conference on computer vision},
pages={740--755},
year={2014},
organization={Springer}
}

@inproceedings{hee2022explaining,
title={On explaining multimodal hateful meme detection models},
author={Hee, Ming Shan and Lee, Roy Ka-Wei and Chong, Wen-Haw},
booktitle={Proceedings of the ACM Web Conference 2022},
pages={3651--3655},
year={2022}
}

@inproceedings{rizzi2023investigating,
title={Investigating the robustness of multimodal hate speech detection against adversarial attacks},
author={Rizzi, Giulia and Fersini, Elisabetta and Gasparini, Francesca},
booktitle={International Conference on Image Analysis and Processing},
year={2023}
}

@inproceedings{lee2021disentangling,
title={Disentangling Hate in Online Memes},
author={Lee, Roy Ka-Wei and Cao, Rui and Fan, Ziqing and Jiang, Jing and Chong, Wen-Haw},
booktitle={Proceedings of the 29th ACM International Conference on Multimedia},
year={2021}
}

@inproceedings{hee2023decoding,
title={Decoding the underlying meaning of multimodal hateful memes},
author={Hee, Ming Shan and Chong, Wen-Haw and Lee, Roy Ka-Wei},
booktitle={Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence},
pages={5995--6003},
year={2023}
}

@inproceedings{lin2024detecting,
title={Detecting Hateful Memes with Multi-modal Information and External Knowledge},
author={Lin, Runji and others},
booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
year={2024}
}

@inproceedings{aggarwal2023hateproof,
title={Hateproof: Are hateful meme detection systems really robust?},
author={Aggarwal, Piush and Chawla, Pranit and Das, Mithun and Saha, Punyajoy and Mathew, Binny and Zesch, Torsten and Mukherjee, Animesh},
booktitle={Proceedings of the ACM Web Conference 2023},
pages={3734--3743},
year={2023}
}

@article{meta2021fhm,
title={The Hateful Memes Challenge: Detecting Hate Speech in Multimodal Memes},
author={Kiela, Douwe and Firooz, Hamed and Mohan, Aravind and Goswami, Vedanuj and Singh, Amanpreet and Ringshia, Pratik and Testuggine, Davide},
journal={NeurIPS},
year={2021}
}

@article{awal2023model,
title={Model-agnostic meta-learning for multilingual hate speech detection},
author={Awal, Md Rabiul and Lee, Roy Ka-Wei and Tanwar, Eshaan and Garg, Tanmay and Chakraborty, Tanmoy},
journal={IEEE Transactions on Computational Social Systems},
volume={11},
number={1},
pages={1086--1095},
year={2023}
}

@inproceedings{cao2024modularized,
title={Modularized networks for few-shot hateful meme detection},
author={Cao, Rui and Lee, Roy Ka-Wei and Jiang, Jing},
booktitle={Proceedings of the ACM on Web Conference 2024},
pages={4575--4584},
year={2024}
}
=====END FILE=====