=====FILE: main.tex=====
\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\geometry{top=2.5cm, bottom=2.5cm, left=2.5cm, right=2.5cm}
\usepackage{times}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{multirow}
\usepackage{hyperref}

\title{Automatic sentence segmentation of clinical record narratives in real-world data}

\author{
Dongfang Xu\textsuperscript{1,*} \and
Davy Weissenbacher\textsuperscript{1,*} \and
Karen O'Connor\textsuperscript{2} \and
Siddharth Rawal\textsuperscript{2} \and
Graciela Gonzalez-Hernandez\textsuperscript{1} \
\
\textsuperscript{1}Cedars-Sinai Medical Center, Los Angeles, CA, USA \
\textsuperscript{2}University of Pennsylvania, Philadelphia, PA, USA \
\texttt{{dongfang.xu, davy.weissenbacher}@cshs.org} \
\texttt{karoc@pennmedicine.upenn.edu} \
\texttt{graciela.gonzalezhernandez@csmc.edu}
}

\date{EMNLP 2024}

\begin{document}

\maketitle

\begin{abstract}
\input{sections/00_abstract}
\end{abstract}

\input{sections/01_introduction}
\input{sections/02_related_work}
\input{sections/03_methods}
\input{sections/04_datasets}
\input{sections/05_experiments}
\input{sections/06_results}

% [MISSING CONTENT]: The remainder of the paper (Results conclusion, Discussion, Conclusion, References, Appendices) is missing from the source file.
\section*{[MISSING CONTENT]}
[The remainder of the document (end of Results, Conclusion, References, Appendices) was not available in the source file.]

\end{document}
=====END FILE=====

=====FILE: sections/00_abstract.tex=====
Sentence segmentation is a linguistic task and is widely used as a pre-processing step in many NLP applications. The need for sentence segmentation is particularly pronounced in clinical notes, where ungrammatical and fragmented texts are common. We propose a straightforward and effective sequence labeling classifier to predict sentence spans using a dynamic sliding window based on the prediction of each input sequence. This sliding window algorithm allows our approach to segment long text sequences on the fly. To evaluate our approach, we annotated 90 clinical notes from the MIMIC-III dataset. Additionally, we tested our approach on five other datasets to assess its generalizability and compared its performance against state-of-the-art systems on these datasets. Our approach outperformed all the systems, achieving an F1 score that is 15% higher than the next best-performing system on the clinical dataset.
=====END FILE=====

=====FILE: sections/01_introduction.tex=====
\section{Introduction}

Sentence segmentation is the task of automatically identifying the boundaries of sentences in a written document, where a sentence is commonly defined as a sequence of grammatically linked words ending with a punctuation mark (PM). It is often the first pre-processing step for other natural language processing (NLP) tasks such as sentiment analysis (Medhat et al., 2014), information extraction (Angeli et al., 2015; Xu et al., 2020; Zhang and Bethard, 2023; Zhang et al., 2024), semantic textual similarity (Agirre et al., 2013), question answering ((Zhang et al., 2021b), and machine translation (Liu et al., 2020). Even tasks that operate at the paragraph or document level, such as coreference resolution (Stylianou and Vlahavas, 2021) or summarization (Pilault et al., 2020), often make use of sentences internally. Errors in segmentation could have detrimental effects on downstream task performance, e.g., in machine translation (Minixhofer et al., 2023), language modeling (Ek et al., 2020), and simultaneous speech translations (Wang et al., 2019).

Detecting sentence boundaries is especially crucial for processing and understanding clinical text, as most clinical NLP tasks depend on this information for annotation and model training (Fan et al., 2013; Gao et al., 2022). Despite its importance, sentence segmentation has received much less attention in the last few decades than other linguistic tasks. For non-clinical text, high-performing baseline systems use simple rule-based (Jurafsky and Martin, 2000; Manning et al., 2014) or machine learning-based (Gillick, 2009; Schweter and Ahmed, 2019) approaches that capture obvious and frequent sentence ending PMs (EPMs) such as [.!?'']. Such baselines leave little room for further improvement on traditional benchmarks derived from formal news(wire) sources or published articles. The focus on formal or edited text assumes EPMs as sentence boundaries, which is not directly applicable to real-world data such as clinical text (Read et al., 2012) or web text. These type of texts often contain fragmented and incomplete sentences, complex graphemic devices (e.g. abbreviations, and acronyms), and markups, which present challenges even for state-of-the-art sentence segmentation approaches, e.g., 70-85% F1 score on English Web Treebank (Straka, 2018; Qi et al., 2020). Another comprehensive evaluation of sentence segmentation in the clinical domain reveals that four standard sentence segmentation tools perform 20-30% worse on clinical texts compared to general-domain texts (Griffis et al., 2016).

Here, we present a sentence segmentation approach specifically tailored for real-world data, particularly clinical notes. Our method uses a sequence labeling classifier to predict sentence spans over a sliding window. During inference, we dynamically slide the window based on the prediction of each input sequence, such that the window always starts with a complete predicted sentence. This allows our approach to segment long text sequences on the fly without needing to pre-split the text. Moreover, the sequence labeling classifier does not rely on PMs for segmentation. To evaluate our approach on real-world clinical texts that can be shared, we annotated 90 clinical notes from MIMIC-III. Additionally, we extensively tested our method on five other datasets to assess its generalizability. Unlike other studies (Wicks and Post, 2021; Udagawa et al., 2023) that have modified datasets for sentence segmentation, we retained the original raw text, preserving their form and document structure.

Our work makes the following contributions:
\begin{itemize}
\item We propose a sentence segmentation approach capable of handling texts from diverse genres and domains without relying on specific text formats or EPMs. Our sliding-window algorithm segments long sequence texts on the fly, eliminating the need for pre-processing.
\item We release a new sentence segmentation dataset based on MIMIC-III corpus. To the best of our knowledge, this is the first manually annotated sentence segmentation dataset using clinical notes.
\item We comprehensively compare our approach against seven widely used off-the-shelf tools across six datasets. Our approach outperforms all these tools on five datasets, with particularly large margins on clinical datasets.
\end{itemize}

The code for our proposed approach and the new dataset are available at \url{[https://bitbucket.org/hlpgonzalezlab/hlp_segmenter](https://bitbucket.org/hlpgonzalezlab/hlp_segmenter)}.
=====END FILE=====

=====FILE: sections/02_related_work.tex=====
\section{Related Work}

Existing sentence segmentation approaches can be categorized into rule- and learning-based approaches.

Rule-based approaches (Aberdeen et al., 1995; Koehn et al., 2007; Dridan and Oepen, 2012; Sadvilkar and Neumann, 2020) utilize handcrafted rules, abbreviation lexicons, and linguistic features to decide whether a PM belongs to a token (an abbreviation or a number), or indicates the end of a sentence. For instance, Stanford CoreNLP toolkit (Manning et al., 2014) utilizes rules such as sentence ending PMs, or two consecutive line breaks to segment text. However, one major limitation of rule-based approaches is that the handcrafted rules are language- or domain-specific, making them difficult to maintain and adapt to new texts.

As an alternative, other systems aim to automatically learn segmentation rules through machine learning algorithms. When working with unlabeled data, unsupervised approaches (Mikheev, 2002; Kiss and Strunk, 2006) automatically curate information about abbreviations and proper names from large corpora and use them to determine whether the token preceding a period is an abbreviation and whether the token following a period is a proper name. One representative algorithm of the approach is in the Punkt system (Kiss and Strunk, 2006), as it computes the likelihood ratio of the truncated words and the following periods to identify abbreviations. An implementation of Punkt is bundled with the NLTK tool (Bird and Loper, 2004). Although these unsupervised approaches do not require extensive lexical resources or manual annotations and are easily adaptable to new domains, they can only segment sentential units (SUs) that use periods as sentence boundaries.

With the increasing availability of annotated corpora, supervised learning approaches have become predominant. One type of supervised approach combines a regular-expression-based detector to generate candidate SUs with a binary classifier. For generating candidate SUs, researchers have focused on only periods (Riley, 1989; Gillick, 2009), multiple EPMs (Reynar and Ratnaparkhi, 1997; Palmer and Hearst, 1997; Schweter and Ahmed, 2019), or more complex regular expressions (Wicks and Post, 2021). For classifying candidate SUs, most approaches employ binary classifiers with various features, e.g., a feedforward neural network with POS tags features (Palmer and Hearst, 1997), an SVM classifier with features such as length and the case of the words occurring before and after the PMs (Gillick, 2009), deep neural models using characters from the surrounding context (Schweter and Ahmed, 2019) of candidate SUs, or a two-layer Transformer encoder using the surrounding context words (Wicks and Post, 2021). However, all these approaches focus on proofread and edited documents, always assuming the existence of EPMS in all SUs. This assumption does not hold for informal, user-generated text or clinical notes with minimal proofreading and post-editing. As a consequence, several studies noted a substantial decline in performance when these systems move to texts with less formal language (Read et al., 2012; Rudrapal et al., 2015).

Another competing supervised approach treats sentence segmentation as a sequence labeling task, assigning a tag to each input unit to mark sentence boundaries (Evang et al., 2013; Toleu et al., 2017; Du et al., 2019; Geng, 2022). This approach has the advantage of not relying on EPMs and can segment ungrammatical and fragmented texts. For example, Elephant (Evang et al., 2013) uses a CRF classifier to jointly segment tokens and sentences. By tagging each character in the input sequence, their classifier can identify SUs ending with various characters. Several works (Du et al., 2019; Rehbein et al., 2020; Udagawa et al., 2023) apply BERT-based sequence labeling classifiers for sentence segmentation. Due to the sequence length constraint of BERT models, these approaches split the original documents/texts into smaller sequences as inputs for BERT. This splitting is achieved either through domain knowledge, such as identifying pauses, speaker turns, or discourse markers from spoken language transcripts (Du et al., 2019), or by using an existing sentence segmentation tool (Udagawa et al., 2023). In contrast, our approach employs a sliding window to segment long sequence text on the fly, requiring no domain knowledge or off-the-shelf tools for pre-processing, which makes it easily applicable to texts from different domains and genres.

The approach proposed by Udagawa et al. (2023) shares similarities with ours in extending sentence segmentation beyond formal, standardized text using BERT-based sequence labeling classifier. Their method involves a two-step process: firstly, applying ERSATZ (Wicks and Post, 2021) - a segmentation tool based on punctuations - to the raw text; and secondly, using a classifier on the segmented text to detect sentence boundaries. However, in their evaluation, they ignore the boundaries of fragmented sentences generated by ERSATZ. Additionally, instead of directly identifying sentence boundaries during the sequence labeling step, as in our approach, they use a dynamic programming algorithm to infer labels for the entire document.
=====END FILE=====

=====FILE: sections/03_methods.tex=====
\section{Methods}

We approach sentence segmentation as a sequence labeling task using a BIO tagging scheme (shown in Figure 1). In this scheme, each token in an input sequence is assigned a tag to mark sentence boundaries: B indicates the Beginning of a sentence, I represents Inside of a sentence, and O denotes Outside of a sentence. We chose this tagging schema as it allows not only to segment sentences from a document but also to differentiate SUs (labelled as B and I) from non-SUs (labelled as O), also known as sentence identification task (Udagawa et al., 2023). Non-SUs typically include metadata from email attachments, markups in web text, irregular series of nouns, repetition of symbols for separating texts, and plain text tables in clinical notes, among other examples. All these non-SUs require additional text cleaning for downstream tasks. Unless otherwise specified, we do not differentiate between sentence identification and sentence segmentation in the following sections.

\begin{figure}[ht]
\centering
\fbox{
\begin{minipage}{0.9\textwidth}
\centering
\textbf{IMAGE NOT PROVIDED} \
\vspace{1em}
[Diagram showing Sliding window algorithm for sentence segmentation. Three sliding windows (SW-1, SW-2, SW-3) over text. Example text: "Neuro <n> Mental status Sedated No response to verbal stimuli". Pred tags: B, 0, 0, B, I, 1, I, 0, B...]
\end{minipage}
}
\caption{Sliding window algorithm for sentence segmentation. We segment the text using three sliding windows sequentially (SW-1, SW-2, and SW-3). Each sliding window contains up to 8 tokens. The final sentence segmentation tags are at the top (Pred) of the diagram.}
\label{fig:sliding_window}
\end{figure}

Formally, let  represent an input sequence that consists of  tokens;  represent a sequence of BIO labels. So the goal of sentence segmentation task is to find a label sequence  which satisfies:
\begin{itemize}
\item , when  is the first token of a SU.
\item , when  is any token within a SU except for the first token.
\item , when  is any token outside of a SU.
\end{itemize}

Pre-trained language models (PLM) (Edunov et al., 2019) have shown great improvements in NLP tasks, encompassing text classification, named entity recognition, or question answering, among others. Here, we use BERT (Devlin et al., 2019) in a sequence labelling configuration, where we feed a list of input tokens  to BERT, followed by a Softmax classification layer to predict the conditional probability of .

\subsection{Sliding window algorithm}

Because of the quadratic computational cost along with the sequence length of the self-attention in transformer architecture (Vaswani et al., 2017), and the pre-training configuration of BERT-style PLMs, BERT models can only take input sequences with up to 512 tokens. Although the development of sparse attention mechanisms in transformer networks has improved the capability of PLMs for long sequence text (Beltagy et al., 2020), it is still challenging to take an entire clinical note as one input sequence. To segment long sequence text using BERT models, we propose a sliding window algorithm to process the input text, and then repetitively tag the text within a smaller sliding window (shown in Figure 1).

\begin{algorithm}[h]
\caption{Sliding window algorithm for sentence segmentation.}
\label{alg:sliding_window}
\begin{algorithmic}[1]
\Function{Segment_Text}{}
\State 
\Repeat
\State 
\While{}
\State 
\State 
\State Concatenate  to 
\State 
\State 
\State 
\State 
\EndWhile
\State 
\State Append  to 
\Until{}
\State \Return 
\EndFunction
\end{algorithmic}
\end{algorithm}

Let  be the maximal sequence length of any PLMs, and  be a sliding window of  tokens from the text input. The main idea of our algorithm is to tag each token within a sliding window, and then slide the text window based on the predicted sentence boundary. Specifically, for each sliding window, we find the start index of the first sentence  by locating the first B label in  (line 9 of algorithm 1), the start index of the second sentence  by locating the second B label in  (line 10), and the end index of the first sentence  by locating the last I label preceding  in  (line 11). We then slide the input window to the start of the second sentence . If there is no second sentence from the current sliding window (line 5), we slide the window by  tokens (line 12), and predict the labels for the new sliding window. We then concatenate the labels of multiple text windows to find the second sentence. During the training, since we already know all the sentence boundary indices beforehand, we generate the training instance by directly moving the sliding window along each sentence, where each text window always starts with the first token of a sentence, and has a length of  tokens.

\begin{figure}[ht]
\centering
\fbox{
\begin{minipage}{0.9\textwidth}
\textbf{IMAGE NOT PROVIDED} \
\vspace{1em}
[Example text from image: \
\textbf{B}Neuro:\textbackslash n$>$ \
\textbackslash n$>$ \
Mental status: Sedated. _ No response to verbal stimuli. Grimaces$<>$ to noxious. No speech output. Not following commands. \textbackslash n$>$ \
\textbackslash n$>$ \
\textbf{B}Cranial Nerves:\textbackslash n$>$ \
\textbf{B}1.: Not tested$<>$ \
\textbf{B}II.: Pupils equally round and minimally reactive to light, 3 to$<>$ 2 mm bilaterally. E_Blinks to threat on right. \textbf{B} Unable to appreciate$<>$ fundi \textbackslash n$>$ \
\textbf{B}III, IV, VI: Assessment of oculocephlic limited by neck$<>$ stiffness. \textbackslash n$>$ \
\textbf{B}V, VII: Obscurred by ETT.\textbackslash n$>$ \
\textbf{B}VIII: Unable to assess. \textbackslash n$>$ \
\textbf{B}IX, X: +Gag.\textbackslash n$>$ \
\textbf{B} [**Doctor First Name 81**]: Unable to assess. \textbackslash n$>$ \
\textbf{B}XII: ETT.\textbackslash n$><>$'' to mark a newline character from the original note.}
\label{fig:annotation_example}
\end{figure}
=====END FILE=====

=====FILE: sections/04_datasets.tex=====
\section{Datasets}

\subsection{MIMIC-III dataset annotation}

To the best of our knowledge, there is no manually annotated sentence segmentation dataset in clinical domain. Zhang et al. (2021a) created a silver-standard treebank from clinical notes in the MIMIC-III using the default CoreNLP tokenizer (Manning et al., 2014), and later train and evaluate the Stanza (Qi et al., 2020) on such treebank for syntactic analysis. However, their treebank dataset was not reviewed by domain experts, and the evaluation on their treebank basically reflects how well other sentence segmentation approaches master the segmentation rules in Stanford CoreNLP library.

There are also other clinical datasets (Uzuner et al., 2007, 2011, 2012; Sun et al., 2013) containing sentence boundary information, where the clinical notes have already been pre-processed with each sentence placed on a separate line. However, this modified structure does not reflect the format of real-world clinical notes. To address this gap, we collected a subset of clinical notes from the MIMIC-III corpus (Johnson et al., 2016), and manually annotated sentence boundaries without changing the original structure of clinical notes.

MIMIC-III contains de-identified clinical notes from 38,597 distinct patients admitted to a Beth Israel Deaconess Medical Center between 2001 and 2012. It covers 15 note types including discharge summary, physician note, radiology report, social work, among others. We randomly sampled 6 notes for each note type for annotation, yielding 90 notes in total. We stratified the notes into training, development, and test sets (57/15/18), respectively.

Clinical text presents unique challenges for syntactic annotation due to the irregular usage of punctuation, incomplete or fragmented sentences, and a blend of structured and narrative text formats, as illustrated in Figure 2. Guidelines designed for syntactic annotation in texts following typical structural and writing conventions might not be suitable for detecting sentence boundaries within the clinical domain. To mitigate these challenges, we developed a detailed annotation guideline and summarized what constitutes a sentence in the clinical note genre (more details in appendix A.1):
\begin{itemize}
\item Grammatically linked words written in an uninterrupted sequence that follow the conventional rules of a sentence in English, with or without an appropriate EPM.
\item A text fragment that conveys a complete thought, e.g., a section header, or each item in a form or bulleted list, such as `Lab Test'', `Results'', or ``Diagnosis'', among many others.
\end{itemize}

One major challenge in our annotation is to distinguish a table from a list in clinical notes. Table text typically contains column headers, row labels, and texts from individual cells. We can not simply separate table text into multiple sentences by rows or cells because interpreting each cell requires an understanding of the original tabular structure, which is not typically included (and usually cannot be included due to technical limitations) in a data export from electronic health record systems such as EPIC. Thus, we assign O labels to the entire table text and leave parsing table text into sentences for future work.

Two annotators independently annotated each note, with the lead annotator being an expert in annotating clinical notes. At the first iteration, the annotators independently annotated the entire 90 notes, and notes without complete agreement were discussed until resolution during the second iteration. During the first iteration (on 15 notes), it took an average of 5.7 minutes to annotate each note. Before resolution, the inter-annotator agreement was 0.89 F1 (Hripcsak and Rothschild, 2005) on sentence boundary annotation which is considered moderate to strong agreement (McHugh, 2012).

\subsection{Other datasets}

To check whether our proposed approach is data-agnostic, we extensively evaluated our approach on other standard corpora from different domains and genres, including 1) biomedical domain with clinical notes (i2b2-2010), and abstracts of biomedical articles (Genia); and 2) the general domain, including various sources of English texts (Brown and WSJ) and web text (EWT). We summarize the dataset statistics in Table 1. Specifically, we examined whether the dataset format had any modifications during pre-processing or remained in its original form. For the general domain corpora, they assume each document is a disjoint union of sentences (no document information and no O tokens). However, since WSJ and EWT provide the original documents where each sentence belongs, we processed their annotations, and mapped each sentence into its original document (Original row in Table 1). We also analyzed statistics related to different sentence structures, such as sentences ending with EPMs, alphanumeric characters, or PMs other than EPMs (OPM). These sentence characteristics contribute to the complexity faced by different sentence segmentation approaches.

\textbf{i2b2-2010} The i2b2-2010 corpus (Uzuner et al., 2011) consists of 426 labeled clinical notes (43,940 sentences). The corpus was released in 2010 i2b2 shared task focused on identifying concepts, assertions, and relations in discharge summaries and progress reports. This corpus had already been pre-processed, with each sentence placed on a separate line for each note. This pre-processing step simplifies both the original i2b2 shared task and the sentence segmentation task, as original clinical texts typically contain multiple newline characters within a sentence and multiple sentences within a single line. For our experiments, we maintain the same train/dev/test splits as in the 2010 i2b2 challenge.

\textbf{Genia} The Genia corpus (Kim et al., 2003) is a collection of 1,999 MEDLINE abstracts with 16,479 sentences related to transcription factors in human blood cells. These abstracts are unstructured text, and meticulously edited to include complete sentences. We use the split in Griffis et al. (2016) and randomly sample 400 and 200 documents for the development and test sets, respectively.

\textbf{EWT} The English Web Treebank (Silveira et al., 2014) comprises 1174 samples of web text sourced from five distinct genres: blog posts, newsgroup threads, emails, product reviews and answers from question-answer websites. Similar to the clinical corpus, EWT contains incomplete and fragmented sentences, but in general domain English language. We use the standard train/dev/test splits.

\textbf{Brown} The Brown corpus (Francis and Kucera, 1964) contains 500 samples of running text of edited American-English prose. Each sample begins at the beginning of a sentence but not necessarily of a paragraph or other larger division, and it ends at the first sentence ending after 2000 words. The text is drawn from a variety of sources such as books, newspapers, magazines, and transcripts of spoken language. Thus, this corpus have much formal sentence units. In our experiments, we load the corpus from the NLTK library (Bird and Loper, 2004), where sentences from each document are separated by empty spaces. We randomly sample 10% and 20% files for the development and test sets, respectively.

\textbf{WSJ} The WSJ corpus (Paul and Baker, 1992) contains 2312 samples of running text primarily sourced from the Wall Street Journal newspaper, covering a wide range of topics related to business, finance, economics, and current affairs. We pre-process this corpus to keep the original format of each running text based on their raw text file. We follow the configuration in Bird and Loper (2004) to keep section 24 for validation, and sections 03-06 for test.

A major difference between these datasets is their sentence structure. For clinical notes, MIMIC-III and i2b2-2010 have only around 39% and 52% of sentences end with EPMs (Sentence-EPM), respectively, compared against around 90% of sentences with EPMs in Brown and WSJ, and 99% of sentences in Genia. For approaches that purely rely on EPMs for sentence segmentation, they could only detect up to 52% of sentences for clinical notes, while 90% for general domain texts. This indicates the limitation of purely using EPM information for sentence segmentation. Clinical notes and web texts (EWT) have more sentences ending with alphanumeric characters (Sentence-Alphanum) or non-sentence ending PMs (Sentence-OPM) than the general domain texts or biomedical articles; they also often use newline characters to separate sentence. This indicates the importance of understanding text contents and text formats for sentence segmentation, especially for clinical notes and web texts.

\begin{table*}[ht]
\centering
\small
\begin{tabular}{l c c c c c c}
\toprule
& \multicolumn{2}{c}{\textbf{Biomedical Domain}} & \multicolumn{4}{c}{\textbf{General Domain}} \
\cmidrule(lr){2-3} \cmidrule(lr){4-7}
& \textbf{MIMIC-III} & \textbf{i2b2-2010} & \textbf{Genia} & \textbf{EWT} & \textbf{Brown} & \textbf{WSJ} \
\midrule
\textbf{Documents} & 90 & 426 & 1,999 & 1,174 & 500 & 2,312 \
Original & Y & N & Y & Y & N & Y \
Train/Dev/Test & 57/15/18 & 265/161/256 & 1,399/400/200 & 540/318/316 & 350/50/100 & 1,876/55/381 \
\midrule
\textbf{Sentence} & 4,142 & 43,940 & 16,479 & 16,621 & 57,340 & 49,208 \
Sentence-EPM & 39.0% & 52.0% & 99.8% & 77.3% & 91.6% & 92.4% \
Sentence-Alphanum & 44.4% & 23.8% & 0.0% & 14.9% & 2.0% & 0.9% \
Sentence-OPM & 16.6% & 24.2% & 0.2% & 8.1% & 6.4% & 6.7% \
Sentence-Sep-Nl & 70.2% & 99.0% & 0.0% & 22.3% & 0.0% & 86.3% \
\bottomrule
\end{tabular}
\caption{Dataset statistics. Original indicates that a dataset has its original format . Sentence-EPM indicates the percentage of sentences ending with a EPM. Sentence-Alphanum indicates the percentage of sentences ending with an alphanumeric character. Sentence-OPM indicates the percentage of sentences ending with a PM other than an EPM. Sentence-Sep-Nl indicates the percentage of sentences separated by at least one newline character.}
\label{tab:datasets}
\end{table*}
=====END FILE=====

=====FILE: sections/05_experiments.tex=====
\section{Experiments}

\subsection{Comparisons with related approaches}
We compared our proposed approach against seven off-the-shelf sentence segmentation systems: NLTK (Bird and Loper, 2004), CoreNLP (Manning et al., 2014), cTAKES (Savova et al., 2010), Syntok\footnote{\url{[https://github.com/fnl/syntok](https://github.com/fnl/syntok)}}, spaCy\footnote{\url{[https://spacy.io/](https://spacy.io/)}}, Stanza (Qi et al., 2020), Trankit (Nguyen et al., 2021). We selected these segmenters because they are state-of-the-art and easy-to-run standard NLP tools, and therefore widely used "as is" by the community when processing text data. We provide a detailed description of each tool in appendix A.2.

\subsection{Experiment details}
As our MIMIC-III dataset contains non-sentential tokens (tagged as O) such as table text, for a fair comparison between these tools and our approach on the MIMIC-III dataset, we created an alternative evaluation, MIMIC-III$_{p}$ (shown in table 2). Specifically, we post-process the segmented output from off-the-shelf tools with six rules that take into account the text structures, such as removing multiple empty spaces or newline characters from the sentence boundary if they are at the end of a sentence. We also remove non-sentential tokens before segmentation during evaluation. For clinical notes (MIMIC-III and i2b2-2010), and biomedical articles (Genia), we chose PubMed-BERT (Gu et al., 2021) for our sequence labeling classifier. PubMed-BERT is a domain-specific language model pre-trained on biomedical text from scratch; it has achieved state-of-the-art performances on multiple biomedical NLP tasks. While for the general domain corpus (EWT, Brown, and WSJ), we chose RoBERTa-base (Liu et al., 2019).

One limitation of BERT-style PLMs is that their tokenizers remove newline characters from input, which makes it challenging to segment text when newline characters are the only sentence separators. To mitigate this issue, we insert the newline character as a special token in the tokenizer to keep the text format signal. Training details are illustrated in appendix A.3. We trained two types of models: 1) Segmenter-Data, where we trained one model on each dataset (six models in total); 2) Segmenter-Domain, where we combined datasets from each domain, and train one model on the biomedical domain, and one model on the general domain.

\subsection{Evaluation}
We evaluated each system by comparing the predicted sentence spans against the gold annotations in the test sets. We measured the performance using the standard F1 evaluation metric, consistent with the evaluation adopted in the 2018 UD Shared Task for sentence boundary detection (Zeman et al., 2018). A sentence span is defined as a pair of offsets representing the first and last characters of a sentence. A predicted sentence span is considered accurate only if both offsets in the predicted pair match those in the gold annotation pair.
=====END FILE=====

=====FILE: sections/06_results.tex=====
\section{Results}

On the MIMIC-III dataset, table 2 shows that our models outperform off-the-shelf tools by large margins .

\begin{table*}[ht]
\centering
\small
\begin{tabular}{l c c c c c c c c}
\toprule
\textbf{Approach} & \textbf{MIMIC-III} & \textbf{MIMIC-III} & \textbf{i2b2-2010} & \textbf{Genia} & \textbf{EWT} & \textbf{Brown} & \textbf{WSJ} & \textbf{Avg. Rank} \
\midrule
NLTK & 39.14 & 70.84 & 39.59 & 97.31 & 66.48 & 64.75 & 81.57 & 6.83 \
CoreNLP & 39.08 & 70.75 & 42.94 & 98.47 & 66.59 & 84.64 & 93.14 & 5.67 \
cTAKES & 21.66 & 26.81 & 92.99 & 70.35 & 32.64 & 69.50 & 76.65 & 7.50 \
Syntok & 37.81 & 70.67 & 45.51 & 96.93 & 66.65 & 82.18 & 90.79 & 6.50 \
spaCy & 16.74 & 47.87 & 23.69 & 98.92 & 60.86 & 88.22 & 16.00 & 6.83 \
Stanza & 40.00 & 72.20 & 53.59 & 97.04 & 89.31 & 86.43 & 93.78 & 4.50 \
Trankit & 51.87 & 60.20 & 58.68 & 97.18 & 91.00 & 88.01 & 97.18 & 3.50 \
\textbf{Our Segmenter-Data} & \textbf{87.86} & \textbf{88.34} & \textbf{97.89} & 99.82 & \textbf{92.42} & \textbf{98.60} & 93.43 & \textbf{1.67} \
\textbf{Our Segmenter-Domain} & 85.41 & 87.03 & 97.71 & \textbf{99.91} & 91.10 & 98.39 & \textbf{93.55} & 2.00 \
\bottomrule
\end{tabular}
\caption{Comparison of our proposed approach against off-the-shelf sentence segmenters. MIMIC-III$*{p}$ is an alternative evaluation on MIMIC-III dataset, where we post-processed the segmented outputs from all the off-the-shelf tools, and removed non-sentential tokens for a fair comparison. The last column Avg. Rank shows the average rank of each segmentation system across the datasets. We excluded the MIMIC-III$*{p}$ column when computing Avg. Rank, as it is not the real-world setting. The system with the best average rank is highlighted in grey; the best F1 scores on each dataset are bolded.}
\label{tab:results}
\end{table*}
=====END FILE=====