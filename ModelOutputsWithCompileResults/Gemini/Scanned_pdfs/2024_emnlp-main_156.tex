=====FILE: main.tex=====
\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\geometry{margin=1in}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{natbib}
\usepackage{hyperref}
\usepackage{authblk}

\title{To Word Senses and Beyond: Inducing Concepts with Contextualized Language Models}

\author{Bastien Liétard}
\author{Pascal Denis}
\author{Mikaela Keller}
\affil{University of Lille, Inria, CNRS, Centrale Lille, \ UMR 9189-CRISTAL, F-59000 Lille, France \ \texttt{first_name.last_name@inria.fr}}

\date{}

\begin{document}

\maketitle

\begin{abstract}
Polysemy and synonymy are two crucial inter-related facets of lexical ambiguity. While both phenomena are widely documented in lexical resources and have been studied extensively in NLP, leading to dedicated systems, they are often being considered independently in practical problems. While many tasks dealing with polysemy (e.g. Word Sense Disambiguation or Induction) highlight the role of word's senses, the study of synonymy is rooted in the study of concepts, i.e. meanings shared across the lexicon. In this paper, we introduce Concept Induction, the unsupervised task of learning a soft clustering among words that defines a set of concepts directly from data. This task generalizes Word Sense Induction. We propose a bi-level approach to Concept Induction that leverages both a local lemma-centric view and a global cross-lexicon view to induce concepts. We evaluate the obtained clustering on SemCor's annotated data and obtain good performance (BCubed  above 0.60). We find that the local and the global levels are mutually beneficial to induce concepts and also senses in our setting. Finally, we create static embeddings representing our induced concepts and use them on the Word-in-Context task, obtaining competitive performance with the State-of-the-Art.
\end{abstract}

\section{Introduction}

A crucial challenge in understanding natural language comes from the fact that the mapping between word forms and lexical meanings is many-to-many, due to polysemy (i.e., the multiplicity of meanings for a given form) and synonymy (i.e., the multiplicity of forms for expressing a given meaning). Both polysemy and synonymy have been thoroughly studied in NLP, but mostly as independent problems, giving rise to dedicated systems. Thus, Word Sense Disambiguation (WSD)\footnote{In this paper, we take polysemy in its most comprehensive definition, also including homonymy.} aims at correctly mapping word occurrences to one of its senses \citep{raganato2017}, while Word Sense Induction (WSI), its unsupervised counterpart, aims at clustering word occurrences into latent senses directly from data \citep{manandhar2010,jurgens2013}. More recently, researchers have proposed the task of Word-in-Context (WiC), which consists in classifying pairs of word occurrences depending on whether they realize the same sense or not \citep{pilehvar2019}. All these works take a word centric view, which aims at identifying or characterizing the different senses of a given word, where these senses are bound to a word.

Another line of work, which takes a broader lexicon-wide perspective, is concerned with identifying synonyms, which are equivalence classes over different words that point to the same concept \citep{zhang2021,ghanem2023}, where concepts are semantic entities that are not bound to a word. In WordNet \citep{miller1995,fellbaum1998}, concepts are called synsets, defined as sets of synonyms. However, outside of lexical resources, synonymy and polysemy are usually considered as independent problems in the NLP literature. Yet, these two views are complementary. In lexicology, they correspond to two perspectives on the word-meaning mapping: semasiology and onomasiology. The former is the word-to-meanings view, where one can observe polysemy by looking at the different meanings a given word has. The latter is the meaning-to-words view, in which one can study synonymy by looking at the inventory of words that speakers use to express the same meaning.

In this paper, we propose a new task, called Concept Induction, that directly aims at learning concepts in an unsupervised manner from raw text. More precisely, this task aims at learning a soft clustering over a target lexicon (i.e., a set of words), in such a way that each cluster corresponds to a (latent) concept. Thus, this task both addresses polysemy (since polysemous words should appear in multiple clusters) and synonymy (since synonymous words should appear in the same cluster(s)). Inducing concepts can be interesting for many external applications, like building lexical resources for low-resources languages \citep{velasco2023}, and can bring a different perspective in computational studies of meaning, moving the usual word-centric focus to a more meaning-centric state.

Our approach to Concept Induction relies on word occurrences for a target lexicon, represented as word embeddings derived from a Contextualized Language Model (in this case, BERT Large \citep{devlin2019}), which are then grouped, using hard clustering algorithms, into concept denoting clusters. While these concept clusters could in principle be obtained directly from word occurrences, we propose a bi-level methodology that leverages both a local, lemma-centric clustering (i.e., operating on only specific word occurrences), and a global, cross-lexicon clustering (i.e., operating on all words occurrences). From this perspective, our approach generalizes, and in fact builds upon classical Word Sense Induction, in that word senses are learned jointly alongside with concepts. We hypothesize that an approach taking both complementary resolutions in account will lead to improved Concept Induction and Word Sense Induction, i.e. that the two objectives can be mutually beneficial.

To validate our approach, we carried out experiments on the SemCor dataset, which provides a set of concepts (taking the form of WordNet synsets) related to word occurrences. We found that our bi-level clustering approach accurately learn concepts, achieving  scores above 0.60 on the task of Concept Induction compared to WordNet's synsets, outperforming competing approaches that use only local and global views. This demonstrates the benefits of our bi-level approach, and its ability to leverage both local and global views when inducing concepts. Interestingly, we show that the benefits go both ways: our proposed approach outperforms lemma-centric approaches when evaluated for WSI. Finally, we show that concept-aware static embeddings derived from our approach are also competitive with state-of-the-art approaches efficient on the Word-in-Context task, while using less training data. Through the new task of concept induction, we also contribute in a new way to the ongoing debate regarding the ability to align vector representations extracted from Contextualized Language Models to the semantic representations posited by (psycho-)linguists. In this vein, we conduct a qualitative evaluation of obtained clusters to ensure they indeed reflect concepts and gather synonyms. The source code we used for experiments is available at \url{[https://github.com/blietard/concept-induction](https://www.google.com/search?q=https://github.com/blietard/concept-induction)}.

\section{Related Work}

\subsection{Lexical resources for concepts}
Princeton's WordNet (PWN) \citep{miller1995,fellbaum1998} is a lexical database that has been the most widely used as a reference for most wordsense-related tasks for many years. In WordNet, the entry corresponding to a lemma has different wordsenses, each of them mapping to a synset. Synsets are WordNet's equivalents of our concepts. Lemmas whose wordsenses belong to the same synset are synonymous. WordNet 3.0 contains 117,659 synsets and is built from the work of psycholinguists and lexicographers, that not only describes synonymy but also other lexical relations such as hypernymy/hyponymy, antonymy, meronymy/holonymy, etc. But the amount of resources needed to create such lexical databases with human experts is considerable, making them a very rare and precious resource. They are not available for a large number of active languages, and even more rare for dead languages \citep{bizzoni2014,khan2022}.

\subsection{Word senses with Language Models}
With the recent development of neural Contextualized Language Models (CLM), several work use their hidden-layers to extract vector representations of word usages and retrieve word senses. These representations are fed to a classification (for WSD) or a clustering (in the case of WSI) algorithm to distinguish the word's senses \citep{scarlini2020,nair2020,saidi2023}. These embeddings-based approaches have applications in other fields: \citet{kutuzov2020} and \citet{martinc2020} use sense clusters found using CLM embeddings to study the change in meaning of words, and \citet{chronis2020} propose a many-Kmeans method to investigate semantic similarity and relatedness. Another line of work uses list of substitute tokens sampled from the CLM head to infer senses \citep{amrami2019,eyal2022} and are successful on WSI benchmarks like \citet{manandhar2010} and \citet{jurgens2013}.

\subsection{Structures of Meaning in CLM}
Recent research probes neural CLMs for alignments between representations from their latent spaces and semantic patterns and relations. Section 7.2 of \citet{haber2024} summarizes findings about polysemy in contextualized CLMs, showing that these models were able to detect polysemy and in some cases distinguish actual polysemy from homonymy. They report that representations from different senses may however overlap. \citet{hanna2021} shows that pretrained BERT embed knowledge of hypernymy but is limited to the more common hyponyms.

\citet{velasco2023} build on top of WSI techniques in an attempt to automatically construct a WordNet for Filipino, thus proposing a modeling of synonymy in this language. However, the evaluation of the synsets they obtained is limited by the lack of sense-annotated data for Filipino, and they could not evaluate the impact of their methodology on the two levels (senses and concepts).

Works like \citet{ethayarajh2019} and \citet{chronis2020} study the kind of information that was distributed across layers. The former concludes that syntactic and word-order information are distributed in the first layers while in deeper layers, representations are heavily influenced by contexts. The latter demonstrates, with a multi-prototypes embedding approach, that semantic similarity is best found in moderately late layers, while relatedness is best found in last layers.

\section{Concept Induction}

Our main motivation behind Concept Induction is to present a view of the mapping between words and their meaning(s).\footnote{This mapping is called patterns of lexification by \citet{francois2022}; see also coexpression and synexpression in the terminology proposed by \citet{haspelmath2023}.} This view is systemic, meaning that it should not be defined for individual words neither for individual concepts, but rather acknowledging these as a whole with interactions and relations. This extends beyond the primary objective of WSI, which defines word senses as pertaining to individual words only and does not explore relations between lemmas or concepts.

\subsection{Basic notions}
Consider a set of target words (or lemmas) and for each lemma, we have a set of occurrences of this word in a context (e.g. a sentence or a phrase). The set of target lemmas is referred to as the lexicon, while the corpus is the set of all occurrences. Our goal is to study the meaning of target words as they are used in the corpus.

In this study we call sense of a word its usage to refer to a concept. A polysemous word has multiple senses, each of them referring to a distinct concept. Two words are said to be synonyms for a given concept when each of them has one of their senses referring to this shared concept. Senses are defined `locally'', i.e. bound to an individual word of the lexicon, as opposed to concepts which are defined `globally'', i.e. across the whole lexicon. An occurrence of a word  realizes one of 's senses.

Consider the words `test'' and `trial'' and the following corpus:
(A) the jury found them guilty in a fair trial.
(B) candidates competed in a trial of skill.
(C) the hero underwent a test of strength.

The corpus is composed of two occurrences of `trial'' and one occurrence of `test.'' In the corpus, `trial'' is polysemous. Its first sense, illustrated in A, refers to a process of law. Its second sense, in B, refers to the concept of the act of undergoing testing. The sense of `test'' in sentence C also corresponds to this concept: it's a case where `test'' and `trials'' are synonymous. Shifting the focus from senses to concepts, we will say that B and C instantiate the same concept, while A is an instance of a different concept.

\subsection{Task definition}
The goal of Concept Induction (CI) is to automatically learn a set of concepts directly from the data, i.e. learning a soft clustering  in the set of target words  that should correspond to the multiple concepts instantiated by occurrences of the corpus.  is a soft clustering because a word can be assigned to several clusters (when it is polysemous). Using a different perspective than WSI, the framework of Concept Induction provides a more complete view on meaning across the lexicon. Both WSI and CI capture polysemy, but CI also reveals synonymy across the lexicon. Like WSI, Concept Induction does not require a pre-defined set of concepts.

\begin{figure}[ht]
\centering
\fbox{\parbox{0.8\linewidth}{\centering IMAGE NOT PROVIDED}}
\caption{Illustration of our framework. The words `trial'' is polysemous and has two senses corresponding to two different concepts, and is synonym with `test'' for this second meaning.}
\label{fig:framework}
\end{figure}

\subsection{Formal framework}
Let  be the lexicon. For all word  in , we denote  the -th occurrence of  in the corpus. We define  the set of  occurrences of . The corpus, denoted , is the union of all .

For a given word  the set  can be partitioned according to its different senses. We denote  the part of occurrences of  in the corpus corresponding to the -th sense of . We refer to these groups of occurrences as the sense clusters of . The set  forms a partition of  and we call  the set of all sense clusters of all words, i.e. .  is a ``local'' (lemma-centric) partition of the whole . The task of Word Sense Induction aims at learning the partition  given a corpus .

In this work, we aim at dividing the corpus into concepts instead of senses. We denote  the group of occurrences of words corresponding to the concept indexed by , and  the partition of  in  concept clusters. Unlike sense clusters of , a concept cluster  can gather occurrences of different words:  is a ``global'' partition.

Each occurrence  of a word  is associated to a sense cluster  and a concept cluster . We can say that a concept corresponding to  is instantiated by occurrence  through the sense corresponding to , or conversely that  uses the sense reflected in  to mean the concept described by concept cluster . All occurrences of sense cluster  appear in the same concept cluster .

In summary,  and  are partitions of  and are naturally constrained as follows:
\begin{enumerate}
\item By definition, a sense in  is associated to one and only one word .
\item An occurrence  realizes exactly one sense .
\item An occurrence  instantiates exactly one concept .
\item In a given sense , all occurrences are assigned to the same concept .
\item All  (i.e. same word) refer to distinct concepts.
\end{enumerate}

From the partition  on occurrences, one can derive  a clustering of the set of words  into concepts. To each concept cluster  we associate a cluster in  that contains all lemmas of  whose occurrences were assigned to . In  a polysemous word with  senses appears in  distinct clusters (one per sense), and synonyms appear in at least one common cluster (one per shared concept).

We denote  the word-level soft-clustering and  the partition of occurrences that are learned on the data. In Figure \ref{fig:framework} we illustrate this framework, using a corpus of occurrences of the words `test'' and `trial''. In this scenario,  {`test'', `trial''} and two concepts are instantiated: a process of law to determine someone's guilt and a challenge to evaluate a skill. The lemma `trial'' exhibits two senses as it has occurrences corresponding to both concepts: `trial'' is polysemous. The second concept is also instantiated by occurrences of `test'', therefore `trial'' and ``test'' show synonymy in this case. This toy example also follows all constraints formulated above.

\section{Methodology}

In this section we describe the methods we propose and evaluate for Concept Induction. We learn a clustering  drawing inspiration from the relations between , ,  and . In particular, the overall objective of our methodology consist in finding  (i.e. partition occurrences into concept clusters) to derive . Section 3.3 highlighted that there are two levels of partitions: a local level (senses) and a global one (concepts). The proposed approaches rely on both levels and the use of a Contextualized Language Model (CLM) to gather representations of occurrences influenced by the context.

\subsection{Proposed Bi-level Method}

\paragraph{Local (lemma-centric) clustering}
Firstly, we propose to learn a word-sense partition for each target words individually. Using the CLM hidden layers, we extract a vector representation (the occurrence embedding) of every occurrence . We then learn a partition  of each  using a clustering algorithm on the embeddings. Each  describes the locally estimated sense clusters of word . Jointly considering these partitions for all , we obtain a partition  of the whole set of occurrences . This partition is local in the sense that each word has its occurrences clustered independently from other words.

\paragraph{Global (cross-lexicon) clustering}
Once we have a local clustering , we turn from considering words independently to consider all words together. In this step, we learn a global clustering by merging local clusters of occurrences. To do so, we average embeddings of all occurrences in the same local cluster to get a single embedding representing each local cluster. Then we run a second clustering algorithm, this time using the averaged embeddings of local clusters. This global clustering defines a new partition  of the the corpus : when two local clusters  and  are merged into the same global cluster  (because their embeddings were clustered together), all their occurrences are assigned to global cluster . From this global occurrence partition  we can easily extract , a word-level soft-clustering of lemmas whose occurrences appear in the same .

This Bi-level method directly implements the system of constraints described in Section 3.3. Only constraint 5 is not enforced by design. Indeed, our local clusters being learned and not informed by an expert, the local clustering step may make errors, especially if the data for a given word are sparse. Allowing the global clustering to merge local clusters enables the correction of local clustering's recall errors using information from the global level. We also want to highlight that the proposed methodology is generic, in the sense that it is not tied to a specific choice of clustering algorithm.

\subsection{Local-only and Global-only}

\paragraph{Local-only Systems}
Sense-inducing systems (WSI approaches) that create only local clusters of occurrences for each word are said to be Local-only systems. We use them as baseline models that only produce word-level clusters of size 1 and do not reflect synonymy, but still learn polysemy.

\paragraph{Global-only Systems}
On the other hand, consider a system in which each occurrence is mapped to its own local cluster (i.e. no actual local clustering step), and the global step divides occurrences directly into global clusters. We refer to this kind of system as Global-only approaches. They allow to evaluate how useful the local clustering step is in the process: we hypothesize that the local step in Bi-level will reduce potential variance in occurrences by aggregating them, increasing Precision compared to Global-only.

\section{Experiments}

In this section, we evaluate the abilities of the proposed methods to induce concepts and compare the proposed bi-level approach to other methods. We investigate the advantages of the bi-level approach not only for the global viewpoint but also in the local setting.

\subsection{Settings}

\paragraph{Data.}
We choose to use the annotated part of the SemCor 3.0\footnote{\url{[http://web.eecs.umich.edu/~mihalcea/downloads.html#semcor](https://www.google.com/search?q=http://web.eecs.umich.edu/~mihalcea/downloads.html%23semcor)}} corpus. This dataset contains occurrences for a wide number of words, and morphosyntactic annotations provide their lemma and their Part-of-Speech tag. Among all lemmas having at least 10 annotated occurrences, we keep only nouns (excluding proper nouns) composed only of alphabetical characters with a minimum length or 3 letters.\footnote{For the sake of simplicity and clarity, this study is focused only on nouns. Indeed, other Parts-of-Speech induce extra difficulties. Verbs for instance required extra preprocessing steps and decisions (e.g. include or exclude gerundive uses, past participle employed like adjectives, etc.). Extension of experiments to other PoS is left to future work.} The resulting lexicon  contains 1,560 different lemmas, for which we gather a corpus  containing a total of 52,997 occurrences. SemCor is also semantically annotated, with each occurrence of a target lemma assigned to a synset in WordNet, that we consider to be the concept it refers to. We derive a reference partition of the occurrences  and a reference soft-clustering of the words  from annotations, for a total of 3,858 concepts.

\paragraph{Representations.}
Following \citet{chronis2020} and \citet{eyal2022}, we use BERT Large \citep{devlin2019}, a masked language model with 24 layers and 345M parameters. This allows for direct comparisons with these approaches. Also, BERT Large was found by \citet{haber2021} to allow for better grouping of sense interpretations than other LLMs. We average subwords' embeddings if needed. It is a common practice in previous work on semantic-related tasks to use the average of the last 4 layers to get embeddings; we decided to adopt the same ``4 layers average pooling'' strategy, but trying with different possible sets of layers (see Appendix C). Therefore, for a set of four layers, we average hidden states across the selected layers to get a single 1024-dimensional vector. We found that layers 14 to 17 obtained the best results on Dev for all methods (global/local-only and bi-level). We leave to further work the use of autoregressive and/or newer Language Models.

\subsection{Results}

The proposed Concept Induction systems reach scores ranging from .56 to .66 on the full data, half of them outperforming the Lemmas baseline, and from .59 to .62 on the Synon. split, outperforming all other systems. While still challenging, it exhibits that it is indeed possible to induce WordNet-based concepts in a corpus using LMs hidden layers vectors. We also see that Kmeans-based approaches are consistently outperformed by Agglomerative methods. This indicates that the representational spaces in LM hidden layers are not organized in a nearly-spherical fashion as Kmeans algorithm assumes, but rather are populated less uniformly. This is reflected in precision and recall: Agglomerative systems reach a higher precision than Kmeans with similar recall.

Overall, results are in favor of Bi-level approaches over Global-only systems, with substantial improvements in  on the full data while obtaining (nearly) identical performance on concepts of multiple lemmas, and large increases in precision while the loss in recall is minimal. This demonstrates that considering the local (lemma-centric) perspective is beneficial to a global (cross-lexicon) view when inducing concepts. The local clustering, with the subsequent representation averaging, helps reducing variance in occurrences and therefore allow to reach higher levels of precision in the global clustering compared to Global-only.

\begin{table}[ht]
\centering
\caption{Concept Induction Results}
\begin{tabular}{lcccccc}
\toprule
& \multicolumn{3}{c}{Full data} & \multicolumn{3}{c}{Synon.} \
&  & P & R &  & P & R \
\midrule
\multicolumn{7}{l}{\textbf{Baselines}} \
Lemmas & .61 & 1.0 & .43 & .61 & 1.0 & .50 \
Oracle WSI & .86 & 1.0 & .75 & .56 & 1.0 & 39.56 \
\midrule
\multicolumn{7}{l}{\textbf{Local-only Systems}} \
Kmeans Local & .70 & .73 & .71 & .49 & .67 & .38 \
Agglo Local & .73 & .92 & .67 & .35 & .92 & .50 \
Eyal et al. (2022) & .44 & .75 & .31 & .38 & .37 & .39 \
\midrule
\multicolumn{7}{l}{\textbf{CI Systems}} \
Kmeans Global & .60 & .48 & .65 & .56 & .68 & .54 \
Kmeans Bi-level & .64 & .59 & .70 & .59 & .82 & .47 \
Agglo Global & .60 & .61 & .60 & .62 & .82 & .50 \
Agglo Bi-level & .66 & .75 & .60 & .62 & .86 & .49 \
\bottomrule
\end{tabular}
\end{table}

\begin{table}[ht]
\centering
\caption{Cluster Statistics}
\begin{tabular}{lcc}
\toprule
\textbf{Cluster size} & \textbf{2-3} & \textbf{4+} \
\midrule
Nb. of annotated clusters & 50 & 23 \
\midrule
\multicolumn{3}{l}{\textbf{Category (% of annotated clusters)}} \
Synonyms & 38 & 17 \
Near-synonyms & 24 & 35 \
Related & 26 & 48 \
Invalid & 08 & 0 \
\bottomrule
\end{tabular}
\end{table}

\begin{table}[ht]
\centering
\caption{WSI Results}
\begin{tabular}{lcc}
\toprule
& \textbf{WSI}  & \textbf{p} \
\midrule
\multicolumn{3}{l}{\textbf{Local-only Systems}} \
Kmeans Local & .61 & NA \
Agglo Local & .77 & .04 \
Eyal et al. (2022) & .46 & .76 \
\midrule
\multicolumn{3}{l}{\textbf{CI Systems}} \
Kmeans Global & .51 & .51 \
Kmeans Bi-level & .78 & .30 \
Agglo Global & .80 & .53 \
Agglo Bi-level & .80 & .46 \
\bottomrule
\end{tabular}
\end{table}

\section{Extrinsic Evaluation with Concept-aware Embeddings}
In their work, \citet{eyal2022} derive sense-aware static embeddings from their WSI method, training them on the Wikipedia dataset and used them for the Word-in-Context (WiC) task. They achieve nearly-SotA results on the dataset proposed by \citet{pilehvar2019}, and report to be outperformed only by methods using external lexical knowledge and resources.

We proceed to the same extrinsic evaluation of our work, constructing concept-aware embeddings using concept clusters of Concept Induction systems (Global-only and Bi-level Agglo). To obtain such embeddings, we average all vectors representing occurrences in SemCor contained each global cluster to get one vector per concept cluster.

The WiC task consists of determining whether two occurrences of a target lemma  correspond to the same sense. The WiC dataset's target words are nouns and verbs, but like in the rest of this paper, we restrict our scope to nouns. To solve the task, we use BERT Large to create representations of the two target occurrences. Each of them is assigned to a concept by finding the closest concept-aware using cosine distance. The decision depends on whether the two occurrences are mapped to the same concept (true) or to distinct ones (false).

Results are displayed in Table 4.

\begin{table}[ht]
\centering
\caption{Accuracy scores on the nouns of the WiC test dataset (Pilehvar and Camacho-Collados, 2019).}
\begin{tabular}{lc}
\toprule
\textbf{Model} & \textbf{Acc.} \
\midrule
Eyal et al. (2022) (CBOW) & 59.3 \
Eyal et al. (2022) (Skip-Grams) & 61.9 \
Ours (Agglo global) & 58.8 \
Ours (Agglo bi-level) & 59.7 \
\bottomrule
\end{tabular}
\end{table}

Our methodology uses pretrained Contextualized Language Models, which are known to encode and replicate social biases contained in their training data and sometimes amplify them. While we do not observe surface-level biases arising when manually annotating concept clusters, it is still an open question of how these social biases may influence or even change results when inducing concepts in SemCor.

\section{Conclusion}
In this paper, we argued that, while word senses allow to investigate polysemy, concepts are a larger perspective that allows the study of polysemy as well as synonymy. We defined Concept Induction, the unsupervised task to learn a soft-clustering of words in a large lexicon, directly from their in-context occurrences in a corpus. Then, we proposed a formulation of this problem in terms of local (lemma-centric) and global (cross-lexicon) complementary views, and tested an approach that uses information from both levels using contextualized Language Models. On concept-annotated SemCor corpus, we found that this bi-level view was beneficial for Concept Induction, and even for Word Sense Induction with a low amount of training data. We validated the quality of obtained clusters with manual evaluation.

\section*{Acknowledgements}
We gratefully thank the anonymous reviewers for their insightful comments. This research was funded by Inria Exploratory Action COMANCHE.

\begin{thebibliography}{99}

\bibitem[Amigó et al.(2009)]{amigo2009}
Enrique Amigó, Julio Gonzalo, Javier Artiles, and Felisa Verdejo. 2009.
\newblock A comparison of extrinsic clustering evaluation metrics based on formal constraints.
\newblock \emph{Information retrieval}, 12:461--486.

\bibitem[Amrami and Goldberg(2019)]{amrami2019}
Asaf Amrami and Yoav Goldberg. 2019.
\newblock Towards better substitution-based word sense induction.

\bibitem[Bagga and Baldwin(1998)]{bagga1998}
Amit Bagga and Breck Baldwin. 1998.
\newblock Entity-based cross-document coreferencing using the vector space model.
\newblock In \emph{36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics}, Volume 1, pages 79--85, Montreal, Quebec, Canada. Association for Computational Linguistics.

\bibitem[Bizzoni et al.(2014)]{bizzoni2014}
Yuri Bizzoni et al. 2014. [MISSING CITATION DETAILS]

\bibitem[Chronis and Erk(2020)]{chronis2020}
Chronis and Erk. 2020. [MISSING CITATION DETAILS]

\bibitem[Devlin et al.(2019)]{devlin2019}
Devlin et al. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.

\bibitem[Ethayarajh(2019)]{ethayarajh2019}
Kawin Ethayarajh. 2019.
\newblock How contextual are contextualized word representations? Comparing the geometry of BERT, ELMo, and GPT-2 embeddings.
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)}, pages 55--65, Hong Kong, China. Association for Computational Linguistics.

\bibitem[Eyal et al.(2022)]{eyal2022}
Matan Eyal, Shoval Sadde, Hillel Taub-Tabib, and Yoav Goldberg. 2022.
\newblock Large scale substitution-based word sense induction.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 4738--4752, Dublin, Ireland. Association for Computational Linguistics.

\bibitem[Fellbaum(1998)]{fellbaum1998}
Christiane Fellbaum. 1998.
\newblock \emph{WordNet: An electronic lexical database}.
\newblock MIT press.

\bibitem[François(2022)]{francois2022}
Alexandre François. 2022.
\newblock Lexical tectonics: Mapping structural change in patterns of lexification.
\newblock \emph{Zeitschrift für Sprachwissenschaft}, 41(1):89--123.

\bibitem[Geeraerts(2010)]{geeraerts2010}
Dirk Geeraerts. 2010.
\newblock \emph{Theories of Lexical Semantics}.
\newblock Oxford University Press.

\bibitem[Ghanem et al.(2023)]{ghanem2023}
Sana Ghanem, Mustafa Jarrar, Radi Jarrar, and Ibrahim Bounhas. 2023.
\newblock A benchmark and scoring algorithm for enriching Arabic synonyms.
\newblock In \emph{Proceedings of the 12th Global Wordnet Conference}, pages 274--283, University of the Basque Country, Donostia - San Sebastian, Basque Country. Global Wordnet Association.

\bibitem[Haber and Poesio(2021)]{haber2021}
Janosch Haber and Massimo Poesio. 2021.
\newblock Patterns of polysemy and homonymy in contextualised language models.
\newblock In \emph{Findings of the Association for Computational Linguistics: EMNLP 2021}, pages 2663--2676, Punta Cana, Dominican Republic. Association for Computational Linguistics.

\bibitem[Haber and Poesio(2024)]{haber2024}
Janosch Haber and Massimo Poesio. 2024.
\newblock Polysemy Evidence from linguistics, behavioral science, and contextualized language models.
\newblock \emph{Computational Linguistics}, 50(1):351--417.

\bibitem[Hanna and Mareček(2021)]{hanna2021}
Michael Hanna and David Mareček. 2021.
\newblock Analyzing BERT's knowledge of hypernymy via prompting.
\newblock In \emph{Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP}, pages 275--282, Punta Cana, Dominican Republic. Association for Computational Linguistics.

\bibitem[Haspelmath(2023)]{haspelmath2023}
Martin Haspelmath. 2023. [MISSING CITATION DETAILS]

\bibitem[Jurgens and Klapaftis(2013)]{jurgens2013}
Jurgens and Klapaftis. 2013. [MISSING CITATION DETAILS]

\bibitem[Khan et al.(2022)]{khan2022}
Khan et al. 2022. [MISSING CITATION DETAILS]

\bibitem[Kutuzov and Giulianelli(2020)]{kutuzov2020}
Andrey Kutuzov and Mario Giulianelli. 2020.
\newblock UiO UvA at SemEval-2020 task 1: Contextualised embeddings for lexical semantic change detection.
\newblock In \emph{Proceedings of the Fourteenth Workshop on Semantic Evaluation}, pages 126--134, Barcelona (online). International Committee for Computational Linguistics.

\bibitem[Manandhar et al.(2010)]{manandhar2010}
Suresh Manandhar, Ioannis Klapaftis, Dmitriy Dligach, and Sameer Pradhan. 2010.
\newblock SemEval-2010 task 14: Word sense induction & disambiguation.
\newblock In \emph{Proceedings of the 5th International Workshop on Semantic Evaluation}, pages 63--68, Uppsala, Sweden. Association for Computational Linguistics.

\bibitem[Martinc et al.(2020)]{martinc2020}
Matej Martinc, Syrielle Montariol, Elaine Zosa, and Lidia Pivovarova. 2020.
\newblock Capturing evolution in word usage: Just add more clusters?
\newblock In \emph{Companion Proceedings of the Web Conference 2020}, pages 343--349, New York, NY, USA. Association for Computing Machinery.

\bibitem[Miller(1995)]{miller1995}
George A. Miller. 1995.
\newblock WordNet: A lexical database for English.
\newblock \emph{Communications of the ACM}, 38(11):39--41.

\bibitem[Nair et al.(2020)]{nair2020}
Nair et al. 2020. [MISSING CITATION DETAILS]

\bibitem[Pilehvar and Camacho-Collados(2019)]{pilehvar2019}
Pilehvar and Camacho-Collados. 2019. [MISSING CITATION DETAILS]

\bibitem[Raganato et al.(2017)]{raganato2017}
Raganato et al. 2017. [MISSING CITATION DETAILS]

\bibitem[Saidi and Jarray(2023)]{saidi2023}
Saidi and Jarray. 2023. [MISSING CITATION DETAILS]

\bibitem[Scarlini et al.(2020)]{scarlini2020}
Scarlini et al. 2020. [MISSING CITATION DETAILS]

\bibitem[Tahmasebi et al.(2021)]{tahmasebi2021}
Nina Tahmasebi, Lars Borin, and Adam Jatowt. 2021.
\newblock Survey of computational approaches to lexical semantic change detection.
\newblock \emph{Computational approaches to semantic change}, 6(1).

\bibitem[Velasco et al.(2023)]{velasco2023}
Dan John Velasco, Axel Alba, Trisha Gail Pelagio, Bryce Anthony Ramirez, Jan Christian Blaise Cruz, Unisse Chua, Briane Paul Samson, and Charibeth Cheng. 2023.
\newblock Towards automatic construction of Filipino WordNet: Word sense induction and synset induction using sentence embeddings.
\newblock In \emph{Proceedings of the First Workshop in South East Asian Language Processing}, pages 1--12, Nusa Dua, Bali, Indonesia. Association for Computational Linguistics.

\bibitem[Zhang et al.(2021)]{zhang2021}
Jingqing Zhang, Luis Bolanos Trujillo, Tong Li, Ashwani Tanwar, Guilherme Freire, Xian Yang, Julia Ive, Vibhor Gupta, and Yike Guo. 2021.
\newblock Self-supervised detection of contextual synonyms in a multi-class setting: Phenotype annotation use case.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing}, pages 8754--8769, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.

\end{thebibliography}

\appendix

\section{Extended BCubed to Evaluate CI and WSI}
The extension of BCubed for overlapping clusters rely on two quantities, Multiplicity Precision (MP) and Multiplicity Recall (MR). In the case of Concept Induction, MP and MR between two lemmas are defined as follows:

[ILLEGIBLE MATH CONTENT]

\section{[MISSING APPENDIX]}

\section{Hyperparameters}

\subsection{[MISSING SECTION]}

\subsection{Hyperparameters}
For \citet{eyal2022}, we tried different resolution, varying it from 1e-3 to 10, for the Louvain clustering but found very little to no effect.
For Kmeans at the local level, we varied the number of clusters  between 2 and 10. For Agglomerative clustering at both levels, we tried single, average and complete linkage.
The distance threshold in Agglo T was indexed on the distribution of distances. We fixed an hyperparameter  and derived  with  the distribution of distances between clustered instances. We made  vary between -4 and +8. For global Kmeans, the number of clusters was indexed using a proportion  on the number of lemmas (e.g. ),  varying from  to . This may help transferring hyperparameters to other dataset in future research.
Best hyperparameters choices are in Table 6.

\section{Concept Clusters Size Distribution}
The distribution of the concept cluster size (in number of lemmas) obtained with Bi-level Agglo system can be found in Figure 2.

Table 6: Best hyperparameters on the Dev split [MISSING TABLE]

\begin{figure}[ht]
\centering
\fbox{\parbox{0.8\linewidth}{\centering IMAGE NOT PROVIDED}}
\caption{Distribution of cluster size (in number of lemmas) obtained by the Bi-level Agglo system.}
\label{fig:cluster_size}
\end{figure}

\section{Scientific Artifacts}
We used WordNet and SemCor, both properties of Princeton University. Licence can be found at \url{https://...} [ILLEGIBLE]

\end{document}
=====END FILE=====