=====FILE: main.tex=====
\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{natbib}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{float}
\usepackage{caption}

\geometry{left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm}

\title{Revisiting Supertagging for Faster HPSG Parsing}
\author{Olga Zamaraeva and Carlos Gómez-Rodríguez}
\date{
Universidade da Coruña, CITIC \
Departamento de Ciencias de la Computación y Tecnologías de la Información \
Campus de Elviña s/n, 15071, A Coruña, Spain \
\texttt{{olga.zamaraeva, carlos.gomez}@udc.es}
}

\begin{document}

\maketitle

\begin{abstract}
We present new supertaggers trained on English grammar-based treebanks and test the effects of the best tagger on parsing speed and accuracy. The treebanks are produced automatically by large manually built grammars and feature high-quality annotation based on a well-developed linguistic theory (HPSG). The English Resource Grammar treebanks include diverse and challenging test datasets, beyond the usual WSJ section 23 and Wikipedia data. HPSG supertagging has previously relied on MaxEnt-based models. We use SVM and neural CRF- and BERT-based methods and show that both SVM and neural supertaggers achieve considerably higher accuracy compared to the baseline and lead to an increase not only in the parsing speed but also the parser accuracy with respect to gold dependency structures. Our fine-tuned BERT-based tagger achieves 97.26% accuracy on 950 sentences from WSJ23 and 93.88% on the out-of-domain technical essay \textit{The Cathedral and the Bazaar} (cb). We present experiments with integrating the best supertagger into an HPSG parser and observe a speedup of a factor of 3 with respect to the system which uses no tagging at all, as well as large recall gains and an overall precision gain. We also compare our system to an existing integrated tagger and show that although the well-integrated tagger remains the fastest, our experimental system can be more accurate. Finally, we hope that the diverse and difficult datasets we used for evaluation will gain more popularity in the field: we show that results can differ depending on the dataset, even if it is an in-domain one. We contribute the complete datasets reformatted for Huggingface token classification.
\end{abstract}

\section{Introduction}

We present new supertaggers for English and use them to improve parsing efficiency for Head-driven Phrase Structure Grammars (HPSG). Grammars have been gaining relevance in the natural language processing (NLP) landscape \citep{Someya2024}, since it is hard to interpret and evaluate the output of NLP systems without robust theories.

Head-Driven Phrase Structure Grammar \citep{PollardSag1994} is a theory of syntax that has been applied in computational linguistic research (see \citealp{BenderEmerson2021}, \S3-\S4). At the core of such research are precision grammars which encode a strict notion of grammaticality; their purpose is to cover and generate only grammatical structures. They include a relatively small set of phrase-structure rules and a large lexicon where lexical entries contain information about the word's syntactic behavior. HPSG treebanks (and the grammars that produce them) encode not only constituency but also dependency and semantic relations and have proven useful in natural language processing, e.g.\ in grammar coaching \citep{FlickingerYu2013, MorgadoDaCosta2016, MorgadoDaCosta2020}, natural language generation \citep{Hajdik2019}, and as training data for high precision semantic parsers \citep{Lin2022, Chen2018, BuysBlunsom2017}. Assuming a good parse ranking model, a treebank is produced automatically by parsing text with the grammar, and any updates are encoded systematically in the grammar, with no need of manual treebank annotation.\footnote{For a good parse ranking model, it is necessary to select ``gold'' parses from a potentially large parse forest at least once. This can be done semi-automatically \citep{Packard2015}.}

HPSG parsing, which is typically bottom-up chart parsing, is both relatively slow and RAM-hungry. Often, more than a second is required to parse a sentence (see Table 7), and sometimes the performance is prohibitively bad for long sentences, with a typical user machine requiring unreasonable amounts of RAM to finish parsing with a large parse chart \citep{Marimon2014, OepenCarroll2002}. It is important to emphasize that this is the state of the art in HPSG parsing, and its speed is one of the reasons why the true potential of HPSG parsing in NLP remains not fully realized despite the evidence that it helps create highly precise training data automatically.

Approaches to speed up HPSG parsing include local ambiguity packing \citep{Tomita1985, Malouf2000, OepenCarroll2002}, on the one hand, and forgoing exact search and reducing the parser search space, on the other \citep{Dridan2008, Dridan2009, Dridan2013}. Here we contribute to the second line of research, aka supertagging, a technique to discard unlikely interpretations of tokens. \citet{Dridan2008} and \citet{Dridan2009, Dridan2013} used maximum entropy-based models trained on a combination of gold and automatically labeled data from English, requiring large-scale computation. They report an efficiency improvement of a factor of 3 for the parser they worked with \citep{Callmeier2000} and accuracy improvements with respect to the ParsEval metric.

We present new models for HPSG supertagging, an SVM-based one, a neural CRF-based one, and a fine-tuned-BERT one, and compare their tagging accuracy with a MaxEnt baseline. We now have more English gold training data thanks to the HPSG grammar engineering consortium's treebanking efforts \citep{Flickinger2000, Oepen2004, Flickinger2011, Flickinger2012}.\footnote{The data is available as part of the 2023 release of the English Resource Grammar (the ERG): \url{[https://github.com/delph-in/docs/wiki/RedwoodsTop](https://www.google.com/search?q=https://github.com/delph-in/docs/wiki/RedwoodsTop)}.} It makes sense to train modern models on this wealth of gold data. Then we use the supertags to filter the parse chart at the lexical analysis stage, so that the parser has fewer possibilities to consider. We report the results of parsing all of the test data associated with the English HPSG treebanks \citep{OepenCarroll2002} in comparison with parsing the same data with the same parsing algorithm but with no tagging at all, as well as with the integrated MEMM-based tagger. If we use the tagger with some exceptions, our system is the most accurate one (using the partial dependency match metric). It is not faster that the MEMM-based tagger integrated into the parser for production mode, although it is of course much faster than parsing without tagging (by a factor of 3).

The paper is organized as follows. In \S2, we give the background necessary for understanding the provenance of our training data. \S3 presents the methodology, starting from previous work (\S3.1). We then describe our training and evaluation data (\S3.2), and finally how we trained the new supertaggers (\S3.3). In \S4, we present the results: first for the accuracy of the supertagger (\S4.1) and then for the parsing experiments, including parsing speed and parsing accuracy (\S4.2).

We trained the neural models with NVIDIA GeForce RTX 2080 GPU, CUDA version 11.2. The SVM model and the MaxEnt baseline were trained using Intel Core i7-9700K 3.60Hz CPU. The parser was run on the same CPU. The code and configurations for the reported results as well as the datasets are online.\footnote{\url{[https://github.com/olzama/neural-supertagging](https://github.com/olzama/neural-supertagging)}} The original data we used is publicly available. Further details can be found in the Appendix.

\section{Background}

Below we explain HPSG lexical types (\S2.1), which serve as the tags that we predict, and in \S2.2, we give the background on the English treebanks which served as our training and evaluation data. \S2.3 is a summary for HPSG parsing and the specific parser that we are using for the experiments.

\subsection{Lexical types}

Any HPSG grammar consists of a hierarchy of types, including phrasal and lexical types, and of a large lexicon which can be used to map surface tokens to lexical types. Each token in the text is recognized by the parser as belonging to one or more of the lexical entries in the lexicon (assuming such an orthographic form is present at all). Lexical entries, in turn, belong to lexical types (Figure 1). Lexical types are similar to POS tags but are more fine grained (e.g.\ a precision grammar may distinguish between multiple types of proper nouns or multiple types of wh-words, etc).

\begin{figure}[ht]
\centering
\fbox{IMAGE NOT PROVIDED}
\caption{Part of the HPSG type hierarchy (simplified; adapted from ERG). NB: This is not a derivation.}
\label{fig:hpsg_type_hierarchy}
\end{figure}

Figure 1 shows the ancestry of two senses of the English word \textit{bark}, a verb (\textit{to bark}) and a noun (\textit{tree bark}). The types differ from each other in features and their values. For example, the HEAD feature value is different for nouns and verbs; one of the characteristics of the main verb type is that it is not a question word; the noun subtype denotes divisible entities, etc. The token \textit{bark} will be interpreted as either a verb or a noun during lexical analysis parsing stage. After the lexical analysis, the bottom-up parser runs a constraint unification-based algorithm \citep{Carpenter1992} to return a (possibly empty) set of parses. To emphasize, a parser in this context is a separate program implementing a parsing algorithm. The grammar is the type hierarchy which the parser takes as input along with the sentence to parse.

\subsection{The ERG treebanks}

The English Resource Grammar (ERG; \citealp{Flickinger2000, Flickinger2011}) is a broad-coverage precision grammar of English implemented in the HPSG formalism. The latest release is from 2023.\footnote{\url{[https://github.com/delph-in/docs/wiki/ErgTop](https://github.com/delph-in/docs/wiki/ErgTop)}} Its intrinsic evaluation relies on a set of English text corpora. Each release of the ERG includes a treebank of those texts parsed by the current version. The parses are created automatically and the gold structure is verified manually. Treebanking in the ERG context is the process of choosing linguistically (semantically) correct structures from the multiple trees corresponding to one string that the grammar may produce. Fast treebanking is made possible by automatically comparing parse forests and by discriminant-based bulk elimination of unwanted trees \citep{Oepen1999, Packard2015}. The treebanks are stored as databases that can be processed with specialized software e.g.\ Pydelphin.\footnote{\url{[https://pydelphin.readthedocs.io/](https://pydelphin.readthedocs.io/)}}

The 2023 ERG release comes with 30 treebanked corpora containing over 1.5 million tokens and 105,155 sentences. In principle, there are 43,505 different lexical types in the ERG (cf.\ 48 tags in the Penn Treebank POS tagset (PTB; \citealp{Marcus1993})) however only 1299 of them are found in the training portion of the treebank. The genres include well-edited text (news, Wikipedia articles, fiction, travel brochures, and technical essays) as well as customer service emails and transcribed phone conversations. There are also constructed test suites illustrating linguistic phenomena such as raising and control. The ERG treebanks present more challenging test data compared to the conventional WSJ23 (which is also included). The ERG 2023's average accuracy (correct structure) over all the corpora is 93.77%; the raw coverage (some structure) is 96.96%. The ERG uses PTB-style punctuation tokens and includes PTB POS tags in all tokens, along with a lexical type (\S2.1).

\subsection{HPSG parsing}

Several parsers for different variations of the HPSG formalism exist. We work with the DELPH-IN formalism \citep{Copestake2002} which is deliberately restricted for theoretical and performance considerations; it only encodes the unification operation natively (and not e.g.\ relational constraints). Still, the parsing algorithms' worst-case complexity is intractable \citep{OepenCarroll2002}. \citet{Carroll1993} (cited in \citealp{BenderEmerson2021}, p.1109) states that the worst-case parsing time for HPSG feature structures is proportional to  where  is the maximum number of children in a phrase structure rule and  is the (potentially large) maximum number of feature structures. The unification operator takes two feature structures as input and outputs one feature structure which satisfies the constraints encoded in both inputs. Given the complex nature of such structures, implementing a fast unification parser is a hard problem. As it is, the existing parsers may take prohibitively long to parse a long sentence (see e.g.\ \citealp{Marimon2014} as well as \S4.2 of this paper).

\section{Methodology}

Supertagging \citep{BangaloreJoshi1999} reduces the parser search space by discarding the less likely interpretations of an orthography. For example, the word \textit{bark} in English can be a verb or a noun, and in \textit{The dog barks} it is a lot less likely to be a noun than a verb (see also Figure 1). In principle, there are at least two possible interpretations of the sentence \textit{The dog barks}, as can be seen in Figure 2. With supertagging, the pragmatically unlikely second interpretation would be discarded by discarding the noun lexical type (\textit{mass-count noun} in Figure 1) possibility for the word \textit{barks}. In HPSG, there are fine-grained lexical types within the POS class (e.g.\ subtypes of common nouns or wh-words), so the search space can be reduced further.

\begin{figure}[ht]
\centering
\fbox{IMAGE NOT PROVIDED}
\caption{Two interpretations of the sentence \textit{The dog barks}. The second one is an unlikely noun phrase fragment, which would be discarded with the supertagging technique. (Trees provided by the English Resource Grammar Delphin-viz online demo.)}
\label{fig:dog_barks_trees}
\end{figure}

In precision grammars, supertagging comes at a cost to coverage and accuracy; selecting a wrong lexical type even for one word means the entire sentence will likely not be parsed correctly. Thus the accuracy of the tagger is crucial. Related to this is the matter of how many possibilities to consider for supertags: the more are considered, the slower the parsing, but the higher the accuracy. In this paper, we experiment with a single, highest-scored tag for each token. However, we combine this strategy (which prioritizes parsing speed) with a list of tokens exempt from supertagging (which increases accuracy).

\subsection{Previous and related work}

\citet{BangaloreJoshi1999} introduced the concept of supertagging. \citet{ClarkCurran2003} showed mathematically that supertagging improves parsing efficiency for a lexicalized formalism (CCG). They used a maximum entropy model; \citet{Xu2015} introduced a neural supertagger for CCG. \citet{Vaswani2016} and \citet{Tian2020} further improved the accuracy of neural-based CCG supertagging achieving an accuracy of 96.25% on WSJ23. \citet{Liu2021} use finer categories within the CCG tagset and report 95.5% accuracy on in-domain test data and 81% and 92.4% accuracy on two out-of-domain datasets (Bioinfer and Wikipedia). \citet{Prange2021} have started exploring the long-tail phenomena related to supertagging and strategies to not discard rare tags. \citet{KogkalidisMoortgat2023} have shown how supertagging, through its relation to underlying grammar principles, improves neural networks' abilities to deal with rare (``out-of-vocabulary'') words.

Supertagging experiments with HPSG parsing speed using hand-engineered grammars are summarized in Table 1. In addition, there were experiments on the use of supertagging for parse ranking with statistically derived HPSG-like grammars \citep{Ninomiya2007, Matsuzaki2007, MiyaoTsujii2008, Zhang2009, Zhang2010, ZhangKrieger2011, Zhang2012}. These statistically derived systems are principally different from the ERG as they do not represent HPSG theory as understood by syntacticians.\footnote{These works do not report experiments on parsing speed; they are concerned with tagging accuracy issues only.} In the context of the ERG, \citet{Dridan2008} represents our baseline SOTA for the tagger accuracy. \citet{Dridan2013} is a related work on ``ubertagging'', which includes multi-word expressions. Specifically, an ubertagger considers various multi-word spans, whereas a supertagger relies on a standard tokenizer. We use the ubertagger that was implemented for the ACE parser for the parsing speed experiments, as the baseline (\S4.2). \citet{Dridan2013} parsing accuracy results, however, are not comparable to ours; she used a different dataset, a different parser, and a different accuracy metric.

\begin{table}[ht]
\centering
\caption{Supertagging effects on HPSG parsing speed.}
\label{tab:speed_effects}
\begin{tabular}{llccc}
\toprule
\textbf{Model} & \textbf{Grammar} & \textbf{Training tok} & \textbf{Tagset size} & \textbf{Speed-up factor} \
\midrule
N-gram \citep{PrinsVanNoord2004} & Alpino (Dutch) & 24 mln & 1,365 & 2 \
HMM \citep{Blunsom2007} & ERG (English) & 113K & 615 & 8.5 \
MEMM \citep{Dridan2009} & ERG (English) & 158K & 676 & 12 \
\bottomrule
\end{tabular}
\end{table}

\subsection{Data}

We train and evaluate our taggers, both for the baseline (\S4.1.1) and for the experiment (\S3.3), on gold lexical types from the ERG 2023 release (\S2.2). We use the train-dev-test split recommended in the release.\footnote{Download redwoods.xls from the ERG repository for details and see \url{[https://github.com/delph-in/docs/wiki/RedwoodsTop](https://www.google.com/search?q=https://github.com/delph-in/docs/wiki/RedwoodsTop)}. This split is different than in \citet{Dridan2009}.} There are 84,894 sentences in the training data, 2,045 in dev, and 7,918 in test. WSJ section 23 is used as test data, as is traditional, but so are a number of other corpora, notably \textit{The Cathedral and the Bazaar} \citep{Raymond1999}, a technical essay which serves as the out-of-domain test data. See Table 2 for the details about the test data. The column titled ``training tokens'' shows the number of tokens for the training dataset which is from the same domain as the test dataset in the row. For example, WSJ23 has 23K tokens and WSJ1-22 have 960K tokens in the ERG treebanks.

\subsection{SVM, LSTM+CRF, and fine-tuned BERT}

We train a liblinear SVM model with default parameters (L2 Squared Hinge loss, , one-v-rest, up to 1,000 training iterations) using the scikit-learn library.

[Content continues describing models, but is missing from snippet. Reconstructed based on context:]
We utilize NCRF++ and a BERT-based model. We fine-tune BERT for the supertagging task.

\subsection{Integration with the ACE Parser}

[Section reconstructed based on context]
The ACE parser is a performant HPSG parser. It is intended for settings which include individual use, including with limited RAM. This parser has default RAM settings which can be modified, and also an in-built ``ubertagger''. While the ubertagger is based on \citet{Dridan2013}, it is not the same thing and its performance has never been published before. In particular, its tagging accuracy is unknown and we did not seek to evaluate it (evaluating a different MaxEnt model instead). The ubertagger was integrated into the ACE parser code with great care, optimizing for performance. We also do not seek to compete with such optimizations in our experiments.

For our experiments, we provide ACE with the tags predicted by the best supertagger (the BERT-based supertagger) along with the character spans corresponding to the token for which the tag was predicted. We then prune all lexical chart edges which correspond to this token span but do not have the predicted lexical type. As such, we follow the general idea of using supertagging for reducing the lexical chart size but we do not use the same code that the integrated ubertagger uses for this procedure. We assume that our code could be further optimized for production.

\subsection{Exceptions for supertagging}

As already mentioned, mistakes in supertagging are very costly for precision grammar parsing; one wrongly predicted lexical type means the entire sentence will not be parsed correctly. After the maxent-based supertaggers were trained by \citet{Dridan2009} and \citet{Dridan2013}, the developer of the English Resource Grammar, Flickinger, experimented with them and has come up with a list of lexical types which the supertagger tended to predict wrong. The list included fine-grained lexical types representing words such as \textit{do}, \textit{many}, \textit{less}, \textit{hard} (among many others).\footnote{The full list can be found in the release of the ERG in the folder titled 'ut' (ubertagging).} Using such exception lists counteracts the effects of supertagging and slows down the parsing, while improving accuracy.

\section{Results}

\subsection{Tagger accuracy and tagging speed}

\subsubsection{Tagging accuracy baseline}

For our baseline, we use a MaxEnt model similar to \citet{Dridan2009}. While \citet{Dridan2009} used off-the-shelf TnT \citep{Brants2000} and C&C \citep{ClarkCurran2003} taggers, we use the off-the-shelf logistic regression library from scikit-learn \citep{Pedregosa2011} which is a popular off-the-shelf tool for classic machine learning algorithms. The baseline tagger accuracy is included in Table 2. The details on how the best baseline model was chosen are in Appendix A. The results are presented in Table 2.

\subsubsection{Tagger accuracy results}

Table 2 shows that the baseline models achieve similar performance to \citet{Dridan2009} (D2009 in Table 2) on in-domain data and are better on out-of-domain data. This may indicate that these models are close to their maximum performance on in-domain data on this task but adding more training data still helps for out-of-domain data. \citet{Dridan2009} models were trained on a subset of our data. \citet{Dridan2009} reports getting 91.47% accuracy on the in-domain data using the TnT tagger \citep{Brants2000}. The SVM and the neural models are better than the baseline models on all test datasets, and fine-tuned BERT is the best overall.

\begin{table}[ht]
\centering
\caption{Tagger accuracy (partially reconstructed).}
\label{tab:tagger_accuracy}
\begin{tabular}{llccccc}
\toprule
\textbf{Dataset} & \textbf{Description} & \textbf{Sent} & \textbf{MaxEnt} & \textbf{SVM} & \textbf{BERT} & \textbf{D2009} \
\midrule
cb & technical essay & 713 & [MISSING] & [MISSING] & 93.88 & [MISSING] \
wsj23 & WSJ section 23 & 950 & [MISSING] & [MISSING] & 97.26 & [MISSING] \
\bottomrule
\end{tabular}
\end{table}

\subsection{Results: Parsing Speed and Accuracy}

We measure the effect of supertagging on parsing speed and accuracy using the ACE parser (\S3.4). Recall that HPSG parsing is chart parsing, and for a large grammar, the charts can be huge.

[Content regarding parsing speed results is partially missing, but discussed in Discussion.]

\section{Discussion}

Our system is strictly faster than the baseline, by a factor of 3, although on two datasets (e-commerce and WSJ) it fails to achieve a speedup factor of 2. The ubertagger is still the fastest overall, remarkably by a factor of 12, on average across all datasets. This is not too surprising because the supertagger is experimental and it is hard for it to compete with the ubertagger which was integrated into the parser for production, with the focus on performance. We believe that the supertagger could be integrated better into the parser's C code in the future. In other words, its current speed is in part a purely C engineering problem.

On the other hand, clearly the exceptions list would have an effect. Since we are excluding 15 types of words from pruning, the supertagger's lexical chart is likely to be bigger than the ubertagger's. This is the expected tension between speed and accuracy that we expected to see, and our supertagger system shows overall benefits in both speed and accuracy. The only dataset on which our system is not the best in accuracy is the e-commerce (ecpr). It appears that for this type of data, tagging is the least effective.

\section{Conclusion}

We used the advancements in HPSG treebanking to train more accurate supertaggers. The ERG is a major project in syntactic theory and an important resource for creating high quality semantic treebanks. It has the potential to contribute to NLP tasks that require high precision and/or interpretability including probing of the LLMs, and thus making HPSG parsing faster is strategic for NLP. We tested the new supertagging models with the state-of-the-art HPSG parser and saw improvements in parsing speed as well as accuracy.

\bibliographystyle{plainnat}
\begin{thebibliography}{99}

\bibitem[Bangalore and Joshi(1999)]{BangaloreJoshi1999}
Srinivas Bangalore and Aravind K. Joshi. 1999.
Supertagging: An approach to almost parsing.
\textit{Computational Linguistics}, 25(2):237--265.

\bibitem[Bender and Emerson(2021)]{BenderEmerson2021}
Emily M. Bender and Guy Emerson. 2021.
Computational linguistics and grammar engineering.
In \textit{The Handbook of Computational Linguistics and Natural Language Processing}.

\bibitem[Blunsom(2007)]{Blunsom2007}
Phil Blunsom. 2007.
Structured Classification for Multilingual and Grammar-Based Processing.
Ph.D. thesis, University of Melbourne.

\bibitem[Brants(2000)]{Brants2000}
Thorsten Brants. 2000.
TnT: a statistical part-of-speech tagger.
In \textit{Proceedings of the Sixth Conference on Applied Natural Language Processing}, pages 224--231.

\bibitem[Buys and Blunsom(2017)]{BuysBlunsom2017}
Jan Buys and Phil Blunsom. 2017.
Robust incremental neural semantic graph parsing.
In \textit{ACL}.

\bibitem[Callmeier(2000)]{Callmeier2000}
Ulrich Callmeier. 2000.
PET: a platform for experimentation with efficient HPSG processing techniques.
\textit{Natural Language Engineering}, 6(1):99--118.

\bibitem[Carpenter(1992)]{Carpenter1992}
Bob Carpenter. 1992.
\textit{The Logic of Typed Feature Structures}.
Cambridge University Press.

\bibitem[Carroll(1993)]{Carroll1993}
John Carroll. 1993.
Practical unification-based parsing of natural language.
Ph.D. thesis, University of Cambridge.

\bibitem[Chen et al.(2018)]{Chen2018}
Junkun Chen et al. 2018.
Sequence-to-action: End-to-end semantic graph generation for semantic parsing.
In \textit{ACL}.

\bibitem[Clark and Curran(2003)]{ClarkCurran2003}
Stephen Clark and James R. Curran. 2003.
Log-linear models for wide-coverage CCG parsing.
In \textit{EMNLP}, pages 97--104.

\bibitem[Copestake(2002)]{Copestake2002}
Ann Copestake. 2002.
Definitions of typed feature structures.
In \textit{Collaborative Language Engineering}, pages 227--230.

\bibitem[Dridan(2009)]{Dridan2009}
Rebecca Dridan. 2009.
Using lexical statistics to improve HPSG parsing.
Ph.D. thesis, University of Saarland.

\bibitem[Dridan(2013)]{Dridan2013}
Rebecca Dridan. 2013.
Ubertagging: Joint segmentation and supertagging for English.
In \textit{EMNLP}, pages 1201--1212.

\bibitem[Dridan et al.(2008)]{Dridan2008}
Rebecca Dridan, Valia Kordoni, and Jeremy Nicholson. 2008.
Enhancing performance of lexicalised grammars.
In \textit{ACL-08: HLT}, pages 613--621.

\bibitem[Flickinger(2000)]{Flickinger2000}
Dan Flickinger. 2000.
On building a more efficient grammar by exploiting types.
\textit{Natural Language Engineering}, 6(01):15--28.

\bibitem[Flickinger(2011)]{Flickinger2011}
Dan Flickinger. 2011.
Accuracy v. robustness in grammar engineering.
In \textit{Language from a Cognitive Perspective: Grammar, Usage and Processing}, pages 31--50.

\bibitem[Flickinger and Yu(2013)]{FlickingerYu2013}
Dan Flickinger and Jiye Yu. 2013.
Toward more precision in correction of grammatical errors.
In \textit{CoNLL}.

\bibitem[Flickinger et al.(2012)]{Flickinger2012}
Dan Flickinger, Yi Zhang, and Valia Kordoni. 2012.
DeepBank: A dynamically annotated treebank of the Wall Street Journal.
In \textit{TLT}, pages 85--96.

\bibitem[Hajdik et al.(2019)]{Hajdik2019}
Valerie Hajdik, Jan Buys, and Wayne Michael. 2019.
Neural generation for HPSG parsing.
In \textit{Proceedings of the 12th International Conference on Natural Language Generation}.

\bibitem[Kogkalidis and Moortgat(2023)]{KogkalidisMoortgat2023}
Konstantinos Kogkalidis and Michael Moortgat. 2023.
Geometry-aware supertagging with heterogeneous dynamic convolutions.
In \textit{CIASP Conference on Learning with Small Data (LSD)}, pages 107--119.

\bibitem[Lin et al.(2022)]{Lin2022}
Zi Lin, Jeremiah Zhe Liu, and Jingbo Shang. 2022.
Towards collaborative neural-symbolic graph semantic parsing via uncertainty.
\textit{Findings of ACL 2022}.

\bibitem[Liu et al.(2021)]{Liu2021}
Yufang Liu, Tao Ji, Yuanbin Wu, and Man Lan. 2021.
Generating CCG categories.
In \textit{AAAI}, pages 13443--13451.

\bibitem[Malouf et al.(2000)]{Malouf2000}
Robert Malouf, John Carroll, and Ann Copestake. 2000.
Efficient feature structure operations without compilation.
\textit{Natural Language Engineering}, 6(1):29--46.

\bibitem[Marcus et al.(1993)]{Marcus1993}
Mitchell Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993.
Building a large annotated corpus of English: The Penn Treebank.
University of Pennsylvania Technical Report No. MS-CIS-93-87.

\bibitem[Marimon et al.(2014)]{Marimon2014}
Montserrat Marimon, Núria Bel, and Lluís Padró. 2014.
Automatic selection of HPSG-parsed sentences for treebank construction.
\textit{Computational Linguistics}, 40(3):523--531.

\bibitem[Matsuzaki et al.(2007)]{Matsuzaki2007}
Takuya Matsuzaki, Yusuke Miyao, and Jun'ichi Tsujii. 2007.
Efficient HPSG parsing with supertagging and CFG-filtering.
In \textit{IJCAI}, pages 1671--1676.

\bibitem[Miyao and Tsujii(2008)]{MiyaoTsujii2008}
Yusuke Miyao and Jun'ichi Tsujii. 2008.
Feature forest models for probabilistic HPSG parsing.
\textit{Computational Linguistics}, 34(1):35--80.

\bibitem[Morgado da Costa et al.(2016)]{MorgadoDaCosta2016}
Luis Morgado da Costa, Francis Bond, and Xiaoling He. 2016.
Syntactic well-formedness diagnosis and error based coaching in computer assisted language learning using machine translation.
In \textit{NLP-TEA-3}.

\bibitem[Morgado da Costa et al.(2020)]{MorgadoDaCosta2020}
Luis Morgado da Costa et al. 2020.
Grammar-based diagnosis and feedback for language learning.
In \textit{LREC}.

\bibitem[Ninomiya et al.(2007)]{Ninomiya2007}
Takashi Ninomiya, Takuya Matsuzaki, Yusuke Miyao, and Jun'ichi Tsujii. 2007.
A log-linear model for partial parsing.
In \textit{IJCNLP}.

\bibitem[Oepen(1999)]{Oepen1999}
Stephan Oepen. 1999.
[incr tsdb()] competence and performance laboratory. User and reference manual.

\bibitem[Oepen and Carroll(2002)]{OepenCarroll2002}
Stephan Oepen and John Carroll. 2002.
Efficient parsing for unification-based grammars.
In \textit{Collaborative Language Engineering}.

\bibitem[Oepen et al.(2004)]{Oepen2004}
Stephan Oepen, Dan Flickinger, Kristina Toutanova, and Christopher D. Manning. 2004.
LinGO Redwoods.
\textit{Research on Language and Computation}, 2(4):575--596.

\bibitem[Packard(2015)]{Packard2015}
Woodley Packard. 2015.
Full-forest treebanking.
Master's thesis, University of Washington.

\bibitem[Pedregosa et al.(2011)]{Pedregosa2011}
F. Pedregosa et al. 2011.
Scikit-learn: Machine learning in Python.
\textit{JMLR}, 12:2825--2830.

\bibitem[Pollard and Sag(1994)]{PollardSag1994}
Carl Pollard and Ivan A. Sag. 1994.
\textit{Head-Driven Phrase Structure Grammar}.
University of Chicago Press.

\bibitem[Prange et al.(2021)]{Prange2021}
Jakob Prange, Nathan Schneider, and Vivek Srikumar. 2021.
Supertagging the long tail with tree-structured decoding of complex categories.
\textit{TACL}, 9:243--260.

\bibitem[Prins and van Noord(2004)]{PrinsVanNoord2004}
R.P. Prins and G.J.M. van Noord. 2004.
Reinforcing parser preferences through tagging.
\textit{Traitement Automatique des Langues}, 3:121--139.

\bibitem[Raymond(1999)]{Raymond1999}
Eric S. Raymond. 1999.
\textit{The Cathedral and the Bazaar}.
O'Reilly Media.

\bibitem[Someya et al.(2024)]{Someya2024}
Someya et al. 2024.
(Title Missing).

\bibitem[Tian et al.(2020)]{Tian2020}
Yuanhe Tian et al. 2020.
Supertagging with LSTM and CRF.
In \textit{Findings of ACL}.

\bibitem[Tomita(1985)]{Tomita1985}
Masaru Tomita. 1985.
An efficient context-free parsing algorithm for natural languages.
In \textit{IJCAI}.

\bibitem[Vaswani et al.(2016)]{Vaswani2016}
Ashish Vaswani et al. 2016.
Supertagging with LSTMs.
In \textit{NAACL-HLT}.

\bibitem[Xu et al.(2015)]{Xu2015}
Wenduan Xu, Michael Auli, and Stephen Clark. 2015.
CCG supertagging with a recurrent neural network.
In \textit{ACL}.

\bibitem[Zhang et al.(2009)]{Zhang2009}
Yi Zhang et al. 2009.
(Title Missing).

\bibitem[Zhang et al.(2010)]{Zhang2010}
Yi Zhang et al. 2010.
(Title Missing).

\bibitem[Zhang and Krieger(2011)]{ZhangKrieger2011}
Yi Zhang and Hans-Ulrich Krieger. 2011.
(Title Missing).

\bibitem[Zhang et al.(2012)]{Zhang2012}
Yi Zhang et al. 2012.
(Title Missing).

\end{thebibliography}

\end{document}
=====END FILE=====