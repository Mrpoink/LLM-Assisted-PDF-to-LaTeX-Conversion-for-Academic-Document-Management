=====FILE: main.tex=====
\documentclass[11pt,a4paper,twocolumn]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\geometry{margin=1in}
\usepackage{times}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{url}
\usepackage{hyperref}

\title{Subword Segmentation in LLMs: Looking at Inflection and Consistency}

\author{
Marion Di Marco\textsuperscript{1} \and Alexander Fraser\textsuperscript{1,2} \
\textsuperscript{1}School of Computation, Information and Technology, \
Technische Universit{"a}t M{"u}nchen (TUM) \
\textsuperscript{2}Munich Center for Machine Learning \
\texttt{{marion.dimarco|alexander.fraser}@tum.de}
}

\date{}

\begin{document}

\maketitle

\begin{abstract}
The role of subword segmentation in relation to capturing morphological patterns in LLMs is currently not well explored. Ideally, one would train large models like GPT using various segmentations and evaluate how well word meanings are captured. Since this is not computationally feasible, we group words according to their segmentation properties and compare how well a model can solve a linguistic task for these groups. We study two criteria: (i) adherence to morpheme boundaries and (ii) the segmentation consistency of the different inflected forms of a lemma. We select word forms with high and low values for these criteria and carry out experiments on GPT-40's ability to capture verbal inflection for 10 languages. Our results indicate that in particular the criterion of segmentation consistency can help to predict the model's ability to recognize and generate the lemma from an inflected form, providing evidence that subword segmentation is relevant.
\end{abstract}

\section{Introduction}

The linguistic abilities of large language models have been studied to a large extent, with many new abilities emerging as language models become ever larger and more powerful. ...

\noindent
\textbf{[MISSING CONTENT]}

\section*{[MISSING SECTION]}

\noindent
\textbf{[MISSING CONTENT]}

\begin{table}[h]
\centering
\caption{Overview of the number of verbs (inflected forms) per language after the filtering step.}
\label{tab:verb-counts}
\begin{tabular}{lr}
\toprule
\textbf{Lang} & \textbf{Verbs} \
\midrule
EN & 23342 \
FR & 57650 \
DE & 21567 \
ES & 15924 \
IT & 49349 \
PT & 27727 \
FI & 22152 \
SV & 14432 \
CS & 20029 \
HU & 37780 \
\bottomrule
\end{tabular}
\end{table}

\appendix

\section{Tags and Abbreviations}
Table 9 lists the abbreviations used in MorphyNet's tags and the respective feature names used in the prompt formulation, based on the documentation in \url{[https://unimorph.github.io/doc/unimorph_schema.pdf](https://www.google.com/search?q=https://unimorph.github.io/doc/unimorph_schema.pdf)}.

\begin{table}[h]
\centering
\caption{Abbreviations used in MorphyNet's tags and respective feature names (Reconstructed from snippet).}
\label{tab:tags}
\begin{tabular}{ll}
\toprule
\textbf{Tag} & \textbf{Feature Name} \
\midrule
V & verb \
V.PTCP & participle \
IND & indicative \
SBIV & subjunctive \
IMP & imperative \
COND & conditional \
POT & potential \
PST & past \
PRS & present \
FUT & future \
SG & singular \
PL & plural \
1 & 1 \
2 & 2 \
3 & 3 \
PFV & perfective \
IPVF & imperfective \
PROG & progressive \
PRF & perfect \
FORM & formal \
INFM & informal \
\bottomrule
\end{tabular}
\end{table}

\begin{thebibliography}{99}

\bibitem{ref1}
[...investiga] tion into the morphological capabilities of a large language model. In \textit{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing}, pages 6508--6524, Singapore. Association for Computational Linguistics.

\bibitem{yehezkel2023}
Shaked Yehezkel and Yuval Pinter. 2023. Incorporating context into subword vocabularies. In \textit{Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics}, pages 623--635, Dubrovnik, Croatia. Association for Computational Linguistics.

\end{thebibliography}

\end{document}
=====END FILE=====