=====FILE: main.tex=====
\documentclass[11pt, a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\usepackage{amsmath, amssymb, amsfonts}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{natbib}
\usepackage{booktabs}
\usepackage{float}
\usepackage{caption}
\usepackage{authblk}

\geometry{left=2.5cm, right=2.5cm, top=2.5cm, bottom=2.5cm}

\title{Dissecting Fine-Tuning Unlearning in Large Language Models}

\author[1,4*]{Yihuai Hong}
\author[2]{Yuelin Zou}
\author[3]{Lijie Hu}
\author[1]{Ziqian Zeng}
\author[3]{Di Wang}
\author[4$\dagger$]{Haiqin Yang}

\affil[1]{South China University of Technology}
\affil[2]{Columbia University}
\affil[3]{King Abdullah University of Science and Technology}
\affil[4]{International Digital Economy Academy (IDEA), China}
\affil[ ]{\texttt{yihuaihong@gmail.com}, \texttt{hqyang@ieee.org}}

\date{}

\begin{document}

\maketitle

\begin{abstract}
\input{sections/00_abstract}
\end{abstract}

\input{sections/01_introduction}
\input{sections/02_background}
\input{sections/03_patching}
\input{sections/04_global_effect}
\input{sections/05_discussion}
\input{sections/06_limitations}
\input{sections/07_acknowledgements}

\bibliographystyle{plain}
\bibliography{refs}

\appendix
\input{sections/appendix}

\end{document}
=====END FILE=====

=====FILE: sections/00_abstract.tex=====
Fine-tuning-based unlearning methods prevail for preventing targeted harmful, sensitive, or copyrighted information within large language models while preserving overall capabilities. However, the true effectiveness of these methods is unclear. In this work, we delve into the limitations of fine-tuning-based unlearning through activation patching and parameter restoration experiments. Our findings reveal that these methods alter the model's knowledge retrieval process, providing further evidence that they do not genuinely erase the problematic knowledge embedded in the model parameters. Instead, the coefficients generated by the MLP components in the model's final layer are the primary contributors to these seemingly positive unlearning effects, playing a crucial role in controlling the model's behaviors. Furthermore, behavioral tests demonstrate that this unlearning mechanism inevitably impacts the global behavior of the models, affecting unrelated knowledge or capabilities. The code is released at \url{[https://github.com/yihuaihong/Dissecting-FT-Unlearning](https://github.com/yihuaihong/Dissecting-FT-Unlearning)}.
=====END FILE=====

=====FILE: sections/01_introduction.tex=====
\section{Introduction}

Large language models (LLMs), due to their extensive pre-training corpora, often inadvertently learn harmful, sensitive, or copyright-protected knowledge (Chang et al., 2023a; Mozes et al., 2023; Eldan and Russinovich, 2023; Ye et al., 2022). Consequently, recent research has focused on developing efficient unlearning methods as a post-training technique to selectively unlearn the specific knowledge (Blanco-Justicia et al., 2024; Liu et al., 2024).

Currently, the core mechanism of these unlearning methods involves finetuning (Eldan and Russinovich, 2023; Jang et al., 2023; Yao et al., 2024; Rafailov et al., 2023), with corresponding adjustments and designs in the loss function to facilitate the unlearning process. Although earlier investigations (Hong et al., 2024; Lee et al., 2024a) have proven that these methods are ineffective at completely erasing model-embedded knowledge, the factors contributing to the misleading success of these techniques remain unclear.

Therefore, in this paper, we try to unveil why existing finetuning-based unlearning methods perform well in behavioral tests by analyzing the mechanisms of internal knowledge recall and flow within models (Meng et al., 2022; Pochinkov and Schoots, 2024; Geva et al., 2021a). Specifically, we investigate which components or parameters carry these unlearning effects. We design activations patching and parameters restoration experiments in three settings, aiming to independently study the impact of unlearning methods on the coefficients and value vectors in the MLPs, as well as on the attention components' states. Our findings further confirm that the methods do not truly alter the knowledge embedded in the value vectors of MLPs, and reveal that they will change how they extract and transfer this knowledge through modifications in the coefficients of MLPs and attention components during unlearning. Notably, the coefficients produced by the MLP in the final layers are primarily responsible for achieving the unlearning effects of finetuning-based methods.

We further test the global behavior impact of these fine-tuning-based unlearning methods on LLaMA2-7B-chat (Touvron et al., 2023) and OLMo-7B (Groeneveld et al., 2024) by implementing them on the respective pretraining datasets of both models, aiming to more closely simulate the erasure of knowledge acquired during the pretraining process. We discover that while these methods appear to effectively unlearn target knowledge, they also inevitably affect the output and behavior related to unrelated knowledge. This unintended consequence stems from the fact that these approaches are based on altering the model's internal knowledge retrieval mechanisms, thereby impacting its global behavior and overall performance.

Ultimately, we conclude once again that current fine-tuning-based unlearning methods cannot completely erase sensitive knowledge embedded in models, particularly within the MLPs, instead adjusting the mechanisms by which the model retrieves knowledge. These methods are vulnerable to recovery attacks in components' activations and unsuitable for true unlearning. We advocate for future unlearning evaluations to concentrate on precise measurement of both the actual storage of targeted knowledge within the model's entire parameter set and the specific dynamics of how this knowledge is retrieved and utilized.
=====END FILE=====

=====FILE: sections/02_background.tex=====
\section{Background and Related Work}

\paragraph{Unlearning in Large Language Models}
Since large language models learn knowledge from different domains and corpora during the pre-training process, it is often found that they contain harmful, sensitive or private knowledge, leading to the possibility that language models produce output behaviors containing corresponding sensitive or harmful information (Liu et al., 2024; Chang et al., 2023a; Mozes et al., 2023). Therefore, unlearning emerges as a timely and important post-pretraining processing method for LLM safety. Currently, the vast majority of LLM unlearning methods use fine-tuning as the primary operational approach. In terms of classifying them by different training objectives, they include gradient direction control (Jang et al., 2023; Yao et al., 2024, 2023) and preference optimization methods (Rafailov et al., 2023; Zhao et al., 2024; Lee et al., 2024b). In terms of classifying them by the parameters covered during training, they include full parameters fine-tuning (Eldan and Russinovich, 2023; Jang et al., 2023; Yao et al., 2024; Rafailov et al., 2023), sparse fine-tuning (Chang et al., 2023b; Stoehr et al., 2024), and parameter-efficient fine-tuning (Lu et al., 2024; Chen and Yang, 2023). Additionally, there are also a few knowledge editing methods (Patil et al., 2024). We present the specific logic details of each method in \S A.

\paragraph{Knowledge Storage in Large Language Models}
Studying how knowledge is stored, transferred, and extracted in LLMs has always been an important direction in the research of LLM's interpretability (Meng et al., 2022; Geva et al., 2021b; Sukhbaatar et al., 2015; Geva et al., 2023). It is known that in transformer-based language models, the MLP is a crucial component for storing the model's factual knowledge, and its sub-layers can be viewed as key-value memories (Geva et al., 2021b). To be specific, the first layer of MLP sub-layers can be viewed as a matrix  formed by key vectors , used to capture a set of patterns in the input sequence, and ultimately outputting the coefficient scores. The second layer can be viewed as a matrix  formed by value vectors , with each value vector containing the corresponding factual knowledge (represented through token distributions). Finally, the MLP's output can be defined as the sum of value vectors weighted by their memory coefficients:
\begin{equation}
M^{l}=f(W_{K}^{l}x^{l})W_{V}^{l}=m^{l}W_{V}^{l}
\end{equation}
where  represents the output of the MLP in the transformer's -th layer for an input hidden state  at that layer with the parameters,  and .  is a non-linearity function.  represents the coefficient scores. The dimension size of hidden states is  and it is  for the intermediate MLP.

In addition to the MLP, primarily responsible for knowledge storage, the attention component is currently considered the main component responsible for knowledge transfer and extraction in language models (Geva et al., 2023). Here, we will not go into detail about its specific structure but only study the impact it has on knowledge extraction. The final computation formula for the hidden states in the language model is defined as:
\begin{equation}
X^{l+1}=X^{l}+M^{l}+A^{l},
\end{equation}
where ,  and  represent the hidden states, MLP's output, and the attention component's output in the transformer's -th layer, respectively.
=====END FILE=====

=====FILE: sections/03_patching.tex=====
\section{Patching Investigation}

\paragraph{Hypothesis and Experimental Design}
Based on Eq. (1) and Eq. (2), we hypothesize that there are three main reasons why the current fine-tuning-based unlearning methods appear successful in behavioral tests and seem to suggest that true unlearning has been achieved:
\footnote{Currently, in most decoder-only models such as GPT-2 (Radford et al., 2019) and GPT-J(Chen et al., 2021), the MLP component has two layers, while in LLaMA (Touvron et al., 2023) it has three layers. However, we can still consider LLaMA's first two layers together as the key matrices, with their output serving as the coefficient scores.}

\begin{enumerate}
\item The coefficients  are changed after fine-tuning, leading to a change in the activations of the MLPs;
\item The value vectors  in MLPs are changed, causing a change in the knowledge they contain;
\item The change that happens in attention components caused the model's focus and the corresponding information extracted by these attention components  to change, thus reducing the target knowledge-related information in the output.
\end{enumerate}

Here, for the sake of simplicity and better understanding, we continue to use the definitions of , , and  as given in Eq. (1) and Eq. (2) in the following. We ignore the minor effects caused by other components or parameters, such as the language model's unembedding matrix and the normalization layers. Based on the possible reasons described above, on the unlearned model, we conduct three different sets of activation patching or components' parameter restoration experiments, trying to recover the output of the target knowledge in the unlearned model. The specific operation process is as follows:

\begin{enumerate}
\item In the first set of experiments, we restore the coefficient scores  corresponding to each MLP component, layer by layer, in the language model, without making any intentional changes to the value vector parameters  of the MLPs or the attention components' states  in any layer.
\item In the second set of experiments, we restore the parameters of value vectors  in MLPs layer by layer, recovering the knowledge they originally contained. In this process, we avoid making intentional changes to the unlearned model's original coefficients  and the attention components' states .
\item In the third set of experiments, we restore the original attention components' states  but without intentionally altering the MLPs' coefficient scores  or the value vectors' parameters , only studying the impact brought by the attention components which are responsible for extracting and transferring knowledge.
\end{enumerate}

To evaluate the extent of knowledge restoration, we propose the metric of Knowledge Recovery Score (KRS):
\begin{equation}
KRS=1-loss^{*o}/loss^{*},
\end{equation}
where the losses are the average of MSE() on  and  and on  and , respectively. MSE() represents the mean squared error (MSE) loss function. ,  and  are the logit distribution of the subsequent token produced by the vanilla model, unlearned model, and unlearned-then-recover model, respectively. The average loss is computed on the next  generated tokens on  knowledge-related questions. Finally, if KRS approaches 1, it indicates  and  that are nearly consistent, representing a higher degree of knowledge recovery. Conversely, a lower KRS suggests a lower degree of that.

\paragraph{Activation Patching and Parameters Restoration Experiments}
We conduct the experiments on two recent LLMs, LLaMA2-7B-chat (Touvron et al., 2023) and OLMo-7B (Groeneveld et al., 2024). We apply two example finetuning-based unlearning methods, DPO (Rafailov et al., 2023) and Gradient Difference (Yao et al., 2024), to perform unlearning on the large language models and calculate the average KRS scores. Inspired by (Eldan and Russinovich, 2023), which tries to unlearn the concept knowledge of ``Harry Potter'' in language models, we extend this experiment by selecting 10 well-known concepts per model from the Concept Vectors Benchmark (Hong et al., 2024), which is a collection of concepts that language models are well-acquainted with and have substantial knowledge about. Examples of them are provided in Table 2 of \S B. For the unlearning training, we use the texts containing the corresponding concepts from Redpjama and Dolma (Soldaini et al., 2024). Redpjama is a replication of the pretraining corpus for the LLaMA model, while Dolma is the open-source pre-training dataset for the OLMo model. Detailed information is provided in \S B. So here we can ensure that the knowledge to be unlearned was at least seen by the model during the pre-training process, and that the training data used more broadly covers the textual sources from which the model acquired the corresponding knowledge about certain concepts.

After obtaining the unlearned model, we follow the steps mentioned in the hypothesis to perform activation patching and parameter restoration experiments on the unlearned models. To calculate the Knowledge Recover Scores, we set  to 30 and  to 10, indicating the generation of the next 30 tokens and the selection of 10 questions related to each concept. To make the recovery effects more pronounced and the whole process easier to observe, we adopt techniques from (Meng et al., 2022, 2023) which implemented causal mediation, setting the size of the recovery window to five. This allows us to observe the average effects of recovering five consecutive layers at a time. Details can be found in \S B.

\begin{figure}[htbp]
\centering
\framebox{\begin{minipage}{0.9\linewidth}
\centering
\vspace{1cm}
\textbf{[IMAGE NOT PROVIDED]} \
\vspace{1cm}
Graph showing Knowledge Recover Score (KRS) on y-axis and Layer on x-axis for different settings.
\end{minipage}}
\caption{Results of KRS on LLaMA and OLMo under three activations patching or parameters restoration settings. We also included another setting that restores both attention and coefficients to compare the final outcomes.}
\label{fig:fig1}
\end{figure}

The specific results are shown in Fig. 1. From our analysis, surprisingly, we observe that when we solely recover the parameters contained in the value vectors of each layer in the unlearned model without interfering with the coefficients or attention components' states, the recovery of the target knowledge is negligible (The KRS scores are all below 0.001). This holds regardless of which layer is recovered, and regardless of the specific model being considered.

However, when recovering the attention components' states in the intermediate layers (from the 15th layer onward) or deeper layers (from the 27th layer onward), we can observe that the average KRS for both models has increased to exceed 0.3 and 0.4, respectively, indicating that a significant portion of the corresponding knowledge has been recovered. What's more, restoring the coefficients of the MLPs in the intermediate layers (from the 20th layer onward) and deeper layers (from the 29th layer) also yields impressive knowledge recovery effects. The layers at which the scores start to increase under the two settings generally align closely with the observation by Geva et al. (2023) that the MLP modules recall knowledge in intermediate layers, and the attention components mostly start to extract and transfer information in the deeper layers. or after the model has completed the relevant knowledge recall. We also tried simultaneously recovering the coefficients and attention states and found that the model can achieve much greater knowledge recovery, with the peak KRS score exceeding 0.9 on both models.

Additionally, it is noteworthy that, simply restoring the coefficient scores of the MLP outputs from the last two or three layers can significantly elevate the KRS of the unlearned LLaMA and OLMo models to 0.8 or above. This suggests that the coefficient scores of the MLPs in the last layers might play a crucial role in the final behavior results of the LLM.

To better isolate the effects of restoring , , and  individually and support the above argument, we present a more rigorous patching and restoration experiment in \S C, with the corresponding results shown in Figure 3. Ultimately, we found that the restoration of the attention states also contributed to the coefficients of the MLP in the final layers, further confirming that these coefficients carry the primary role of achieving the effects of finetuning-based unlearning. It also indicates that fine-tuning largely adjusts the model's behavior by modifying the coefficients of the deep MLP layers, likely because this enables faster adaptation compared to other knowledge adjustment mechanisms, such as altering knowledge encoded in the MLP itself. This phenomenon and the potential defensive strategy have not been discussed in the previous literature, warranting further investigation in future studies.

Overall, these results all further confirm that the finetuning-based unlearning methods essentially do not modify the model knowledge contained in the value vectors, but adjust the way knowledge is called during the fine-tuning process, either by adjusting the coefficients to modulate the MLP activation or by adjusting the attention to extract and transfer knowledge.
=====END FILE=====

=====FILE: sections/04_global_effect.tex=====
\section{Global Negative Effect of Fine-Tuning Unlearning}

In the previous section, we demonstrated that these finetuning-based methods alter the model's final behavior by adjusting the MLP output coefficients in the final layers. Therefore, we hypothesize that this behavioral change will has a global effect, potentially impacting the output of unrelated knowledge as well. In this section, we verify this hypothesis through the following experiments.

We apply four fine-tuning-based unlearning methods to the concepts used in \S 3 on their pretraining text sources (from RedPajama and Dolma) with the goal of erasing the learned knowledge during pretraining through a reverse process. These methods are as follows: DPO (Rafailov et al., 2023), NPO (Zhao et al., 2024), NPO+KL (Zhao et al., 2024) and Gradient Difference (Yao et al., 2024). The details of these baselines and data statistics are shown in \S A and \S B. We evaluate the unlearning effectiveness of these methods on the concepts' related QA pairs and the unlearning impact on unrelated QA pairs, reporting the average scores of BLEU (Papineni et al., 2002) by comparing the model's response before and after unlearning. In Figure 2, we report their performance at the end of each training epoch respectively.

\begin{figure}[htbp]
\centering
\framebox{\begin{minipage}{0.9\linewidth}
\centering
\vspace{1cm}
\textbf{[IMAGE NOT PROVIDED]} \
\vspace{1cm}
Plots showing Target QA BLEU and Unrelated QA BLEU vs Training Epoch for LLaMA and OLMo.
\end{minipage}}
\caption{Unlearning testing results on LLaMA and OLMo for each training epoch.}
\label{fig:fig2}
\end{figure}

We can observe that for finetuning-based methods, as the number of training epochs increases, aiming to achieve a lower target QA BLEU score, the corresponding unrelated QA BLEU score also decreases accordingly, exhibiting a positive correlation. This suggests that the impact of finetuning-based methods on the model's output behavior is global. While unlearning the target knowledge, they inadvertently alter the output behavior or manner for unrelated knowledge to a certain degree.
=====END FILE=====

=====FILE: sections/05_discussion.tex=====
\section{Discussion and Conclusion}

We have deeply investigated the reasons why fine-tuning-based unlearning methods seemingly succeeded in behavior-based testing for large language model unlearning: Through activation patching and parameter restoration experiments, we find that these methods alter the way knowledge is extracted by changing MLP activations or model's attention, ultimately affecting the output. This is evidenced by the fact that the model's output regarding the target knowledge is largely restored after patching the activations and the attention components' states. Furthermore, we conduct experiments on the pretraining datasets of two models, to test the models' capabilities after unlearning, verifying that in addition to unlearning the corresponding knowledge, fine-tuning-based methods that by altering the way the model accesses knowledge, will significantly impair the model's other unrelated capabilities, causing a certain degree of capability degradation.
=====END FILE=====

=====FILE: sections/06_limitations.tex=====
\section{Limitations}

In the experiments detailed in \S 3, we have disregarded the potential unlearning impact caused by parameter changes in other model components during the fine-tuning process. This decision is based on the observation that the impact of such changes appears to be minimal. For instance, during our parameter comparison analysis, we found that the changes in the unembedding matrix and normalization layer parameters resulted in cosine similarity values above 0.999. This suggests that the modifications to these components are quite small in magnitude. However, it remains unclear whether even such minimal parameter changes can still have any meaningful effect on the model's overall behavior and knowledge. Further verification and analysis would be needed to conclusively determine the extent to which these ancillary parameter updates might influence the unlearning outcome.
=====END FILE=====

=====FILE: sections/07_acknowledgements.tex=====
\section*{Acknowledgements}

The work was fully supported by the IDEA Information and Super Computing Centre (ISCC), National Natural Science Foundation of China (Grant No. 62406114), the Guangzhou Basic and Applied Basic Research Foundation (Grant No. 2023A04J1687), and the Fundamental Research Funds for the Central Universities (Grant No. 2024ZYGXZR074). Di Wang and Lijie Hu are supported in part by the funding BAS/1/1689-01-01, URF/1/4663-01-01, REI/1/5232-01-01, REI/1/5332-01-01, and URF/1/5508-01-01 from KAUST, and funding from KAUST - Center of Excellence for Generative AI, under award number 5940.
=====END FILE=====

=====FILE: sections/appendix.tex=====
\section{Details in Existing Unlearning Methods}
In this section, we provide a more detailed introduction to the LLM unlearning methods we used in \S 3 and \S 4.
\begin{itemize}
\item Gradient Difference (Yao et al., 2024), based on Gradient Ascent, it adds a regularization term to minimize the KL divergence between the unlearned and the original LLM on a reference text dataset, thus preventing the model from catastrophic deterioration of its general capability.
\item Direct Preference Optimization (DPO) (Rafailov et al., 2023), which maximizes the log-likelihood ratio between generating the preferred and the unfavored responses, while retaining a small shift from the original LLM predictive distribution.
\item Negative Preference Optimization (NPO) (Zhao et al., 2024), which discards the favored responses and only minimizes the prediction probability of the unfavored answers.
\item NPO+KL which adds to NPO a KL divergence loss between the model's outputs before and after unlearning.
\end{itemize}

\section{Unlearning Experiment's Corpus}
Here, we present detailed information about the data used for activation patching experiments and the unlearning experiments conducted in \S 3 and \S 4. We select 10 well-known concepts from Concept Vectors Benchmark (Hong et al., 2024) and extract 6,000 corresponding training data segments containing knowledge about the respective concepts per model from the pre-training datasets of Redpjama and Dolma. These extracted data segments are used for unlearn training of the two models respectively. For each concept, we also include ten related questions from the Concept Vectors Benchmark, along with 50 unrelated questions sampled from other unrelated concepts. These are used in \S 4 to evaluate the unlearning effectiveness from the behavior perspective on the specific concepts, as well as to assess whether the model's unrelated capabilities were affected. We have manually checked and verified that the vanilla LLaMA and OLMo models can accurately answer these selected questions, indicating that the models possess the knowledge. All the statistics and examples are shown in Table 1 and Table 2, respectively.

\begin{table}[h]
\centering
\caption{Statistics [TABLE NOT PROVIDED]}
\label{tab:table1}
\framebox{\begin{minipage}{0.5\linewidth}
\centering
\vspace{1cm}
[TABLE 1 CONTENT MISSING]
\vspace{1cm}
\end{minipage}}
\end{table}

\begin{table}[h]
\centering
\caption{Example extracted data from the Redpjama and Dolma pre-training datasets.}
\label{tab:table2}
\begin{tabular}{p{0.3\linewidth} p{0.3\linewidth} p{0.3\linewidth}}
\toprule
\textbf{Concept} & \textbf{Snippet from Pre-training Data} & \textbf{Related Question} \
\midrule
Olympic Games(OLMo) & The modern Olympic Games or Olympics (French: Jeux olympiques) are the leading international sporting events featuring summer and winter sports competitions in which thousands of athletes from around the world participate in a variety of competitions... & When were the first modern Olympic Games held? \
Diabetes (OLMo) & Diabetes mellitus, often known simply as diabetes, is a group of common endocrine diseases characterized by sustained high blood sugar levels. Diabetes is due to either the pancreas not producing enough insulin, or the cells of the body becoming unresponsive to the hormone's effects... & What is diabetes? \
Pakistan? & [MISSING] & What is the capital city of Pakistan? \
\bottomrule
\end{tabular}
\end{table}

\section{More Rigorous Patching Investigation}
In \S 3, during our activation patching and parameters restoration experiments, we restore , , or 
\textbf{[CONTENT TRUNCATED / MISSING]}
=====END FILE=====

=====FILE: refs.bib=====
@article{blanco2024digital,
title={Digital forgetting in large language models: A survey of unlearning methods},
author={Blanco-Justicia, Alberto and Jebreel, Najeeb and Manzanares, Benet and S{'a}nchez, David and Domingo-Ferrer, Josep and Collell, Guillem and Tan, Kuan Eeik},
journal={arXiv preprint arXiv:2404.02062},
year={2024}
}

@inproceedings{chang2023speak,
title={Speak, memory: An archaeology of books known to chatGPT/GPT-4},
author={Chang, Kent K and Cramer, Mackenzie Hanh and Soni, Sandeep and Bamman, David},
booktitle={The 2023 Conference on Empirical Methods in Natural Language Processing},
year={2023}
}

@article{chang2023do,
title={Do localization methods actually localize memorized data in llms?},
author={Chang, Ting-Yun and Thomason, Jesse and Jia, Robin},
journal={arXiv preprint arXiv:2311.09060},
year={2023}
}

@inproceedings{chen2023unlearn,
title={Unlearn what you want to forget: Efficient unlearning for llms},
author={Chen, Jiaao and Yang, Diyi},
booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
pages={12041--12052},
year={2023}
}

@article{chen2021evaluating,
title={Evaluating large language models trained on code},
author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and others},
journal={arXiv preprint arXiv:2107.03374},
year={2021}
}

@article{eldan2023who,
title={Who's harry potter? approximate unlearning in llms},
author={Eldan, Ronen and Russinovich, Mark},
journal={arXiv preprint arXiv:2310.02238},
year={2023}
}

@inproceedings{geva2023dissecting,
title={Dissecting recall of factual associations in auto-regressive language models},
author={Geva, Mor and Bastings, Jasmijn and Filippova, Katja and Globerson, Amir},
booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
pages={12216--12235},
year={2023}
}

@inproceedings{geva2021transformer,
title={Transformer feed-forward layers are key-value memories},
author={Geva, Mor and Schuster, Roei and Berant, Jonathan and Levy, Omer},
booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
pages={5484--5495},
year={2021}
}

@article{groeneveld2024olmo,
title={Olmo: Accelerating the science of language models},
author={Groeneveld, Dirk and Beltagy, Iz and Walsh, Pete and Bhagia, Akshita and Kinney, Rodney and Tafjord, Oyvind and Jha, Ananya Harsh and Ivison, Hamish and Magnusson, Ian and Wang, Yizhong and others},
journal={arXiv preprint arXiv:2402.00838},
year={2024}
}

@article{hong2024intrinsic,
title={Intrinsic evaluation of unlearning using parametric knowledge traces},
author={Hong, Yihuai and Yu, Lei and Yang, Haiqin and Ravfogel, Shauli and Geva, Mor},
journal={arXiv preprint arXiv:2406.11614},
year={2024}
}

@inproceedings{jang2023knowledge,
title={Knowledge unlearning for mitigating privacy risks in language models},
author={Jang, Joel and Yoon, Dongkeun and Yang, Sohee and Cha, Sungmin and Lee, Moontae and Logeswaran, Lajanugen and Seo, Minjoon},
booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
pages={14389--14408},
year={2023}
}

@inproceedings{lee2024mechanistic,
title={A mechanistic understanding of alignment algorithms: A case study on DPO and toxicity},
author={Lee, Andrew and Bai, Xiaoyan and Pres, Itamar and Wattenberg, Martin and Kummerfeld, Jonathan K and Mihalcea, Rada},
booktitle={Forty-first International Conference on Machine Learning},
year={2024}
}

@article{lee2024mechanistic_arxiv,
title={A mechanistic understanding of alignment algorithms: A case study on dpo and toxicity},
author={Lee, Andrew and Bai, Xiaoyan and Pres, Itamar and Wattenberg, Martin and Kummerfeld, Jonathan K and Mihalcea, Rada},
journal={arXiv preprint arXiv:2401.01967},
year={2024}
}

@article{liu2024rethinking,
title={Rethinking machine unlearning for large language models},
author={Liu, Sijia and Yao, Yuanshun and Jia, Jinghan and Casper, Stephen and Baracaldo, Nathalie and Hase, Peter and Xu, Xiaojun and Yao, Yuguang and Li, Hang and Varshney, Kush R and others},
journal={arXiv preprint arXiv:2402.08787},
year={2024}
}

@article{lu2024eraser,
title={Eraser: Jailbreaking defense in large language models via unlearning harmful knowledge},
author={Lu, Weikai and Zeng, Ziqian and Wang, Jianwei and Lu, Zhengdong and Chen, Zelin and Zhuang, Huiping and Chen, Cen},
journal={arXiv preprint arXiv:2404.05880},
year={2024}
}

@inproceedings{meng2022locating,
title={Locating and editing factual associations in GPT},
author={Meng, Kevin and Bau, David and Andonian, Alex J and Belinkov, Yonatan},
booktitle={Advances in Neural Information Processing Systems},
year={2022}
}

@inproceedings{meng2023mass,
title={Mass-editing memory in a transformer},
author={Meng, Kevin and Sharma, Arnab Sen and Andonian, Alex J and Belinkov, Yonatan and Bau, David},
booktitle={The Eleventh International Conference on Learning Representations},
year={2023}
}

@article{mozes2023use,
title={Use of llms for illicit purposes: Threats, prevention measures, and vulnerabilities},
author={Mozes, Maximilian and He, Xuanli and Kleinberg, Bennett and Griffin, Lewis D},
journal={arXiv preprint arXiv:2308.12833},
year={2023}
}

@inproceedings{papineni2002bleu,
title={Bleu: a method for automatic evaluation of machine translation},
author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
booktitle={Proceedings of the 40th annual meeting of the Association for Computational Linguistics},
pages={311--318},
year={2002}
}

@inproceedings{patil2024can,
title={Can sensitive information be deleted from LLMs? Objectives for defending against extraction attacks},
author={Patil, Vaidehi and Hase, Peter and Bansal, Mohit},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024}
}

@article{pochinkov2024dissecting,
title={Dissecting language models: Machine unlearning via selective pruning},
author={Pochinkov, Nicholas and Schoots, Nandi},
journal={arXiv preprint arXiv:2403.01267},
year={2024}
}

@misc{radford2019language,
title={Language models are unsupervised multitask learners},
author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
year={2019},
publisher={OpenAI blog}
}

@inproceedings{rafailov2023direct,
title={Direct preference optimization: Your language model is secretly a reward model},
author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023}
}

@article{soldaini2024dolma,
title={Dolma: An open corpus of three trillion tokens for language model pretraining research},
author={Soldaini, Luca and Kinney, Rodney and Bhagia, Akshita and Schwenk, Dustin and Atkinson, David and Authur, Russell and Bogin, Ben and Chandu, Khyathi and Dumas, Jennifer and Elazar, Yanai and others},
journal={arXiv preprint arXiv:2402.00159},
year={2024}
}

@article{stoehr2024localizing,
title={Localizing paragraph memorization in language models},
author={Stoehr, Niklas and Gordon, Mitchell and Zhang, Chiyuan and Lewis, Owen},
journal={arXiv preprint arXiv:2403.19851},
year={2024}
}

@article{sukhbaatar2015end,
title={End-to-end memory networks},
author={Sukhbaatar, Sainbayar and Weston, Jason and Fergus, Rob and others},
journal={Advances in neural information processing systems},
volume={28},
year={2015}
}

@article{touvron2023llama,
title={Llama 2: Open foundation and fine-tuned chat models},
author={Touvron, Hugo and Martin, Louis and Stone, Kevin R and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
journal={arXiv preprint arXiv:2307.09288},
year={2023}
}

@inproceedings{yao2023large,
title={Large language model unlearning},
author={Yao, Yuanshun and Xu, Xiaojun and Liu, Yang},
booktitle={Socially Responsible Language Modelling Research},
year={2023}
}

@article{yao2024large,
title={Large language model unlearning},
author={Yao, Yuanshun and Xu, Xiaojun and Liu, Yang},
journal={arXiv preprint arXiv:2310.10683},
year={2024}
}

@inproceedings{ye2022learning,
title={Learning with recoverable forgetting},
author={Ye, Jingwen and Fu, Yifang and Song, Jie and Yang, Xingyi and Liu, Songhua and Jin, Xin and Song, Mingli and Wang, Xinchao},
booktitle={European Conference on Computer Vision},
pages={87--103},
year={2022},
organization={Springer}
}

@article{zhao2024towards,
title={Towards comprehensive and efficient post safety alignment of large language models via safety patching},
author={Zhao, Weixiang and Hu, Yulin and Li, Zhuojun and Deng, Yang and Zhao, Yanyan and Qin, Bing and Chua, Tat-Seng},
journal={arXiv preprint arXiv:2405.13820},
year={2024}
}
=====END FILE=====

=====FILE: figures/README.txt=====
The original PDF contained figures that could not be automatically extracted.
Figure 1: Results of KRS on LLaMA and OLMo.
Figure 2: Unlearning testing results on LLaMA and OLMo for each training epoch.
Figure 3: More rigorous patching investigation results (in Appendix C, missing).
=====END FILE=====