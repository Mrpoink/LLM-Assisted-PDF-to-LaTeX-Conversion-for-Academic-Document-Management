ACK

=====FILE: main.tex=====
\documentclass[10pt, a4paper, twocolumn]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{authblk}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{float}
\usepackage{caption}

% Page layout to match conference paper roughly
\geometry{left=2cm,right=2cm,top=2.5cm,bottom=2.5cm}

\title{Factuality of Large Language Models: A Survey}

\author[1]{Yuxia Wang}
\author[2]{Minghan Wang}
\author[1]{Muhammad Arslan Manzoor}
\author[3]{Fei Liu}
\author[1]{Georgi Georgiev}
\author[1]{Rocktim Jyoti Das}
\author[1]{Preslav Nakov}

\affil[1]{MBZUAI}
\affil[2]{Monash University}
\affil[3]{Google}
\affil[4]{Sofia University}
\affil[ ]{\texttt{{yuxia.wang, preslav.nakov}@mbzuai.ac.ae}}

\date{}

\begin{document}

\maketitle

\input{sections/abstract}
\input{sections/1_introduction}
\input{sections/2_background}
\input{sections/3_evaluating}
\input{sections/4_improving}
\input{sections/5_multimodal}

\bibliographystyle{plain}
\bibliography{refs}

\end{document}
=====END FILE=====

=====FILE: sections/abstract.tex=====
\begin{abstract}
Large language models (LLMs), especially when instruction-tuned for chat, have become part of our daily lives, freeing people from the process of searching, extracting, and integrating information from multiple sources by offering a straightforward answer to a variety of questions in a single place. Unfortunately, in many cases, LLM responses are factually incorrect, which limits their applicability in real-world scenarios. As a result, research on evaluating and improving the factuality of LLMs has attracted a lot of attention recently. In this survey, we critically analyze existing work with the aim to identify the major challenges and their associated causes, pointing out to potential solutions for improving the factuality of LLMs, and analyzing the obstacles to automated factuality evaluation for open-ended text generation. We further offer an outlook on where future research should go.
\end{abstract}
=====END FILE=====

=====FILE: sections/1_introduction.tex=====
\section{Introduction}

Large language models (LLMs) have become an integral part of our daily lives. When instruction-tuned for chat, they have enabled digital assistants that can free people from the need to search, extract, and integrate information from multiple sources by offering straightforward answers in a single chat. While people naturally expect LLMs to always present reliable information that is consistent with real-world knowledge, LLMs tend to fabricate ungrounded statements, resulting in misinformation (Tonmoy et al., 2024), which limits their utility.

Thus, assessing and improving the factuality of the text generated by LLMs has become an emerging and crucial research area, aiming to identify potential errors and to advance the development of more reliable LLMs (Chen et al., 2023). To this end, researchers have collected multiple datasets, introduced a variety of measures to evaluate the factuality of LLMs, and proposed numerous strategies leveraging external knowledge through retrieval, self-reflection, and early refinement in model generation to mitigate factual errors (Tonmoy et al., 2024).

Numerous surveys (Tonmoy et al., 2024; Huang et al., 2023a; Wang et al., 2023b) have explored factuality or hallucinations in large language models across various modalities. While they either lack in-depth discussion or are too specific to grasp the fundamental challenges, promising solutions in factuality evaluation and enhancement, and some ambiguous concepts in LLM factuality. We summarized these surveys in Table \ref{tab:surveys}.

Our survey aims to bridge this gap by providing an in-depth analysis of LLM factuality, with an emphasis on recent studies to reflect the rapidly evolving nature of the field. We offer a comprehensive overview of different categorizations, evaluation methods, and mitigation techniques for LLM factuality in both language and vision modalities. Additionally, we explore a novel research avenue that seeks to improve LLM calibration. This includes making models aware of their knowledge limitations and enhancing the reliability of their output confidence.

\begin{table*}[t]
\centering
\caption{Comparison of different surveys on the factuality of LLMs. Eval: Evaluation; Improve: Improvement.}
\label{tab:surveys}
\resizebox{\textwidth}{!}{
\begin{tabular}{l c c c c c p{8cm}}
\toprule
\textbf{Survey} & \textbf{Date} & \textbf{Pages} & \textbf{Eval} & \textbf{Improve} & \textbf{Multimodal} & \textbf{Contributions and limitations} \
\midrule
Our work & 15-June-2024 & 9 & \checkmark & \checkmark & \checkmark & Discusses ambiguous concepts in LLM factuality, compares and analyzes evaluation and enhancement approaches from academic and practical perspectives, outlining major challenges and promising avenues to explore. \
(Tonmoy et al., 2024) & 08-Jan-2024 & 19 & X & X & X & Summarizes recent work in terms of mitigating LLM hallucinations, but lacks comparison between different approaches and discussions to identify open questions and challenges. \
(Gao et al., 2023b) & 18-Dec-2023 & 26 & X & \checkmark & X & Summarizes three RAG paradigms: naïve, advanced, and modular RAG, with key elements and evaluation methods for the three major components in RAG (retriever, generator, and augmentation). \
(Huang et al., 2023b) & 09-Nov-2023 & 49 & \checkmark & \checkmark & X & Analyzes the reasons for hallucinations, and presents a comprehensive overview of hallucination detection methods, benchmarks, and approaches to mitigate hallucinations. \
(Wang et al., 2023b) & 18-Oct-2023 & 44 & \checkmark & \checkmark & X & Detailed literature review of factuality improvement and enhancement methods covering both retrieval augmentation and non-retrieval augmentation, missing discussion of major bottleneck issues in LLM factuality and promising directions to investigate. \
(Rawte et al., 2023b) & 18-Sept-2023 & 11 & X & X & \checkmark & Extensively elucidates the problem of hallucination across all major modalities of foundation models, including text (general, multilingual, domain-specific LLMs), image, video, and audio. However, inadequate coverage of approaches, in-depth categorization and comparison between methods. \
(Zhang et al., 2023c) & 03-Sept-2023 & 32 & X & X & X & Organized by different training stages of LLMs, discusses potential sources of LLM hallucinations and in-depth review of recent work on addressing the problem. \
(Guo et al., 2022) & Feb-2022 & 29 & X & X & X & Focused on the automated fact-checking pipeline \
\bottomrule
\end{tabular}
}
\end{table*}
=====END FILE=====

=====FILE: sections/2_background.tex=====
\section{Background}

Hallucination and factuality, while conceptually distinct, often occur in similar contexts and are sometimes used interchangeably, rendering them intricately intertwined, posing a challenge in discerning their distinct boundaries, and causing a considerable amount of misconception. In this section, we seek to disambiguate and refine our understanding of these two closely aligned concepts, thereby preventing misinterpretation and reducing potential confusion. Additionally, we further include two closely-related axes: relevance and trustworthiness for LLM evaluation to illustrate their nuance in relation to factuality.

\paragraph{Hallucination vs. Factuality}
The concept of hallucination in the context of traditional natural language generation tasks is typically referred to as the phenomenon in which the generated content appears nonsensical or unfaithful to the provided source content (Ji et al., 2023). One concrete example is made-up information in an abstractive summary with additional insights beyond the scope of the original source document.

In the age of LLMs, the term hallucination has been reimagined, encompassing any deviation from factual reality or the inclusion of fabricated elements within generated texts (Tonmoy et al., 2024; Rawte et al., 2023b). (Zhang et al., 2023c) define hallucination as the characteristic of LLMs to generate content that diverges from the user input, contradicts previously generated context, or mis-aligns with established world knowledge. (Huang et al., 2023b) merge the input and context-conflicting types of hallucinations and further take logical inconsistency into account to form faithfulness hallucination. Another category is factuality hallucination, referring to the discrepancy between generated content and verifiable real-world facts, manifesting as (1) factual inconsistency and (2) factual fabrication.

Factuality, on the other hand, is concerned with a model's ability to learn, acquire, and utilize factual knowledge. (Wang et al., 2023b) characterize factuality issues as the probability of LLMs producing content inconsistent with established facts. It is important to note that hallucination content may not always involve factual missteps. Though a piece of generated text may exhibit divergence from the initial prompt's specifics, it falls into hallucinations, not necessarily a factual issue if the content is accurate.

It is crucial to distinguish between factual errors and instances of hallucination. The former involves inaccurate information whereas the latter can present unanticipated and yet factually substantiated content (Wang et al., 2023b).

\textbf{Summary:} Factuality is the ability of LLMs to generate content consistent with factual information and world knowledge. Although both hallucinations and factuality may impact the credibility of LLMs in the context of content generation, they present distinct challenges. Hallucinations occur when LLMs produce baseless or untruthful content, not grounded in the given source. In contrast, factuality errors arise when the model fails to accurately learn and utilize factual knowledge. It is possible for a model to be factually correct yet still produce hallucinations by generating content that is either off-topic or more detailed than what is requested.

\paragraph{Trustworthiness/Reliability vs. Factuality}
In the context of LLMs, factuality (Wang et al., 2023b) refers to a model's capability of generating contents of factual information, grounded in reliable sources (e.g., dictionaries, Wikipedia or textbooks), with commonsense, world and domain-specific knowledge taken into account. In contrast, ``trustworthiness'' (Sun et al., 2024) extends beyond mere factual accuracy and is measured on eight dimensions: truthfulness, safety, fairness, robustness, privacy, ethics, transparency, and accountability.
=====END FILE=====

=====FILE: sections/3_evaluating.tex=====
\section{Evaluating Factuality}

Evaluating LLM factuality on open-ended generations presents a non-trivial challenge, discerning the degree to which a generated textual statement aligns with objective reality. Studies employ various benchmarks, evaluation strategies and metrics to achieve this goal.

\subsection{Datasets and Metrics}

While (Zhang et al., 2023c) outlined tasks and measures for hallucination evaluation, there is no comparative analysis of existing datasets to assess various aspects in regards to model factuality (e.g., knowledge grounding, fast-changing facts, snowballing hallucinations, robustness to false premises, and uncertainty awareness). We categorize the datasets in the format of discrimination or generation, and highlights the challenges in automatic evaluation for long-form open-ended generations.

Current benchmarks largely assess the factuality in LLMs based on two capabilities: proficiency in distinguishing factual accuracy in a context and ability to generate factually sound content. The former typically comes in the form of a multi-choice question, with the expected response being a label of one of A, B, C, and D. For instance, HotpotQA, StrategyQA, MMLU. This form of evaluation has been widely used to measure the general knowledge proficiency and factual accuracy of LLMs, largely thanks to its automation-friendly nature. Under this evaluation formulation, model responses are easily parsed and compared with gold standard labels, enabling the calculation of accuracy or F1 scores against established benchmarks.

Precisely assessing the factuality of free-form LLM outputs remains a significant challenge due to the inherent limitations of automatic methods in the face of open-ended generation and the absence of definitive gold standard responses within an expansive output space. To make automatic evaluation feasible, many studies constrain the generation space to (1) Yes/No; (2) short-form phrase; and (3) a list of entities through controlling the categories of questions and generation length.

Perhaps the most demanding, yet inherently realistic scenario is free-form long text generation, such as biography generation. For this, the most commonly used and reliable methods rely on human experts following specific guidelines, and automatic fact-checkers based on retrieved information, such as FactScore, FacTool and Factcheck-GPT, to facilitate efficient and consistent evaluation. These automatic fact-checkers generally first decompose a document into a set of atomic claims, and then verify one by one whether the claim is true or false based on the retrieved evidence, either from offline Wikipedia or online Web pages. The percentage of true claims over all statements in a document is used to reflect the factual status of a response (refer to FactScore). The averaged Factscore over a dataset is in turn used to assess a model's factuality accuracy. However, there is no guarantee that automatic fact-checkers are 100% accurate in their verification process. (Wang et al., 2023c) show that even the state-of-the-art verifier, equipped with GPT-4 and supporting evidence retrieved with Google search, can only achieve an F1 score of 0.63 in identifying false claims and  using PerplexityAI (compared with human-annotated labels for claims: true or false).

\textbf{Summary:} We categorize datasets that evaluate LLM factuality into four types, depending on the answer space and the difficulty degree on which accurate automatic quantification can be performed (see Table \ref{tab:datasets}). They are: (I) open-domain, free-form, long-term responses (FactScore: the percentage of the correct claims verified by human or automated fact-checker); (II) Yes/No answer w/wt explanation (extract Yes/No, metrics for binary classification); (III) short-form answer (Exact match the answer with gold labels and calculate accuracy) or the listing answer (recall@K); and (IV) multi-choice QA (metrics for multi-class classification).

\begin{table*}[t]
\centering
\caption{Four types of datasets used to evaluate LLM factuality. I: open-ended generation; II: Yes/No answer; III: short-term or list of entities answer; IV: A, B, C, D multiple Choice QA. Labeled datasets under type I are mostly generated by ChatGPT, and FactScore-Bio (ChatGPT, InstGPT and PerplexityAI). ER: Human-annotated Error Rate. Freq: usage frequency as evaluation set in our first 50 references.}
\label{tab:datasets}
\resizebox{\textwidth}{!}{
\begin{tabular}{c l l c c l c}
\toprule
\textbf{Type} & \textbf{Dataset} & \textbf{Topic} & \textbf{Size} & \textbf{ER%} & \textbf{Evaluation and Metrics used in Original Paper} & \textbf{Freq} \
\midrule
I & \begin{tabular}{@{}l@{}}FactScore-Bio (Min and et al., 2023) \ FactcheckGPT (Wang et al., 2023c) \ FacTool-QA (Chern et al., 2023) \ FELM-WK (Chen et al., 2023) \ HaluEval (Li and et al., 2023a) \ FreshQA (Vu et al., 2023) \ SelfAware (Yin et al., 2023b)\end{tabular} & \begin{tabular}{@{}l@{}}Biography \ Open-ended questions \ Knowledge-based QA \ Knowledge-based QA \ Open-ended questions \ Open-ended questions \ Open-ended questions\end{tabular} & \begin{tabular}{@{}c@{}}549 \ 94 \ 50 \ 184 \ 5000 \ 499 \ 3369\end{tabular} & \begin{tabular}{@{}c@{}}42.6 \ 64.9 \ 54.0 \ 46.2 \ 12.3 \ 68.0\end{tabular} & \begin{tabular}{@{}l@{}}Human annotation and automated fact-checkers \ Human annotation \ Human annotation and automated fact-checkers \ Human annotation, Accuracy and F1 score \ Human annotation, AUROC + LLM judge + PARENT \ Human annotation \ Evaluate the LLM awareness of unknown by F1-score\end{tabular} & \begin{tabular}{@{}c@{}}1 \ 2 \ 1 \ 3 \ 2 \ 1\end{tabular} \
\midrule
II & Snowball (Zhang et al., 2023b) & Yes/No question & 1500 & 9.4 & Exact match + Accuracy/F1-score & \
\midrule
III & \begin{tabular}{@{}l@{}}Wiki-category List (Dhuliawala et al., 2023) \ Multispan QA (Dhuliawala et al., 2023)\end{tabular} & \begin{tabular}{@{}l@{}}Name some [Mexican films] \ Short-term Answer\end{tabular} & \begin{tabular}{@{}c@{}}55 \ 428\end{tabular} & & \begin{tabular}{@{}l@{}}Precision/recall@5 \ Exact match + F1 score\end{tabular} & \
\midrule
IV & \begin{tabular}{@{}l@{}}TruthfulQA (Lin et al., 2022) \ HotpotQA (Yang and et al., 2018) \ StrategyQA (Geva et al., 2021) \ MMLU (Hendrycks et al., 2021)\end{tabular} & \begin{tabular}{@{}l@{}}False belief or misconception \ Multi-step reasoning \ Multi-step reasoning \ Knowledge\end{tabular} & \begin{tabular}{@{}c@{}}817 \ 113k \ 2780 \ 15700\end{tabular} & & \begin{tabular}{@{}l@{}}Accuracy \ Exact match + F1 score \ Recall@10 \ Accuracy\end{tabular} & \begin{tabular}{@{}c@{}}5 \ 11 \ 3 \ 4\end{tabular} \
\bottomrule
\end{tabular}
}
\end{table*}

\subsection{Other Metrics}

In addition to evaluating the methods discussed above, (Lee et al., 2022) quantified the hallucinations using two metrics, both requiring document-level ground-truth: (1) hallucinated named entities error measures the percentage of named entities in the generations that do not appear in the ground-truth document; (2) entailment ratio evaluates the number of generations that can be entailed by the ground-truth reference, over all generations.

(Rawte et al., 2023a) defined the hallucination vulnerability index (HVI), which takes a spectrum of factors into account, to evaluate and rank LLMs. Some factuality measurement tasks, such as claim extraction and evidence retrieval are non-trivial to automate. (Rawte et al., 2023a) curated publicly available LLM hallucination mitigation benchmark, where LLM generations are scored by humans when automated external knowledge retrieval fails to resolve a claim clearly. While widely used for factuality evaluation, this hybrid approach may suffer from human annotation bias.
=====END FILE=====

=====FILE: sections/4_improving.tex=====
\section{Improving Factuality}

Improving the factuality of an LLM often requires updating its internal knowledge, editing fake, outdated and biased elements, thereby making its output reflect a revised collection of facts, maximizing the probability of P(truth prompt). One option is to adopt gradient-based methods to update model parameters to encourage desired model output. This includes pre-training, supervised fine-tuning and RLXF. We can also explore injecting a new fact into LLMs or overwriting the false knowledge stored in LLM memory by in-context learning (ICL).

When models store factually correct knowledge but produce errors, they can in some cases rectify them through self-reasoning, reflection, and multi-agent debates. We discuss these methods throughout the lifecycle of an LLM, ranging from pre-training, to inference, to post-processing. Another important element is retrieval augmentation, which enhances the generation capabilities of LLMs by anchoring them in external knowledge that may not be stored or contradict the information in LLM parametric memory. It can be incorporated at various stages throughout model training and the subsequent inference process (Gao et al., 2023b), and is therefore not discussed individually.

\subsection{Pre-training}

LLMs store a vast amount of world knowledge in their parameters through the process of pre-training. The quality of the pre-training data plays a crucial role and misinformation could potentially cause LLMs to generate false responses, motivating the utilization of high-quality textual corpora. However, the prohibitively massive amount of pre-training data, typically consisting of trillions of tokens, renders manual filtering and editing impractically laborious. To this end, automated filtering methods have been proposed. For instance, (Brown et al., 2020) introduce a method to only focus on a small portion of the CommonCrawl dataset that exhibits similarity to high-quality reference corpora.

(Touvron et al., 2023) propose to enhance factual robustness of mixed corpora by up-sampling documents from the most reliable sources, thereby amplifying knowledge accuracy and mitigating hallucinations. During the pre-training phase of phi-1.5, (Li and et al., 2023b) synthesize ``textbook-like'' data, consists of and rich in high-quality commonsense reasoning and world knowledge. While careful corpus curation remains the cornerstone of pre-training for enhanced factuality, the task becomes increasingly challenging with the expansion of dataset scale and the growing demand for linguistic diversity. It is therefore crucial to develop novel strategies that guarantee the consistency of factual knowledge across diverse cultural landscapes.

(Borgeaud et al., 2021) propose RETRO, a retrieval augmented pre-training approach. An auto-regressive LLM is trained from scratch with a retrieval module that is practically scalable to large-scale pre-training by retrieving billions of tokens. RETRO shows better accuracy and is less prone to hallucinate compared to GPT (Wang et al., 2023a). While limitations lie in that RETRO performance could be compromised if the retrieval database contains inaccurate, biased or outdated information. 25% additional computation is required for the pre-training of LLMs with retrieval.

\subsection{Tuning and RLXF}

Continued domain-specific SFT has shown to be effective for enhancing factuality, particularly in the absence of such knowledge during pre-training. For instance, (Elaraby et al., 2023) enhance the factual accuracy of LLMs through knowledge injection (KI). Knowledge, in the form of entity summaries or entity triplets, is incorporated through SFT by either intermediate tuning, i.e. first on knowledge and then on instruction data; or combined tuning, i.e. on the mixture of both. While some improvements are exhibited, the method alone can be insufficient to fully mitigate factual errors.

For general-purpose LLMs, SFT is typically employed to improve the instruction-following capabilities as opposed to factual knowledge which is mostly learned in pre-training. However, this process may inadvertently reveal areas of knowledge not covered in the pre-training, causing the risk of behavior cloning, where a model feigns understanding and responds with hallucinations to questions it has little knowledge of (Torabi et al., 2018). R-tuning (Zhang et al., 2023a) is proposed to address this issue with two pivotal steps: first, assessing the knowledge gap between the model's parametric knowledge and the instruction tuning data, and second, creating a refusal-aware dataset for SFT. It enables LLMs to abstain from answering queries beyond their parametric knowledge scope. On the other hand, BeInfo (Razumovskaia et al., 2023) improve factual alignment through the form of behavioral fine-tuning. The creation of the behavioral tuning dataset emphasizes two goals: selectivity (choosing correct information from the knowledge source) and response adequacy (informing the user when no relevant information is available or asking for clarification). Both methods effectively control LLMs on non-parametric questions but require extra effort in dataset curation and might hinder the models' retention of parametric knowledge.

Sycophancy (Sharma et al., 2023), another source of factuality errors, often arises from misalignments during SFT and RLHF(Ouyang et al., 2022). This is partially attributed to human annotators' tendency to award higher scores to responses they like rather than those that are factually accurate. (Wei et al., 2023) explore the correlation of sycophancy with model scaling and instruction tuning. They propose a synthetic-data intervention method, using various NLP tasks to teach models that truthfulness is independent of user opinions. However, one limitation is that the generalizability of their approach remains unclear for varied prompt formats and diverse user opinions.

(Tian et al., 2023) utilize direct preference optimization (DPO) (Rafailov et al., 2023) with the feedback of factuality score either from automatic fact-checkers or LLMs predictive confidence. In-domain evaluation shows promising results on biographies and medical queries, but generalization performance across domains and unseen domains is under-explored. (Köksal et al., 2023) propose hallucination-augmented recitations (HAR). It encourages the model to attribute to the contexts rather than its parametric knowledge, by tuning the model on the counterfactual dataset created leveraging LLM hallucinations. This approach offers a novel way to enhance LLM attribution and grounding in open-book QA. However, challenges lie in refining counterfactual generation for consistency and expanding its application to broader contexts.

\paragraph{Retrieval Augmentation}
Incorporating retrieval mechanisms during fine-tuning has been shown to enhance the LLM factuality on downstream tasks, particularly in open-domain QA. DPR (Karpukhin et al., 2020) refines a dual-encoder framework, consisting of two BERT models. It employs a contrastive loss to align the hidden representations of questions and their corresponding answers, obtained through the respective encoder models. RAG (Lewis et al., 2020) and FiD (Izacard and Grave, 2020) study a fine-tuning recipe for retrieval-augmented generation models, focusing on open-domain QA tasks. WebGPT (Nakano et al., 2021) fine-tunes GPT-3 (Brown et al., 2020) by RLHF, providing questions with factually correct long-form reference generation. The implementation in a text-based web-browsing environment allows the model to search and navigate the web.

\subsection{Inference}

We categorize approaches to improve factuality during inference into two: (1) optimizing decoding strategies to strengthen model factuality; and (2) empowering LLM learned ability by either in-context learning (ICL) or self-reasoning.

\subsubsection{Decoding Strategy}
Sampling from the top subword candidates with a cumulative probability of p, known as nucleus sampling (top-p) (Holtzman et al., 2020), sees a decrease in factuality performance compared to greedy decoding, despite higher diversity. This is likely due to its over-encouragement of randomness. Building on the hypothesis that sampling randomness may damage factuality when generating the latter part of a sentence than the beginning, (Lee et al., 2022) introduce factual-nucleus sampling, which dynamically reduces the nucleus-p value as generation progresses to limit diversity and improve factuality, modulating factual integrity and textual diversity.

Apart from randomness, some errors arise when knowledge conflicts, where context contradicts information present in the model's prior knowledge. Context-aware decoding (CAD) (Shi et al., 2023) prioritizes current context over prior knowledge, and employs contrastive ensemble logits, adjusting the weight of the probability distribution when predicting the next token with or without context. Despite the factuality boost, CAD is a better fit for tasks involving knowledge conflicts and heavily reliant on high-quality context.

In contrast, DoLa (Chuang et al., 2023) takes into account both upper and lower (earlier) layers, as opposed to only the final (mature) layer. This method dynamically selects intermediate layers at each decoding step, in which an appropriate premature layer contains less factual information with maximum divergence among the subset of the early layers. This method effectively harnesses the distinct contributions of each layer to factual generations. However, DoLa increases the decoding time by 1.01x to 1.08x and does not utilize external knowledge, which limits its ability to correct misinformation learned during training.

\subsubsection{ICL and Self-reasoning}
In context learning (ICL) allows an LLM to leverage and learn from demonstration examples in its context to perform a particular task without the need to update model parameters. (Zheng et al., 2023) present that it is possible to perform knowledge editing via ICL through facts included in demonstration examples, thereby correcting fake or outdated facts. The objective of demonstration examples is to teach LLMs how to: (1) identify and copy an answer; (2) generalize using in-context facts; (3) ignore irrelevant facts in context. While it is rather easy for LLMs to copy answers from contexts, changing predictions of questions related to the new facts accordingly, and keeping the original predictions if the question is irrelevant to the modified facts, remains tough.

Another line of research leverages the self-reasoning capability of LLMs. (Du et al., 2023) improve LLM factuality through multi-agent debate. This approach first instantiates a number of agents and then makes them debate over answers returned by other agents until a consensus is reached. One interesting finding is that more agents and longer debates tend to lead to better results. This approach is orthogonal and can be applied in addition to many other generation methods, such as complex prompting strategy (e.g., CoT (Wei et al., 2022), ReAct (Yao et al., 2023), Reflexion (Shinn et al., 2023)) and retrieval augmentation.

Take-away: Zheng et al. (2023) evaluate the effectiveness of knowledge editing on subject-relation-object triplets, an unrealistic setting compared to open-ended free-form text assessment. Previous methods (Mitchell et al., 2021; Meng et al., 2022) use finetuning over texts containing specific text to improve factuality. The relationship between SFT and ICL may also been an interesting avenue to explore. More specifically, we seek answers to two research questions: (1) What types of facts and to what extent can facts be edited effectively, learned by LLMs through ICL? (2) Would SFT do a better job at learning from examples that are difficult for ICL? More broadly, what is the best way to insert new facts or edit false knowledge stored in LLMs. The community may also benefit from an in-depth comparative analysis of the effectiveness of improving factuality between SFT and ICL (perhaps also RLXF).

Retrieval Augmentation can be applied before, during, and after model generation. One commonly used option is to apply retrieval augmentation prior to response generation. For questions requiring up-to-date world knowledge to answer, (Vu et al., 2023) augment LLM prompts with web-retrieved information and demonstrate the effectiveness on improving accuracy on FreshQA, where ChatGPT and GPT-4 struggle due to their lack of up-to-date information. (Gao et al., 2023a) place all relevant paragraphs in the context and encourage the model to cite supporting evidence, instructing LLMs to understand retrieved documents and generate correct citations, thereby improving reliability and factuality. Pre-generation retrieval augmentation is beneficial as the generation process is conditioned on the retrieval results, implicitly constraining the output space. While improving factual accuracy, this comes at the cost of spontaneous and creative responses, largely limiting the capabilities of LLMs.

\begin{figure*}[t]
\centering
\fbox{
\begin{minipage}{0.9\textwidth}
\centering
\textbf{[IMAGE NOT PROVIDED]} \
\textit{Original content: Figure 1: Fact-checker framework: claim processor, retriever, and verifier, with optional step of summarizing and explaining in gray.}
\end{minipage}
}
\caption{Fact-checker framework: claim processor, retriever, and verifier, with optional step of summarizing and explaining in gray.}
\label{fig:fact_checker}
\end{figure*}

An alternative method is to verify and rectify factual errors after the model generates all content. However, LLMs have been shown to be susceptible to hallucination snowballing (Zhang et al., 2023b), a common issue where a model attempts to make its response consistent with previously generated content even if it is factually incorrect. Striking a balance between preserving creative elements and avoiding error propagation, EVER (Kang et al., 2023) and ``a stitch in time saves nine'' (Varshney et al., 2023) actively detect and correct factual errors during generation sentence by sentence. The former leverages retrieved evidence for verification, and the latter incorporates the probability of dominant concepts in detection. Their findings suggest that timely correcting errors during generation can prevent snowballing and further improve factuality. Nonetheless, the primary concern for this iterative process of generate-verify-correct in real-time systems is latency, making it difficult to meet the high-throughput and responsiveness demand (Kang et al., 2023).

\subsection{Automatic Fact Checkers}

An automatic fact-checking framework typically consists of three components: claim processor, retriever, and verifier as shown in Figure \ref{fig:fact_checker}, though the implementation of verification pipelines may differ. For example, FACTOR (Muhlgay et al., 2023) and FactScore (Min and et al., 2023) only detect falsehoods without correction. While RARR depends on web-retrieved information (Gao et al., 2022), and CoVe (Dhuliawala et al., 2023) only relies on LLM parametric knowledge (Dhuliawala et al., 2023) to perform both detection and correction, albeit at a coarse granularity, editing the entire document. Compared to fine-grained verification over claims, it is unable to spot false spans precisely and tends to result in poor preservation of the original input.

FacTool (Chern et al., 2023) and Factcheck-GPT (Wang et al., 2023c) edit atomic claims. While the former breaks a document down to independent checkworthy claims with three steps: decomposition, decontextualization and checkworthiness identification, the latter employs GPT-4 to extract verifiable claims directly.

Evaluating the effectiveness of fact-checkers remains challenging, making the improvement of such systems a difficult task.

\paragraph{Engineering and Practical Considerations}
Automatic fact-checking involve tasks of extracting atomic check-worthy claims, collecting evidence either by leveraging the knowledge stored in the model parameters or retrieved externally, and verification. While straightforward to implement, this pipeline may be susceptible to error propagation. Major bottleneck lies in the absence of automatic evaluation measures to assess the quality of intermediate steps, in particular, the claim processor and evidence retriever as there is no gold standard.

The input to a claim processor is a document and the expected output is a list of atomic check-worthy claims or atomic verifiable facts. There is no consensus on the granularity of `atomic claims'', making consistent decomposition difficult. Additionally, the concept of check-worthy and verifiable claims are subjective. Consequently, the definition of an atomic check-worthy claim remains a highly debatable topic. This naturally leads to different `gold'' human-annotated atomic claims annotated following various guidelines and distinct implementation approaches to decompose a document.

Given a document, even if assuming a ground-truth list of atomic claims, it is an open question how to assess the quality of automatically derived decomposition results. (Wang et al., 2023c) assess the agreement in the number of claims between ground truth and predictions, followed by examining the semantic similarity between two claims at the same index when the claim count aligns. Entailment ratio presented in Section 3.2 is also applicable (Lee et al., 2022). While it is much simpler when the evidence is constrained (e.g., to Wikipedia documents as is the case for FEVER (Thorne et al., 2018)), accurate retrieval of evidence from the Internet and subsequently quantifying the quality of such retrieval results remain challenging. Similar to the assessment of atomic claims, gold-labeled evidence is unavailable and infeasible to obtain in the expansive open search space. The only step where we can confidently evaluate its quality is the accuracy of verification, a simple binary true/false label given a document/claim.

In conclusion, perhaps the most significant hurdle for the development and improvement of automatic fact-checkers lies in the automated assessment and quantification of the quality at intermediate stages.
=====END FILE=====

=====FILE: sections/5_multimodal.tex=====
\section{Factuality of Multimodal LLMs}

Factuality or hallucination in Multimodal Large Language Models refers to the phenomenon of generated responses being inconsistent with the image content. Current research on multimodal factuality can be further categorized into three types:

1. Existence Factuality: incorrectly claiming the existence of certain objects in the image.
2. Attribute Factuality: describing the attributes of certain objects in a wrong way, e.g. identifying the colour of a car incorrectly.
3. Relationship Factuality: false descriptions of relationships between objects, such as relative positions and interactions.

\paragraph{Evaluation}
CHAIR (Rohrbach et al., 2018) is the first benchmark for assessing the accuracy of object existence within captions, focusing on a pre-defined set of objects in the COCO dataset (Lin et al., 2014). However, this approach can be misleading since the COCO dataset is frequently used in training sets, providing a limited perspective when used as the sole basis for evaluation. In contrast, POPE (Li et al., 2023) evaluates object hallucination with multiple binary choice prompts, both positive and negative, querying if a specific object exists in the image. More recently, (Li et al., 2023) proposed GPT4-Assisted Visual Instruction Evaluation (GAVIE) to evaluate the visual hallucination. Additionally, (Gunjal et al., 2023) demonstrated the use of human evaluation to avoid inaccuracies and systematic biases.

\paragraph{Mitigation}
The methods for improving factuality in MLLMs can be broadly categorized into the categories: finetuning-based method, inference time correction and representation learning. Fine-tuning methods such as LRV-Instruction (Liu et al., 2023) and LLaVA-RLHF (Sun et al., 2023) follow an intuitive and straightforward solution of collecting specialized data such as positive and negative instructions or human preference pairs. This data is used for finetuning the model, thus resulting in models with fewer hallucinated responses.

\noindent\textit{[MISSING]: Remainder of section and subsequent sections (Conclusion, References) are missing from the source.}
=====END FILE=====

=====FILE: refs.bib=====
@article{tonmoy2024,
title={A comprehensive survey of hallucination mitigation techniques in large language models},
author={Tonmoy, S. M. Towhidul Islam and Zaman, S. M. Mehedi and others},
journal={CoRR},
volume={abs/2401.01313},
year={2024}
}

@article{huang2023a,
title={A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions},
author={Huang, Lei and Yu, Weijiang and Ma, Weitao and Zhong, Weihong and Feng, Zhangyin and Wang, Haotian and Chen, Qianglong and Peng, Weihua and Feng, Xiaocheng and Qin, Bing and others},
journal={arXiv preprint arXiv:2311.05232},
year={2023}
}

@article{wang2023b,
title={Survey on factuality in large language models: Knowledge, retrieval and domain-specificity},
author={Wang, Cunxiang and Liu, Xiaoze and Yue, Yuanhao and Tang, Xiangru and Zhang, Tianhang and Jiayang, Cheng and Yun, Yao and Gong, Yiqiao and Dong, Xuanjing},
journal={arXiv preprint arXiv:2310.07521},
year={2023}
}

@article{chen2023,
title={Felm: Benchmarking factuality evaluation of large language models},
author={Chen, Shiqi and Zhao, Yiran and Zhang, Baotian and Geng, Yuxiao and Yang, Yang and Zhai, Chengqiang},
journal={Advances in Neural Information Processing Systems},
volume={36},
year={2024}
}

@article{gao2023b,
title={Retrieval-augmented generation for large language models: A survey},
author={Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang and Pan, Jinliu and Bi, Yuxi and Dai, Yi and Sun, Jiawei and Guo, Haofen},
journal={arXiv preprint arXiv:2312.10997},
year={2023}
}

@article{rawte2023b,
title={The troubling emergence of hallucination in large language models-an extensive definition, quantification, and prescriptive remorse survey},
author={Rawte, Vipula and Sheth, Amit and Das, Amitava},
journal={arXiv preprint arXiv:2310.04988},
year={2023}
}

@article{zhang2023c,
title={Siren's song in the ai ocean: A survey on hallucination in large language models},
author={Zhang, Yue and Li, Yafu and Cui, Leyang and Cai, Deng and Liu, Lemao and Fu, Tingchen and Huang, Minlie and Shi, Shuming},
journal={arXiv preprint arXiv:2309.01219},
year={2023}
}

@article{guo2022,
title={A survey on automated fact-checking},
author={Guo, Zhijiang and Schlichtkrull, Michael and Vlachos, Andreas},
journal={Transactions of the Association for Computational Linguistics},
volume={10},
pages={178--206},
year={2022},
publisher={MIT Press}
}
=====END FILE=====