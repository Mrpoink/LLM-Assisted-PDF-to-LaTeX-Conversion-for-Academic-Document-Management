=====FILE: main.tex=====
\documentclass[twocolumn,11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\geometry{top=2.5cm, bottom=2.5cm, left=2cm, right=2cm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{authblk}
\usepackage{hyperref}

\title{MTA4DPR: Multi-Teaching-Assistants Based Iterative Knowledge Distillation for Dense Passage Retrieval}

\author[1,2]{Qixi Lu}
\author[1]{Endong Xun}
\author[2]{Gongbo Tang\thanks{Corresponding author}}

\affil[1]{Beijing Advanced Innovation Center for Language Resources, Beijing Language and Culture University, China}
\affil[2]{School of Information Science, Beijing Language and Culture University, China}
\affil[ ]{\texttt{lqxaixxh@gmail.com}, \texttt{{edxun, gongbo.tang}@blcu.edu.cn}}

\date{}

\begin{document}

\maketitle

\begin{abstract}
Although Dense Passage Retrieval (DPR) models have achieved significantly enhanced performance, their widespread application is still hindered by the demanding inference efficiency and high deployment costs. Knowledge distillation is an efficient method to compress models, which transfers knowledge from strong teacher models to weak student models. Previous studies have proved the effectiveness of knowledge distillation in DPR. However, there often remains a significant performance gap between the teacher and the distilled student. To narrow this performance gap, we propose MTA4DPR, a Multi-Teaching-Assistants based iterative knowledge distillation method for Dense Passage Retrieval, which transfers knowledge from the teacher to the student with the help of multiple assistants in an iterative manner; with each iteration, the student learns from more performant assistants and more difficult data. The experimental results show that our 66M student model achieves the state-of-the-art performance among models with same parame[MISSING]
\end{abstract}

\section{Introduction}
[MISSING CONTENT]

\section{Methodology}
[MISSING CONTENT]

\section{Experiments}
[MISSING CONTENT]

\section{Conclusion}
[MISSING CONTENT]

\bibliographystyle{acl_natbib}
\bibliography{refs}

\appendix

\section{Algorithm}

\begin{algorithm}[H]
\caption{MTA4DPR Training Process}
\begin{algorithmic}[1]
\REQUIRE : the teacher model; : the assistant models; : the student model; : the query set; : the passage set; : maximum number of training iterations; : maximum number of training steps; : Learning rate;
\ENSURE 
\STATE 
\WHILE{}
\STATE  \COMMENT{[ILLEGIBLE]}
\STATE [MISSING]
\ENDWHILE
\end{algorithmic}
\end{algorithm}

\end{document}
=====END FILE=====

=====FILE: refs.bib=====
@inproceedings{yang2022cross,
title={Cross-image relational knowledge distillation for semantic segmentation},
author={Yang, Chuanguang and Zhou, Helong and An, Zhulin and Jiang, Xue and Xu, Yongjun and Zhang, Qian},
booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
pages={12319--12328},
year={2022}
}

@inproceedings{yuan2021reinforced,
title={Reinforced multi-teacher selection for knowledge distillation},
author={Yuan, Fei and Shou, Linjun and Pei, Jian and Lin, Wutao and Gong, Ming and Fu, Yan and Jiang, Daxin},
booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
volume={35},
pages={14284--14291},
year={2021}
}

@inproceedings{zeng2022curriculum,
title={Curriculum learning for dense retrieval distillation},
author={Zeng, Hansi and Zamani, Hamed and Vinay, Vishwa},
booktitle={Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages={1979--1983},
year={2022}
}
=====END FILE=====