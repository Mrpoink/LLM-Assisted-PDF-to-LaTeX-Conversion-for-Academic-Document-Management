=====FILE: main.tex=====
\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\usepackage{authblk}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{tabularx}
\usepackage{natbib}

% Page geometry
\geometry{margin=1in}

\title{Related Work and Citation Text Generation: A Survey}

\author[1,2]{Xiangci Li}
\author[1]{Jessica Ouyang}
\affil[1]{University of Texas at Dallas}
\affil[2]{Amazon Web Services}
\affil[ ]{\texttt{lixiangci8@gmail.com}, \texttt{jessica.ouyang@utdallas.edu}}

\date{}

\begin{document}

\maketitle

\begin{abstract}
To convince readers of the novelty of their research paper, authors must perform a literature review and compose a coherent story that connects and relates prior works to the current work. This challenging nature of literature review writing makes automatic related work generation (RWG) academically and computationally interesting, and also makes it an excellent test bed for examining the capability of SOTA natural language processing (NLP) models. Since the initial proposal of the RWG task, its popularity has waxed and waned, following the capabilities of mainstream NLP approaches. In this work, we survey the zoo of RWG historical works, summarizing the key approaches and task definitions and discussing the ongoing challenges of RWG.
\end{abstract}

\section{Introduction}

Academic research is an exploratory activity to solve problems that have never been resolved before. Each academic research paper must sit at the frontier of the field and present novelties that have not been addressed by prior work; to convince readers of the novelty of the current work, the authors must perform a literature review to compare their work with the prior work. In natural language processing (NLP), a short literature review is usually conducted under the ``R... [MISSING]''

\section*{[MISSING SECTION]}
[CONTENT MISSING]

\section*{[MISSING SECTION]}
[CONTENT MISSING]

\begin{table*}[t]
\centering
\caption{Comparison of evaluation metrics used in RWG works.}
\label{tab:metrics}
\begin{tabularx}{\textwidth}{lXp{6cm}}
\toprule
\textbf{Metric} & \textbf{Definition} & \textbf{Papers} \
\midrule
Coherence & Whether the citation text is coherent with the citing paper's context & Xing et al. (2020); Ge et al. (2021); Chen et al. (2021, 2022); Li et al. (2022, 2023); Liu et al. (2023); Mandal et al. (2024); Li and Ouyang (2024) \
\midrule
Informativeness & Does the related work convey important facts about the topic question? & Deng et al. (2021); Chen et al. (2021, 2022); Liu et al. (2023) \
\midrule
Succinctness & Does the related work avoid [ILLEGIBLE] & Deng et al. (2021); Chen et al. (2021); Liu et al. (2023) \
\midrule
Overall & Overall quality & Xing et al. (2020); Ge et al. (2021); Li et al. (2022, 2023); Mandal et al. (2024); Li and Ouyang (2024) \
\midrule
Intuitiveness & How intuitive is the related work section for readers to grasp the key content? & Wang et al. (2018) \
\midrule
QA & Retain the key information? & Chen et al. (2021, 2022); Liu et al. (2023) \
\midrule
Specific & Whether the explanation describes a specific relationship between the two works & Luu et al. (2021); Jung et al. (2022) \
\midrule
Factuality, # of errors & Does the output contain factual errors? & Li and Ouyang (202...) \
\bottomrule
\end{tabularx}
\end{table*}

\section*{[MISSING SECTIONS]}
[CONTENT MISSING]

\end{document}
=====END FILE=====