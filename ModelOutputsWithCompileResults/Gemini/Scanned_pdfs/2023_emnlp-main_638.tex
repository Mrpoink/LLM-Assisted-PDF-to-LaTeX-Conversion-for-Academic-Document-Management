ACK

=====FILE: main.tex=====
\documentclass[10pt,twocolumn,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{url}
\usepackage{geometry}
\usepackage{caption}
\usepackage{float}
\usepackage{newunicodechar}

% Adjust geometry to match conference paper
\geometry{
top=2.5cm,
bottom=2.5cm,
left=2cm,
right=2cm
}

% Fix for Tibetan characters if needed, or use placeholders
% Since we don't have a Tibetan font guaranteed, we will use placeholders or standard text
\newcommand{\tibetan}[1]{{\fontfamily{lmr}\selectfont #1}}

\title{Simple and Effective Input Reformulations for Translation}
\author{Brian Yu, Hansen Lillemark, Kurt Keutzer \
University of California, Berkeley \
Berkeley Artificial Intelligence Research (BAIR) \
\texttt{{bri25yu, hlillemark, keutzer}@berkeley.edu}}
\date{}

\begin{document}

\maketitle

\input{sections/00_abstract}
\input{sections/01_introduction}
\input{sections/02_related_work}
\input{sections/03_experiments_tib2eng}
\input{sections/04_experiments_flores}
\input{sections/05_conclusion}
\input{sections/06_limitations_ethics}

\bibliographystyle{alpha}
\begin{thebibliography}{99}

\bibitem{brown2020}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020.
\newblock Language models are few-shot learners.
\newblock \textit{Advances in neural information processing systems}, 33:1877--1901.

\bibitem{shoeybi2020}
Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. 2020.
\newblock Megatron-lm: Training multi-billion parameter language models using model parallelism.
\newblock \textit{arXiv preprint arXiv:1909.08053}.

\bibitem{xue2021}
Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2021.
\newblock mT5: A massively multilingual pre-trained text-to-text transformer.
\newblock In \textit{Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, pages 483--498.

\bibitem{hoffmann2022}
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. 2022.
\newblock Training compute-optimal large language models.
\newblock \textit{arXiv preprint arXiv:2203.15556}.

\bibitem{chowdhery2022}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022.
\newblock Palm: Scaling language modeling with pathways.
\newblock \textit{arXiv preprint arXiv:2204.02311}.

\bibitem{zhang2022a}
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022a.
\newblock Opt: Open pre-trained transformer language models.
\newblock \textit{arXiv preprint arXiv:2205.01068}.

\bibitem{chung2022}
Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022.
\newblock Scaling instruction-finetuned language models.
\newblock \textit{arXiv preprint arXiv:2210.11416}.

\bibitem{workshop2023}
BigScience Workshop. 2023.
\newblock Bloom: A 176b-parameter open-access multilingual language model.
\newblock \textit{arXiv preprint arXiv:2211.05100}.

\bibitem{touvron2023}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023.
\newblock Llama: Open and efficient foundation language models.
\newblock \textit{arXiv preprint arXiv:2302.13971}.

\bibitem{nllb2022}
NLLB Team, Marta R. Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, et al. 2022.
\newblock No language left behind: Scaling human-centered machine translation.
\newblock \textit{arXiv preprint arXiv:2207.04672}.

\bibitem{sennrich2016}
Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016.
\newblock Neural machine translation of rare words with subword units.
\newblock In \textit{Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 1715--1725.

\bibitem{fadaee2017}
Marzieh Fadaee, Arianna Bisazza, and Christof Monz. 2017.
\newblock Data augmentation for low-resource neural machine translation.
\newblock In \textit{Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)}, pages 567--573.

\bibitem{papineni2002}
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.
\newblock Bleu: a method for automatic evaluation of machine translation.
\newblock In \textit{Proceedings of the 40th Annual Meeting on Association for Computational Linguistics}, pages 311--318.

\bibitem{loshchilov2019}
Ilya Loshchilov and Frank Hutter. 2019.
\newblock Decoupled weight decay regularization.
\newblock In \textit{International Conference on Learning Representations}.

\bibitem{zhang2022b}
Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. 2022b.
\newblock Automatic chain of thought prompting in large language models.
\newblock \textit{arXiv preprint arXiv:2210.03493}.

\bibitem{zhong2021a}
Ruiqi Zhong, Kristy Lee, Zheng Zhang, and Dan Klein. 2021a.
\newblock Adapting language models for zero-shot learning by meta-tuning on dataset and prompt collections.
\newblock In \textit{Findings of the Association for Computational Linguistics: EMNLP 2021}, pages 2856--2878.

\bibitem{zhong2021b}
Zexuan Zhong, Dan Friedman, and Danqi Chen. 2021b.
\newblock Factual probing is [MASK]: Learning vs. learning to recall.
\newblock In \textit{Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, pages 5017--5033.

\bibitem{zhou2023a}
Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, and Ed Chi. 2023a.
\newblock Least-to-most prompting enables complex reasoning in large language models.
\newblock \textit{International Conference on Learning Representations}.

\bibitem{zhou2023b}
Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. 2023b.
\newblock Large language models are human-level prompt engineers.
\newblock \textit{International Conference on Learning Representations}.

\end{thebibliography}

\appendix
\input{sections/07_appendix}

\end{document}
=====END FILE=====

=====FILE: sections/00_abstract.tex=====
\begin{abstract}
Foundation language models learn from their finetuning input context in different ways. In this paper, we reformulate inputs during finetuning for challenging translation tasks, leveraging model strengths from pretraining in novel ways to improve downstream performance. These reformulations are simple data level modifications, require no additional collection of training data or modification of data at inference time. They can be applied either on single language pair translation tasks or massively multilingual translation tasks. Experiments with these techniques demonstrate significant performance improvements up to 3.5 chrF++ on the Flores200 translation benchmark. We hope our research accessibly improves finetuning data efficiency, enabling more effective training to scalably improve state-of-the-art performance. Our code is released here.
\end{abstract}
=====END FILE=====

=====FILE: sections/01_introduction.tex=====
\section{Introduction}

Foundation language models (FLMs) are powerful and task-agnostic models. They are pretrained on language understanding objectives, enabling strong performance on downstream language tasks (Brown et al., 2020; Shoeybi et al., 2020; Xue et al., 2021; Hoffmann et al., 2022; Chowdhery et al., 2022; Zhang et al., 2022a; Chung et al., 2022; Workshop, 2023; Touvron et al., 2023). FLMs are then either prompted or finetuned for downstream use.

In this paper, we present three different data efficient techniques for improving translation performance, applied to the multilingual FLM mT5 during finetuning (Xue et al., 2021). In our first approach, we train mT5 on a Classical Tibetan to English (tib2eng) translation task. mT5 struggles heavily in the initial training steps. Thus, for the first 20% of finetuning, we apply the ``Partial Output English Scaffold'' or POSE reformulation, shown in Figure \ref{fig:reformulations}. Tib2eng translation examples consist of a Classical Tibetan source and English target translation pair. POSE simply appends a prefix of the target English output to the Classical Tibetan input. We see qualitative improvements in the variance of the training curves. When evaluated on the same test set with no reformulations, POSE significantly increases overall translation performance compared to the direct finetuning baseline, up to 10.3% / 2.8 BLEU.

The POSE setup had many adjustable hyperparameters relating to task difficulty, task curriculum, and substring selection for scaffolding. We find that input reformulation setups should consist of 20% less informative examples, and 80% harder and more informative examples. More ablation details can be found below.

Second, we approach the massively multilingual Flores200 translation benchmark (NLLB-Team et al., 2022). mT5 does not struggle in the initial steps of finetuning on Flores200 in the same way it did on tib2eng. Even so, we begin by replicating the tib2eng POSE setup on Flores200 by appending a partial output of the target translation to the input translation. As expected, this setup matched but did not improve upon the baseline performance.

The Flores200 benchmark consists of parallel examples of the same sentence in different languages. In our second approach, we extend the tib2eng POSE reformulation to create the ``Parallel Scaffold in English'' or ParSE reformulation, shown in Figure \ref{fig:reformulations}. ParSE appends the corresponding full parallel English translation (provided by Flores200) to the input. Following the tib2eng setup, we use a data mix of 20% baseline (less informative) and 80% ParSE (more informative) examples. ParSE significantly improves translation performance, up to 17.2% / 3.5 chrF++.

We postulate that POSE and ParSE improve translation performance in part because they enable mT5 to attend to an in-distribution pretrain language with strong monolingual performance. In our third approach, we explore the efficacy of parallel scaffolding that does not require strong monolingual performance using the ``Mixed-language Parallel Scaffold'' or MiPS reformulation, shown in Figure \ref{fig:reformulations}. MiPS appends a different parallel translation to both the input and output for a total of 4 distinct languages per input. Again, we use a data mix of 20% baseline and 80% MiPS examples. MiPS also improves translation performance, up to 9.1% / 1.6 chrF++. Scaffolding with the strongest performing pretraining language (ParSE) outperforms scaffolding with a mix of other languages (MiPS).

Finally, we perform analysis on the languages in the translation set. Using a balanced dataset like Flores200 allows mT5 to partially overcome pretraining dataset size biases. Naturally, translating into lower resource languages is more difficult than translating into higher resource languages, but we find that the ParSE and MiPS reformulations improve translation into all languages across the board, rather than disproportionately improving performance on high resource languages.

In summary, we propose input reformulations on translation tasks. These reformulations require no additional data, have few hyperparameters, and are simple to implement. When finetuning on a single language pair translation task, if the target output language is in the model's pretraining dataset distribution, the POSE reformulation can be applied. When translating between multiple language pairs, the ParSE reformulation can be applied to the strongest performing pretraining language.

\begin{figure*}[t]
\centering
\begin{small}
\begin{tabular}{l}
\textbf{Baseline} \
German: Das ist gut. \
English: \
\midrule
\textbf{Partial Output English Scaffold (POSE)} \
German: Das ist gut. \
English: That is \
\midrule
\textbf{Parallel Scaffold in English (ParSE)} \
German: Das ist gut. \
English: That is good. \
Spanish: \
\midrule
\textbf{Mixed-language Parallel Scaffold (MiPS)} \
German to Spanish: Das ist gut. \
Chinese to English: [Chinese Text] \ % Using placeholder for Chinese
That is good. \
\bottomrule
\textbf{mT5} \
That is good. \
Está bien. \
Está bien. \
That is good. \
\end{tabular}
\end{small}
\caption{Task reformulations. Baseline: a direct translation pair. POSE: append a prefix of the target translation to the input translation. ParSE: append a parallel English translation to the input translation. MiPS: append a different parallel translation to both the input and output.}
\label{fig:reformulations}
\end{figure*}
=====END FILE=====

=====FILE: sections/02_related_work.tex=====
\section{Related work}

Our work can be viewed as a data efficiency technique for translation. Past works in translation have explored data augmentation (Sennrich et al., 2016; Fadaee et al., 2017), sample re-weighting (Shu et al., 2019; Ren et al., 2019; Gu et al., 2018), and curriculum learning (Kocmi and Bojar, 2017; Zhang et al., 2018; Platanios et al., 2019; Zhang et al., 2019; NLLB-Team et al., 2022). These approaches vary in effectiveness, are not generalizable, and introduce complexity into the training process. Curriculum learning approaches in particular are typically complicated and unsuccessful, because they are designed using intuition on how humans treat inputs, which may differ from how models treat inputs. In contrast, our input reformulations are simple and can be directly applied to any sequence-to-sequence task.

Previous work has explored prompting a frozen language model using manually curated prompts (Brown et al., 2020; Touvron et al., 2023; Petroni et al., 2019). Results are typically sensitive to the exact prompt used. This technique cannot be applied to larger corpora because it is limited by the number of examples that can feasibly fit into a single input context. Other works have explored finetuning with a fixed prompt without leveraging the target output as a part of the input (Radford et al., 2018, 2019; Dong et al., 2019; Devlin et al., 2019; Lewis et al., 2019; Sun et al., 2019; Liu et al., 2019; Clark et al., 2020; Yang et al., 2020; Raffel et al., 2020; Gao et al., 2021; Schick and Schütze, 2021; au2 et al., 2021; Xue et al., 2021; He et al., 2021; Taori et al., 2023).

Following the success of fixed prompt techniques, other works proposed prompt tuning setups (Shin et al., 2020; Schick et al., 2020; Li and Liang 2021; Hambardzumyan et al., 2021; Lester et al., 2021; Zhong et al., 2021b; Wallace et al., 2021; Haviv et al., 2021; Jiang et al., 2020; Chen et al., 2022; Qin and Eisner, 2021; Liu et al., 2021; Han et al., 2021; Zhong et al., 2021a; Lu et al., 2022; Ben-David et al., 2022; Wang et al., 2022a; Zhou et al., 2023b). These prompt tuning setups were typically used in the context of compute efficiency: training a smaller number of prompt-related parameters to input into a larger frozen language model. These setups are an orthogonal improvement to our proposed input reformulations.

Previous approaches also investigated dataset improvements for better downstream task performance. These approaches gathered additional data for model training to augment the model's input context (Chung et al., 2022; Wei et al., 2023; Wang et al., 2023a; Iyer et al., 2023; Min et al., 2022; Wei et al., 2022; Wang et al., 2022b; Gu et al., 2023; Wang et al., 2023b; Zhang et al., 2022b; Press et al., 2023; Zhou et al., 2023a). They require large, specific, and high quality datasets to be collected. On the other hand, our input reformulations require no additional data.

Overall, our approach differs from previously explored approaches by avoiding prompts and leveraging the target output as a part of the input reformulation. Our input reformulations are a data-level change that can be easily applied to any training setup.
=====END FILE=====

=====FILE: sections/03_experiments_tib2eng.tex=====
\section{Experiments on a difficult single language pair translation task}

\subsection{Setup}
We perform experiments on a Classical Tibetan to English (tib2eng) dataset. Critically, Classical Tibetan is not found in mT5's pretraining dataset, while English is. As a result, the tib2eng dataset is challenging for mT5. Additionally, mT5's tokenizer was not trained on Tibetan. We use mT5's current tokenizer and use the byte-level fallback capabilities of the underlying SentencePiece tokenizer to encode unknown tokens (Xue et al., 2021). We use the BLEU metric (Papineni et al., 2002) for evaluation.

The dataset consists of 450k train, 5k validation, and 5k test translation pairs. The tokenized Tibetan inputs are mean 72 and median 51 tokens long; we use a maximum sequence length of 256. We train for 10k steps and a batch size of 512 translation pairs (about 35k tokens per batch, about 350M tokens total), equivalent to 11 epochs. We use the AdamW (Loshchilov and Hutter, 2019) optimizer with parameters   and weight decay 0. We use a constant learning rate schedule with no warmup. The models converge successfully under this data compute budget. We ablate over learning rates in {1e-3, 2e-3, 3e-3} for 600M and 1B parameter models (the default finetuning learning rate for mT5 is 1e-3 (Xue et al., 2021)) and {3e-4, 5e-4, 1e-3} for 3B parameter models, where we found lower learning rates to be empirically better.

We perform evaluation on the models and save checkpoints every 200 steps, for a total of 50 evaluations, and we use the highest scoring checkpoint for all results. Models were trained on GPU nodes of either 8 NVIDIA A5000 24GB GPUs or 8 NVIDIA A6000 48GB GPUs. The typical train time varied from 8 hours for the smallest models to 80 hours for the largest. We leverage the Deepspeed library \url{[https://www.deepspeed.ai/](https://www.deepspeed.ai/)} for training in the half precision bf16, as well as for effective multi-GPU training.

In all the following results tables, we report the highest test set BLEU scores and standard deviation (std) values over learning rates.

\subsection{Motivation}
We begin by training baseline mT5 models on the tib2eng dataset. The resulting training curves are shown in Figure \ref{fig:training_curves} (referenced as Figure 3 in text) with the blue colored curves. Clearly, mT5 struggles in the first 2000 steps or 20% of the training steps. With the intuition of reducing task difficulty, we design an easier task reformulation to apply only in the first 20% of training.

First, we select a prefix from the target English translation. The length of this prefix is uniformly randomly chosen over the full length of the English translation. Then, we append this English prefix to the Classical Tibetan translation input. Intuitively, we `scaffold'' the Classical Tibetan input with a partial English translation. We use a partial prefix of the English translation so the model doesn't degenerate into simply outputting all the English in the input. We name this reformulation `Partial Output Scaffold English'' or POSE. An example of POSE is found in Figure 2. The next 4 subsections cover ablations over the finetuning reformulation setup. For direct results on the POSE task, which ended up being the most successful, see section 3.7.

\begin{figure}[ht]
\centering
\fbox{IMAGE NOT PROVIDED}
\caption{POSE reformulation applied to the tib2eng translation task. Changes are highlighted in red.}
\label{fig:pose_example}
\end{figure}

\subsection{Modulating task difficulty}
The POSE reformulation is easier than the baseline task. In order to modulate task difficulty, we ablate over different amounts of training examples that use this reformulation: 0% (baseline), 20%, 50%, and 100% (all reformulated).

Results are found in Table \ref{tab:difficulty}. The best condition involves reformulating the first 20% of training examples, achieving 24.6 BLEU, 1.3 BLEU higher than the baseline. We hypothesize that making the task too easy e.g. 50% or 100% reformulated makes the task less informative, which hurts downstream performance. All of the reformulated runs have low variance across the learning rates, suggesting that models are better conditioned while training on easier tasks.

\begin{table}[ht]
\centering
\begin{tabular}{llrr}
\toprule
\textbf{Difficulty} & \textbf{% reform} & \textbf{BLEU} & \textbf{Std} \
\midrule
Least difficult & 100% & 21.1 & 0.29 \
& 50% & 23.9 & 0.05 \
& 20% & 24.6 & 0.26 \
Most difficult & 0% & 23.5 & 1.64 \
\bottomrule
\end{tabular}
\caption{Task difficulty experiment results on mT5 600M.}
\label{tab:difficulty}
\end{table}

\subsection{Optimizing the curriculum}
We attempt to optimize the curriculum using human intuition in 3 setups.
\begin{itemize}
\item (Curriculum 1): Instead of reformulating only the first 20% of training examples (i.e. all examples in the first 2000 steps), we rigidly add 100% of the output to the input at the beginning of training, and linearly scale down to 0% added at the end of training.
\item (Curriculum 2): Instead of reformulating 100% of training examples in the first 2000 steps, we reformulate 80% of the inputs for the first 2000 steps, linearly scale down from 80% reformulated to 40% reformulated for the next 4000 steps, and reformulate no examples for the last 4000 steps.
\item (Curriculum 3): Instead of using uniformly random length prefixes for the first 20% of training examples, we rigidly add 100% of the output to the input and linearly scale down to 0% at the end of 2000 steps.
\end{itemize}

Results are found in Table \ref{tab:curriculum}. Even though these setups have merit using human intuition, mT5 performs markedly worse on all of them in either performance, stability, or both. The best performing runs perform better than POSE, but at the cost of stability.

\begin{table}[ht]
\centering
\begin{tabular}{lrr}
\toprule
\textbf{Setup} & \textbf{BLEU} & \textbf{Std} \
\midrule
Baseline & 23.5 & 1.64 \
POSE & 24.6 & 0.26 \
(Curriculum 1) & 17.4 & 0.85 \
(Curriculum 2) & 24.9 & 0.74 \
(Curriculum 3) & 24.7 & 2.50 \
\bottomrule
\end{tabular}
\caption{Curriculum experiment results on mT5 600M.}
\label{tab:curriculum}
\end{table}

\subsection{Modulating scaffold substring}
Rather than using just a prefix of the target English output, we experiment with setups that append both a portion of the target English prefix and a portion of the target English suffix (``prefix+suffix'' reformulation). The total selected length remains the same for the prefix+suffix experiments. The prefix+suffix input reformulation is still in natural language.

Results are shown in Table \ref{tab:substring}. We find that using just the prefix is optimal.

\begin{table}[ht]
\centering
\begin{tabular}{llrr}
\toprule
\textbf{Substring} & \textbf{% reform} & \textbf{BLEU} & \textbf{Std} \
\midrule
Baseline & 0% & 23.5 & 1.64 \
Prefix & 20% & 24.6 & 0.26 \
Prefix+suffix & 12% & 24.8 & 0.55 \
& 20% & 24.5 & 0.90 \
& 40% & 24.0 & 0.12 \
\bottomrule
\end{tabular}
\caption{Prefix+suffix experiment results on mT5 600M.}
\label{tab:substring}
\end{table}

\subsection{Matching the pretraining task}
We hypothesize that matching the pretraining task smooths performance similar to the POSE reformulation. We experiment on 4 masking setups: (Mask 1) mask in the first 20% of finetuning steps with ; (Mask 2) mask in the last 20% of finetuning steps with ; (Mask 3) mask in the last 50% of finetuning steps with ; and (Mask 4) span-mask in the last 50% of finetuning steps with .

Results are found in Table \ref{tab:masks}. Masking setups have less variance compared to the baseline or previous best setup, most likely because they are closer to the pretraining task distribution. Setup (Mask 1) performs better than the POSE reformulation with slightly higher variance. However, we retain the POSE reformulation as the best because it is simpler than setup (Mask 1). The other masking setups (Mask 2), (Mask 3), followed by 20%. We chose to stick with the 20% experiment due to the lower variance.

\begin{table}[ht]
\centering
\begin{tabular}{lrr}
\toprule
\textbf{Setup} & \textbf{BLEU} & \textbf{Std} \
\midrule
Baseline & 23.5 & 1.64 \
POSE & 24.6 & 0.26 \
(Mask 1) & 24.9 & 0.35 \
(Mask 2) & 23.6 & 0.20 \
(Mask 3) & 23.0 & 0.15 \
(Mask 4) & 23.4 & 0.04 \
\bottomrule
\end{tabular}
\caption{Masking experiment results on mT5 600M. (Table Caption Inferred from Context, likely Table 4)}
\label{tab:masks}
\end{table}

\subsection{Main results}
Table \ref{tab:main_tib2eng} shows the main results on the tib2eng translation task for mT5. Values shown are test set BLEU scores. The difference shown is the improvement gained by using the input finetuning reformulations. The NLLB column is the test set BLEU score for the corresponding sized NLLB model.

\begin{table}[ht]
\centering
\begin{tabular}{lcccc}
\toprule
\textbf{Params} & \textbf{NLLB} & \textbf{Baseline} & \textbf{POSE} & \textbf{Diff} \
\midrule
600M & 29.3 & 23.5 & 24.6 & +1.1 \
1B & 32.3 & 27.2 & 28.3 & +1.1 \
3B & 34.4 & 27.3 & 30.1 & +2.8 \
\bottomrule
\end{tabular}
\caption{Main results on the tib2eng translation task for mT5. Values shown are test set BLEU scores.}
\label{tab:main_tib2eng}
\end{table}
=====END FILE=====

=====FILE: sections/04_experiments_flores.tex=====
\section{Experiments on Massively Multilingual Translation}

mT5 has strong English performance because it was pretrained on orders of magnitude more English data than other languages. So, we look to leverage this strong capability in an input reformulation.

The Flores200 benchmark consists of parallel examples of the same sentence in different languages. We extend the tib2eng POSE reformulation to the ``Parallel Scaffold in English'' or ParSE reformulation. ParSE appends a full parallel English translation to the input translation. For the ParSE setup, we provide the intuition that English is used as a pivot language between the two other languages.

We explore the efficacy of parallel scaffolding without using English using the ``Mixed-language Parallel Scaffold'' or MiPS reformulation. MiPS appends a different parallel translation to both the input and output for a total of 4 distinct language translations per input. For simplicity, we use any combination of languages in Flores200, regardless if they're in or out of mT5's pretraining distribution. Examples of the ParSE and MiPS reformulations are shown in Figures 1 and 4.

\begin{table}[ht]
\centering
\begin{tabular}{lcccc}
\toprule
\textbf{Params} & \textbf{NLLB} & \textbf{Baseline} & \textbf{ParSE} & \textbf{MiPS} \
\midrule
600M & 39.5 & 17.6 & 20.7 & 19.2 \
1B & 41.5 & 20.3 & 23.8 & 21.6 \
3B & 41.8 & 23.2 & 25.1 & 23.6 \
\bottomrule
\end{tabular}
\caption{Results on Flores200. (Table caption inferred from context)}
\label{tab:flores_results}
\end{table}

The performance improvement using ParSE when translating from English into other languages is much more pronounced. This can be seen visually in Figure 6 for the rightmost datapoint in each plot in the top row. The corresponding numbers in Table 7 for 3B models shows the increase for from-English is . This makes intuitive sense since the model has seen significantly more English in the input during finetuning. We break down the performance of different model sizes and reformulation setups in Table 7.

\begin{figure}[ht]
\centering
\fbox{IMAGE NOT PROVIDED}
\caption{Pretrain dataset size vs chrf++ score.}
\label{fig:flores_graphs}
\end{figure}
=====END FILE=====

=====FILE: sections/05_conclusion.tex=====
\section{Conclusion}

In this paper, we propose simple and effective input reformulations for translation tasks. We show that these reformulations improve performance on both single language pair and massively multilingual translation tasks. We hope our research accessibly improves finetuning data efficiency, enabling more effective training to scalably improve state-of-the-art performance.
=====END FILE=====

=====FILE: sections/06_limitations_ethics.tex=====
\section{Limitations}
Our work focuses on translation tasks. While we believe the intuition of leveraging pretraining strengths through input reformulation extends to other tasks, we have not experimentally verified this. Additionally, our experiments are conducted on mT5, a specific FLM. While mT5 is a representative model, results may vary on other architectures.

\section{Ethics Statement}
We use public datasets and models. We do not foresee any specific ethical concerns with our proposed method beyond general concerns with large language models.

\section*{Acknowledgements}
We thank the reviewers for their helpful feedback.
=====END FILE=====

=====FILE: sections/07_appendix.tex=====
\section{Appendix}
\label{sec:appendix}

\subsection{Flores200 in- and out- pretrain results}
Table \ref{tab:flores_breakdown} shows the breakdown of model and setup performance over different splits of the Flores200 dataset. `In'' refers to a language that was found in the mT5 pretraining dataset and `out'' refers to a language that was not. `To Eng'' and `From Eng'' is refererd to as xx-eng and eng-xx in some other papers, respectively. Notably, the proposed techniques improve `To Eng'' performance up to 4.2 chrF++ and `From Eng'' performance up to 9.4 chrF++, in the 600M case. We hypothesize this difference in improvement is due to the finetuning task including more English examples in the input, helping with downstream English translations as well as other language translations.

\begin{table*}[t]
\centering
\begin{tabular}{llcccccc}
\toprule
\textbf{Params} & \textbf{S} & \textbf{In-in} & \textbf{Out-in} & \textbf{In-out} & \textbf{Out-out} & \textbf{To Eng} & \textbf{From Eng} \
\midrule
600M & Baseline & 20.5 & 24.5 & 22.6 & 28.3 & 30.9 & 21.8 \
& ParSE & 23.2 & 27.6 & 25.4 & 31.5 & 35.1 & 31.2 \
& MiPS & 22.3 & 26.2 & 23.9 & 29.8 & 32.5 & 24.5 \
\midrule
1B & Baseline & 24.2 & 28.1 & 25.8 & 31.9 & 34.8 & 25.6 \
& ParSE & 26.9 & 31.5 & 28.8 & 35.1 & 38.6 & 33.5 \
& MiPS & 25.5 & 29.3 & 26.9 & 33.1 & 36.3 & 27.2 \
\midrule
3B & Baseline & 27.3 & 31.1 & 28.7 & 34.6 & 37.9 & 28.9 \
& ParSE & 28.8 & 32.9 & 30.1 & 36.2 & 39.4 & 35.2 \
& MiPS & 28.2 & 31.9 & 29.4 & 35.4 & 38.7 & 30.1 \
\bottomrule
\end{tabular}
\caption{Breakdown of model and setup performance over different splits of the Flores200 dataset.}
\label{tab:flores_breakdown}
\end{table*}
=====END FILE=====