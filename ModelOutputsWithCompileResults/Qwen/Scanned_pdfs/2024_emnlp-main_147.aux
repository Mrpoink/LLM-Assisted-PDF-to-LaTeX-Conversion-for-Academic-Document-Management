\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{Touvron2023}
\citation{OpenAI2023}
\citation{Cobbe2021}
\citation{Saparov2023}
\citation{Schlegel2022b}
\citation{Madusanka2023}
\citation{AlKhamissi2022}
\citation{He2023}
\citation{Brown2020}
\citation{Vaswani2017}
\citation{Devlin2019}
\citation{Gururangan2018}
\citation{Schlegel2022a}
\citation{Dua2019}
\citation{Huang2023b}
\citation{Deng2024}
\citation{Chiang2024}
\citation{Perez2023}
\citation{Yang2018a}
\citation{Welbl2018}
\citation{Inoue2020}
\citation{Lewis2020}
\citation{Kintsch1988}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{3}{section.1}\protected@file@percent }
\citation{Min2019a}
\citation{Bowman2022}
\citation{Sakarvadia2023}
\citation{Liu2023}
\citation{Yang2024}
\citation{Schlegel2020}
\citation{Min2019a}
\citation{Yang2018b}
\citation{Min2019a}
\citation{Trivedi2020}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{4}{section.2}\protected@file@percent }
\citation{Min2019b}
\citation{Perez2020}
\citation{Ding2021}
\citation{Tang2021}
\citation{Gardner2020}
\citation{Sun2023}
\citation{Li2024}
\citation{Huang2023a}
\citation{Chomsky1965}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Example of a decomposed multi-hop question.}}{6}{figure.1}\protected@file@percent }
\newlabel{fig:decomp}{{1}{6}{Example of a decomposed multi-hop question}{figure.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Methodology}{6}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}I. Acquiring the main entity}{6}{subsection.3.1}\protected@file@percent }
\citation{Schlegel2021}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}II. Extracting the details}{7}{subsection.3.2}\protected@file@percent }
\citation{Liu2019}
\citation{Reimers2019}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Instantiation of our proposed method. With ``arena'' as main entity of sub-question 1, we extract ``home'' to be replaced with ``playoff''. Then, we use the modified sequence with the original sub-question 2 (masking the answer ``Androscoggin Bank Colisee'') as prompt to GPT-4 to generate the distractor paragraphs 1 and 2. The distractor paragraphs generated have ``Maple Leaf Arena'' as the bridging entity in the false reasoning chain which leads to the wrong answer ``15,000''.}}{8}{figure.2}\protected@file@percent }
\newlabel{fig:example}{{2}{8}{Instantiation of our proposed method. With ``arena'' as main entity of sub-question 1, we extract ``home'' to be replaced with ``playoff''. Then, we use the modified sequence with the original sub-question 2 (masking the answer ``Androscoggin Bank Colisee'') as prompt to GPT-4 to generate the distractor paragraphs 1 and 2. The distractor paragraphs generated have ``Maple Leaf Arena'' as the bridging entity in the false reasoning chain which leads to the wrong answer ``15,000''}{figure.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}III. Creating the distractor paragraphs}{8}{subsection.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Data Quality}{9}{subsection.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Experiment Setup}{9}{section.4}\protected@file@percent }
\citation{Touvron2023}
\citation{Yang2018b}
\citation{Jiang2019}
\citation{Trivedi2020}
\citation{Chiang2024}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Do LLMs suffer from the same flaws as fine-tuned models?}{10}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Do LLMs get distracted by seemingly plausible alternate reasoning paths?}{10}{subsection.4.2}\protected@file@percent }
\citation{Tang2021}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Comparing normal and chain-of-thought prompts using Llama-2-13B as baseline}}{11}{table.1}\protected@file@percent }
\newlabel{tab:baseline}{{1}{11}{Comparing normal and chain-of-thought prompts using Llama-2-13B as baseline}{table.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}What are the effects of the different parameters?}{11}{subsection.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Experiment Results}{11}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Do LLMs suffer from the same flaws as fine-tuned models?}{11}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.1}I. Setting up the baseline}{11}{subsubsection.5.1.1}\protected@file@percent }
\citation{Tang2021}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Results of Llama-2-13B on SubQA dataset}}{12}{table.2}\protected@file@percent }
\newlabel{tab:subqa}{{2}{12}{Results of Llama-2-13B on SubQA dataset}{table.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Breakdown of the results on running SubQA}}{12}{table.3}\protected@file@percent }
\newlabel{tab:subqa_breakdown}{{3}{12}{Breakdown of the results on running SubQA}{table.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.2}II. Reasoning shortcuts using SubQA}{12}{subsubsection.5.1.2}\protected@file@percent }
\citation{Jiang2019}
\citation{Zellers2018}
\citation{Zellers2019}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Llama-2-13B performance on DiRe when using a normal (non-CoT) prompt and priming with few-shot examples.}}{13}{table.4}\protected@file@percent }
\newlabel{tab:dire}{{4}{13}{Llama-2-13B performance on DiRe when using a normal (non-CoT) prompt and priming with few-shot examples}{table.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces F1 score of Llama-2-13B, Llama-2-70B and Mixtral-8x7B-Instruct-v0.1 when attacked with 2000 examples of AddDoc in the few-shot setting.}}{13}{table.5}\protected@file@percent }
\newlabel{tab:adddoc}{{5}{13}{F1 score of Llama-2-13B, Llama-2-70B and Mixtral-8x7B-Instruct-v0.1 when attacked with 2000 examples of AddDoc in the few-shot setting}{table.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.3}III. Reasoning shortcuts in DiRe}{13}{subsubsection.5.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.4}IV. Reasoning failures when presented with distracting paragraphs from AddDoc}{13}{subsubsection.5.1.4}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Results of Llama-2-13B, Mixtral-8x7B-Instruct-v0.1, Llama-2-70B, GPT-3.5 and longformer (fine-tuned on the training set) on the original HotpotQA dev set (ori) and our adversarially constructed examples (adv). All the tests for the LLMs are done in the few-shot chain of thought prompt setting. EM and F1 Performance Scores are reported. F1 scores are further broken down by (left to right): the number of ``fake'' paragraphs; whether ``fake'' paragraphs are related; the type of entity modified, if adversarial paragraphs are unrelated, and if both the adversarial paragraphs are generated from the second sub-question of two different fake sub-question pair.}}{14}{table.6}\protected@file@percent }
\newlabel{tab:main_results}{{6}{14}{Results of Llama-2-13B, Mixtral-8x7B-Instruct-v0.1, Llama-2-70B, GPT-3.5 and longformer (fine-tuned on the training set) on the original HotpotQA dev set (ori) and our adversarially constructed examples (adv). All the tests for the LLMs are done in the few-shot chain of thought prompt setting. EM and F1 Performance Scores are reported. F1 scores are further broken down by (left to right): the number of ``fake'' paragraphs; whether ``fake'' paragraphs are related; the type of entity modified, if adversarial paragraphs are unrelated, and if both the adversarial paragraphs are generated from the second sub-question of two different fake sub-question pair}{table.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Do LLMs get distracted when faced with seemingly plausible alternatives?}{14}{subsection.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Analysing the effects of different parameters}{14}{subsection.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.1}Count of distractor paragraphs}{14}{subsubsection.5.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.2}Are the paragraphs related?}{15}{subsubsection.5.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.3}Modified type}{15}{subsubsection.5.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.4}Are the paragraphs unrelated and only belong to the 2nd subquestion?}{15}{subsubsection.5.3.4}\protected@file@percent }
\citation{Jiang2019}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{16}{section.6}\protected@file@percent }
\bibstyle{acl_natbib}
\bibcite{AlKhamissi2022}{AlKhamissi et~al.(2022)}
\bibcite{Bowman2022}{Bowman(2022)}
\bibcite{Brown2020}{Brown et~al.(2020)}
\bibcite{Chiang2024}{Chiang and Lee(2024)}
\bibcite{Chiang2024b}{Chiang et~al.(2024)}
\bibcite{Chomsky1965}{Chomsky(1965)}
\bibcite{Cobbe2021}{Cobbe et~al.(2021)}
\bibcite{DeMarneffe2014}{De~Marneffe et~al.(2014)}
\bibcite{Deng2024}{Deng et~al.(2024)}
\bibcite{Devlin2019}{Devlin et~al.(2019)}
\bibcite{Ding2021}{Ding et~al.(2021)}
\bibcite{Dua2019}{Dua et~al.(2019)}
\bibcite{Gardner2020}{Gardner et~al.(2020)}
\bibcite{Gururangan2018}{Gururangan et~al.(2018)}
\bibcite{He2023}{He et~al.(2023)}
\bibcite{Huang2023a}{Huang et~al.(2023a)}
\bibcite{Huang2023b}{Huang et~al.(2023b)}
\bibcite{Inoue2020}{Inoue et~al.(2020)}
\bibcite{Jiang2019}{Jiang and Bansal(2019)}
\bibcite{Kintsch1988}{Kintsch(1988)}
\bibcite{Lewis2020}{Lewis et~al.(2020)}
\bibcite{Li2024}{Li et~al.(2024)}
\bibcite{Liu2019}{Liu et~al.(2019)}
\bibcite{Liu2023}{Liu et~al.(2023)}
\bibcite{Madusanka2023}{Madusanka et~al.(2023)}
\bibcite{Min2019a}{Min et~al.(2019a)}
\bibcite{Min2019b}{Min et~al.(2019b)}
\bibcite{OpenAI2023}{OpenAI(2023)}
\bibcite{Perez2020}{Perez et~al.(2020)}
\bibcite{Perez2023}{Perez et~al.(2023)}
\bibcite{Qi2020}{Qi et~al.(2020)}
\bibcite{Reimers2019}{Reimers and Gurevych(2019)}
\bibcite{Sakarvadia2023}{Sakarvadia et~al.(2023)}
\bibcite{Saparov2023}{Saparov et~al.(2023)}
\bibcite{Schlegel2020}{Schlegel et~al.(2020)}
\bibcite{Schlegel2021}{Schlegel et~al.(2021)}
\bibcite{Schlegel2022a}{Schlegel et~al.(2022a)}
\bibcite{Schlegel2022b}{Schlegel et~al.(2022b)}
\bibcite{Shi2023}{Shi et~al.(2023)}
\bibcite{Sun2023}{Sun et~al.(2023)}
\bibcite{Tang2021}{Tang et~al.(2021)}
\bibcite{Touvron2023}{Touvron et~al.(2023)}
\bibcite{Trivedi2020}{Trivedi et~al.(2020)}
\bibcite{Vaswani2017}{Vaswani et~al.(2017)}
\bibcite{Wang2023}{Wang et~al.(2023)}
\bibcite{Wei2023}{Wei et~al.(2023)}
\bibcite{Welbl2018}{Welbl et~al.(2018)}
\bibcite{Yang2024}{Yang et~al.(2024)}
\bibcite{Yang2018a}{Yang et~al.(2018a)}
\bibcite{Yang2018b}{Yang et~al.(2018b)}
\bibcite{Zellers2018}{Zellers et~al.(2018)}
\bibcite{Zellers2019}{Zellers et~al.(2019)}
\@writefile{toc}{\contentsline {section}{\numberline {A}System Prompt for Q/A task}{24}{appendix.A}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {B}System Prompt for Creating Fake Paragraphs}{24}{appendix.B}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {C}Dependency Type Definitions}{25}{appendix.C}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {D}Reproducibility}{25}{appendix.D}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {E}User Study to Verify Adversarial Paragraphs}{25}{appendix.E}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Definitions based on Universal Dependencies}}{26}{table.7}\protected@file@percent }
\newlabel{tab:deps}{{7}{26}{Definitions based on Universal Dependencies}{table.7}{}}
\citation{Shi2023}
\citation{Wang2023}
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces The three different metrics for accuracy}}{27}{table.8}\protected@file@percent }
\newlabel{tab:userstudy}{{8}{27}{The three different metrics for accuracy}{table.8}{}}
\@writefile{lot}{\contentsline {table}{\numberline {9}{\ignorespaces The confidence level of a question being contradictory}}{27}{table.9}\protected@file@percent }
\newlabel{tab:contradictions}{{9}{27}{The confidence level of a question being contradictory}{table.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {F}Do Existing Techniques Make Models More Robust?}{27}{appendix.F}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {10}{\ignorespaces Effect of self-consistency on F1 score}}{28}{table.10}\protected@file@percent }
\newlabel{tab:robustness}{{10}{28}{Effect of self-consistency on F1 score}{table.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {G}Performance of SOTA LLM}{28}{appendix.G}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {11}{\ignorespaces F1 scores of GPT-4 for 2 and 4 fake paragraphs}}{28}{table.11}\protected@file@percent }
\newlabel{tab:gpt4}{{11}{28}{F1 scores of GPT-4 for 2 and 4 fake paragraphs}{table.11}{}}
\gdef \@abspage@last{28}
