\relax 
\citation{devlin2019bert}
\citation{muennighoff2022crosslingual}
\citation{alves2024tower}
\citation{liu2020multilingual}
\citation{xue2021mt5}
\citation{gorman2019need}
\citation{wu2019beto}
\citation{wang2019cross}
\citation{cao2020multilingual}
\citation{wu2020explicit}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{}\protected@file@percent }
\citation{vaswani2017attention}
\citation{ott2019fairseq}
\citation{lewis2020bart}
\citation{devlin2019bert}
\citation{radford2019language}
\citation{conneau2019cross}
\citation{ziemski2016united}
\citation{tiedemann2012parallel}
\citation{sennrich2016neural}
\citation{kingma2017adam}
\citation{hou2024bridging}
\citation{smetanin2019sentiment}
\citation{elsahar2015building}
\citation{fetahu2023multiconer}
\citation{malmasi2022multiconer}
\citation{mohit2012recall}
\@writefile{toc}{\contentsline {section}{\numberline {2}Methods and Settings}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Models and objectives}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Pretraining conditions}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Downstream evaluation}{3}{}\protected@file@percent }
\citation{nivre2020universal}
\citation{conneau2018xnli}
\citation{williams2018broad}
\citation{loshchilov2017decoupled}
\@writefile{toc}{\contentsline {section}{\numberline {3}Results}{4}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Double-stack models}{4}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Accuracy ($\times 100$) of double-stack models ($\pm $ s.d. over 5 runs) -- Probing}}{4}{}\protected@file@percent }
\newlabel{tab:double_stack_probing}{{1}{4}{}{table.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Accuracy ($\times 100$) of double-stack models ($\pm $ s.d. over 5 runs) -- Fine-tuning}}{4}{}\protected@file@percent }
\newlabel{tab:double_stack_finetuning}{{2}{4}{}{table.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Single-stack models}{4}{}\protected@file@percent }
\citation{conneau2019cross}
\citation{liu2020multilingual}
\citation{xue2021mt5}
\citation{kale2021mt5}
\citation{fang2021filter}
\citation{chi2021mt6}
\citation{alves2024tower}
\citation{ustin2024aya}
\citation{conneau2020emerging}
\citation{siddhant2020evaluating}
\citation{choudhury2021linguistically}
\citation{fierro2022factual}
\citation{hrimmerl2023exploring}
\citation{conneau2019cross}
\citation{ji2024can}
\citation{rust2021how}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Accuracy ($\times 100$) of single-stack models ($\pm $ s.d. over 5 runs) -- Probing}}{5}{}\protected@file@percent }
\newlabel{tab:single_stack_probing}{{3}{5}{}{table.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Accuracy ($\times 100$) of single-stack models ($\pm $ s.d. over 5 runs) -- Fine-tuning}}{5}{}\protected@file@percent }
\newlabel{tab:single_stack_finetuning}{{4}{5}{}{table.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Discussion}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Related works}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{5}{}\protected@file@percent }
\bibstyle{acl_natbib}
\bibcite{allset2023chinese}{AllSet Learning2023}
\@writefile{toc}{\contentsline {section}{\numberline {A}Overview of pretraining objectives}{6}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {B}Datasets statistics}{6}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {C}Detailed results}{6}{}\protected@file@percent }
\bibcite{alves2024tower}{Alves et al.2024}
\bibcite{cao2020multilingual}{Cao et al.2020}
\bibcite{chi2021mt6}{Chi et al.2021}
\bibcite{choudhury2021linguistically}{Choudhury and Deshpande2021}
\bibcite{conneau2019cross}{Conneau and Lample2019}
\bibcite{conneau2018xnli}{Conneau et al.2018}
\bibcite{conneau2020emerging}{Conneau et al.2020}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Overview of the different objectives considered in this study. Top two rows: two-stacks (encoder-decoder) models; bottom three rows: single-stack (encoder-only or decoder-only) models.}}{7}{}\protected@file@percent }
\newlabel{tab:objectives}{{5}{7}{}{table.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Number of sentences in pretraining corpora.}}{7}{}\protected@file@percent }
\newlabel{tab:datastats}{{6}{7}{}{table.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Statistics of datasets used for downstream evaluation tasks.}}{7}{}\protected@file@percent }
\newlabel{tab:downstream_datasets}{{7}{7}{}{table.7}{}}
\bibcite{devlin2019bert}{Devlin et al.2019}
\bibcite{elsahar2015building}{ElSahar and El-Beltagy2015}
\bibcite{fang2021filter}{Fang et al.2021}
\bibcite{fetahu2023multiconer}{Fetahu et al.2023}
\bibcite{fierro2022factual}{Fierro and S{\o }gaard2022}
\bibcite{gorman2019need}{Gorman and Bedrick2019}
\bibcite{guillaume2019conversion}{Guillaume et al.2019}
\bibcite{hrimmerl2023exploring}{Hrimmerl et al.2023}
\bibcite{hou2024bridging}{Hou et al.2024}
\bibcite{ji2024can}{Ji et al.2024}
\bibcite{kale2021mt5}{Kale et al.2021}
\bibcite{kingma2017adam}{Kingma and Ba2017}
\bibcite{lewis2020bart}{Lewis et al.2020}
\bibcite{liu2020multilingual}{Liu et al.2020}
\bibcite{loshchilov2017decoupled}{Loshchilov and Hutter2017}
\bibcite{lyashevskaya2018ud}{Lyashevskaya et al.2018}
\bibcite{malmasi2022multiconer}{Malmasi et al.2022}
\bibcite{mcdonald2013universal}{McDonald et al.2013}
\bibcite{mohit2012recall}{Mohit et al.2012a}
\bibcite{muennighoff2022crosslingual}{Muennighoff et al.2022}
\bibcite{nivre2020universal}{Nivre et al.2020}
\bibcite{ott2019fairseq}{Ott et al.2019}
\bibcite{radford2019language}{Radford et al.2019}
\bibcite{rust2021how}{Rust et al.2021}
\bibcite{sennrich2016neural}{Sennrich et al.2016}
\bibcite{siddhant2020evaluating}{Siddhant et al.2020}
\bibcite{smetanin2019sentiment}{Smetanin and Komarov2019}
\bibcite{tiedemann2012parallel}{Tiedemann2012}
\bibcite{vaswani2017attention}{Vaswani et al.2017}
\bibcite{wang2019cross}{Wang et al.2019}
\bibcite{williams2018broad}{Williams et al.2018}
\bibcite{wong2011quantitative}{Wong et al.2011}
\bibcite{wu2019beto}{Wu and Dredze2019}
\bibcite{wu2020explicit}{Wu and Dredze2020}
\bibcite{xue2021mt5}{Xue et al.2021}
\bibcite{zeldes2017gum}{Zeldes2017}
\bibcite{zeman2023udchinese}{Zeman et al.2023}
\bibcite{ziemski2016united}{Ziemski et al.2016}
\bibcite{ustin2024aya}{Usti\"{u}n et al.2024}
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces Macro F1 score using probing technique.}}{11}{}\protected@file@percent }
\newlabel{tab:probing_f1}{{8}{11}{}{table.8}{}}
\@writefile{lot}{\contentsline {table}{\numberline {9}{\ignorespaces Macro F1 score after model fine-tuning.}}{11}{}\protected@file@percent }
\newlabel{tab:finetuning_f1}{{9}{11}{}{table.9}{}}
\gdef \@abspage@last{11}
