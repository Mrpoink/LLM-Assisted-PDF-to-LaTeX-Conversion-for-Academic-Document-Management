=====FILE: main.tex=====
\documentclass[11pt,letterpaper]{article}
\usepackage{times}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{array}
\usepackage{amsmath}
\usepackage{url}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{multirow}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{geometry}
\geometry{margin=1in}

\title{Factuality of Large Language Models: A Survey}

\author{
Yuxia Wang$^{1}$, Minghan Wang$^{2}$, Muhammad Arslan Manzoor$^{1}$, Fei Liu$^{3}$, \\
Georgi Georgiev$^{1}$, Rocktim Jyoti Das$^{1}$, Preslav Nakov$^{1}$ \\
$^{1}$MBZUAI \quad $^{2}$Monash University \quad $^{3}$Google \quad $^{4}$Sofia University \\
\texttt{\{yuxia.wang, preslav.nakov\}@mbzuai.ac.ae}
}

\begin{document}

\maketitle

\begin{abstract}
Large language models (LLMs), especially when instruction-tuned for chat, have become part of our daily lives, freeing people from the process of searching, extracting, and integrating information from multiple sources by offering a straightforward answer to a variety of questions in a single place. Unfortunately, in many cases, LLM responses are factually incorrect, which limits their applicability in real-world scenarios. As a result, research on evaluating and improving the factuality of LLMs has attracted a lot of attention recently. In this survey, we critically analyze existing work with the aim to identify the major challenges and their associated causes, pointing out to potential solutions for improving the factuality of LLMs, and analyzing the obstacles to automated factuality evaluation for open-ended text generation. We further offer an outlook on where future research should go.
\end{abstract}

\section{Introduction}
Large language models (LLMs) have become an integral part of our daily lives. When instruction-tuned for chat, they have enabled digital assistants that can free people from the need to search, extract, and integrate information from multiple sources by offering straightforward answers in a single chat. While people naturally expect LLMs to always present reliable information that is consistent with real-world knowledge, LLMs tend to fabricate ungrounded statements, resulting in misinformation \cite{Tonmoy2024}, which limits their utility. Thus, assessing and improving the factuality of the text generated by LLMs has become an emerging and crucial research area, aiming to identify potential errors and to advance the development of more reliable LLMs \cite{Chen2023}.

To this end, researchers have collected multiple datasets, introduced a variety of measures to evaluate the factuality of LLMs, and proposed numerous strategies leveraging external knowledge through retrieval, self-reflection, and early refinement in model generation to mitigate factual errors \cite{Tonmoy2024}. Numerous surveys \cite{Tonmoy2024,Huang2023a,Wang2023b} have explored factuality or hallucinations in large language models across various modalities. While they either lack in-depth discussion or are too specific to grasp the fundamental challenges, promising solutions in factuality evaluation and enhancement, and some ambiguous concepts in LLM factuality. We summarized these surveys in Table~\ref{tab:survey_comparison}.

Our survey aims to bridge this gap by providing an in-depth analysis of LLM factuality, with an emphasis on recent studies to reflect the rapidly evolving nature of the field. We offer a comprehensive overview of different categorizations, evaluation methods, and mitigation techniques for LLM factuality in both language and vision modalities. Additionally, we explore a novel research avenue that seeks to improve LLM calibration. This includes making models aware of their knowledge limitations and enhancing the reliability of their output confidence.

\section{Background}
Hallucination and factuality, while conceptually distinct, often occur in similar contexts and are sometimes used interchangeably, rendering them intricately intertwined, posing a challenge in discerning their distinct boundaries, and causing a considerable amount of misconception. In this section, we seek to disambiguate and refine our understanding of these two closely aligned concepts, thereby preventing misinterpretation and reducing potential confusion. Additionally, we further include two closely-related axes: relevance and trustworthiness for LLM evaluation to illustrate their nuance in relation to factuality.

\subsection{Hallucination vs. Factuality}
The concept of hallucination in the context of traditional natural language generation tasks is typically referred to as the phenomenon in which the generated content appears nonsensical or unfaithful to the provided source content \cite{Ji2023}. One concrete example is made-up information in an abstractive summary with additional insights beyond the scope of the original source document.

In the age of LLMs, the term hallucination has been reimagined, encompassing any deviation from factual reality or the inclusion of fabricated elements within generated texts \cite{Tonmoy2024,Rawte2023b}. \cite{Zhang2023c} define hallucination as the characteristic of LLMs to generate content that diverges from the user input, contradicts previously generated context, or mis-aligns with established world knowledge. \cite{Huang2023b} merge the input- and context-conflicting types of hallucinations and further take logical inconsistency into account to form \textit{faithfulness hallucination}. Another category is \textit{factuality hallucination}, referring to the discrepancy between generated content and verifiable real-world facts, manifesting as (1) factual inconsistency and (2) factual fabrication.

Factuality, on the other hand, is concerned with a model's ability to learn, acquire, and utilize factual knowledge. \cite{Wang2023b} characterize factuality issues as the probability of LLMs producing content inconsistent with established facts. It is important to note that hallucination content may not always involve factual missteps. Though a piece of generated text may exhibit divergence from the initial prompt's specifics, it falls into hallucinations, not necessarily a factual issue if the content is accurate.

It is crucial to distinguish between factual errors and instances of hallucination. The former involves inaccurate information whereas the latter can present unanticipated and yet factually substantiated content \cite{Wang2023b}.

\textbf{Summary:} Factuality is the ability of LLMs to generate content consistent with factual information and world knowledge. Although both hallucinations and factuality may impact the credibility of LLMs in the context of content generation, they present distinct challenges. Hallucinations occur when LLMs produce baseless or untruthful content, not grounded in the given source. In contrast, factuality errors arise when the model fails to accurately learn and utilize factual knowledge. It is possible for a model to be factually correct yet still produce hallucinations by generating content that is either off-topic or more detailed than what is requested.

\subsection{Trustworthiness/Reliability vs. Factuality}
In the context of LLMs, factuality \cite{Wang2023b} refers to a model's capability of generating contents of factual information, grounded in reliable sources (e.g., dictionaries, Wikipedia or textbooks), with commonsense, world and domain-specific knowledge taken into account. In contrast, \textit{trustworthiness} \cite{Sun2024} extends beyond mere factual accuracy and is measured on eight dimensions: truthfulness, safety, fairness, robustness, privacy, ethics, transparency, and accountability.

\begin{table*}[t]
\centering
\caption{Comparison of different surveys on the factuality of LLMs. Eval: Evaluation; Improve: Improvement.}
\label{tab:survey_comparison}
\begin{tabular}{lccccccccc}
\toprule
Survey & Date & Pages & Eval & Improve & Multimodal & Contributions and limitations & Our work \\
\midrule
\cite{Tonmoy2024} & 15-June-2024 & 49 & \checkmark & \checkmark & & Discusses ambiguous concepts in LLM factuality, compares and analyzes evaluation and enhancement approaches from academic and practical perspectives, outlining major challenges and promising avenues to explore. & \checkmark \\
\cite{Gao2023b} & 08-Jan-2024 & 44 & \checkmark & \checkmark & & Summarizes recent work in terms of mitigating LLM hallucinations, but lacks comparison between different approaches and discussions to identify open questions and challenges. & \checkmark \\
\cite{Huang2023b} & 18-Dec-2023 & 11 & \checkmark & & & Summarizes three RAG paradigms: naive, advanced, and modular RAG, with key elements and evaluation methods for the three major components in RAG (retriever, generator, and augmentation). & \checkmark \\
\cite{Wang2023b} & 09-Nov-2023 & 32 & \checkmark & \checkmark & & Analyzes the reasons for hallucinations, and presents a comprehensive overview of hallucination detection methods, benchmarks, and approaches to mitigate hallucinations. & \checkmark \\
\cite{Rawte2023b} & 18-Oct-2023 & 29 & \checkmark & \checkmark & & Detailed literature review of factuality improvement and enhancement methods covering both retrieval augmentation and non-retrieval augmentation, missing discussion of major bottleneck issues in LLM factuality and promising directions to investigate. & \checkmark \\
\cite{Zhang2023c} & 18-Sept-2023 & 26 & \checkmark & \checkmark & \checkmark & Extensively elucidates the problem of hallucination across all major modalities of foundation models, including text (general, multilingual, domain-specific LLMs), image, video, and audio. However, inadequate coverage of approaches, in-depth categorization and comparison between methods. & \checkmark \\
\cite{Guo2022} & 03-Sept-2023 & 19 & \checkmark & & & Organized by different training stages of LLMs, discusses potential sources of LLM hallucinations and in-depth review of recent work on addressing the problem. & \checkmark \\
\cite{Min2023} & Feb-2022 & 4 & \checkmark & & & Focused on the automated fact-checking pipeline & \checkmark \\
\bottomrule
\end{tabular}
\end{table*}

\section{Evaluating Factuality}
Evaluating LLM factuality on open-ended generations presents a non-trivial challenge, discerning the degree to which a generated textual statement aligns with objective reality. Studies employ various benchmarks, evaluation strategies and metrics to achieve this goal.

\subsection{Datasets and Metrics}
Perhaps the most demanding, yet inherently realistic scenario is free-form long text generation, such as biography generation. For this, the most commonly used and reliable methods rely on human experts following specific guidelines, and automatic fact-checkers based on retrieved information, such as FactScore, Factool and Factcheck-GPT, to facilitate efficient and consistent evaluation.

These automatic fact-checkers generally first decompose a document into a set of atomic claims, and then verify one by one whether the claim is true or false based on the retrieved evidence, either from offline Wikipedia or online Web pages. The percentage of true claims over all statements in a document is used to reflect the factual status of a response (refer to FactScore). The averaged FactScore over a dataset is in turn used to assess a model's factuality accuracy. However, there is no guarantee that automatic fact-checkers are 100\% accurate in their verification process. \cite{Wang2023c} show that even the state-of-the-art verifier equipped with GPT-4 and supporting evidence retrieved with Google search, can only achieve an F1 score of 0.63 in identifying false claims and F1=0.53 using PerplexityAI (compared with human-annotated labels for claims: true or false).

\begin{table}[t]
\centering
\caption{Four types of datasets used to evaluate LLM factuality. I: open-ended generation; II: Yes/No answer; III: short-term or list of entities answer; IV: A, B, C, D multiple Choice QA. Labeled datasets under type I are mostly generated by ChatGPT, and FactScore-Bio (ChatGPT, InstGPT and PerplexityAI). ER: Human-annotated Error Rate. Freq: usage frequency as evaluation set in our first 50 references.}
\label{tab:dataset_types}
\begin{tabular}{llrrll}
\toprule
Type & Dataset & Topic & Size & ER\% & Evaluation and Metrics \\
\midrule
I & FactScore-Bio \cite{Min2023} & Biography & 549 & 42.6 & Human annotation and automated fact-checkers \\
  & Factcheck-GPT \cite{Wang2023c} & Open-ended questions & 94 & 64.9 & Human annotation \\
  & Factool-QA \cite{Chen2023} & Knowledge-based QA & 50 & 54.0 & Human annotation and automated fact-checkers \\
  & HaluEval \cite{Li2023a} & Open-ended questions & 184 & 46.2 & Human annotation, AUROC + LLM judge + PARENT \\
  & FreshQA \cite{Yin2023} & Open-ended questions & 5000 & -- & Human annotation \\
  & SelfAware \cite{Yin2023b} & Open-ended questions & 499 & -- & Evaluate the LLM awareness of unknown by F1-score \\
\midrule
II & Snowball \cite{Zhang2023b} & Yes/No question & 1500 & 9.1 & Exact match + Accuracy/F1-score \\
\midrule
III & Wiki-category & Multispan QA & 55 & -- & Precision/recall@5 \\
  & List (Mexican films) \cite{Dhuliawala2023} & List of entities & 428 & -- & Exact match + F1 score \\
\midrule
IV & TruthfulQA \cite{Lin2022} & False belief or misconception & 817 & 5 & Accuracy \\
  & HotpotQA \cite{Yang2018} & Multi-step reasoning & 113k & 11 & Exact match + F1 score \\
  & StrategyQA \cite{Geva2021} & Multi-step reasoning & 2780 & 3 & Recall@10 \\
  & MMLU \cite{Hendrycks2021} & Knowledge & 15700 & 4 & Accuracy \\
\bottomrule
\end{tabular}
\end{table}

Summary: We categorize datasets that evaluate LLM factuality into four types, depending on the answer space and the difficulty degree on which accurate automatic quantification can be performed (see Table~\ref{tab:dataset_types}). They are: (I) open-domain, free-form, long-term responses (FactScore: the percentage of the correct claims verified by human or automated fact-checker); (II) Yes/No answer w/wo explanation (extract Yes/No, metrics for binary classification); (III) short-form answer (Exact match the answer with gold labels and calculate accuracy) or the listing answer (recall@K); and (IV) multi-choice QA (metrics for multi-class classification).

\subsection{Other Metrics}
In addition to evaluating the methods discussed above, \cite{Lee2022} quantified the hallucinations using two metrics, both requiring document-level groundtruth: (1) hallucinated named entities error measures the percentage of named entities in the generations that do not appear in the ground-truth document; (2) entailment ratio evaluates the number of generations that can be entailed by the ground-truth reference, over all generations.

\cite{Rawte2023a} defined the hallucination vulnerability index (HVI), which takes a spectrum of factors into account, to evaluate and rank LLMs.

Some factuality measurement tasks, such as claim extraction and evidence retrieval are non-trivial to automate. \cite{Rawte2023a} curated publicly available LLM hallucination mitigation benchmark, where LLM generations are scored by humans when automated external knowledge retrieval fails to resolve a claim clearly. While widely used for factuality evaluation, this hybrid approach may suffer from human annotation bias.

\section{Improving Factuality}
Improving the factuality of an LLM often requires updating its internal knowledge, editing fake, outdated and biased elements, thereby making its output reflect a revised collection of facts, maximizing the probability of $P(\text{truth}|\text{prompt})$. One option is to adopt gradient-based methods to update model parameters to encourage desired model output. This includes pre-training, supervised fine-tuning and RLHF. We can also explore injecting a new fact into LLMs or overwriting the false knowledge stored in LLM memory by in-context learning (ICL). When models store factually correct knowledge but produce errors, they can in some cases rectify them through self-reasoning, reflection, and multi-agent debates.

We discuss these methods throughout the lifecycle of an LLM, ranging from pre-training, to inference, to post-processing. Another important element is retrieval augmentation, which enhances the generation capabilities of LLMs by anchoring them in external knowledge that may not be stored or contradict the information in LLM parametric memory. It can be incorporated at various stages throughout model training and the subsequent inference process \cite{Gao2023b}, and is therefore not discussed individually.

\subsection{Pre-training}
LLMs store a vast amount of world knowledge in their parameters through the process of pre-training. The quality of the pre-training data plays a crucial role and misinformation could potentially cause LLMs to generate false responses, motivating the utilization of high-quality textual corpora. However, the prohibitively massive amount of pre-training data, typically consisting of trillions of tokens, renders manual filtering and editing impractically laborious. To this end, automated filtering methods have been proposed. For instance, \cite{Brown2020} introduce a method to only focus on a small portion of the CommonCrawl dataset that exhibits similarity to high-quality reference corpora. \cite{Touvron2023} propose to enhance factual robustness of mixed corpora by up-sampling documents from the most reliable sources, thereby amplifying knowledge accuracy and mitigating hallucinations. During the pre-training phase of phi-1.5, \cite{Li2023b} synthesize ``textbook-like'' data, consisting of and rich in high-quality commonsense reasoning and world knowledge. While careful corpus curation remains the cornerstone of pre-training for enhanced factuality, the task becomes increasingly challenging with the expansion of dataset scale and the growing demand for linguistic diversity. It is therefore crucial to develop novel strategies that guarantee the consistency of factual knowledge across diverse cultural landscapes.

\cite{Borgeaud2021} propose RETRO, a retrieval augmented pre-training approach. An auto-regressive LLM is trained from scratch with a retrieval module that is practically scalable to large-scale pre-training by retrieving billions of tokens. RETRO shows better accuracy and is less prone to hallucinate compared to GPT \cite{Wang2023a}. While limitations lie in that RETRO performance could be compromised if the retrieval database contains inaccurate, biased or outdated information. Additional computation is required for the pre-training of LLMs with retrieval.

\subsection{Fine-tuning and RLHF}
Continued domain-specific SFT has shown to be effective for enhancing factuality, particularly in the absence of such knowledge during pre-training. For instance, \cite{Elaraby2023} enhance the factual accuracy of LLMs through knowledge injection (KI). Knowledge, in the form of entity summaries or entity triplets, is incorporated through SFT by either intermediate tuning, i.e. first on knowledge and then on instruction data; or combined tuning, i.e. on the mixture of both. While some improvements are exhibited, the method alone can be insufficient to fully mitigate factual errors.

For general-purpose LLMs, SFT is typically employed to improve the instruction-following capabilities as opposed to factual knowledge which is mostly learned in pre-training. However, this process may inadvertently reveal areas of knowledge not covered in the pre-training, causing the risk of behavior cloning, where a model feigns understanding and responds with hallucinations to questions it has little knowledge of \cite{Torabi2018}. R-tuning \cite{Zhang2023a} is proposed to address this issue with two pivotal steps: first, assessing the knowledge gap between the model's parametric knowledge and the instruction tuning data, and second, creating a refusal-aware dataset for SFT. It enables LLMs to abstain from answering queries beyond their parametric knowledge scope. On the other hand, Belief \cite{Razumovskaia2023} improve factual alignment through the form of behavioral fine-tuning. The creation of the behavioral tuning dataset emphasizes two goals: selectivity (choosing correct information from the knowledge source) and response adequacy (informing the user when no relevant information is available or asking for clarification). Both methods effectively control LLMs on non-parametric questions but require extra effort in dataset curation and might hinder the models' retention of parametric knowledge.

Sycophancy \cite{Sharma2023}, another source of factuality errors, often arises from mis-alignments during SFT and RLHF \cite{Ouyang2022}. This is partially attributed to human annotators' tendency to award higher scores to responses they like rather than those that are factually accurate. \cite{Wei2023} explore the correlation of sycophancy with model scaling and instruction tuning. They propose a synthetic-data intervention method, using various NLP tasks to teach models that truthfulness is independent of user opinions. However, one limitation is that the generalizability of their approach remains unclear for varied prompt formats and diverse user opinions.

\cite{Tian2023} utilize direct preference optimization (DPO) \cite{Rafailov2023} with the feedback of factuality score either from automatic fact-checkers or LLMs predictive confidence. In-domain evaluation shows promising results on biographies and medical queries, but generalization performance across domains and unseen domains is under-explored. \cite{Kdksal2023} propose hallucination-augmented recitations (HAR). It encourages the model to attribute to the contexts rather than its parametric knowledge, by tuning the model on the counterfactual dataset created leveraging LLM hallucinations. This approach offers a novel way to enhance LLM attribution and grounding in open-book QA. However, challenges lie in refining counterfactual generation for consistency and expanding its application to broader contexts.

Retrieval Augmentation Incorporating retrieval mechanisms during fine-tuning has been shown to enhance the LLM factuality on downstream tasks, particularly in open-domain QA. DPR \cite{Karpukhin2020} refines a dual-encoder framework, consisting of two BERT models. It employs a contrastive loss to align the hidden representations of questions and their corresponding answers, obtained through the respective encoder models. RAG \cite{Lewis2020} and FiD \cite{Izacard2020} study a fine-tuning recipe for retrieval-augmented generation models, focusing on open-domain QA tasks. WebGPT \cite{Nakano2021} fine-tunes GPT-3 \cite{Brown2020} by RLHF, providing questions with factually correct long-form reference generation. The implementation in a text-based web-browsing environment allows the model to search and navigate the web.

\subsection{Inference}
We categorize approaches to improve factuality during inference into two: (1) optimizing decoding strategies to strengthen model factuality; and (2) empowering LLM learned ability by either in-context learning (ICL) or self-reasoning.

\subsubsection{Decoding Strategy}
Sampling from the top subword candidates with a cumulative probability of $p$, known as nucleus sampling (top-$p$) \cite{Holtzman2020}, sees a decrease in factuality performance compared to greedy decoding, despite higher diversity. This is likely due to its over-encouragement of randomness. Building on the hypothesis that sampling randomness may damage factuality when generating the latter part of a sentence than the beginning, \cite{Lee2022} introduce factual-nucleus sampling, which dynamically reduces the nucleus-$p$ value as generation progresses to limit diversity and improve factuality, modulating factual integrity and textual diversity.

Apart from randomness, some errors arise when knowledge conflicts, where context contradicts information present in the model's prior knowledge. Context-aware decoding (CAD) \cite{Shi2023} prioritizes current context over prior knowledge, employs contrastive ensemble logits, adjusting the weight of the probability distribution when predicting the next token with or without context. Despite the factuality boost, CAD is a better fit for tasks involving knowledge conflicts and heavily reliant on high-quality context.

In contrast, DoLa \cite{Chuang2023} takes into account both upper and lower (earlier) layers, as opposed to only the final (mature) layer. This method dynamically selects intermediate layers at each decoding step, in which an appropriate premature layer contains less factual information with maximum divergence among the subset of the early layers. This method effectively harnesses the distinct contributions of each layer to factual generations. However, DoLa increases the decoding time by 1.01x to 1.08x and does not utilize external knowledge, which limits its ability to correct misinformation learned during training.

\subsubsection{ICL and Self-reasoning}
In-context learning (ICL) allows an LLM to leverage and learn from demonstration examples in its context to perform a particular task without the need to update model parameters. \cite{Zheng2023} present that it is possible to perform knowledge editing via ICL through facts included in demonstration examples, thereby correcting fake or outdated facts. The objective of demonstration examples is to teach LLMs how to: (1) identify and copy an answer; (2) generalize using in-context facts; (3) ignore irrelevant facts in context. While it is rather easy for LLMs to copy answers from contexts, changing predictions of questions related to the new facts accordingly, and keeping the original predictions if the question is irrelevant to the modified facts, remains tough.

Another line of research leverages the self-reasoning capability of LLMs. \cite{Du2023} improve LLM factuality through multi-agent debate. This approach first instantiates a number of agents and then makes them debate over answers returned by other agents until a consensus is reached. One interesting finding is that more agents and longer debates tend to lead to better results. This approach is orthogonal and can be applied in addition to many other generation methods, such as complex prompting strategy (e.g., CoT \cite{Wei2022}, ReAct \cite{Yao2023}, Reflexion \cite{Shinn2023}) and retrieval augmentation.

Take-away: \cite{Zheng2023} evaluate the effectiveness of knowledge editing on subject-relation-object triplets, an unrealistic setting compared to open-ended free-form text assessment. Previous methods \cite{Mitchell2021,Meng2022} use finetuning over texts containing specific text to improve factuality. The relationship between SFT and ICL may also been an interesting avenue to explore. More specifically, we seek answers to two research questions: (1) What types of facts and to what extent can facts be edited effectively, learned by LLMs through ICL? (2) Would SFT do a better job at learning from examples that are difficult for ICL? More broadly, what is the best way to insert new facts or edit false knowledge stored in LLMs. The community may also benefit from an in-depth comparative analysis of the effectiveness of improving factuality between SFT and ICL (perhaps also RLHF).

Retrieval Augmentation can be applied before, during, and after model generation. One commonly used option is to apply retrieval augmentation prior to response generation. For questions requiring up-to-date world knowledge to answer, \cite{Vu2023} augment LLM prompts with web-retrieved information and demonstrate the effectiveness on improving accuracy on FreshQA, where ChatGPT and GPT-4 struggle due to their lack of up-to-date information. \cite{Gao2023a} place all relevant paragraphs in the context and encourage the model to cite supporting evidence, instructing LLMs to understand retrieved documents and generate correct citations, thereby improving reliability and factuality.

Pre-generation retrieval augmentation is beneficial as the generation process is conditioned on the retrieval results, implicitly constraining the output space. While improving factual accuracy, this comes at the cost of spontaneous and creative responses, largely limiting the capabilities of LLMs.

An alternative method is to verify and rectify factual errors after the model generates all content. However, LLMs have been shown to be susceptible to hallucination snowballing \cite{Zhang2023b}, a common issue where a model attempts to make its response consistent with previously generated content even if it is factually incorrect.

Striking a balance between preserving creative elements and avoiding error propagation, EVER \cite{Kang2023} and ``a stitch in time saves nine'' \cite{Varshney2023} actively detect and correct factual errors during generation sentence by sentence. The former leverages retrieved evidence for verification, and the latter incorporates the probability of dominant concepts in detection. Their findings suggest that timely correcting errors during generation can prevent snowballing and further improve factuality. Nonetheless, the primary concern for this iterative process of generate-verify-correct in real-time systems is latency, making it difficult to meet the high-throughput and responsiveness demand \cite{Kang2023}.

\subsection{Automatic Fact Checkers}
An automatic fact-checking framework typically consists of three components: claim processor, retriever, and verifier as shown in Figure~\ref{fig:factchecker}, though the implementation of verification pipelines may differ. For example, FACTOR \cite{Muhlgay2023} and FactScore \cite{Min2023} only detect falsehoods without correction. While RARR depends on web-retrieved information \cite{Gao2022}, and CoVe \cite{Dhuliawala2023} only relies on LLM parametric knowledge \cite{Dhuliawala2023} to perform both detection and correction, albeit at a coarse granularity, editing the entire document. Compared to fine-grained verification over claims, it is unable to spot false spans precisely and tends to result in poor preservation of the original input. Factool \cite{Chen2023} and Factcheck-GPT \cite{Wang2023c} edit atomic claims. While the former breaks a document down to independent checkworthy claims with three steps: decomposition, decontextualization and checkworthiness identification, the latter employs GPT-4 to extract verifiable claims directly.

\begin{figure}[t]
\centering
\fbox{\parbox{0.9\linewidth}{\centering IMAGE NOT PROVIDED\\Fact-checker framework: claim processor, retriever, and verifier, with optional step of summarizing and explaining in gray.}}
\caption{Fact-checker framework: claim processor, retriever, and verifier, with optional step of summarizing and explaining in gray.}
\label{fig:factchecker}
\end{figure}

Evaluating the effectiveness of fact-checkers remains challenging, making the improvement of such systems a difficult task.

\textbf{Engineering and Practical Considerations} Automatic fact-checking involve tasks of extracting atomic check-worthy claims, collecting evidence either by leveraging the knowledge stored in the model parameters or retrieved externally, and verification. While straightforward to implement, this pipeline may be susceptible to error propagation.

Major bottleneck lies in the absence of automatic evaluation measures to assess the quality of intermediate steps, in particular, the claim processor and evidence retriever as there is no gold standard. The input to a claim processor is a document and the expected output is a list of atomic check-worthy claims or atomic verifiable facts. There is no consensus on the granularity of ``atomic claims'', making consistent decomposition difficult. Additionally, the concept of check-worthy and verifiable claims are subjective. Consequently, the definition of an atomic check-worthy claim remains a highly debatable topic. This naturally leads to different ``gold'' human-annotated atomic claims annotated following various guidelines and distinct implementation approaches to decompose a document.

Given a document, even if assuming a ground-truth list of atomic claims, it is an open question how to assess the quality of automatically derived decomposition results. \cite{Wang2023c} assess the agreement in the number of claims between ground truth and predictions, followed by examining the semantic similarity between two claims at the same index when the claim count aligns. Entailment ratio presented in Section 3.2 is also applicable \cite{Lee2022}.

While it is much simpler when the evidence is constrained (e.g., to Wikipedia documents as is the case for FEVER \cite{Thorne2018}), accurate retrieval of evidence from the Internet and subsequently quantifying the quality of such retrieval results remain challenging. Similar to the assessment of atomic claims, gold-labeled evidence is unavailable and infeasible to obtain in the expansive open search space.

The only step where we can confidently evaluate its quality is the accuracy of verification, a simple binary true/false label given a document/claim. In conclusion, perhaps the most significant hurdle for the development and improvement of automatic fact-checkers lies in the automated assessment and quantification of the quality at intermediate stages.

\section{Factuality of Multimodal LLMs}
Factuality or hallucination in Multimodal Large Language Models refers to the phenomenon of generated responses being inconsistent with the image content. Current research on multimodal factuality can be further categorized into three types:

\begin{enumerate}
    \item \textbf{Existence Factuality}: incorrectly claiming the existence of certain objects in the image.
    \item \textbf{Attribute Factuality}: describing the attributes of certain objects in a wrong way, e.g. identifying the colour of a car incorrectly.
    \item \textbf{Relationship Factuality}: false descriptions of relationships between objects, such as relative positions and interactions.
\end{enumerate}

\textbf{Evaluation} CHAIR \cite{Rohrbach2018} is the first benchmark for assessing the accuracy of object existence within captions, focusing on a predefined set of objects in the COCO dataset \cite{Lin2014}. However, this approach can be misleading since the COCO dataset is frequently used in training sets, providing a limited perspective when used as the sole basis for evaluation. In contrast, POPE \cite{Li2023} evaluates object hallucination with multiple binary choice prompts, both positive and negative, querying if a specific object exists in the image. More recently, \cite{Li2023} proposed GPT4-Assisted Visual Instruction Evaluation (GAVIE) to evaluate the visual hallucination. Additionally, \cite{Gunjal2023} demonstrated the use of human evaluation to avoid inaccuracies and systematic biases.

\textbf{Mitigation} The methods for improving factuality in MLLMs can be broadly categorized into the categories: finetuning-based method, inference time correction and representation learning.

Fine-tuning methods such as LRV-Instruction \cite{Liu2023} and LLaVA-RLHF \cite{Sun2023} follow an intuitive and straightforward solution of collecting specialized data such as positive and negative instructions or human preference pairs. This data is used for finetuning the model, thus resulting in models with fewer hallucinated responses. Whereas inference time approaches mitigate factuality by correcting output generation. Woodpecker \cite{Yin2023a} and LURE \cite{Zhou2023} use specialized models to rectify model generation. There are other works such as HallE-Switch \cite{Zhai2023}, VCD \cite{Leng2023}, and HACL \cite{Jiang2023} that analyse and improve feature representation to improve factuality.

\section{Challenges and Future Directions}
We first identify three major challenges for improving the factuality of LLMs, and then we point to several promising directions for future work.

\textbf{Language models learn a language distribution, not facts.} The training objective of language modeling is to maximize the probability of a sentence, as opposed to that of a factual statement. While capable of generating seemingly coherent and fluent outputs upon convergence, models are not guaranteed to always return a factual response.

\textbf{Automatic evaluation of the factual accuracy of open-ended generations remains challenging.} Existing studies on factuality enhancement use different benchmarks and evaluation measures, making fair comparisons difficult, which motivates the need for a unified automated evaluation framework that uses the same collection of datasets and metrics. Current approaches rely on either human evaluation or results of automated fact-checkers such as FactScore and Factool \cite{Min2023,Chen2023}. However, automatically quantifying the quality of automated fact-checkers is itself an open question, resulting in a chicken and egg situation.

\textbf{Latency and multi-hop reasoning could be the bottleneck of RAG systems.} Retrievers serve as the core component in RAG systems, and the effectiveness of RAGs is largely influenced by the quality (coverage and relevance) of the retrieved documents. Latency and difficulties in gathering the most pertinent evidence are the primary challenges in retrieval. While this is partly due to the inability of ranking algorithms to retrieve such documents, certain facts require information gathered from various sources and multi-hop reasoning.

\subsection{Potential Future Directions}
\textbf{Mitigation in inference:} We observe that models can often generate a correct answer in multiple trials even if some attempts are wrong \cite{Tian2023}. This motivates us to ask how to provide an anchor that can guide LLM decoding to the factually correct path? Iteratively detecting, correcting, and generating during generation has been demonstrated to be effective to mitigate hallucinations. If simply correcting the first one or two sentences, how much improvements can we expect for subsequent generations? Can factually correct and relevant sentences, phrases or concepts serve as anchors?

\textbf{Development of better retrieval algorithms:} Integrating Retrieval-Augmented Generation (RAG) into Large Language Models (LLMs) is challenging due to the prevalence of unreliable information, such as fake news, on the internet. This compromises the accuracy of the knowledge retrieved, resulting in LLMs generating responses based on incorrect input. Consequently, future research should focus on improving retrieval techniques to enhance the factuality of LLM-generated responses.

\textbf{Improving the efficiency and the accuracy of automated fact-checkers:} The key breakthrough in effectively evaluating the factual accuracy of LLMs lies in establishing accurate and efficient fact-checkers. This requires improvement of the quality of the evidence used for making veracity decisions. Moreover, many recent methods rely on the factuality of stronger models such as GPT-4 for claim verification. Not only is this computationally expensive, but it also tends to be highly sensitive to minor prompt changes and LLM updates. A small task-specific and well fine-tuned NLI model can be a more viable, robust, and cost-efficient option.

\section{Conclusion}
We presented an overview on the factuality of LLMs, surveying a number of studies covering topics such as evaluation and improvement methods (applicable at various stages: pre-training, SFT, inference and post-processing) along with their respective challenges. We also identified three major issues and pointed out to promising future research directions.

\subsection*{Limitations}
Despite conducting an extensive literature review to encompass all existing research on the factuality of LLMs, some studies may have been omitted due to the rapidly evolving nature of this research area. We endeavored to include all pertinent studies and references wherever feasible. This survey only briefly touches upon the factuality issues associated with vision language models. However, there is room for a more in-depth exploration of mitigation techniques specific to vision-language models. Additionally, comprehensive discussions are also necessary for language models that incorporate other modalities, such as video and speech.

\begin{thebibliography}{99}

\bibitem[Borgeaud et al.2021]{Borgeaud2021}
Sebastian Borgeaud, Arthur Mensch, Jordan Hoffman, et al. 2021. Improving language models by retrieving from trillions of tokens. In \textit{ICML}.

\bibitem[Brown et al.2020]{Brown2020}
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, et al. 2020. Language models are few-shot learners. In \textit{NeurIPS 2020}.

\bibitem[Chen et al.2023]{Chen2023}
Shiqi Chen, Yiran Zhao, Jinghan Zhang, et al. 2023. Factool: Factuality detection in generative AI---A tool augmented framework for multi-task and multi-domain scenarios. \textit{CoRR}, abs/2307.13528.

\bibitem[Chuang et al.2023]{Chuang2023}
Yung-Sung Chuang, Yujia Xie, Hongyin Luo, et al. 2023. DoLa: Decoding by contrasting layers improves factuality in large language models. \textit{CoRR}, abs/2309.03883.

\bibitem[Dhuliawala et al.2023]{Dhuliawala2023}
Shehzaad Dhuliawala, Mojtaba Komeili, et al. 2023. Chain-of-verification reduces hallucination in large language models. \textit{arXiv preprint} arXiv:2309.11495.

\bibitem[Du et al.2023]{Du2023}
Yilun Du, Shuang Li, Antonio Torralba, Joshua B. Tenenbaum, Igor Mordatch. 2023. Improving factuality and reasoning in language models through multiagent debate. \textit{CoRR}, abs/2305.14325.

\bibitem[Elaraby et al.2023]{Elaraby2023}
Mohamed Elaraby, Mengyin Lu, Jacob Dunn, et al. 2023. Halo: Estimation and reduction of hallucinations in open-source weak large language models. \textit{CoRR}, abs/2308.11764.

\bibitem[Gao et al.2022]{Gao2022}
Luyu Gao, Zhuyun Dai, Panupong Pasupat, et al. 2022. Attributed text generation via post-hoc research and revision. \textit{arXiv preprint} arXiv:2210.08726.

\bibitem[Gao et al.2023a]{Gao2023a}
Tianyu Gao, Howard Yen, Jiatong Yu, Danqi Chen. 2023a. Enabling large language models to generate text with citations. In \textit{EMNLP}, pages 6465--6488.

\bibitem[Gao et al.2023b]{Gao2023b}
Yunfan Gao, Yun Xiong, et al. 2023b. Retrieval-augmented generation for large language models: A survey. \textit{CoRR}, abs/2312.10997.

\bibitem[Geva et al.2021]{Geva2021}
Mor Geva, Daniel Khashabi, et al. 2021. Did Aristotle use a laptop? A question answering benchmark with implicit reasoning strategies. \textit{TACL}, 9:346--361.

\bibitem[Gunjal et al.2023]{Gunjal2023}
Anish Gunjal, Jihan Yan, Erhan Bas. 2023. Detecting and preventing hallucinations in large vision language models. In \textit{AAAI Conference on Artificial Intelligence}.

\bibitem[Guo et al.2022]{Guo2022}
Zhijiang Guo, Michael Schlichtkrull, Andreas Vlachos. 2022. A survey on automated fact-checking. \textit{TACL}, 10:178--206.

\bibitem[Hendrycks et al.2021]{Hendrycks2021}
Dan Hendrycks, Collin Burns, et al. 2021. Measuring massive multitask language understanding. In \textit{ICLR 2021}.

\bibitem[Holtzman et al.2020]{Holtzman2020}
Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, Yejin Choi. 2020. The curious case of neural text degeneration. In \textit{ICLR}.

\bibitem[Huang et al.2023a]{Huang2023a}
Lei Huang, Werjiang Yu, Weitao Ma, Wei-hong Zhong, et al. 2023a. A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. \textit{CoRR}, abs/2311.05232.

\bibitem[Huang et al.2023b]{Huang2023b}
Lei Huang, Werjiang Yu, Weitao Ma, Wei-hong Zhong, et al. 2023b. A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. \textit{CoRR}, abs/2311.05232.

\bibitem[Izacard and Grave2020]{Izacard2020}
Gautier Izacard, Edouard Grave. 2020. Leveraging passage retrieval with generative models for open domain question answering. \textit{ArXiv}, abs/2007.01282.

\bibitem[Ji et al.2023]{Ji2023}
Ziwei Ji, Nayeon Lee, Rita Frieske, et al. 2023. Survey of hallucination in natural language generation. \textit{ACM Comput. Surv.}, 55(12):248:1--248:38.

\bibitem[Jiang et al.2023]{Jiang2023}
Chaoya Jiang, Haiyang Xu, Mengfan Dong, Jiaxing Chen, Wei Ye, Mingshi Yan, Qinghao Ye, Ji Zhang, Fei Huang, Shikun Zhang. 2023. Hallucination augmented contrastive learning for multimodal large language model. \textit{ArXiv}, abs/2312.06968.

\bibitem[Kang et al.2023]{Kang2023}
Haoqiang Kang, Juntong Ni, Huaxiu Yao. 2023. EVER: Mitigating hallucination in large language models through real-time verification and rectification. \textit{CoRR}, abs/2311.09114.

\bibitem[Karpukhin et al.2020]{Karpukhin2020}
Vladimir Karpukhin, Barlas Oguz, Sewon Min, et al. 2020. Dense passage retrieval for open-domain question answering. \textit{ArXiv}, abs/2004.04906.

\bibitem[Kdksal et al.2023]{Kdksal2023}
Abdullatif Kdksal, Renat Aksitov, Chung-Ching Chang. 2023. Hallucination augmented recitations for language models. \textit{arXiv preprint} arXiv:2311.07424.

\bibitem[Lee et al.2022]{Lee2022}
Nayeon Lee, Wei Ping, Peng Xu, et al. 2022. Factuality enhanced language models for open-ended text generation. \textit{NeurIPS}, 35:34586--34599.

\bibitem[Leng et al.2023]{Leng2023}
Sicong Leng, Hang Zhang, Guanzheng Chen, Xin Li, Shijian Lu, Chunyan Miao, Li Bing. 2023. Mitigating object hallucinations in large vision-language models through visual contrastive decoding. \textit{ArXiv}, abs/2311.16922.

\bibitem[Lewis et al.2020]{Lewis2020}
Patrick Lewis, Ethan Perez, et al. 2020. Retrieval-augmented generation for knowledge-intensive NLP tasks. \textit{ArXiv}, abs/2005.11401.

\bibitem[Li et al.2023a]{Li2023a}
Junyi Li, Xiaoxue Cheng, et al. 2023a. HaluEval: A large-scale hallucination evaluation benchmark for large language models. \textit{CoRR}, abs/2305.11741.

\bibitem[Li et al.2023]{Li2023}
Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, Ji-Rong Wen. 2023. Evaluating object hallucination in large vision-language models. In \textit{Conference on Empirical Methods in Natural Language Processing}.

\bibitem[Li et al.2023b]{Li2023b}
Yuanzhi Li, Sébastien Bubeck, et al. 2023b. Textbooks are all you need II: phi-1.5 technical report. \textit{CoRR}, abs/2309.05463.

\bibitem[Lin et al.2014]{Lin2014}
Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, C. Lawrence Zitnick. 2014. Microsoft coco: Common objects in context. In \textit{European Conference on Computer Vision}.

\bibitem[Lin et al.2022]{Lin2022}
Stephanie Lin, Jacob Hilton, Owain Evans. 2022. TruthfulQA: Measuring how models mimic human falsehoods. In \textit{ACL}, pages 3214--3252.

\bibitem[Liu et al.2023]{Liu2023}
Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, Lijuan Wang. 2023. Mitigating hallucination in large multi-modal models via robust instruction tuning.

\bibitem[Meng et al.2022]{Meng2022}
Kevin Meng, David Bau, Alex Andonian, Yonatan Belinkov. 2022. Locating and editing factual associations in GPT. In \textit{Neural Information Processing Systems}.

\bibitem[Min et al.2023]{Min2023}
Sewon Min, Kalpesh Krishna, et al. 2023. FactScore: Fine-grained atomic evaluation of factual precision in long form text generation. \textit{CoRR}, abs/2305.14251.

\bibitem[Mitchell et al.2021]{Mitchell2021}
Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, Christopher D. Manning. 2021. Fast model editing at scale. \textit{ArXiv}, abs/2110.11309.

\bibitem[Muhlgay et al.2023]{Muhlgay2023}
Dor Muhlgay, Ori Ram, Inbal Magar, et al. 2023. Generating benchmarks for factuality evaluation of language models. \textit{CoRR}, abs/2307.06908.

\bibitem[Nakano et al.2021]{Nakano2021}
Reiichiro Nakano, Jacob Hilton, et al. 2021. WebGPT: Browser-assisted question-answering with human feedback. \textit{ArXiv}, abs/2112.09332.

\bibitem[Ouyang et al.2022]{Ouyang2022}
Long Ouyang, Jeff Wu, Xu Jiang, et al. 2022. Training language models to follow instructions with human feedback. \textit{ArXiv}, abs/2203.02155.

\bibitem[Rafailov et al.2023]{Rafailov2023}
Rafael Rafailov, Archit Sharma, et al. 2023. Direct preference optimization: Your language model is secretly a reward model. \textit{CoRR}, abs/2305.18290.

\bibitem[Rawte et al.2023a]{Rawte2023a}
Vipula Rawte, Swagata Chakraborty, Agnibha Pathak, et al. 2023a. The troubling emergence of hallucination in large language models---an extensive definition, quantification, and prescriptive remediations. In \textit{EMNLP 2023}, pages 2541--2553.

\bibitem[Rawte et al.2023b]{Rawte2023b}
Vipula Rawte, Amit P. Sheth, Amitava Das. 2023b. A survey of hallucination in large foundation models. \textit{CoRR}, abs/2309.05922.

\bibitem[Razumovskaia et al.2023]{Razumovskaia2023}
Evgeniia Razumovskaia, Ivan Vulic, Pavle Markovic, et al. 2023. Dial beinfo for faithfulness: Improving factuality of information-seeking dialogue via behavioural fine-tuning. \textit{CoRR}, abs/2311.09800.

\bibitem[Rohrbach et al.2018]{Rohrbach2018}
Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, Trevor Darrell, Kate Saenko. 2018. Object hallucination in image captioning. In \textit{Conference on Empirical Methods in Natural Language Processing}.

\bibitem[Sharma et al.2023]{Sharma2023}
Mrinank Sharma, Meg Tong, Tomasz Korbak, David Duvenaud, Amanda Askell, et al. 2023. Towards understanding sycophancy in language models. \textit{CoRR}, abs/2310.13548.

\bibitem[Shi et al.2023]{Shi2023}
Weijia Shi, Xiaochuang Han, et al. 2023. Trusting your evidence: Hallucinate less with context-aware decoding. \textit{arXiv preprint} arXiv:2305.14739.

\bibitem[Shinn et al.2023]{Shinn2023}
Noah Shinn, Federico Cassano, Gopinath et al. 2023. Reflexion: Language agents with verbal reinforcement learning. In \textit{NeurIPS}.

\bibitem[Sun et al.2024]{Sun2024}
Lichao Sun, Yue Huang, Haoran Wang, et al. 2024. TrustLLM: Trustworthiness in large language models. \textit{ArXiv}, abs/2401.05561.

\bibitem[Thorne et al.2018]{Thorne2018}
James Thorne, Andreas Vlachos, et al. 2018. FEVER: a large-scale dataset for fact extraction and VERification. In \textit{NAACL}, pages 809--819.

\bibitem[Tian et al.2023]{Tian2023}
Katherine Tian, Eric Mitchell, et al. 2023. Fine-tuning language models for factuality. \textit{arXiv preprint} arXiv:2311.08401.

\bibitem[Tonmoy et al.2024]{Tonmoy2024}
S. M. Towhidul Islam Tonmoy, S. M. Mehedi Zaman, et al. 2024. A comprehensive survey of hallucination mitigation techniques in large language models. \textit{CoRR}, abs/2401.01313.

\bibitem[Torabi et al.2018]{Torabi2018}
Faraz Torabi, Garrett Warnell, Peter Stone. 2018. Behavioral cloning from observation. In \textit{IJCAI}, pages 4950--4957.

\bibitem[Touvron et al.2023]{Touvron2023}
Hugo Touvron, Louis Martin, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. \textit{CoRR}, abs/2307.09288.

\bibitem[Varshney et al.2023]{Varshney2023}
Neeraj Varshney, Wenlin Yao, et al. 2023. A stitch in time saves nine: Detecting and mitigating hallucinations of LLMs by validating low-confidence generation. \textit{CoRR}, abs/2307.03987.

\bibitem[Vu et al.2023]{Vu2023}
Tu Vu, Mohit Iyyer, et al. 2023. FreshLLMs: Refreshing large language models with search engine augmentation. \textit{arXiv preprint} arXiv:2310.03214.

\bibitem[Wang et al.2023a]{Wang2023a}
Boxin Wang, Wei Ping, et al. 2023a. Shall we pretrain autoregressive language models with retrieval? a comprehensive study. In \textit{EMNLP}.

\bibitem[Wang et al.2023b]{Wang2023b}
Cunxiang Wang, Xiaoze Liu, et al. 2023b. Survey on factuality in large language models: Knowledge, retrieval and domain-specificity. \textit{ArXiv}, abs/2310.07521.

\bibitem[Wang et al.2023c]{Wang2023c}
Yuxia Wang, Revanth Gangi Reddy, et al. 2023c. Factcheck-GPT: End-to-end fine-grained document-level fact-checking and correction of LLM output. \textit{CoRR}, abs/2311.09000.

\bibitem[Wei et al.2022]{Wei2022}
Jason Wei, Xuezhi Wang, Dale Schuurmans, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. In \textit{NeurIPS 2022}.

\bibitem[Wei et al.2023]{Wei2023}
Jerry Wei, Da Huang, Yifeng Lu, et al. 2023. Simple synthetic data reduces sycophancy in large language models. \textit{CoRR}, abs/2308.03958.

\bibitem[Yang et al.2018]{Yang2018}
Zhilin Yang, Peng Qi, et al. 2018. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In \textit{EMNLP 2018}, pages 2369--2380.

\bibitem[Yao et al.2023]{Yao2023}
Shunyu Yao, Jeffrey Zhao, Dian Yu, et al. 2023. React: Synergizing reasoning and acting in language models. In \textit{ICLR}.

\bibitem[Yin et al.2023a]{Yin2023a}
Shukang Yin, Chaoyou Fu, et al. 2023a. Woodpecker: Hallucination correction for multimodal large language models. \textit{CoRR}, abs/2310.16045.

\bibitem[Yin et al.2023b]{Yin2023b}
Zhangyue Yin, Qiushi Sun, Zipeng Guo, et al. 2023b. Do large language models know what they don't know? In \textit{ACL}, pages 8653--8665.

\bibitem[Zhai et al.2023]{Zhai2023}
Bohan Zhai, Stratis Yang, Xiangchen Zhao, Chenfeng Xu, Sheng Shen, Dongdi Zhao, Kurt Keutzer, Manling Li, Tan Yan, Xiangyu Fan. 2023. HallE-switch: Rethinking and controlling object existence hallucinations in large vision language models for detailed caption. \textit{ArXiv}, abs/2310.01719.

\bibitem[Zhang et al.2023a]{Zhang2023a}
Hanning Zhang, Shizhe Diao, et al. 2023a. R-tuning: Teaching large language models to refuse unknown questions. \textit{CoRR}, abs/2311.09671.

\bibitem[Zhang et al.2023b]{Zhang2023b}
Muru Zhang, Ofir Press, et al. 2023b. How language model hallucinations can snowball. \textit{CoRR}, abs/2305.13534.

\bibitem[Zhang et al.2023c]{Zhang2023c}
Yue Zhang, Yafu Li, et al. 2023c. Siren's song in the AI ocean: A survey on hallucination in large language models. \textit{CoRR}, abs/2309.01219.

\bibitem[Zheng et al.2023]{Zheng2023}
Ce Zheng, Lei Li, et al. 2023. Can we edit factual knowledge by in-context learning? In \textit{EMNLP}, pages 4862--4876.

\bibitem[Zhou et al.2023]{Zhou2023}
Yiming Zhou, Chenhang Cui, Jaehong Yoon, Linjun Zhang, Zhun Deng, Chelsea Finn, Mohit Bansal, Huaxiu Yao. 2023. Analyzing and mitigating object hallucination in large vision-language models. \textit{ArXiv}, abs/2310.00754.

\end{thebibliography}

\end{document}
=====END FILE=====