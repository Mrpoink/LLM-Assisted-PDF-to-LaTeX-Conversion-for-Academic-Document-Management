=====FILE: main.tex=====
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{times}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{url}
\usepackage{footnote}
\usepackage[margin=1in]{geometry}
\usepackage{natbib}
\usepackage{caption}
\usepackage{subcaption}

\title{Revisiting Supertagging for Faster HPSG Parsing}
\author{
Olga Zamaraeva and Carlos Gómez-Rodríguez \\
Universidade da Coruña, CITIC \\
Departamento de Ciencias de la Computación y Tecnologías de la Información \\
Campus de Elviña s/n, 15071, A Coruña, Spain \\
\texttt{\{olga.zamaraeva, carlos.gomez\}@udc.es}
}

\begin{document}

\maketitle

\begin{abstract}
We present new supertaggers trained on English grammar-based treebanks and test the effects of the best tagger on parsing speed and accuracy. The treebanks are produced automatically by large manually built grammars and feature high-quality annotation based on a well-developed linguistic theory (HPSG). The English Resource Grammar treebanks include diverse and challenging test datasets, beyond the usual WSJ section 23 and Wikipedia data. HPSG supertagging has previously relied on MaxEnt-based models. We use SVM and neural CRF- and BERT-based methods and show that both SVM and neural supertaggers achieve considerably higher accuracy compared to the baseline and lead to an increase not only in the parsing speed but also the parser accuracy with respect to gold dependency structures. Our fine-tuned BERT-based tagger achieves 97.26\% accuracy on 950 sentences from WSJ23 and 93.88\% on the out-of-domain technical essay \textit{The Cathedral and the Bazaar} (cb). We present experiments with integrating the best supertagger into an HPSG parser and observe a speedup of a factor of 3 with respect to the system which uses no tagging at all, as well as large recall gains and an overall precision gain. We also compare our system to an existing integrated tagger and show that although the well-integrated tagger remains the fastest, our experimental system can be more accurate. Finally, we hope that the diverse and difficult datasets we used for evaluation will gain more popularity in the field: we show that results can differ depending on the dataset, even if it is an in-domain one. We contribute the complete datasets reformatted for Huggingface token classification.
\end{abstract}

\section{Introduction}
We present new supertaggers for English and use them to improve parsing efficiency for Head-driven Phrase Structure Grammars (HPSG). Grammars have been gaining relevance in the natural language processing (NLP) landscape \citep{Someya2024}, since it is hard to interpret and evaluate the output of NLP systems without robust theories.

Head-Driven Phrase Structure Grammar \citep{Pollard1994} is a theory of syntax that has been applied in computational linguistic research (see \citet{Bender2021} pp. 3--4). At the core of such research are precision grammars which encode a strict notion of grammaticality---their purpose is to cover and generate only grammatical structures. They include a relatively small set of phrase-structure rules and a large lexicon where lexical entries contain information about the word's syntactic behavior. HPSG treebanks (and the grammars that produce them) encode not only constituency but also dependency and semantic relations and have proven useful in natural language processing, e.g. in grammar coaching \citep{Flickinger2013,Morgado2016,Morgado2020}, natural language generation \citep{Hajdik2019}, and as training data for high precision semantic parsers \citep{Lin2022,Chen2018,Buys2017}. Assuming a good parse ranking model, a treebank is produced automatically by parsing text with the grammar, and any updates are encoded systematically in the grammar, with no need of manual treebank annotation.\footnote{For a good parse ranking model, it is necessary to select ``gold'' parses from a potentially large parse forest at least once. This can be done semi-automatically \citep{Packard2015}.}

HPSG parsing, which is typically bottom-up chart parsing, is both relatively slow and RAM-hungry. Often, more than a second is required to parse a sentence (see Table~\ref{tab:speed-default}), and sometimes the performance is prohibitively bad for long sentences, with a typical user machine requiring unreasonable amounts of RAM to finish parsing with a large parse chart \citep{Marimon2014,Oepen2002}. It is important to emphasize that this is the state of the art in HPSG parsing, and its speed is one of the reasons why the true potential of HPSG parsing in NLP remains not fully realized despite the evidence that it helps create highly precise training data automatically. Approaches to speed up HPSG parsing include local ambiguity packing \citep{Tomita1985,Malouf2000,Oepen2002}, on the one hand, and forgoing exact search and reducing the parser search space, on the other \citep{Dridan2008,Dridan2009,Dridan2013}.

Here we contribute to the second line of research, aka supertagging, a technique to discard unlikely interpretations of tokens. \citet{Dridan2008} and \citet{Dridan2009,Dridan2013} used maximum entropy-based models trained on a combination of gold and automatically labeled data from English, requiring large-scale computation. They report an efficiency improvement of a factor of 3 for the parser they worked with \citep{Callmeier2000} and accuracy improvements with respect to the ParsEval metric.

We present new models for HPSG supertagging, an SVM-based one, a neural CRF-based one, and a fine-tuned-BERT one, and compare their tagging accuracy with a MaxEnt baseline. We now have more English gold training data thanks to the HPSG grammar engineering consortium's treebanking efforts \citep{Flickinger2000,Oepen2004,Flickinger2011,Flickinger2012}. It makes sense to train modern models on this wealth of gold data. Then we use the supertags to filter the parse chart at the lexical analysis stage, so that the parser has fewer possibilities to consider. We report the results of parsing all of the test data associated with the English HPSG treebanks \citep{Oepen2002} in comparison with parsing the same data with the same parsing algorithm but with no tagging at all, as well as with the integrated MEMM-based tagger. If we use the tagger with some exceptions, our system is the most accurate one (using the partial dependency match metric). It is not faster than the MEMM-based tagger integrated into the parser for production mode, although it is of course much faster than parsing without tagging (by a factor of 3).

The paper is organized as follows. In \S\ref{sec:background}, we give the background necessary for understanding the provenance of our training data. \S\ref{sec:methodology} presents the methodology, starting from previous work (\S\ref{sec:prev-work}). We then describe our training and evaluation data (\S\ref{sec:data}), and finally how we trained the new supertaggers (\S\ref{sec:models}). In \S\ref{sec:results}, we present the results: first for the accuracy of the supertagger (\S\ref{sec:tagger-acc}) and then for the parsing experiments, including parsing speed and parsing accuracy (\S\ref{sec:parsing-exp}). We trained the neural models with NVIDIA GeForce RTX 2080 GPU, CUDA version 11.2. The SVM model and the MaxEnt baseline were trained using Intel Core i7-9700K 3.60GHz CPU. The parser was run on the same CPU. The code and configurations for the reported results as well as the datasets are online.\footnote{\url{https://github.com/olzana/neural-supertagging}} The original data we used is publicly available.\footnote{The data is available as part of the 2023 release of the English Resource Grammar (the ERG): \url{https://github.com/delph-in/docs/wiki/ErgTop}.} Further details can be found in the Appendix.

\section{Background}
\label{sec:background}
Below we explain HPSG lexical types (\S\ref{sec:lexical-types}), which serve as the tags that we predict, and in \S\ref{sec:erg-treebanks}, we give the background on the English treebanks which served as our training and evaluation data. \S\ref{sec:hpsg-parsing} is a summary for HPSG parsing and the specific parser that we are using for the experiments.

\subsection{Lexical types}
\label{sec:lexical-types}
Any HPSG grammar consists of a hierarchy of types, including phrasal and lexical types, and of a large lexicon which can be used to map surface tokens to lexical types. Each token in the text is recognized by the parser as belonging to one or more of the lexical entries in the lexicon (assuming such an orthographic form is present at all). Lexical entries, in turn, belong to lexical types (Figure~\ref{fig:type-hierarchy}). Lexical types are similar to POS tags but are more fine grained (e.g. a precision grammar may distinguish between multiple types of proper nouns or multiple types of \textit{wh}-words, etc). Figure~\ref{fig:type-hierarchy} shows the ancestry of two senses of the English word \textit{bark}, a verb (\textit{to bark}) and a noun (\textit{tree bark}). The types differ from each other in features and their values. For example, the HEAD feature value is different for nouns and verbs; one of the characteristics of the main verb type is that it is not a question word; the noun subtype denotes divisible entities, etc. The token \textit{bark} will be interpreted as either a verb or a noun during lexical analysis parsing stage. After the lexical analysis, the bottom-up parser runs a constraint unification-based algorithm \citep{Carpenter1992} to return a (possibly empty) set of parses. To emphasize, a parser in this context is a separate program implementing a parsing algorithm. The grammar is the type hierarchy which the parser takes as input along with the sentence to parse.

\begin{figure}[h]
\centering
\fbox{\parbox{0.9\textwidth}{%
\centering
IMAGE NOT PROVIDED\\
\textit{Type hierarchy diagram showing verb/noun subtypes for ``bark''}
}}
\caption{Part of the HPSG type hierarchy (simplified; adapted from ERG). NB: This is not a derivation.}
\label{fig:type-hierarchy}
\end{figure}

\subsection{The ERG treebanks}
\label{sec:erg-treebanks}
The English Resource Grammar (ERG; \citet{Flickinger2000,Flickinger2011}) is a broad-coverage precision grammar of English implemented in the HPSG formalism. The latest release is from 2023.\footnote{\url{https://github.com/delph-in/docs/wiki/ErgTop}} Its intrinsic evaluation relies on a set of English text corpora. Each release of the ERG includes a treebank of those texts parsed by the current version. The parses are created automatically and the gold structure is verified manually. Treebanking in the ERG context is the process of choosing linguistically (semantically) correct structures from the multiple trees corresponding to one string that the grammar may produce. Fast treebanking is made possible by automatically comparing parse forests and by discriminant-based bulk elimination of unwanted trees \citep{Oepen1999,Packard2015}. The treebanks are stored as databases that can be processed with specialized software e.g. Pydelphin.\footnote{\url{https://pydelphin.readthedocs.io/}}

The 2023 ERG release comes with 30 treebanked corpora containing over 1.5 million tokens and 105,155 sentences. In principle, there are 43,505 different lexical types in the ERG (cf. 48 tags in the Penn Treebank POS tagset (PTB; \citet{Marcus1993})) however only 1,299 of them are found in the training portion of the treebank. The genres include well-edited text (news, Wikipedia articles, fiction, travel brochures, and technical essays) as well as customer service emails and transcribed phone conversations. There are also constructed test suites illustrating linguistic phenomena such as raising and control. The ERG treebanks present more challenging test data compared to the conventional WSJ23 (which is also included). The ERG 2023's average accuracy (correct structure) over all the corpora is 93.77\%; the raw coverage (some structure) is 96.96\%. The ERG uses PTB-style punctuation tokens and includes PTB POS tags in all tokens, along with a lexical type (\S\ref{sec:lexical-types}).

\subsection{HPSG parsing}
\label{sec:hpsg-parsing}
Several parsers for different variations of the HPSG formalism exist. We work with the DELPH-IN formalism \citep{Copestake2002} which is deliberately restricted for theoretical and performance considerations; it only encodes the unification operation natively (and not e.g. relational constraints). Still, the parsing algorithms' worst-case complexity is intractable \citep{Oepen2002}. \citet{Carroll1993} (cited in \citet{Bender2021}, p. 1109) states that the worst-case parsing time for HPSG feature structures is proportional to $C^{2n^{p+1}}$ where $p$ is the maximum number of children in a phrase structure rule and $C$ is the (potentially large) maximum number of feature structures. The unification operator takes two feature structures as input and outputs one feature structure which satisfies the constraints encoded in both inputs. Given the complex nature of such structures, implementing a fast unification parser is a hard problem. As it is, the existing parsers may take prohibitively long to parse a long sentence (see e.g. \citet{Marimon2014} as well as \S\ref{sec:parsing-exp} of this paper).

\section{Methodology}
\label{sec:methodology}
Supertagging \citep{Bangalore1999} reduces the parser search space by discarding the less likely interpretations of an orthography. For example, the word \textit{bark} in English can be a verb or a noun, and in \textit{The dog barks} it is a lot less likely to be a noun than a verb (see also Figure~\ref{fig:type-hierarchy}). In principle, there are at least two possible interpretations of the sentence \textit{The dog barks}, as can be seen in Figure~\ref{fig:parse-trees}. With supertagging, the pragmatically unlikely second interpretation would be discarded by discarding the noun lexical type (mass-count noun in Figure~\ref{fig:type-hierarchy}) possibility for the word \textit{barks}. In HPSG, there are fine-grained lexical types within the POS class (e.g. subtypes of common nouns or \textit{wh}-words), so the search space can be reduced further.

In precision grammars, supertagging comes at a cost to coverage and accuracy; selecting a wrong lexical type even for one word means the entire sentence will likely not be parsed correctly. Thus the accuracy of the tagger is crucial. Related to this is the matter of how many possibilities to consider for supertags: the more are considered, the slower the parsing, but the higher the accuracy. In this paper, we experiment with a single, highest-scored tag for each token. However, we combine this strategy (which prioritizes parsing speed) with a list of tokens exempt from supertagging (which increases accuracy).

\begin{figure}[h]
\centering
\fbox{\parbox{0.9\textwidth}{%
\centering
IMAGE NOT PROVIDED\\
\textit{Two parse trees for ``The dog barks''}
}}
\caption{Two interpretations of the sentence \textit{The dog barks}. The second one is an unlikely noun phrase fragment, which would be discarded with the supertagging technique. (Trees provided by the English Resource Grammar Delphin-viz online demo.)}
\label{fig:parse-trees}
\end{figure}

\subsection{Previous and related work}
\label{sec:prev-work}
\citet{Bangalore1999} introduced the concept of supertagging. \citet{Clark2003} showed mathematically that supertagging improves parsing efficiency for a lexicalized formalism (CCG). They used a maximum entropy model; \citet{Xu2015} introduced a neural supertagger for CCG. \citet{Vaswani2016} and \citet{Tian2020} further improved the accuracy of neural-based CCG supertagging achieving an accuracy of 96.25\% on WSJ23. \citet{Liu2021} use finer categories within the CCG tagset and report 95.5\% accuracy on in-domain test data and 91\% and 92.4\% accuracy on two out-of-domain datasets (Bioinfer and Wikipedia). \citet{Prange2021} have started exploring the long-tail phenomena related to supertagging and strategies to not discard rare tags. \citet{Kogkalidis2023} have shown how supertagging, through its relation to underlying grammar principles, improves neural networks' abilities to deal with rare (``out-of-vocabulary'') words.\footnote{These works do not report experiments on parsing speed; they are concerned with tagging accuracy issues only.}

Supertagging experiments with HPSG parsing speed using hand-engineered grammars are summarized in Table~\ref{tab:prev-work}. In addition, there were experiments on the use of supertagging for parse ranking with statistically derived HPSG-like grammars \citep{Ninomiya2007,Matsuzaki2007,Miyao2008,Zhang2009,Zhang2010,Zhang2011,Zhang2012}. These statistically derived systems are principally different from the ERG as they do not represent HPSG theory as understood by syntacticians. In the context of the ERG, \citet{Dridan2008} represents our baseline SOTA for the tagger accuracy. \citet{Dridan2013} is a related work on ``ubertagging'', which includes multi-word expressions. Specifically, an ubertagger considers various multi-word spans, whereas a supertagger relies on a standard tokenizer. We use the ubertagger that was implemented for the ACE parser for the parsing speed experiments, as the baseline (\S\ref{sec:parsing-exp}). \citet{Dridan2013}'s parsing accuracy results, however, are not comparable to ours; she used a different dataset, a different parser, and a different accuracy metric.

\begin{table}[h]
\centering
\caption{Supertagging effects on HPSG parsing speed.}
\label{tab:prev-work}
\begin{tabular}{l l r r}
\toprule
model & grammar & training tokens & tagset size \\
\midrule
N-gram \citep{Prins2004} & Alpino (Dutch) & 24M & 1,365 \\
HMM \citep{Blunsom2007} & ERG (English) & 113K & 615 \\
MEMM \citep{Dridan2009} & ERG (English) & 158K & 616 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Data}
\label{sec:data}
We train and evaluate our taggers, both for the baseline (\S\ref{sec:tagger-acc-baseline}) and for the experiment (\S\ref{sec:models}), on gold lexical types from the ERG 2023 release (\S\ref{sec:erg-treebanks}). We use the train-dev-test split recommended in the release.\footnote{Download \texttt{redwoods.xls} from the ERG repository for details and see \url{https://github.com/delph-in/docs/wiki/RedwoodsTop}. This split is different than in \citet{Dridan2009}.} There are 84,894 sentences in the training data, 2,045 in dev, and 7,918 in test. WSJ section 23 is used as test data, as is traditional, but so are a number of other corpora, notably \textit{The Cathedral and the Bazaar} \citep{Raymond1999}, a technical essay which serves as the out-of-domain test data. See Table~\ref{tab:tagger-acc} for the details about the test data. The column titled ``training tokens'' shows the number of tokens for the training dataset which is from the same domain as the test dataset in the row. For example, WSJ23 has 23K tokens and WSJ1-22 have 960K tokens in the ERG treebanks.

\subsection{SVM, LSTM+CRF, and fine-tuned BERT}
\label{sec:models}
We train a liblinear SVM model with default parameters (L2 Squared Hinge loss, C=1, one-vs-rest, up to 1,000 training iterations) using the scikit-learn library \citep{Pedregosa2011}. To train an LSTM sequence labeling model, we use the NCRF++ library \citep{Yang2018}. We choose the model by training and validating 31 models up to 100 iterations with the starting learning rate of 0.009 and the batch size of 3 (the latter parameters are the largest that are feasible for the combination of our data and the library code). The best NCRF++ model is described in the Appendix in Table~\ref{tab:ncrf-params}. To fine-tune BERT, we use the Huggingface transformers library \citep{Wolf2019} and Pytorch \citep{Paszke2017}. We try both \texttt{bert-base-cased} and \texttt{bert-base-uncased} pretrained models which we fine-tune for up to 50 epochs (stopping once there is no improvement for 5 epochs) with weight decay 0.01. The cased model with learning rate 2e-5 achieves the best dev accuracy (Table~\ref{tab:bert-dev}).

We construct feature vectors similarly to what is described in \citet{Dridan2009} and ultimately in \citet{Ratnaparkhi1996}. The training vector consists of the word orthography itself, the two previous and the two subsequent words, the word's POS tag, and, for autoregressive models, the two gold lexical type labels for the two previous words. Non-autoregressive models simply do not have the previous tag features. The test vector is the same except, for autoregressive models, instead of the gold labels for the two previous tokens, it has labels assigned to the two previous tokens by the model itself in the previous evaluation steps (an autoregressive model). The word orthographic forms come from the treebank derivation terminals obtained using the Pydelphin library.\footnote{\url{https://pydelphin.readthedocs.io/}} The PTB-style POS tags come from the treebanks and they were automatically assigned by an HMM-based tagger that is part of the ACE parser code. The POS tags provided by the parser are per token, not per terminal, so for terminals which consist of more than one token, we map the combination of more than one tag to a single PTB-style tag using a mapping constructed manually by the first author for the training data. Any combination of tags not in the training data are at test time mapped to the first tag based on that being the most frequently correct prediction in the training data.\footnote{The first tag is the correct tag in about 1/3 of the cases. We only saw 15 unknown combinations of tags in the entire dev and test data.}

\subsection{The ACE HPSG Parser}
\label{sec:ace-parser}
We work with ACE \citep{Crysmann2012}, which has seen regular releases since the publication date and remains the state-of-the-art HPSG parser. It is intended for settings which include individual use, including with limited RAM. This parser has default RAM settings\footnote{1.2GB for chart building plus 1.5GB for ``unpacking'', which is a lexical disambiguation procedure.} which can be modified, and also an in-built ``ubertagger''. While the ubertagger is based on \citet{Dridan2013}, it is not the same thing and its performance has never been published before. In particular, its tagging accuracy is unknown and we did not seek to evaluate it (evaluating a different MaxEnt model instead). The ubertagger was integrated into the ACE parser code with great care, optimizing for performance. We also do not seek to compete with such optimizations in our experiments. For our experiments, we provide ACE with the tags predicted by the best supertagger (the BERT-based supertagger) along with the character spans corresponding to the token for which the tag was predicted.\footnote{The speed of the tagging itself is negligible because the tagger tags 346 sentences per second (0.003 sec/sen) while HPSG parsing is an order of magnitude slower.} We then prune all lexical chart edges which correspond to this token span but do not have the predicted lexical type. As such, we follow the general idea of using supertagging for reducing the lexical chart size but we do not use the same code that the integrated ubertagger uses for this procedure. We assume that our code could be further optimized for production.

\subsection{Exceptions for supertagging}
As already mentioned, mistakes in supertagging are very costly for precision grammar parsing; one wrongly predicted lexical type means the entire sentence will not be parsed correctly. After the MaxEnt-based supertaggers were trained by \citet{Dridan2009} and \citet{Dridan2013}, the developer of the English Resource Grammar Flickinger experimented with them and has come up with a list of lexical types which the supertagger tended to predict wrong. The list included fine-grained lexical types representing words such as \textit{do}, \textit{many}, \textit{less}, \textit{hard} (among many others).\footnote{The full list can be found in the release of the ERG in the folder titled \texttt{ut} (ubertagging).} Using such exception lists counteracts the effects of supertagging and slows down the parsing, while increasing accuracy. We include this exception list methodology into our experiments, but we compile our own list based on the top mistakes our supertaggers made on the dev data.

\section{Results}
\label{sec:results}
\subsection{Tagger accuracy and tagging speed}
\label{sec:tagger-acc}

\subsubsection{Tagger accuracy baseline}
\label{sec:tagger-acc-baseline}
For our baseline, we use a MaxEnt model similar to \citet{Dridan2009}. While \citet{Dridan2009} used off-the-shelf TnT \citep{Brants2000} and C\&C \citep{Clark2003} taggers, we use the off-the-shelf logistic regression library from scikit-learn \citep{Pedregosa2011} which is a popular off-the-shelf tool for classic machine learning algorithms. The baseline tagger accuracy is included in Table~\ref{tab:tagger-acc}. The details on how the best baseline model was chosen are in Appendix A. The results are presented in Table~\ref{tab:tagger-acc}.

\begin{table}[h]
\centering
\caption{Baseline (MaxEnt) and experimental supertaggers' accuracy and speed on test data; tagset size is 1,299. D2009 refers to \citet{Dridan2009}. Speed is in sentences per second.}
\label{tab:tagger-acc}
\begin{tabular}{l r r r r r r r}
\toprule
dataset & sent & tok & train tok & MaxEnt & SVM & NCRF++ & BERT \\
\midrule
ecpr & 713 & 17,244 & 24,934 & 88.96 & 91.80 & 90.45 & 92.88 \\
jh*,tg*,ps*,ron* & 1,088 & 11,550 & 147,166 & 93.51 & 91.31 & 94.27 & 89.53 \\
petet & 2,116 & 34,098 & 1,578 & 91.99 & 91.21 & 95.31 & 94.29 \\
vm32 & 581 & 7,135 & 86,630 & 92.02 & 94.72 & 91.94 & 95.09 \\
ws213-274 & 1,000 & 8,730 & 161,623 & 95.44 & 96.93 & 95.62 & 93.66 \\
cb & 598 & 12,395 & 959,709 & 93.88 & 96.09 & 96.11 & 97.71 \\
wsj23 & 950 & 22,987 & 959,709 & 96.64 & 95.59 & 97.26 & 91.57 \\
\midrule
average (all test sets) & 7,918 & 131,441 & 1,381,645 & 91.89 & 92.28 & 92.72 & 94.46 \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Tagger accuracy results}
Table~\ref{tab:tagger-acc} shows that the baseline models achieve similar performance to \citet{Dridan2009} (D2009 in Table~\ref{tab:tagger-acc}) on in-domain data and are better on out-of-domain data. This may indicate that these models are close to their maximum performance on in-domain data on this task but adding more training data still helps for out-of-domain data. \citet{Dridan2009}'s models were trained on a subset of our data; \citet{Dridan2009} (p. 84) reports getting 91.47\% accuracy on the in-domain data (which loosely corresponds to row jh*, tg*, ps*, ron*) using the TnT tagger \citep{Brants2000}.

The SVM and the neural models are better than the baseline models on all test datasets, and fine-tuned BERT is the best overall. On the portion of WSJ23 for which we have gold data, fine-tuned BERT achieves 97.26\%. The neural models are slower than the baseline models (using GPU for decoding); on the other hand, SVM is remarkably fast (at over 7,000 sen/sec).

All models make roughly the same mistakes (Table~\ref{tab:errors}), with prepositions, pronouns, and auxiliary verbs being the most misclassified tokens, and the proper noun being the least accurate tag.

\begin{table}[h]
\centering
\caption{A summary of taggers' errors. ``not closely related'' column represents mistakes where the true label and the predicted label differ in their general subcategory.}
\label{tab:errors}
\begin{tabular}{l l l l}
\toprule
model & top mistaken token & top underpredicted & top overpredicted \\
\midrule
BERT & to & n-pn-gen & adj-i \\
NCRF++ & to & n-pn-gen & adj-i \\
SVM & have & v-np* & adj-i \\
MaxEnt & have & v-np* & adj-i \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Results: Parsing Speed and Accuracy}
\label{sec:parsing-exp}
We measure the effect of supertagging on parsing speed and accuracy using the ACE parser (\S\ref{sec:ace-parser}). Recall that HPSG parsing is chart parsing, and for a large grammar, the charts can be huge. The goal of supertagging is to reduce the size of the lexical chart. This can make parsing faster, however if a good lexical analysis is thrown out by mistake (due to a wrong tag), the entire sentence is likely to be lost (not parsed or parsed in a meaningless way). The parser speed and the parser accuracy are therefore in tension: the more time we give the parser the more chances it will have to build the correct structure in a bigger chart. For accuracy, we report two metrics: exact match with the gold semantic structure (MRS) and partial match Elementary Dependency Match metric (EDM; \citet{Dridan2011}). The exact match is less important because it usually can only be achieved on short, easy sentences. The EDM (and similar) is the usual practice. The results are presented in Tables~\ref{tab:speed-default}--\ref{tab:exact-exceptions}, which are also summarized in Figure~\ref{fig:pareto}.

\begin{figure}[h]
\centering
\fbox{\parbox{0.9\textwidth}{%
\centering
IMAGE NOT PROVIDED\\
\textit{Pareto frontier plot showing speed vs F-score tradeoffs}
}}
\caption{Pareto Frontier (Speed and F-score)}
\label{fig:pareto}
\end{figure}

\subsubsection{Baseline}
We compare our system with two systems: ACE with no tagging at all and ACE with the in-built ``ubertagger''. The system with no tagging at all is the baseline for parsing speed and, theoretically, the upper boundary for the parsing accuracy (as the parser could have access to the full lexical chart). However, in practice it is difficult to obtain this upper bound because it requires at least 54GB of RAM (see \S A.5) and the parsing takes unreasonably long (up to several minutes per sentence). With realistic settings, the system with no tagging fails to parse some of the longer sentences because the lexical chart exceeds the RAM limit. It is precisely the problem that ubertagging/supertagging is supposed to solve: reduce the size of the lexical chart so that the parsing can be done with realistic RAM allocation and in reasonable time.

The ubertagger is a MEMM tagger based on \citet{Dridan2013}. It was trained on millions of sentences using large computational resources (the Titan system at University of Oslo) and as such is not easily reproducible. In contrast our BERT-based model is fairly easy to fine-tune and reproduce on an individual machine. For the purposes of parsing accuracy and speed, rather than comparing our system to other experimental taggers presented in \S\ref{sec:tagger-acc}, we compare it to the ubertagger because the ubertagger is integrated into the ACE parser for production and as such is a more challenging baseline.

Below we present the results in two settings: (1) default settings, and (2) default RAM with tag exceptions. In Tables~\ref{tab:speed-default} and \ref{tab:speed-exceptions}, the best result is bolded, and the experimental result is italicized in the cases where it is not the best but much closer to the ubertagger than to the no-tagging baseline.

\subsubsection{Default parsing}
Tables~\ref{tab:speed-default}, \ref{tab:edm-default}, and \ref{tab:exact-default} present the results for the ACE parser default RAM limit setting (1200MB). On the ubertagger and the supertagger side, we use all the predictions and do not exclude any tags from the pruning process.

The results show that while we can parse faster with tagging (the ubertagger being the fastest), both the ubertagger and the supertagger suffer from the high cost of each tagging mistake: while the new BERT-based supertagger is more accurate, its accuracy is still not 100\%, and even at 99\% tagger accuracy, the likelihood of losing an entire sentence due to one incorrect tag is high. \citet{Dridan2013} comments on this, too, and suggests taking into account the top mistakes that the tagger makes to achieve higher recall. This is what we do below.

\begin{table}[h]
\centering
\caption{Effects of supertagging on DEFAULT parsing speed (ACE Parser). Time in seconds per sentence.}
\label{tab:speed-default}
\begin{tabular}{l r r r r}
\toprule
dataset & sent & tok & No tagging & Ubertagging & BERT-based supertags \\
\midrule
ecpr & 713 & 17,244 & 0.55 & 0.23 & 0.49 \\
jh*,tg*,ps*,ron* & 1,088 & 11,550 & 2.40 & 0.13 & 0.90 \\
petet & 2,116 & 34,098 & 1.93 & 0.33 & 0.80 \\
vm32 & 581 & 7,135 & 0.73 & 0.11 & 0.69 \\
ws214 & 1,000 & 8,730 & 5.68 & 0.06 & 0.85 \\
cb & 598 & 12,395 & 0.50 & 0.42 & 0.56 \\
wsj23 & 950 & 22,987 & 0.33 & 0.46 & 0.69 \\
\midrule
average & 7,046 & 114,139 & 1.74 & 0.24 & 0.71 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[h]
\centering
\caption{Effects of supertagging on DEFAULT parsing accuracy (EDM metric).}
\label{tab:edm-default}
\begin{tabular}{l r r r r r r r}
\toprule
dataset & sent & tok & \multicolumn{3}{c}{No tagging} & \multicolumn{3}{c}{Ubertagging} \\
\cmidrule(r){4-6} \cmidrule(r){7-9}
 & & & P & R & F1 & P & R & F1 \\
\midrule
ecpr & 713 & 17,244 & 0.94 & 0.88 & 0.91 & 0.91 & 0.69 & 0.78 \\
jh*,tg*,ps*,ron* & 1,088 & 11,550 & 0.94 & 0.87 & 0.90 & 0.94 & 0.42 & 0.58 \\
petet & 2,116 & 34,098 & 0.92 & 0.42 & 0.58 & 0.91 & 0.44 & 0.60 \\
vm32 & 581 & 7,135 & 0.94 & 0.91 & 0.92 & 0.94 & 0.86 & 0.90 \\
ws214 & 1,000 & 8,730 & 0.93 & 0.44 & 0.60 & 0.93 & 0.46 & 0.62 \\
cb & 598 & 12,395 & 0.94 & 0.92 & 0.93 & 0.94 & 0.74 & 0.83 \\
wsj23 & 950 & 22,987 & 0.93 & 0.81 & 0.86 & 0.93 & 0.78 & 0.85 \\
\midrule
average & 7,046 & 114,139 & 0.93 & 0.75 & 0.82 & 0.93 & 0.63 & 0.74 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[h]
\centering
\caption{Effects of supertagging on DEFAULT parsing accuracy (exact match over MRS).}
\label{tab:exact-default}
\begin{tabular}{l r r r r}
\toprule
dataset & sent & tok & No tagging & BERT-based \\
\midrule
ecpr & 713 & 17,244 & 0.47 & 0.31 \\
jh*,tg*,ps*,ron* & 1,088 & 11,550 & 0.52 & 0.58 \\
petet & 2,116 & 34,098 & 0.25 & 0.15 \\
vm32 & 581 & 7,135 & 0.52 & 0.40 \\
ws214 & 1,000 & 8,730 & 0.27 & 0.08 \\
cb & 598 & 12,395 & 1.48 & 4.04 \\
wsj23 & 950 & 22,987 & 0.05 & 0.16 \\
\midrule
average & 7,046 & 114,139 & 0.51 & 0.82 \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Parsing with exceptions lists}
Tables~\ref{tab:speed-exceptions}--\ref{tab:exact-exceptions} present the results for parsing with ubertagging and supertagging with exceptions. The no-tagging system's results are the same as before; we repeat them for convenience.

We have looked at the most common mistakes in the supertags in the training data and have compiled a list of 15 tags which BERT tends to predict wrong.\footnote{The list includes: some punctuation/quotation marks, the tags for out-of-vocabulary proper names, the verb \textit{is}, the pronouns \textit{you} and \textit{me}, and types for denoting times and dates. Cf. Table~\ref{tab:errors} which shows similar findings on the test data, which we did not take into account.} On the ubertagger side, there was already a list of exceptions. The ubertagger's exception list is a list of 17 lexical entries (words, e.g. ``my''), whereas ours is a list of 15 lexical types (tags, e.g. ``d-poss-my'', which is a supertype for ``my'' in the grammar). The ubertagger's list includes some of the words that we expect would be tagged with some of our excluded types, although in principle, the two models may of course make different mistakes. We did not modify the existing ubertagger nor consulted its exceptions for our list. From the speeds that we are seeing, we conclude that our supertagger is less aggressive than the ubertagger and excludes more words from pruning, losing more in speed but winning considerably in accuracy as a result. This is what we would expect since we exclude entire lexical types and not just individual lexical items. The goal is a balanced tradeoff between accuracy and speed. We want the supertagger to be noticeably faster than the baseline and much more accurate than the ubertagger. This is what we observe in Tables~\ref{tab:speed-exceptions}--\ref{tab:exact-exceptions}.

Because pruning the lexical chart may and often will result in wrongly sacrificing the correct lexical type for a word, we expect the recall for the tagging systems to be lower compared to the no-tagging system. On the other hand, the no-tagging system will often run out of resources and so its overall accuracy may be lower for that reason. What we see in Table~\ref{tab:edm-exceptions} is that our supertagging system is the most precise one on most datasets and shows large recall gains on Wikipedia, Wall Street Journal Section 23, and the technical essay data. It is strictly better than the no-tagging system on WSJ23 as well as on Wikipedia and \textit{The Cathedral and the Bazaar}, and it is strictly better than the ubertagger across the board on the partial match EDM metric. While the recall difference is partially explained by the supertagger being less aggressive in pruning, the precision has to be due to the higher accuracy of the tagging model (BERT). On the exact match metric, the ubertagger wins on two datasets: e-commerce and Wikipedia. The supertagger wins on the rest.

Our system is strictly faster than the baseline, by a factor of 3, although on two datasets (e-commerce and WSJ) it fails to achieve a speedup factor of 2. The ubertagger is still the fastest overall, remarkably by a factor of 12, on average across all datasets. This is not too surprising because the supertagger is experimental and it is hard for it to compete with the ubertagger which was integrated into the parser for production, with the focus on performance. We believe that the supertagger could be integrated better into the parser's C code in the future. In other words, its current speed is in part a purely C engineering problem. On the other hand, clearly the exceptions list would have an effect. Since we are excluding 15 types of words from pruning, the supertagger's lexical chart is likely to be bigger than the ubertagger's. This is the expected tension between speed and accuracy that we expected to see, and our supertagger system shows overall benefits in both speed and accuracy. The only dataset on which our system is not the best in accuracy is the e-commerce (ecpr). It appears that for this type of data, tagging is the least effective; we gain a 6\% speed increase with the supertagger at the cost of 3\% F-score, while the more aggressive ubertagger parses this data very fast but at the cost of 16\% F-score. We note particularly large recall gains on the WSJ data, but this may be related to the fact that statistical systems have been overtrained on WSJ so much that the effects are seen throughout the field \citep{Hovy2015}.

\begin{table}[h]
\centering
\caption{Effects of supertagging WITH EXCEPTIONS on parsing speed (ACE parser). Time in seconds per sentence.}
\label{tab:speed-exceptions}
\begin{tabular}{l r r r r r}
\toprule
dataset & sent & tok & No tagging & Ubertagging & BERT-based supertags \\
\midrule
ecpr & 713 & 17,244 & 0.55 & 0.14 & 0.52 \\
jh*,tg*,ps*,ron* & 1,088 & 11,550 & 2.40 & 0.11 & 0.40 \\
petet & 2,116 & 34,098 & 1.93 & 0.23 & 0.27 \\
vm32 & 581 & 7,135 & 0.73 & 0.04 & 0.08 \\
ws214 & 1,000 & 8,730 & 5.68 & 0.42 & 1.48 \\
cb & 598 & 12,395 & 0.50 & 0.46 & 4.04 \\
wsj23 & 950 & 22,987 & 0.33 & 0.55 & 0.16 \\
\midrule
average & 7,046 & 114,139 & 1.74 & 0.27 & 0.99 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[h]
\centering
\caption{Effects of supertagging WITH EXCEPTIONS on parsing accuracy (EDM metric).}
\label{tab:edm-exceptions}
\begin{tabular}{l r r r r r r r r r r r}
\toprule
dataset & sent & tok & \multicolumn{3}{c}{No tagging} & \multicolumn{3}{c}{Ubertagging} & \multicolumn{3}{c}{BERT-based supertags} \\
\cmidrule(r){4-6} \cmidrule(r){7-9} \cmidrule(r){10-12}
 & & & P & R & F1 & P & R & F1 & P & R & F1 \\
\midrule
ecpr & 713 & 17,244 & 0.92 & 0.86 & 0.89 & 0.81 & 0.89 & 0.85 & 0.90 & 0.74 & 0.81 \\
jh*,tg*,ps*,ron* & 1,088 & 11,550 & 0.95 & 0.93 & 0.94 & 0.97 & 0.94 & 0.95 & 0.93 & 0.92 & 0.93 \\
petet & 2,116 & 34,098 & 0.93 & 0.70 & 0.80 & 0.94 & 0.93 & 0.94 & 0.92 & 0.88 & 0.90 \\
vm32 & 581 & 7,135 & 0.91 & 0.69 & 0.78 & 0.94 & 0.95 & 0.95 & 0.91 & 0.93 & 0.92 \\
ws214 & 1,000 & 8,730 & 0.93 & 0.45 & 0.60 & 0.94 & 0.92 & 0.93 & 0.93 & 0.89 & 0.91 \\
cb & 598 & 12,395 & 0.92 & 0.38 & 0.54 & 0.86 & 0.39 & 0.53 & 0.93 & 0.67 & 0.78 \\
wsj23 & 950 & 22,987 & 0.89 & 0.65 & 0.75 & 0.92 & 0.69 & 0.79 & 0.93 & 0.79 & 0.86 \\
\midrule
average & 7,046 & 114,139 & 0.92 & 0.67 & 0.76 & 0.91 & 0.82 & 0.85 & 0.92 & 0.83 & 0.86 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[h]
\centering
\caption{Effects of supertagging WITH EXCEPTIONS on parsing accuracy (exact match over MRS).}
\label{tab:exact-exceptions}
\begin{tabular}{l r r r r r}
\toprule
dataset & sent & tok & No tagging & Ubertagging & BERT-based supertags \\
\midrule
ecpr & 713 & 17,244 & 0.55 & 0.33 & 0.46 \\
jh*,tg*,ps*,ron* & 1,088 & 11,550 & 0.55 & 0.23 & 0.13 \\
petet & 2,116 & 34,098 & 0.26 & 0.17 & 0.42 \\
vm32 & 581 & 7,135 & 0.48 & 0.38 & 0.66 \\
ws214 & 1,000 & 8,730 & 0.61 & 0.67 & 0.22 \\
cb & 598 & 12,395 & 0.22 & 0.27 & 0.48 \\
wsj23 & 950 & 22,987 & 0.38 & 0.58 & 0.67 \\
\midrule
average & 7,046 & 114,139 & 0.44 & 0.37 & 0.43 \\
\bottomrule
\end{tabular}
\end{table}

\section{Conclusion and future work}
We used the advancements in HPSG treebanking to train more accurate supertaggers. The ERG is a major project in syntactic theory and an important resource for creating high quality semantic treebanks. It has the potential to contribute to NLP tasks that require high precision and/or interpretability including probing of the LLMs, and thus making HPSG parsing faster is strategic for NLP. We tested the new supertagging models with the state-of-the-art HPSG parser and saw improvements in parsing speed as well as accuracy. We consider the results on multiple domains, well beyond the WSJ Section 23. We show promising results but also confirm that domain remains important, and purely statistical systems are brittle and often require rule-based additions in real-life scenarios. We contribute the ERG datasets converted to huggingface transformers format intended for token classification, along with the code which can be adapted for other purposes.

\section{Limitations}
Our paper is concerned with training supertagging models on an English HPSG treebank. The limitations therefore are associated mainly with the training of the models including neural networks, and with the building of broad-coverage grammars such as the English Resource Grammar. Crucially, while our method does not require industry-scale computational resources, training a neural classifier such as ours still requires a certain amount of training data, and this means that our method assumes that a large HPSG treebank is available for training. The availability of such a treebank, in turn, depends directly on the availability of a broad-coverage grammar. While choosing the gold trees for the treebank can be done relatively fast using treebanking tools once the grammar parsed the corpus, building a broad-coverage grammar itself requires an investment of years of expert work. At the moment, such an investment was made only for a few languages (English, Spanish, Japanese, Chinese), English being the largest one. Furthermore, the coverage of a precision grammar is never perfect and regular grammar updates are needed. A limitation related to using neural networks is that while the NCRF++ library can in principle be very efficient on some tasks (e.g. POS tagging), with our data and large label set it proved relatively slow, and so ideally a more efficient neural architecture may be required for future work in this direction.

\section*{Acknowledgments}
We acknowledge the European Union's Horizon Europe Framework Programme which funded this research under the Marie Skłodowska-Curie postdoctoral fellowship grant HORIZON-MSCA-2021-PF-01 (GAUSS, grant agreement No 101063104); and the European Research Council (ERC), which has funded this research under the Horizon Europe research and innovation programme (SALSA, grant agreement No 101100615). We also acknowledge grants SCANNER-UDC (PID2020-113230RB-C21) funded by MICIU/AEI 0.13039/501100011033; GAP (PID2022-139308OA-I00) funded by MICIU/AEI 0.13039/501100011033 and ERDF EU; LATCHING (PID2023-147129OB-C22) funded by MICIU/AEI 0.13039/501100011033 and ERDF EU; and TSI-100925-2023-L funded by Ministry for Digital Transformation and Civil Service and ``NextGenerationEU'' PRTR; as well as funding by Xunta de Galicia (ED431C 2024/102), and Centro de Investigación de Galicia ``CITIC'', funded by the Xunta de Galicia through the collaboration agreement between the Consellería de Cultura, Educación, Formación Profesional e Universidades and the Galician universities for the reinforcement of the research centres of the Galician University System (CIGUS).

\bibliographystyle{acl_natbib}
\bibliography{refs}

\appendix
\section{Appendix A}
\subsection{Tuning ranges}
BERT \citep{Devlin2019} was fine-tuned using transformers \citep{Wolf2019} and pytorch \citep{Paszke2017} using 4 learning rates: 1e-5, 2e-5, 3e-5, and 5e-6. Cased and uncased pretrained BERT models were tried.

\subsection{Computational resources}
We trained the neural models with a single NVIDIA GeForce RTX 2080 GPU, CUDA version 11.2. The SVM model and the MaxEnt baseline were trained using Intel Core i7-9700K 3.60GHz CPU (using single core processing for each model). We have experimented with Stochastic Gradient Descent (SGD) optimizer along with AdaGrad, Adam, and AdaDelta. The ranges for parameter values can be found in Table~\ref{tab:ncrf-params}. The decoding time (sentences per second) for the models can be found in Table~\ref{tab:tagger-acc}. The training times are presented in Table~\ref{tab:training-times}. The energy costs as estimated by the Python library carbontracker \citep{Anthony2020} are in Table~\ref{tab:energy}.

\begin{table}[h]
\centering
\caption{NCRF++ model parameters.}
\label{tab:ncrf-params}
\begin{tabular}{l l}
\toprule
Parameter & Value \\
\midrule
LSTM layers & 4 \\
hidden dim & 800 \\
word embeddings & glove.840B \\
word emb. dim & 300 \\
char emb. dim & 50 \\
momentum & 0 \\
dropout & 0.5 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[h]
\centering
\caption{Training times for models used to choose the best baseline and best experimental models.}
\label{tab:training-times}
\begin{tabular}{l l r}
\toprule
Model type & models trained for tuning & total time (sec) \\
\midrule
SVM (Scikit-learn) & 1 & 3,664 \\
MaxEnt (Scikit-learn) & 14 & 106,922 \\
NCRF++ & 31 & 955,500 (approx.) \\
BERT & 5 & 100,000 (approx.) \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[h]
\centering
\caption{Energy cost estimate for training the final NCRF++ model in 38 epochs (31 were trained in total, number of epochs varied) and for BERT 50 epochs.}
\label{tab:energy}
\begin{tabular}{l r r}
\toprule
Measurement & NCRF++ & BERT \\
\midrule
Process used (kWh) & 5.55 & 3.5 \\
Carbon emissions (kg CO\textsubscript{2}) & 1.63 & 5.5 \\
Equivalent km driven & 13 km & 5.5 km \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Development accuracies}
The development (validation set) accuracies are presented in Tables~\ref{tab:maxent-dev}, \ref{tab:ncrf-dev}, and \ref{tab:bert-dev}. The best models are bolded. NCRF++ has nondeterministic components, and the average dev accuracy of the best (bolded) model in Table~\ref{tab:ncrf-dev}; the average accuracy is 95.15\%; standard deviation 0.07088.

\begin{table}[h]
\centering
\caption{Development (validation) set accuracies for MaxEnt and SVM.}
\label{tab:maxent-dev}
\begin{tabular}{l r}
\toprule
Model & dev accuracy (\%) \\
\midrule
multinomial L2 SAG & 91.59 \\
multinomial L2 SAG autoreg & 91.41 \\
OVR L2 SAG & 91.18 \\
OVR L2 SAG autoreg & 91.27 \\
multinomial L2 SAGA & 91.53 \\
multinomial L2 SAGA autoreg & 88.56 \\
OVR L2 SAGA & 91.17 \\
OVR L2 SAGA autoreg & 91.26 \\
\textbf{OVR L1 SAGA} & \textbf{92.17} \\
OVR L1 SAGA autoreg & 92.12 \\
multinomial L1 SAGA & 92.12 \\
multinomial L1 SAGA autoreg & 91.17 \\
SVM & 91.94 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[h]
\centering
\caption{Development (validation) set accuracies for neural models (NCRF++). Best model bolded.}
\label{tab:ncrf-dev}
\begin{tabular}{l l l r r}
\toprule
optimizer & LSTM layers & embed & epochs & dev accuracy (\%) \\
\midrule
SGD & 4 & glove840B & 19 & 93.20 \\
SGD & 1 & glove840B & 17 & 92.82 \\
SGD & 4 & glove840B & 23 & 93.69 \\
SGD & 1 & glove840B & 20 & 94.27 \\
SGD & 4 & glove840B & 18 & 93.86 \\
SGD & 1 & glove840B & 19 & 91.68 \\
SGD & 4 & glove840B & 20 & 93.07 \\
SGD & 4 & glove840B & 18 & 94.04 \\
SGD & 2 & glove840B & 19 & 94.74 \\
SGD & 4 & glove840B & 19 & 94.62 \\
SGD & 4 & glove840B & 13 & 94.56 \\
SGD & 4 & glove840B & 16 & 94.68 \\
SGD & 5 & glove840B & 12 & 94.35 \\
SGD & 4 & glove840B & 21 & 94.66 \\
SGD & 3 & glove840B & 21 & 94.92 \\
SGD & 2 & glove840B & 19 & 95.01 \\
SGD & 1 & glove840B & 16 & 94.60 \\
SGD & 3 & glove840B & 14 & 94.65 \\
SGD & 3 & glove840B & 15 & 94.86 \\
SGD & 2 & glove840B & 19 & 95.00 \\
SGD & 2 & glove840B & 8 & 94.57 \\
SGD & 2 & glove840B & 21 & 94.67 \\
SGD+0.3 momentum & 3 & glove840B & 12 & 94.69 \\
SGD & 2 & glove840B & 17 & 94.95 \\
\textbf{SGD} & \textbf{2} & \textbf{glove840B} & \textbf{11} & \textbf{95.12} \\
adagrad & 1 & glove840B & 42 & 92.00 \\
adagrad & 4 & glove840B & 1 & 92.42 \\
adam & 1 & glove840B & stuck on 73 & 86.85 \\
adadelta & 4 & random & did not finish & [ILLEGIBLE] \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[h]
\centering
\caption{Development (validation) set accuracies for fine-tuned BERT models. Best model bolded.}
\label{tab:bert-dev}
\begin{tabular}{l r}
\toprule
Model & dev accuracy (\%) \\
\midrule
\textbf{BERT cased LR 2e-5} & \textbf{96.46} \\
BERT cased LR 1e-5 & 96.37 \\
BERT cased LR 3e-5 & 96.31 \\
BERT cased LR 5e-6 & 96.34 \\
BERT uncased LR 2e-5 & 95.97 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{MaxEnt model}
Below we describe in detail how we trained the baseline MaxEnt models.

\subsubsection{MaxEnt model selection}
Rather than comparing our experimental numbers with numbers obtained by \citet{Dridan2009}, we create our own baseline because we want to be able to compare classic models with neural models with the same amount of training data. We experimented with autoregressive and non-autoregressive MaxEnt models and in the end chose one MaxEnt as the baseline.

\subsubsection{MaxEnt classifiers}
We use scikit-learn Python library \citep{Pedregosa2011} to train the baseline MaxEnt classifiers. The scikit-learn classifiers are optimized for processing a large number of observations. For that reason, we organized our evaluation data (dev and test) so as to maximize the number of observations passed to the classifier at each step. \citet{Dridan2009}'s models were autoregressive; we also implemented autoregressive baseline models, and in order to make them faster at test time, we organized the evaluation data by the word's position in the sentence. So the classifier would first process all the first words in all sentences, then all the second words, etc. For non-autoregressive models, which we also tried in order to find the best-performing baseline model, we just pass the classifier the entire list of observations in their original order.

We choose the single baseline MaxEnt model from the following types of models, by validation on the dev set: (1) MaxEnt autoregressive models which at test time, if more than one sentence is passed to the classifier, first classify all first tokens in all sentences, then all second tokens, etc; (2) MaxEnt non-autoregressive models where the observation tokens are organized in the same way as in (1); (3) MaxEnt non-autoregressive models where tokens are not reordered in any way and are stored consecutively. The best model happens to be of type (3).

All models achieve above 91\% accuracy on the dev set. The validation (dev) data consists of one Wikipedia section and one e-commerce corpus (2,267 sentences and 25,076 tokens total), with both domains represented also in the training data. Our best performing MaxEnt baseline model is a non-autoregressive `One-versus-rest' (OVR) model with L1 regularization and SAGA optimizer (92.21\% accuracy on the dev set).

\subsection{Parsing with more RAM}
To give the baseline system an opportunity to build the full lexical chart, more than 50GB RAM is required (24+30), according to our experiments with a subset of the WSJ training data that includes 25 sentences some of which are very long and ambiguous \citep{Yuret2010}, presented below in Table~\ref{tab:ram-coverage}. On this dataset, even with 54GB RAM, 100\% coverage is not achieved, and the parsing speed becomes intractable (77 sec/sen).

Since spending 77 sec/sen is not viable, we did not run the full experiments with 54GB RAM. We present below a subset of experiments, showing the baseline F-score gain due to higher recall. The ubertagger and the supertagger do not end up with such large lexical charts and thus do not benefit from more RAM, so we do not repeat the results from \S\ref{sec:parsing-exp} in Table~\ref{tab:ram-fscore}.

The `Verbmobil' (phone conversations) and the `ecpr' (e-commerce) datasets are easy to parse fast (as we see from Table~\ref{tab:speed-exceptions}) and on such data, using more RAM may be justified with the baseline system, however other types of data lead to the parsing time increasing noticeably. On the travel brochures data (jhk), the baseline system achieves an F-score of 87\% at the cost of spending 8.68 seconds per sentence, while our supertagger achieves 86\% with only 0.78 seconds/sentence and with only 2.7GB of RAM. Figure~\ref{fig:pareto} summarizes the results presented in Tables~\ref{tab:speed-default}--\ref{tab:edm-exceptions}, showing that if we optimize for both speed and F-score, the best models include our model and the ubertagger models.

\begin{table}[h]
\centering
\caption{Baseline (no tagging) coverage gain with more RAM on the PETE dataset (ERG version).}
\label{tab:ram-coverage}
\begin{tabular}{l r r}
\toprule
RAM & coverage & speed (sec/sen) \\
\midrule
2.7GB (default) & 11/25 (44\%) & 0.74 \\
54GB & 19/25 (76\%) & 77 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[h]
\centering
\caption{Baseline (no tagging) recall gains and speed loss with generous RAM.}
\label{tab:ram-fscore}
\begin{tabular}{l r r r r}
\toprule
dataset & 2.7GB RAM F-score & 54GB RAM F-score & 2.7GB RAM speed & 54GB RAM speed \\
\midrule
ecpr & 0.78 & 0.85 & 0.74 & 2.61 \\
jhk & 0.87 & 0.90 & 1.96 & 8.68 \\
petet & 0.92 & 0.92 & 0.74 & 4.34 \\
vm32 & 0.98 & 0.99 & 0.99 & 0.99 \\
wsj23 & 0.98 & 0.99 & 0.99 & 0.99 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[h]
\centering
\caption{Parsing with 54GB RAM (F-scores only).}
\label{tab:ram-full}
\begin{tabular}{l r r r}
\toprule
dataset & No tagging & Ubertagging & BERT supertagging \\
\midrule
ecpr & 0.87 & 0.75 & 0.81 \\
jhk & 0.93 & 0.87 & 0.88 \\
petet & 0.87 & 0.85 & 0.92 \\
vm32 & 0.98 & 0.99 & 0.99 \\
wsj23 & 0.99 & 0.92 & 0.94 \\
\bottomrule
\end{tabular}
\end{table}

\end{document}
=====END FILE=====

=====FILE: refs.bib=====
@inproceedings{Someya2024,
  author = {Taiga Someya and Ryo Yoshida and Yohei Oseki},
  title = {Targeted syntactic evaluation on the chomsky hierarchy},
  booktitle = {Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)},
  pages = {15595--15605},
  year = {2024}
}

@book{Pollard1994,
  author = {Carl Pollard and Ivan A. Sag},
  title = {Head-Driven Phrase Structure Grammar},
  series = {Studies in Contemporary Linguistics},
  publisher = {The University of Chicago Press and CSLI Publications},
  address = {Chicago, IL and Stanford, CA},
  year = {1994}
}

@incollection{Bender2021,
  author = {Emily M. Bender and Guy Emerson},
  title = {Computational linguistics and grammar engineering},
  booktitle = {Head-Driven Phrase Structure Grammar: The handbook},
  editor = {Stephan M{\"u}ller and Anne Abeill{\'e} and Robert D. Borsley and Jean-Pierre Koenig},
  year = {2021}
}

@phdthesis{Blunsom2007,
  author = {Philip Blunsom},
  title = {Structured classification for multilingual natural language processing},
  school = {University of Melbourne},
  year = {2007}
}

@article{Brants2000,
  author = {Thorsten Brants},
  title = {{TnT}---a statistical part-of-speech tagger},
  journal = {arXiv preprint cs/0003055},
  year = {2000}
}

@inproceedings{Buys2017,
  author = {Jan Buys and Phil Blunsom},
  title = {Robust incremental neural semantic graph parsing},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages = {1215--1226},
  year = {2017}
}

@article{Callmeier2000,
  author = {Ulrich Callmeier},
  title = {{PET}---a platform for experimentation with efficient {HPSG} processing techniques},
  journal = {Natural Language Engineering},
  volume = {6},
  number = {1},
  pages = {99--107},
  year = {2000}
}

@book{Carpenter1992,
  author = {Robert Carpenter},
  title = {The logic of typed feature structures: with applications to unification grammars, logic programs and constraint resolution},
  volume = {32},
  publisher = {Cambridge University Press},
  year = {1992}
}

@phdthesis{Carroll1993,
  author = {John Carroll},
  title = {Practical unification-based parsing of natural language},
  school = {University of Cambridge},
  year = {1993}
}

@inproceedings{Chen2018,
  author = {Yufei Chen and Weiwei Sun and Xiaojun Wan},
  title = {Accurate {SHRG}-based semantic parsing},
  booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages = {408--418},
  address = {Melbourne, Australia},
  year = {2018}
}

@inproceedings{Clark2003,
  author = {Stephen Clark and James R. Curran},
  title = {Log-linear models for wide-coverage {CCG} parsing},
  booktitle = {Proceedings of the 2003 conference on Empirical methods in natural language processing},
  pages = {97--104},
  year = {2003}
}

@incollection{Copestake2002,
  author = {Ann Copestake},
  title = {Definitions of typed feature structures},
  booktitle = {Collaborative Language Engineering},
  editor = {Stephan Oepen and Dan Flickinger and Jun'ichi Tsujii and Hans Uszkoreit},
  pages = {227--230},
  publisher = {CSLI Publications},
  address = {Stanford, CA},
  year = {2002}
}

@inproceedings{Crysmann2012,
  author = {Berthold Crysmann and Woodley Packard},
  title = {Towards efficient {HPSG} generation for {G}erman, a non-configurational language},
  booktitle = {COLING},
  pages = {695--710},
  year = {2012}
}

@inproceedings{Devlin2019,
  author = {Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  title = {{BERT}: Pre-training of deep bidirectional transformers for language understanding},
  booktitle = {Proceedings of NAACL-HLT},
  pages = {4171--4186},
  year = {2019}
}

@phdthesis{Dridan2009,
  author = {Rebecca Dridan},
  title = {Using lexical statistics to improve {HPSG} parsing},
  school = {University of Saarland},
  year = {2009}
}

@inproceedings{Dridan2013,
  author = {Rebecca Dridan},
  title = {Ubertagging: Joint segmentation and supertagging for english},
  booktitle = {Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing},
  pages = {1201--1212},
  year = {2013}
}

@inproceedings{Dridan2008,
  author = {Rebecca Dridan and Valia Kordoni and Jeremy Nicholson},
  title = {Enhancing performance of lexicalised grammars},
  booktitle = {Proceedings of ACL-08: HLT},
  pages = {613--621},
  year = {2008}
}

@inproceedings{Dridan2011,
  author = {Rebecca Dridan and Stephan Oepen},
  title = {Parser evaluation using elementary dependency matching},
  booktitle = {Proceedings of the 12th International Conference on Parsing Technologies},
  pages = {225--230},
  address = {Dublin, Ireland},
  year = {2011}
}

@article{Flickinger2000,
  author = {Dan Flickinger},
  title = {On building a more efficient grammar by exploiting types},
  journal = {Natural Language Engineering},
  volume = {6},
  number = {01},
  pages = {15--28},
  year = {2000}
}

@incollection{Flickinger2011,
  author = {Dan Flickinger},
  title = {Accuracy v. robustness in grammar engineering},
  booktitle = {Language from a Cognitive Perspective: Grammar, Usage and Processing},
  editor = {Emily M. Bender and Jennifer E. Arnold},
  pages = {31--50},
  publisher = {CSLI Publications},
  address = {Stanford, CA},
  year = {2011}
}

@inproceedings{Flickinger2013,
  author = {Dan Flickinger and Jiye Yu},
  title = {Toward more precision in correction of grammatical errors},
  booktitle = {Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task},
  pages = {68--73},
  year = {2013}
}

@inproceedings{Flickinger2012,
  author = {Dan Flickinger and Yi Zhang and Valia Kordoni},
  title = {{Deepbank}. a dynamically annotated treebank of the wall street journal},
  booktitle = {Proceedings of the 11th International Workshop on Treebanks and Linguistic Theories},
  pages = {85--96},
  year = {2012}
}

@inproceedings{Hajdik2019,
  author = {Valerie Hajdik and Jan Buys and Michael W. Goodman and Emily M. Bender},
  title = {Neural text generation from rich semantic representations},
  booktitle = {Proceedings of NAACL-HLT},
  pages = {2259--2266},
  year = {2019}
}

@inproceedings{Hovy2015,
  author = {Dirk Hovy and Anders S{\o}gaard},
  title = {Tagging performance correlates with author age},
  booktitle = {Proceedings of the 53rd annual meeting of the Association for Computational Linguistics and the 7th international joint conference on natural language processing (volume 2: Short papers)},
  pages = {483--488},
  year = {2015}
}

@inproceedings{Kogkalidis2023,
  author = {Konstantinos Kogkalidis and Michael Moortgat},
  title = {Geometry-aware supertagging with heterogeneous dynamic convolutions},
  booktitle = {Proceedings of the 2023 CoNLL Conference on Learning with Small Data (LSD)},
  pages = {107--119},
  year = {2023}
}

@inproceedings{Lin2022,
  author = {Zi Lin and Jeremiah Zhe Liu and Jingbo Shang},
  title = {Towards collaborative neural-symbolic graph semantic parsing via uncertainty},
  booktitle = {Findings of the Association for Computational Linguistics: ACL 2022},
  year = {2022}
}

@inproceedings{Liu2021,
  author = {Yufang Liu and Tho Ji and Yuanbin Wu and Man Lan},
  title = {Generating {CCG} categories},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {35},
  pages = {13443--13451},
  year = {2021}
}

@article{Malouf2000,
  author = {Robert Malouf and John Carroll and Ann Copestake},
  title = {Efficient feature structure operations without compilation},
  journal = {Natural Language Engineering},
  volume = {6},
  number = {1},
  pages = {29--46},
  year = {2000}
}

@techreport{Marcus1993,
  author = {Mitchell Marcus and Beatrice Santorini and Mary Ann Marcinkiewicz},
  title = {Building a large annotated corpus of {E}nglish: {T}he {P}enn {T}reebank},
  institution = {University of Pennsylvania Department of Computer and Information Science},
  number = {MS-CIS-93-87},
  year = {1993}
}

@article{Marimon2014,
  author = {Montserrat Marimon and N{\'u}ria Bel and Llu{\'\i}s Padr{\'o}},
  title = {Automatic selection of {HPSG}-parsed sentences for freebank construction},
  journal = {Computational Linguistics},
  volume = {40},
  number = {3},
  pages = {523--531},
  year = {2014}
}

@inproceedings{Matsuzaki2007,
  author = {Takuya Matsuzaki and Yusuke Miyao and Jun'ichi Tsujii},
  title = {Efficient {HPSG} parsing with supertagging and {CFG}-filtering},
  booktitle = {Proceedings of the 20th international joint conference on Artificial intelligence},
  pages = {1671--1676},
  year = {2007}
}

@article{Miyao2008,
  author = {Yusuke Miyao and Jun'ichi Tsujii},
  title = {Feature forest models for probabilistic {HPSG} parsing},
  journal = {Computational Linguistics},
  volume = {34},
  number = {1},
  pages = {35--80},
  year = {2008}
}

@inproceedings{Morgado2016,
  author = {Luis Morgado da Costa and Francis Bond and Xiaoling He},
  title = {Syntactic well-formedness diagnosis and error-based coaching in computer assisted language learning using machine translation},
  booktitle = {Proceedings of the 3rd Workshop on Natural Language Processing Techniques for Educational Applications (NLPTEA2016)},
  pages = {107--116},
  year = {2016}
}

@inproceedings{Morgado2020,
  author = {Luis Morgado da Costa and Roger Y.P. Winder and Shu Yun Li and Benedict Christopher Lim Tzer Liang and Joseph Mackinnon and Francis Bond},
  title = {Automated writing support using deep linguistic parsers},
  booktitle = {Proceedings of The 12th Language Resources and Evaluation Conference},
  pages = {369--377},
  year = {2020}
}

@inproceedings{Ninomiya2007,
  author = {Takashi Ninomiya and Takuya Matsuzaki and Yusuke Miyao and Jun'ichi Tsujii},
  title = {A log-linear model with an n-gram reference distribution for accurate {HPSG} parsing},
  booktitle = {Proceedings of the 10th International Conference on Parsing Technologies},
  pages = {60--68},
  year = {2007}
}

@manual{Oepen1999,
  author = {Stephan Oepen},
  title = {[incr tsdb()] competence and performance laboratory. User and reference manual},
  year = {1999}
}

@incollection{Oepen2002,
  author = {Stephan Oepen and John Carroll},
  title = {Efficient parsing for unification-based grammars},
  booktitle = {Collaborative Language Engineering},
  editor = {Stephan Oepen and Dan Flickinger and Jun'ichi Tsujii and Hans Uszkoreit},
  publisher = {CSLI Press},
  year = {2002}
}

@article{Oepen2004,
  author = {Stephan Oepen and Dan Flickinger and Kristina Toutanova and Christopher D. Manning},
  title = {{LinGO Redwoods}},
  journal = {Research on Language and Computation},
  volume = {2},
  number = {4},
  pages = {515--596},
  year = {2004}
}

@mastersthesis{Packard2015,
  author = {Woodley Packard},
  title = {Full-forest treebanking},
  school = {University of Washington},
  year = {2015}
}

@misc{Paszke2017,
  author = {Adam Paszke and Sam Gross and Soumith Chintala and Gregory Chanan and Edward Yang and Zachary DeVito and Zeming Lin and Alban Desmaison and Luca Antiga and Adam Lerer},
  title = {Automatic differentiation in {PyTorch}},
  year = {2017}
}

@article{Pedregosa2011,
  author = {F. Pedregosa and G. Varoquaux and A. Gramfort and V. Michel and B. Thirion and O. Grisel and M. Blondel and P. Prettenhofer and R. Weiss and V. Dubourg and J. Vanderplas and A. Passos and D. Cournapeau and M. Brucher and M. Perrot and E. Duchesnay},
  title = {Scikit-learn: Machine learning in {Python}},
  journal = {Journal of Machine Learning Research},
  volume = {12},
  pages = {2825--2830},
  year = {2011}
}

@inproceedings{Prange2021,
  author = {Jakob Prange and Nathan Schneider and Vivek Srikumar},
  title = {Supertagging the long tail with tree-structured decoding of complex categories},
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {9},
  pages = {243--260},
  year = {2021}
}

@article{Prins2004,
  author = {R.P. Prins and G.J.M. van Noord},
  title = {Reinforcing parser preferences through tagging},
  journal = {Traitement Automatique des Langues},
  volume = {3},
  pages = {121--139},
  year = {2004}
}

@inproceedings{Ratnaparkhi1996,
  author = {Adwait Ratnaparkhi},
  title = {A maximum entropy model for part-of-speech tagging},
  booktitle = {EMNLP},
  volume = {1},
  pages = {133--142},
  year = {1996}
}

@article{Raymond1999,
  author = {Eric Raymond},
  title = {The cathedral and the bazaar},
  journal = {Knowledge, Technology \& Policy},
  volume = {12},
  number = {3},
  pages = {23--49},
  year = {1999}
}

@inproceedings{Tian2020,
  author = {Yuanhe Tian and Yan Song and Fei Xia},
  title = {Supertagging combinatory categorial grammar with attentive graph convolutional networks},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages = {6037--6044},
  year = {2020}
}

@inproceedings{Tomita1985,
  author = {Masaru Tomita},
  title = {An efficient context-free parsing algorithm for natural languages},
  booktitle = {IJCAI},
  volume = {2},
  pages = {756--764},
  year = {1985}
}

@inproceedings{Vaswani2016,
  author = {Ashish Vaswani and Yonatan Bisk and Kenji Sagae and Ryan Musa},
  title = {Supertagging with lstms},
  booktitle = {Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages = {232--237},
  year = {2016}
}

@misc{Wolf2019,
  author = {Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and R{\'e}mi Louf and Morgan Funtowicz and others},
  title = {Huggingface's transformers: State-of-the-art natural language processing},
  journal = {arXiv preprint arXiv:1910.03771},
  year = {2019}
}

@inproceedings{Xu2015,
  author = {Wenduan Xu and Michael Auli and Stephen Clark},
  title = {{CCG} supertagging with a recurrent neural network},
  booktitle = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
  pages = {250--255},
  year = {2015}
}

@inproceedings{Yang2018,
  author = {Jie Yang and Yue Zhang},
  title = {{NCRF++}: An open-source neural sequence labeling toolkit},
  booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics},
  year = {2018}
}

@inproceedings{Yuret2010,
  author = {Deniz Yuret and Aydin Han and Zehra Turgut},
  title = {{Semeval-2010} task 12: Parser evaluation using textual entailments},
  booktitle = {Proceedings of the 5th International Workshop on Semantic Evaluation},
  pages = {51--56},
  year = {2010}
}

@article{Zhang2012,
  author = {Yao-Zhong Zhang and Takuya Matsuzaki and Jun'ichi Tsujii},
  title = {Structure-guided supertagger learning},
  journal = {Natural Language Engineering},
  volume = {18},
  number = {2},
  pages = {205--234},
  year = {2012}
}

@inproceedings{Zhang2009,
  author = {Yao-Zhong Zhang and Takuya Matsuzaki and Jun'ichi Tsujii},
  title = {{HPSG} supertagging: A sequence labeling view},
  booktitle = {Proceedings of the 11th International Conference on Parsing Technologies (IWPT'09)},
  pages = {210--213},
  year = {2009}
}

@inproceedings{Zhang2010,
  author = {Yao-Zhong Zhang and Takuya Matsuzaki and Jun'ichi Tsujii},
  title = {A simple approach for {HPSG} supertagging using dependency information},
  booktitle = {Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics},
  pages = {645--648},
  year = {2010}
}

@inproceedings{Zhang2011,
  author = {Yi Zhang and Hans-Ulrich Krieger},
  title = {Large-scale corpus-driven {PCFG} approximation of an {HPSG}},
  booktitle = {Proceedings of the 12th international conference on parsing technologies},
  pages = {198--208},
  year = {2011}
}

@inproceedings{Bangalore1999,
  author = {Srinivas Bangalore and Aravind Joshi},
  title = {Supertagging: An approach to almost parsing},
  journal = {Computational Linguistics},
  volume = {25},
  number = {2},
  pages = {237--265},
  year = {1999}
}

@misc{Anthony2020,
  author = {Lasse F. Wolff Anthony and Benjamin Kanding and Raghavendra Selvan},
  title = {Carbontracker: Tracking and predicting the carbon footprint of training deep learning models},
  booktitle = {ICML Workshop on Challenges in Deploying and monitoring Machine Learning Systems},
  eprint = {2007.03051},
  archivePrefix = {arXiv},
  year = {2020}
}
=====END FILE=====