\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{zhang2020pegasus}
\citation{qin2019stack}
\citation{wang2021bentailment}
\citation{brown2020language}
\citation{gao2021simcse}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}\protected@file@percent }
\newlabel{sec:introduction}{{1}{2}{Introduction}{section.1}{}}
\citation{schroff2015facenet}
\citation{chopra2005learning}
\citation{khosla2020supervised}
\citation{reimers2019sentence}
\citation{devlin2018bert}
\citation{kramer1991nonlinear}
\citation{vincent2010stacked}
\citation{germain2015made}
\citation{miao2015neural}
\citation{li2019stable}
\citation{wang2021atsdae}
\citation{wang2021atsdae}
\citation{savinov2021step}
\citation{chopra2005learning}
\citation{schroff2015facenet}
\citation{sohn2016improved}
\citation{gutmann2010noise}
\citation{khosla2020supervised}
\citation{reimers2019sentence}
\citation{conneau2018senteval}
\citation{gao2021simcse}
\citation{brown2020language}
\citation{wang2021bentailment}
\citation{finn2017model}
\citation{qin2019stack}
\citation{garg2021towards}
\citation{sun2020fine}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{4}{section.2}\protected@file@percent }
\newlabel{sec:related}{{2}{4}{Related Work}{section.2}{}}
\citation{liu2019roberta}
\citation{wang2020minilm}
\@writefile{toc}{\contentsline {section}{\numberline {3}Model}{5}{section.3}\protected@file@percent }
\newlabel{sec:model}{{3}{5}{Model}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}DAE: Denoising Autoencoder phase}{5}{subsection.3.1}\protected@file@percent }
\newlabel{subsec:dae}{{3.1}{5}{DAE: Denoising Autoencoder phase}{subsection.3.1}{}}
\newlabel{eq:dae_loss}{{1}{5}{DAE: Denoising Autoencoder phase}{equation.1}{}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:dae}{{1a}{6}{First stage: Denoising Autoencoder}{figure.caption.1}{}}
\newlabel{sub@fig:dae}{{a}{6}{First stage: Denoising Autoencoder}{figure.caption.1}{}}
\newlabel{fig:cl}{{1b}{6}{Second stage: Supervised Contrastive Learning}{figure.caption.1}{}}
\newlabel{sub@fig:cl}{{b}{6}{Second stage: Supervised Contrastive Learning}{figure.caption.1}{}}
\newlabel{fig:ft}{{1c}{6}{Third stage: Fine-tuning}{figure.caption.1}{}}
\newlabel{sub@fig:ft}{{c}{6}{Third stage: Fine-tuning}{figure.caption.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces (a) First stage: Denoising Autoencoder architecture where the encoder and the decoder are based on the pre-trained Transformers. The middle layer following the pooling together with the encoder model will be the resulting model of this stage. (b) Second stage: Supervised Contrastive Learning phase. We add a pooling layer to the previous one to learn the new clustered representation. The set of blocks denoted as DAECL will be employed and adapted as a base model in the fine-tuning phase. (c) Third stage: Classification phase through fine-tuning}}{6}{figure.caption.1}\protected@file@percent }
\newlabel{fig:phases}{{1}{6}{(a) First stage: Denoising Autoencoder architecture where the encoder and the decoder are based on the pre-trained Transformers. The middle layer following the pooling together with the encoder model will be the resulting model of this stage. (b) Second stage: Supervised Contrastive Learning phase. We add a pooling layer to the previous one to learn the new clustered representation. The set of blocks denoted as DAECL will be employed and adapted as a base model in the fine-tuning phase. (c) Third stage: Classification phase through fine-tuning}{figure.caption.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}CL: Contrastive Learning phase}{6}{subsection.3.2}\protected@file@percent }
\newlabel{subsec:cl}{{3.2}{6}{CL: Contrastive Learning phase}{subsection.3.2}{}}
\newlabel{eq:label}{{2}{6}{CL: Contrastive Learning phase}{equation.2}{}}
\newlabel{eq:cl_loss}{{3}{6}{CL: Contrastive Learning phase}{equation.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Imbalance correction}{6}{subsubsection.3.2.1}\protected@file@percent }
\newlabel{subsubsec:imbalance}{{3.2.1}{6}{Imbalance correction}{subsubsection.3.2.1}{}}
\newlabel{eq:ratio_func}{{4}{7}{Imbalance correction}{equation.4}{}}
\newlabel{eq:newratio_a}{{5}{7}{Imbalance correction}{equation.5}{}}
\newlabel{eq:newratio_b}{{6}{7}{Imbalance correction}{equation.6}{}}
\newlabel{eq:ratio_one}{{7}{7}{Imbalance correction}{equation.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}FT: fine-tuning phase}{7}{subsection.3.3}\protected@file@percent }
\newlabel{subsec:ft}{{3.3}{7}{FT: fine-tuning phase}{subsection.3.3}{}}
\citation{coucke2018snips}
\citation{qin2019stack}
\citation{socher2013recursive}
\citation{zhang2015character}
\newlabel{eq:softmax}{{8}{8}{FT: fine-tuning phase}{equation.8}{}}
\newlabel{eq:ce_loss}{{9}{8}{FT: fine-tuning phase}{equation.9}{}}
\newlabel{eq:binary_ce}{{10}{8}{FT: fine-tuning phase}{equation.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}Joint}{8}{subsubsection.3.3.1}\protected@file@percent }
\newlabel{subsubsec:joint}{{3.3.1}{8}{Joint}{subsubsection.3.3.1}{}}
\newlabel{eq:joint_loss}{{11}{8}{Joint}{equation.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experimental Setup}{8}{section.4}\protected@file@percent }
\newlabel{sec:setup}{{4}{8}{Experimental Setup}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Datasets}{8}{subsection.4.1}\protected@file@percent }
\newlabel{subsec:datasets}{{4.1}{8}{Datasets}{subsection.4.1}{}}
\citation{maas2011learning}
\citation{wang2020minilm}
\citation{liu2019roberta}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Statistics for Train, Validation and Test dataset splits.}}{9}{table.caption.2}\protected@file@percent }
\newlabel{tab:dataset_stats}{{1}{9}{Statistics for Train, Validation and Test dataset splits}{table.caption.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Average and max lengths for each of the datasets mentioned in the paper.}}{9}{table.caption.3}\protected@file@percent }
\newlabel{tab:dataset_lengths}{{2}{9}{Average and max lengths for each of the datasets mentioned in the paper}{table.caption.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Models and Training}{9}{subsection.4.2}\protected@file@percent }
\newlabel{subsec:models}{{4.2}{9}{Models and Training}{subsection.4.2}{}}
\citation{qin2019stack}
\citation{sun2020self}
\citation{phuong2022cae}
\citation{karl2022transformers}
\citation{wang2021bentailment}
\citation{sun2020fine}
\citation{qin2019stack}
\citation{sun2020self}
\citation{phuong2022cae}
\citation{karl2022transformers}
\citation{wang2021bentailment}
\citation{sun2020fine}
\citation{sun2020fine}
\citation{sun2020fine}
\citation{wang2021bentailment}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Performance accuracy on different datasets using RoBERTa and all-MiniLM-L12-v2 models in \%. 3-Phase refers to our main 3 stages approach, while Joint denotes one whose loss is based on the combination of the first two losses, and FT corresponds to the fine-tuning. We also add some SOTA results from other papers: S-P denotes Stack-Propagation \cite  {qin2019stack}, Self-E is used to denote \cite  {sun2020self}, CAE is used to denote \cite  {phuong2022cae}, STC-DeBERTa refers to \cite  {karl2022transformers}, EFL points to \cite  {wang2021bentailment} (this one uses RoBERTa-Large), and FTBERT is \cite  {sun2020fine}.}}{10}{table.caption.4}\protected@file@percent }
\newlabel{tab:main_results}{{3}{10}{Performance accuracy on different datasets using RoBERTa and all-MiniLM-L12-v2 models in \%. 3-Phase refers to our main 3 stages approach, while Joint denotes one whose loss is based on the combination of the first two losses, and FT corresponds to the fine-tuning. We also add some SOTA results from other papers: S-P denotes Stack-Propagation \cite {qin2019stack}, Self-E is used to denote \cite {sun2020self}, CAE is used to denote \cite {phuong2022cae}, STC-DeBERTa refers to \cite {karl2022transformers}, EFL points to \cite {wang2021bentailment} (this one uses RoBERTa-Large), and FTBERT is \cite {sun2020fine}}{table.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Metrics}{10}{subsection.4.3}\protected@file@percent }
\newlabel{subsec:metrics}{{4.3}{10}{Metrics}{subsection.4.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Results}{10}{section.5}\protected@file@percent }
\newlabel{sec:results}{{5}{10}{Results}{section.5}{}}
\citation{phuong2022cae}
\citation{sun2020self}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Ablation results. As before, 3-Phase, Joint, and FT correspond to the 3 stages approach, joint losses, and Fine-tuning, respectively. Here, DAE+FT denotes the denoising autoencoder together with fine-tuning, CL+FT denotes the contrastive Siamese training together with fine-tuning, No Imb. means 3-phase but skipping the imbalance correction, and Extra Imb. refers to an increase of the imbalance correction to $\text  {minratio}=1.5$ and $\text  {maxratio}=3.5$.}}{11}{table.caption.5}\protected@file@percent }
\newlabel{tab:ablation}{{4}{11}{Ablation results. As before, 3-Phase, Joint, and FT correspond to the 3 stages approach, joint losses, and Fine-tuning, respectively. Here, DAE+FT denotes the denoising autoencoder together with fine-tuning, CL+FT denotes the contrastive Siamese training together with fine-tuning, No Imb. means 3-phase but skipping the imbalance correction, and Extra Imb. refers to an increase of the imbalance correction to $\text {minratio}=1.5$ and $\text {maxratio}=3.5$}{table.caption.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Ablation study}{11}{subsection.5.1}\protected@file@percent }
\newlabel{subsec:ablation}{{5.1}{11}{Ablation study}{subsection.5.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{12}{section.6}\protected@file@percent }
\newlabel{sec:conclusion}{{6}{12}{Conclusion}{section.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Future work}{12}{section.7}\protected@file@percent }
\newlabel{sec:future}{{7}{12}{Future work}{section.7}{}}
\newlabel{sec:limitations}{{7}{12}{Limitations}{section*.6}{}}
\bibstyle{acl_natbib}
\bibdata{refs}
\newlabel{sec:ack}{{7}{13}{Acknowledgments}{section*.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A}Appendix}{13}{appendix.A}\protected@file@percent }
\newlabel{app:appendix}{{A}{13}{Appendix}{appendix.A}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}Hyper-parameters}{13}{subsection.A.1}\protected@file@percent }
\newlabel{app:hyperparams}{{A.1}{13}{Hyper-parameters}{subsection.A.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}Other metrics}{13}{subsection.A.2}\protected@file@percent }
\newlabel{app:metrics}{{A.2}{13}{Other metrics}{subsection.A.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Hyper-parameters configurations and search space of the experiments. $^1$ means these are not real epochs since the input data is not always the same. The data was masked on the fly; therefore, each epoch differs. $^2$ We used an early stopping approach for the FT phase. $^3$ We only consider the epsilon hyperparameter in the AdamW optimizer for FT; the other two phases use the default value from the Transformers library (1e-06). $^4$ This hyper-parameter was estimated initially with the training dataset with a large margin. This was applied for datasets with very short sentences, like SNIPS. $^5$ This hyper-parameter estimates the max length of the sequences using the 10\% of the examples. This estimation is multiplied by 1.2 and is added as the maximum size of the sequences for the embedding layers. The difference is that it was done on the fly and not preserved in this case.}}{14}{table.caption.8}\protected@file@percent }
\newlabel{tab:hyperparams}{{5}{14}{Hyper-parameters configurations and search space of the experiments. $^1$ means these are not real epochs since the input data is not always the same. The data was masked on the fly; therefore, each epoch differs. $^2$ We used an early stopping approach for the FT phase. $^3$ We only consider the epsilon hyperparameter in the AdamW optimizer for FT; the other two phases use the default value from the Transformers library (1e-06). $^4$ This hyper-parameter was estimated initially with the training dataset with a large margin. This was applied for datasets with very short sentences, like SNIPS. $^5$ This hyper-parameter estimates the max length of the sequences using the 10\% of the examples. This estimation is multiplied by 1.2 and is added as the maximum size of the sequences for the embedding layers. The difference is that it was done on the fly and not preserved in this case}{table.caption.8}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Precision and Recall values. The best values are shown in bold.}}{15}{table.caption.9}\protected@file@percent }
\newlabel{tab:prec_rec}{{6}{15}{Precision and Recall values. The best values are shown in bold}{table.caption.9}{}}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces F1 values for the best results. The best values are shown in bold.}}{15}{table.caption.10}\protected@file@percent }
\newlabel{tab:f1}{{7}{15}{F1 values for the best results. The best values are shown in bold}{table.caption.10}{}}
\gdef \@abspage@last{15}
