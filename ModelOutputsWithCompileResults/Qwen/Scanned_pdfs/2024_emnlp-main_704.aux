\relax 
\citation{Banerjee2020}
\citation{Savenkov2022}
\citation{Luong2015}
\citation{McCloskey1989}
\citation{Thompson2019}
\citation{Gu2020}
\citation{Papineni2002}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{}\protected@file@percent }
\newlabel{sec:introduction}{{1}{2}{}{section.1}{}}
\citation{Aharoni2020}
\citation{Zhang2019}
\citation{Sharaf2020}
\citation{Saunders2022}
\citation{Barone2017}
\citation{Shao2022}
\citation{Chu2017}
\citation{Xu2019}
\citation{Thompson2019,Gu2020}
\citation{vanderWees2015}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Related work}{3}{}\protected@file@percent }
\newlabel{sec:related_work}{{1.1}{3}{}{subsection.1.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}What does adapted NMT forget?}{3}{}\protected@file@percent }
\newlabel{sec:what_forgets}{{2}{3}{}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Measuring vocabulary-shift forgetting}{3}{}\protected@file@percent }
\newlabel{sec:measuring_forgetting}{{2.1}{3}{}{subsection.2.1}{}}
\citation{Aharoni2020}
\citation{Cettolo2012}
\citation{Neubig2011}
\citation{Rikters2019}
\citation{Rei2020}
\citation{Hasler2021}
\citation{Dyer2013}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Intentionally triggering forgetting: Lower quality and detrimental vocabulary shift}{4}{}\protected@file@percent }
\newlabel{sec:triggering_forgetting}{{2.2}{4}{}{subsection.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Which tokens are forgotten, and what replaces them?}{4}{}\protected@file@percent }
\newlabel{sec:token_replacement}{{2.3}{4}{}{subsection.2.3}{}}
\citation{Bojanowski2017}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Segment counts and absolute generic model BLEU and COMET on the generic domain test sets and on each in-domain test set.}}{5}{}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:datasets}{{1}{5}{}{table.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Measuring forgetting on generic test sets for de-en and en-ja domains after adaptation. Higher values indicate more forgetting.}}{5}{}\protected@file@percent }
\newlabel{tab:forgetting_metrics}{{2}{5}{}{table.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces High $\text  {ForgetGenUse}$ tokens for de-en domains---counts are for that token in the in-domain adaptation dataset. Left columns: Output from generic model. Right columns: Most frequent aligned replacements post-adaptation.}}{6}{}\protected@file@percent }
\newlabel{tab:forgotten_tokens}{{3}{6}{}{table.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Out-of-domain tokens are forgotten more}{6}{}\protected@file@percent }
\newlabel{sec:ood_tokens}{{2.4}{6}{}{subsection.2.4}{}}
\citation{vanderWees2015,Saunders2022}
\citation{Sharaf2020}
\citation{Pham2020}
\citation{Varis2021}
\citation{Wan2022}
\citation{Saunders2020}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Calculating $\text  {ForgetGenUse}$ over tokens that are out-of-domain (OOD) vs in-domain (ID) for each domain.}}{7}{}\protected@file@percent }
\newlabel{tab:ood_forgetting}{{4}{7}{}{table.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Why does forgetting vary by domain?}{7}{}\protected@file@percent }
\newlabel{sec:why_forgetting}{{3}{7}{}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Controlling for dataset size}{7}{}\protected@file@percent }
\newlabel{sec:dataset_size}{{3.1}{7}{}{subsection.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Controlling segment length and quality}{7}{}\protected@file@percent }
\newlabel{sec:segment_length}{{3.2}{7}{}{subsection.3.2}{}}
\citation{Khayrallah2018}
\citation{Lin1991}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Forgetting when adapting on subsampled (-s) domains. All de-en sets except Kor, and all en-ja except BSD, subsampled randomly to approximately the same token count as Kor/BSD respectively.}}{8}{}\protected@file@percent }
\newlabel{tab:subsampled_forgetting}{{5}{8}{}{table.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Forgetting on generic sets, adapting on subsampled datasets. We sample randomly (-s) or sample the shortest (ss) lines by source plus target token count. Sub-ssf pre-filters the shortest lines using LASER.}}{8}{}\protected@file@percent }
\newlabel{tab:short_segments}{{6}{8}{}{table.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Corpus-level score domain heuristics}{8}{}\protected@file@percent }
\newlabel{sec:corpus_heuristics}{{3.3}{8}{}{subsection.3.3}{}}
\citation{Lu2020}
\citation{Chu2017}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Corpus-level score domain heuristics, with forgetting measures for reference. Generic NLL and vocab JSD: closer to 0 is more similar to generic. Final lines: vocab coverage for downsampled domains of Table~\ref {tab:subsampled_forgetting}.}}{9}{}\protected@file@percent }
\newlabel{tab:heuristics}{{7}{9}{}{table.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Understanding generic data mix-in}{9}{}\protected@file@percent }
\newlabel{sec:mix_in}{{4}{9}{}{section.4}{}}
\citation{Haque2020,Hasler2021}
\citation{Gu2020}
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces Number of generic mix-in lines for each strategy. Random 1:1 is by definition the same size as the in-domain dataset, and Minimal Mix-in is often far smaller.}}{10}{}\protected@file@percent }
\newlabel{tab:mix_in_sizes}{{8}{10}{}{table.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Minimal mix-in mitigates 80\% of the forgetting}{10}{}\protected@file@percent }
\newlabel{sec:minimal_mix_in}{{4.1}{10}{}{subsection.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Minimal mix-in, better in-domain scores}{10}{}\protected@file@percent }
\newlabel{sec:in_domain_scores}{{4.2}{10}{}{subsection.4.2}{}}
\citation{Zhang2023}
\citation{Pang2024}
\bibstyle{acl_natbib}
\bibdata{refs}
\@writefile{lot}{\contentsline {table}{\numberline {9}{\ignorespaces Forgetting metrics on generic test sets, varying the mix-in dataset when fine-tuning for 20K iterations in each case. Lower is better for all metrics. Negative scores indicate improvement.}}{11}{}\protected@file@percent }
\newlabel{tab:mix_in_results}{{9}{11}{}{table.9}{}}
\@writefile{lot}{\contentsline {table}{\numberline {10}{\ignorespaces $\Delta $BLEU and $\Delta $COMET on in-domain test sets for the same experiments as in Table~\ref {tab:mix_in_results}. Higher is better.}}{11}{}\protected@file@percent }
\newlabel{tab:in_domain_results}{{10}{11}{}{table.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusions}{11}{}\protected@file@percent }
\newlabel{sec:conclusions}{{5}{11}{}{section.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Limitations}{11}{}\protected@file@percent }
\newlabel{sec:limitations}{{5.1}{11}{}{subsection.5.1}{}}
\citation{Vaswani2018}
\citation{Sennrich2016}
\citation{Kocmi2023}
\citation{Kocmi2022}
\citation{Morishita2022}
\citation{Post2018}
\@writefile{toc}{\contentsline {section}{\numberline {A}Experimental setup}{12}{}\protected@file@percent }
\newlabel{sec:experimental_setup}{{A}{12}{}{section.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {11}{\ignorespaces Pre-trained model specifications}}{12}{}\protected@file@percent }
\newlabel{tab:model_specs}{{11}{12}{}{table.11}{}}
\gdef \@abspage@last{12}
