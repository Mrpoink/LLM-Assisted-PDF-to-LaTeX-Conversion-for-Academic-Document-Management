\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{brown2020language}
\citation{shoeybi2020megatron}
\citation{xue2021mt5}
\citation{hoffmann2022training}
\citation{chowdhery2022palm}
\citation{zhang2022opt}
\citation{chung2022scaling}
\citation{workshop2023bloom}
\citation{touvron2023llama}
\citation{xue2021mt5}
\citation{nllb2022}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}\protected@file@percent }
\citation{sennrich2016improving}
\citation{fadaee2017data}
\citation{shu2019meta}
\citation{ren2019learning}
\citation{gu2018meta}
\citation{kocmi2017curriculum}
\citation{zhang2018empirical}
\citation{platanios2019competence}
\citation{zhang2019curriculum}
\citation{nllb2022}
\citation{brown2020language}
\citation{touvron2023llama}
\citation{petroni2019language}
\citation{radford2018improving}
\citation{radford2019language}
\citation{dong2019unified}
\citation{devlin2019bert}
\citation{lewis2019bart}
\citation{sun2019ernie}
\citation{liu2019roberta}
\citation{clark2020electra}
\citation{yang2020xlnet}
\citation{raffel2020exploring}
\citation{gao2021making}
\citation{liu2021gpt}
\citation{he2021deberta}
\citation{taori2023alpaca}
\citation{shin2020autoprompt}
\citation{schick2020exploiting}
\citation{li2021prefix}
\citation{hambardzumyan2021warp}
\citation{lester2021power}
\citation{zhong2021adapting}
\citation{wallace2021bertese}
\citation{jiang2020can}
\citation{qin2021learning}
\citation{liu2021what}
\citation{han2021ptr}
\citation{haviv2021bertese}
\citation{zhong2021factual}
\citation{lu2022fantastically}
\citation{bendavid2022pada}
\citation{wang2022iteratively}
\citation{zhou2023large}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related work}{3}{section.2}\protected@file@percent }
\citation{chung2022scaling}
\citation{wei2023chain}
\citation{wang2023self}
\citation{iyer2023optiml}
\citation{min2022metaicl}
\citation{wei2022finetuned}
\citation{wang2022supernaturalinstructions}
\citation{gu2023pretraining}
\citation{wang2023selfinstr}
\citation{zhang2022automatic}
\citation{press2023measuring}
\citation{zhou2023least}
\citation{xue2021mt5}
\citation{papineni2002bleu}
\citation{loshchilov2019decoupled}
\citation{xue2021mt5}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Task reformulations}}{4}{figure.1}\protected@file@percent }
\newlabel{fig:reformulations}{{1}{4}{Task reformulations}{figure.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Experiments on a difficult single language pair translation task}{4}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Setup}{4}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Motivation}{5}{subsection.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces POSE reformulation example}}{5}{figure.2}\protected@file@percent }
\newlabel{fig:pose_example}{{2}{5}{POSE reformulation example}{figure.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Modulating task difficulty}{5}{subsection.3.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Task difficulty experiment results on mT5 600M.}}{5}{table.1}\protected@file@percent }
\newlabel{tab:difficulty}{{1}{5}{Task difficulty experiment results on mT5 600M}{table.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Optimizing the curriculum}{5}{subsection.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Modulating scaffold substring}{5}{subsection.3.5}\protected@file@percent }
\citation{nllb2022}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Curriculum experiment results on mT5 600M.}}{6}{table.2}\protected@file@percent }
\newlabel{tab:curriculum}{{2}{6}{Curriculum experiment results on mT5 600M}{table.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Prefix+suffix experiment results on mT5 600M.}}{6}{table.3}\protected@file@percent }
\newlabel{tab:prefix_suffix}{{3}{6}{Prefix+suffix experiment results on mT5 600M}{table.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6}Matching the pretraining task}{6}{subsection.3.6}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Matching pretraining experiment results on mT5 600M with masking.}}{6}{table.4}\protected@file@percent }
\newlabel{tab:masking}{{4}{6}{Matching pretraining experiment results on mT5 600M with masking}{table.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7}Final results and comparison to state-of-the-art}{6}{subsection.3.7}\protected@file@percent }
\newlabel{sec:final_results}{{3.7}{6}{Final results and comparison to state-of-the-art}{subsection.3.7}{}}
\citation{nllb2022}
\citation{goyal2021flores}
\citation{guzman2019flores}
\citation{popovic2015chrf}
\citation{nllb2022}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Main results on the tib2eng translation task for mT5. Values shown are test set BLEU scores. The difference shown is the improvement gained by using the input finetuning reformulations. The NLLB column is the test set BLEU score for the corresponding sized NLLB model.}}{7}{table.5}\protected@file@percent }
\newlabel{tab:tib2eng_main}{{5}{7}{Main results on the tib2eng translation task for mT5. Values shown are test set BLEU scores. The difference shown is the improvement gained by using the input finetuning reformulations. The NLLB column is the test set BLEU score for the corresponding sized NLLB model}{table.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experiments on a massively multilingual translation task}{7}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Setup}{7}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Designing task reformulations}{7}{subsection.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Tib2eng translation results}}{8}{figure.3}\protected@file@percent }
\newlabel{fig:tib2eng_results}{{3}{8}{Tib2eng translation results}{figure.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces ParSE and MiPS examples}}{8}{figure.4}\protected@file@percent }
\newlabel{fig:par_mips_example}{{4}{8}{ParSE and MiPS examples}{figure.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Results}{8}{subsection.4.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Results on the Flores200 translation task for mT5. Values shown are test set chrF++ scores. The NLLB column is the task performance of a corresponding size NLLB model.}}{8}{table.6}\protected@file@percent }
\newlabel{tab:flores200_main}{{6}{8}{Results on the Flores200 translation task for mT5. Values shown are test set chrF++ scores. The NLLB column is the task performance of a corresponding size NLLB model}{table.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Analysis on mT5's pretraining dataset and Flores200}{8}{subsection.4.4}\protected@file@percent }
\citation{lample2019cross}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Flores200 translation results}}{9}{figure.5}\protected@file@percent }
\newlabel{fig:flores200_results}{{5}{9}{Flores200 translation results}{figure.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{9}{section.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Pretraining dataset analysis}}{10}{figure.6}\protected@file@percent }
\newlabel{fig:pretrain_analysis}{{6}{10}{Pretraining dataset analysis}{figure.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Breakdown of model and setup performance over different splits of the Flores200 dataset. ``In'' refers to a language that was found in the mT5 pretraining dataset and ``out'' refers to a language that was not. ``To Eng'' and ``From Eng'' is referred to as xx-eng and eng-xx in some other papers, respectively.}}{10}{table.7}\protected@file@percent }
\newlabel{tab:flores200_breakdown}{{7}{10}{Breakdown of model and setup performance over different splits of the Flores200 dataset. ``In'' refers to a language that was found in the mT5 pretraining dataset and ``out'' refers to a language that was not. ``To Eng'' and ``From Eng'' is referred to as xx-eng and eng-xx in some other papers, respectively}{table.7}{}}
\bibstyle{acl_natbib}
\bibcite{brown2020language}{Brown et al.2020}
\bibcite{chowdhery2022palm}{Chowdhery et al.2022}
\bibcite{chung2022scaling}{Chung et al.2022}
\bibcite{clark2020electra}{Clark et al.2020}
\bibcite{devlin2019bert}{Devlin et al.2019}
\bibcite{dong2019unified}{Dong et al.2019}
\bibcite{fadaee2017data}{Fadaee et al.2017}
\bibcite{gao2021making}{Gao et al.2021}
\bibcite{goyal2021flores}{Goyal et al.2021}
\bibcite{gu2018meta}{Gu et al.2018}
\bibcite{guzman2019flores}{Guzm\'{a}n et al.2019}
\bibcite{hambardzumyan2021warp}{Hambardzumyan et al.2021}
\bibcite{han2021ptr}{Han et al.2021}
\bibcite{haviv2021bertese}{Haviv et al.2021}
\bibcite{he2021deberta}{He et al.2021}
\bibcite{hoffmann2022training}{Hoffmann et al.2022}
\bibcite{iyer2023optiml}{Iyer et al.2023}
\bibcite{jiang2020can}{Jiang et al.2020}
\bibcite{kocmi2017curriculum}{Kocmi and Bojar2017}
\bibcite{lample2019cross}{Lample and Conneau2019}
\bibcite{lester2021power}{Lester et al.2021}
\bibcite{lewis2019bart}{Lewis et al.2019}
\bibcite{li2021prefix}{Li and Liang2021}
\bibcite{liu2019roberta}{Liu et al.2019}
\bibcite{liu2021what}{Liu et al.2021}
\bibcite{loshchilov2019decoupled}{Loshchilov and Hutter2019}
\bibcite{lu2022fantastically}{Lu et al.2022}
\bibcite{min2022metaicl}{Min et al.2022}
\bibcite{nllb2022}{NLLB Team2022}
\bibcite{papineni2002bleu}{Papineni et al.2002}
\bibcite{petroni2019language}{Petroni et al.2019}
\bibcite{platanios2019competence}{Platanios et al.2019}
\bibcite{popovic2015chrf}{Popovi\'{c}2015}
\bibcite{press2023measuring}{Press et al.2023}
\bibcite{qin2021learning}{Qin and Eisner2021}
\bibcite{radford2018improving}{Radford et al.2018}
\bibcite{radford2019language}{Radford et al.2019}
\bibcite{raffel2020exploring}{Raffel et al.2020}
\bibcite{ren2019learning}{Ren et al.2019}
\bibcite{sennrich2016improving}{Sennrich et al.2016}
\bibcite{schick2020exploiting}{Schick and Sch\"{u}tze2020}
\bibcite{schick2021exploiting}{Schick and Sch\"{u}tze2021}
\bibcite{shin2020autoprompt}{Shin et al.2020}
\bibcite{shoeybi2020megatron}{Shoeybi et al.2020}
\bibcite{shu2019meta}{Shu et al.2019}
\bibcite{sun2019ernie}{Sun et al.2019}
\bibcite{taori2023alpaca}{Taori et al.2023}
\bibcite{touvron2023llama}{Touvron et al.2023}
\bibcite{wallace2021bertese}{Wallace et al.2021}
\bibcite{wang2022iteratively}{Wang et al.2022a}
\bibcite{wang2022supernaturalinstructions}{Wang et al.2022b}
\bibcite{wang2023self}{Wang et al.2023a}
\bibcite{wang2023selfinstr}{Wang et al.2023b}
\bibcite{wei2022finetuned}{Wei et al.2022}
\bibcite{wei2023chain}{Wei et al.2023}
\bibcite{workshop2023bloom}{Workshop2023}
\bibcite{xue2021mt5}{Xue et al.2021}
\bibcite{yang2020xlnet}{Yang et al.2020}
\bibcite{zhang2018empirical}{Zhang et al.2018}
\bibcite{zhang2019curriculum}{Zhang et al.2019}
\bibcite{zhang2022opt}{Zhang et al.2022a}
\bibcite{zhang2022automatic}{Zhang et al.2022b}
\bibcite{zhong2021adapting}{Zhong et al.2021a}
\bibcite{zhong2021factual}{Zhong et al.2021b}
\bibcite{zhou2023least}{Zhou et al.2023a}
\bibcite{zhou2023large}{Zhou et al.2023b}
\@writefile{toc}{\contentsline {section}{\numberline {A}Flores200 in- and out- pretrain results}{17}{appendix.A}\protected@file@percent }
\gdef \@abspage@last{17}
