\relax 
\citation{barrault-etal-2020-findings}
\citation{savenkov-lopez-2022-state}
\citation{luong-manning-2015-stanford}
\citation{mccloskey-cohen-1989-catastrophic}
\citation{thompson-etal-2019-overcoming}
\citation{gu-feng-2020-investigating}
\citation{papineni-etal-2002-bleu}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{}\protected@file@percent }
\newlabel{sec:introduction}{{1}{2}{}{section.1}{}}
\citation{aharoni-goldberg-2020-unsupervised}
\citation{zhang-etal-2019-curriculum}
\citation{sharaf-etal-2020-meta}
\citation{saunders-2022-domain}
\citation{barone-etal-2017-regularization}
\citation{shao-feng-2022-overcoming}
\citation{chu-etal-2017-empirical}
\citation{xu-etal-2019-lexical}
\citation{thompson-etal-2019-overcoming}
\citation{gu-feng-2020-investigating}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Related work}{3}{}\protected@file@percent }
\newlabel{sec:related_work}{{1.1}{3}{}{subsection.1.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}What does adapted NMT forget?}{3}{}\protected@file@percent }
\newlabel{sec:what_forgets}{{2}{3}{}{section.2}{}}
\citation{van-der-wees-etal-2015-whats}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Measuring vocabulary-shift forgetting}{4}{}\protected@file@percent }
\newlabel{sec:vocab_shift}{{2.1}{4}{}{subsection.2.1}{}}
\newlabel{eq:forgetgenuse}{{1}{4}{}{equation.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Intentionally triggering forgetting: Lower quality and detrimental vocabulary shift}{4}{}\protected@file@percent }
\newlabel{sec:trigger_forgetting}{{2.2}{4}{}{subsection.2.2}{}}
\citation{cettolo-etal-2012-wit3}
\citation{neubig-2011-kyoto}
\citation{rikters-etal-2019-designing}
\citation{rei-etal-2020-comet}
\citation{hasler-etal-2021-improving}
\citation{dyer-etal-2013-simple}
\citation{bojanowski-etal-2017-enriching}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Which tokens are forgotten, and what replaces them?}{5}{}\protected@file@percent }
\newlabel{sec:token_replacements}{{2.3}{5}{}{subsection.2.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Segment counts and absolute generic model BLEU and COMET on the generic domain test sets and on each in-domain test set.}}{6}{}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:datasets}{{1}{6}{}{table.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Measuring forgetting on generic test sets for de-en and en-ja.}}{6}{}\protected@file@percent }
\newlabel{tab:forgetting_metrics}{{2}{6}{}{table.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces High $ForgetGenUse$ tokens for de-en domains---counts are for that token in the in-domain adaptation dataset. Left columns: Output from generic model. Right columns: Most frequent aligned replacements post-adaptation.}}{7}{}\protected@file@percent }
\newlabel{tab:token_replacements}{{3}{7}{}{table.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Out-of-domain tokens are forgotten more}{7}{}\protected@file@percent }
\newlabel{sec:ood_forgetting}{{2.4}{7}{}{subsection.2.4}{}}
\citation{van-der-wees-etal-2015-whats}
\citation{saunders-2022-domain}
\citation{sharaf-etal-2020-meta}
\citation{pham-etal-2020-study}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Calculating $ForgetGenUse$ over tokens that are out-of-domain (OOD) vs in-domain (ID) for each domain.}}{8}{}\protected@file@percent }
\newlabel{tab:ood_forgetting}{{4}{8}{}{table.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Why does forgetting vary by domain?}{8}{}\protected@file@percent }
\newlabel{sec:why_forgetting}{{3}{8}{}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Controlling for dataset size}{8}{}\protected@file@percent }
\newlabel{sec:dataset_size}{{3.1}{8}{}{subsection.3.1}{}}
\citation{varis-bojar-2021-sequence}
\citation{wan-etal-2022-challenges}
\citation{saunders-byrne-2020-addressing}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Forgetting when adapting on subsampled (-s) domains. All de-en sets except Kor, and all en-ja except BSD, subsampled randomly to approximately the same token count as Kor/BSD respectively.}}{9}{}\protected@file@percent }
\newlabel{tab:subsampled_forgetting}{{5}{9}{}{table.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Controlling segment length and quality}{9}{}\protected@file@percent }
\newlabel{sec:segment_length}{{3.2}{9}{}{subsection.3.2}{}}
\citation{lin-1991-divergence}
\citation{chu-etal-2017-empirical}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Forgetting on generic sets, adapting on subsampled datasets. We sample randomly (-s) or sample the shortest (-ss) lines by source plus target token count. Sub-ssf pre-filters the shortest lines using LASER.}}{10}{}\protected@file@percent }
\newlabel{tab:length_quality}{{6}{10}{}{table.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Corpus-level score domain heuristics}{10}{}\protected@file@percent }
\newlabel{sec:corpus_heuristics}{{3.3}{10}{}{subsection.3.3}{}}
\citation{haque-etal-2020-terminology}
\citation{hasler-etal-2021-improving}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Corpus-level score domain heuristics, with forgetting measures for reference. Generic NLL and vocab JSD: closer to 0 is more similar to generic. Final lines: vocab coverage for downsampled domains of Table~\ref {tab:subsampled_forgetting}.}}{11}{}\protected@file@percent }
\newlabel{tab:heuristics}{{7}{11}{}{table.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Understanding generic data mix-in}{11}{}\protected@file@percent }
\newlabel{sec:mix_in}{{4}{11}{}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Less than 10\% of the mix-in data can mitigate 80\% of the forgetting}{11}{}\protected@file@percent }
\newlabel{sec:minimal_mix_in}{{4.1}{11}{}{subsection.4.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces Number of generic mix-in lines for each strategy. Random 1:1 is by definition the same size as the in-domain dataset, and Minimal Mix-in is often far smaller.}}{12}{}\protected@file@percent }
\newlabel{tab:mix_in_sizes}{{8}{12}{}{table.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Minimal mix-in, better in-domain scores}{12}{}\protected@file@percent }
\newlabel{sec:in_domain_scores}{{4.2}{12}{}{subsection.4.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {9}{\ignorespaces Forgetting metrics on generic test sets, varying the mix-in dataset when fine-tuning for 20K iterations in each case. Lower is better for all metrics. Negative scores indicate improvement.}}{13}{}\protected@file@percent }
\newlabel{tab:mix_in_forgetting}{{9}{13}{}{table.9}{}}
\@writefile{lot}{\contentsline {table}{\numberline {10}{\ignorespaces $\Delta $BLEU and $\Delta $COMET on in-domain test sets for the same experiments as in Table~\ref {tab:mix_in_forgetting}. Higher is better.}}{13}{}\protected@file@percent }
\newlabel{tab:in_domain_scores}{{10}{13}{}{table.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusions}{13}{}\protected@file@percent }
\newlabel{sec:conclusions}{{5}{13}{}{section.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Limitations}{13}{}\protected@file@percent }
\newlabel{sec:limitations}{{6}{13}{}{section.6}{}}
\citation{zhang-etal-2023-machine}
\bibstyle{acl_natbib}
\bibdata{refs}
\citation{vaswani-etal-2018-tensor2tensor}
\citation{sennrich-etal-2016-neural}
\citation{kocmi-etal-2023-findings}
\citation{kocmi-etal-2022-findings}
\citation{morishita-etal-2022-jparacrawl}
\citation{post-2018-call}
\@writefile{toc}{\contentsline {section}{\numberline {A}Experimental setup}{14}{}\protected@file@percent }
\newlabel{sec:experimental_setup}{{A}{14}{}{section.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {11}{\ignorespaces Pre-trained model specifications}}{14}{}\protected@file@percent }
\newlabel{tab:model_specs}{{11}{14}{}{table.11}{}}
\gdef \@abspage@last{14}
