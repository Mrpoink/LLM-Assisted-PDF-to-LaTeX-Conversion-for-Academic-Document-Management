\relax 
\citation{hu2021lora}
\citation{devlin2019bert}
\citation{liu2019roberta}
\citation{wang2019glue}
\citation{hu2021lora}
\citation{sun2017meprop}
\citation{touvron2023llama}
\citation{kopf2023openassistant}
\citation{lialin2023scaling}
\citation{pfeiffer2020adapterhub}
\citation{houlsby2019parameter}
\citation{hu2021lora}
\citation{zaken2021bitfit}
\citation{sun2017meprop}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Number of parameters for different layers in models based on the Transformer.}}{2}{}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:params}{{1}{2}{}{table.1}{}}
\citation{cichocki2016tensor}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Method}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Preliminary Phase: Finding Transition Matrices}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Signal Propagation in SparseGradLinear Layer}{3}{}\protected@file@percent }
\citation{wang2019glue}
\citation{kopf2023openassistant}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Correspondence of variables in Torch Autograd for a regular Linear layer and SparseGradLinear.}}{4}{}\protected@file@percent }
\newlabel{tab:autograd}{{2}{4}{}{table.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Sparse-by-Dense Matrix Multiplication}{4}{}\protected@file@percent }
\newlabel{eq:sparse_dense}{{1}{4}{}{equation.1}{}}
\newlabel{eq:coo_restore}{{2}{4}{}{equation.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Time and Memory Consumption per Training Iteration}{4}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Experiments}{4}{}\protected@file@percent }
\citation{liu2019roberta}
\citation{akiba2019optuna}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Training speed and memory requirements averaged on the GLUE benchmark. The last two rows of the Table report the results for the SparseGradmethod with Sparse-by-Dense (SD) and Regular (Reg) matrix multiplication, respectively.}}{5}{}\protected@file@percent }
\newlabel{tab:perf}{{3}{5}{}{table.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Natural Language Understanding with BERT and RoBERTa}{5}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Average scores over the GLUE benchmark for BERT and RoBERTa$_{\text  {base}}$ models.}}{5}{}\protected@file@percent }
\newlabel{tab:glue_base}{{4}{5}{}{table.4}{}}
\citation{touvron2023llama}
\citation{kopf2023openassistant}
\citation{zheng2023judging}
\citation{zheng2023judging}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Comparative results of RoBERTa$_{\text  {large}}$ for 20-epoch task-specific fine-tuning.}}{6}{}\protected@file@percent }
\newlabel{tab:glue_large}{{5}{6}{}{table.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Conversations with LLaMa-2}{6}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Comparative results for LLaMa-2 on the OpenAssistant dataset.}}{6}{}\protected@file@percent }
\newlabel{tab:llama2}{{6}{6}{}{table.6}{}}
\citation{cichocki2016tensor}
\bibstyle{acl_natbib}
\bibdata{refs}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{7}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Acknowledgements}{7}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8}Limitations}{7}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {9}Ethics Statement}{7}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {A}Appendix A}{7}{}\protected@file@percent }
\newlabel{app:perf_details}{{A}{7}{}{section.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B}Appendix B}{7}{}\protected@file@percent }
\newlabel{app:glue_results}{{B}{7}{}{section.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {C}Appendix C}{7}{}\protected@file@percent }
\newlabel{app:sparsity_ablation}{{C}{7}{}{section.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces The training step execution speed, measured in steps per second (where a higher value indicates faster execution), is reported for the RoBERTa$_{\text  {base}}$ model. The last two rows describe the SparseGradMethod with Sparse-by-Dense multiplication and with Regular matrix multiplication.}}{8}{}\protected@file@percent }
\newlabel{tab:speed_appendix}{{7}{8}{}{table.7}{}}
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces Peak memory measurement in MB for training loop for the model RoBERTa$_{\text  {base}}$.}}{8}{}\protected@file@percent }
\newlabel{tab:memory_appendix}{{8}{8}{}{table.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {D}Appendix D}{8}{}\protected@file@percent }
\newlabel{app:hyperparams}{{D}{8}{}{section.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {E}Appendix E}{8}{}\protected@file@percent }
\newlabel{app:examples}{{E}{8}{}{section.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {9}{\ignorespaces Comparative results of BERT model for 20-epoch task-specific fine-tuning.}}{9}{}\protected@file@percent }
\newlabel{tab:bert_results}{{9}{9}{}{table.9}{}}
\@writefile{lot}{\contentsline {table}{\numberline {10}{\ignorespaces Comparative results of RoBERTa for 20-epoch task-specific fine-tuning.}}{9}{}\protected@file@percent }
\newlabel{tab:roberta_results}{{10}{9}{}{table.10}{}}
\@writefile{lot}{\contentsline {table}{\numberline {11}{\ignorespaces GLUE score as a function of the weight gradient sparsity in BERT}}{10}{}\protected@file@percent }
\newlabel{tab:bert_sparsity}{{11}{10}{}{table.11}{}}
\@writefile{lot}{\contentsline {table}{\numberline {12}{\ignorespaces GLUE score as a function of the weight gradient sparsity in RoBERTa}}{10}{}\protected@file@percent }
\newlabel{tab:roberta_sparsity}{{12}{10}{}{table.12}{}}
\@writefile{lot}{\contentsline {table}{\numberline {13}{\ignorespaces Best training parameters on GLUE benchmark for BERT model.}}{10}{}\protected@file@percent }
\newlabel{tab:bert_params}{{13}{10}{}{table.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The first row illustrates signal propagation in the original Linear Layer, while the second row illustrates propagation with the proposed SparseGradLinear layer.}}{11}{}\protected@file@percent }
\newlabel{fig:signal_prop}{{1}{11}{}{figure.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Gradients on the 5-th BERT MLP: $U \frac  {\partial L}{\partial W^T} V^T$ (right) is more sparse than the original $\frac  {\partial L}{\partial W^T}$ (left).}}{12}{}\protected@file@percent }
\newlabel{fig:grad_sparsity}{{2}{12}{}{figure.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Strided structure of $\frac  {\partial L}{\partial \tilde  {Y}}$ (left) and visualizations of \% nonzero elements in $\frac  {\partial L}{\partial \tilde  {Y}}$ throughout training (right).}}{12}{}\protected@file@percent }
\newlabel{fig:strided}{{3}{12}{}{figure.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {14}{\ignorespaces Best training parameters on GLUE benchmark for RoBERTa model.}}{12}{}\protected@file@percent }
\newlabel{tab:roberta_params}{{14}{12}{}{table.14}{}}
\@writefile{lot}{\contentsline {table}{\numberline {15}{\ignorespaces Best training parameters on GLUE benchmark for RoBERTa-large model.}}{13}{}\protected@file@percent }
\newlabel{tab:roberta_large_params}{{15}{13}{}{table.15}{}}
\gdef \@abspage@last{13}
