\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{fan2021pretraining}
\citation{guo2022semantic}
\citation{lin2021pretrained}
\citation{karpukhin2020dense}
\citation{lee2020learning}
\citation{zhu2021adaptive}
\citation{gao2022neural}
\citation{yu2021few}
\citation{devlin2018bert}
\citation{liu2019roberta}
\citation{xiong2020approximate}
\citation{lu2021less}
\citation{hofstatter2021efficiently}
\citation{gao2021unsupervised}
\citation{ren2021rocketqav2}
\citation{ma2022pre}
\citation{liu2022retromae}
\citation{wu2022contextual}
\citation{wang2022simlm}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}\protected@file@percent }
\citation{gao2021unsupervised}
\citation{wu2022contextual}
\citation{nguyen2016ms}
\citation{gao2022unsupervised}
\citation{nogueira2019doc2query}
\citation{dai2019context}
\citation{mao2020generation}
\citation{bai2020sparterm}
\citation{formal2021spladev2}
\citation{formal2021splade}
\citation{mallia2021learning}
\citation{shen2022lexmae}
\citation{nogueira2019doc2query}
\citation{nguyen2016ms}
\citation{craswell2020overview}
\citation{craswell2020overview2020}
\citation{thakur2021beir}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces An example of low-relevance passages within a document from the MS-MARCO corpus. The two passages are weakly correlated in content.}}{3}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:weak_corr}{{1}{3}{An example of low-relevance passages within a document from the MS-MARCO corpus. The two passages are weakly correlated in content}{figure.caption.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces A comparison of context-supervised pre-training and query-as-context pre-training.}}{4}{figure.caption.3}\protected@file@percent }
\newlabel{fig:method}{{2}{4}{A comparison of context-supervised pre-training and query-as-context pre-training}{figure.caption.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Preliminary: Context-supervised Pre-training}{4}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Pre-training Corpus}{4}{subsection.2.1}\protected@file@percent }
\newlabel{eq:corpus}{{1}{4}{Pre-training Corpus}{equation.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Masked Language Modeling (MLM)}{4}{subsection.2.2}\protected@file@percent }
\newlabel{eq:passage}{{2}{4}{Masked Language Modeling (MLM)}{equation.2}{}}
\newlabel{eq:hidden}{{3}{4}{Masked Language Modeling (MLM)}{equation.3}{}}
\newlabel{eq:mlm}{{4}{4}{Masked Language Modeling (MLM)}{equation.4}{}}
\citation{gao2021unsupervised}
\citation{gao2021unsupervised}
\citation{wu2022contextual}
\citation{wu2022contextual}
\citation{nogueira2019doc2query}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}coCondenser}{5}{subsection.2.3}\protected@file@percent }
\newlabel{eq:contrastive}{{5}{5}{coCondenser}{equation.5}{}}
\newlabel{eq:aux_mlm}{{6}{5}{coCondenser}{equation.6}{}}
\newlabel{eq:cocondenser_loss}{{7}{5}{coCondenser}{equation.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}CoT-MAE}{5}{subsection.2.4}\protected@file@percent }
\newlabel{eq:decoder_input}{{8}{5}{CoT-MAE}{equation.8}{}}
\newlabel{eq:decoder_hidden}{{9}{5}{CoT-MAE}{equation.9}{}}
\newlabel{eq:ctx_mlm}{{10}{5}{CoT-MAE}{equation.10}{}}
\newlabel{eq:cotmae_loss}{{11}{5}{CoT-MAE}{equation.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Query-as-context Pre-training}{5}{section.3}\protected@file@percent }
\citation{gao2021unsupervised}
\citation{wu2022contextual}
\citation{gao2022tevatron}
\citation{thakur2021beir}
\citation{gao2021unsupervised}
\citation{wu2022contextual}
\citation{wu2022contextual}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Illustration of the fine-tuning pipeline. The query-as-context pre-trained model is used to initialize the dual-encoder retrievers.}}{6}{figure.caption.4}\protected@file@percent }
\newlabel{fig:fine_tune}{{3}{6}{Illustration of the fine-tuning pipeline. The query-as-context pre-trained model is used to initialize the dual-encoder retrievers}{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Pre-training}{6}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Fine-tuning}{6}{subsection.3.2}\protected@file@percent }
\newlabel{eq:finetune_loss}{{12}{6}{Fine-tuning}{equation.12}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experiment}{6}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Pre-training}{6}{subsection.4.1}\protected@file@percent }
\citation{gao2021unsupervised}
\citation{gao2021condenser}
\citation{nguyen2016ms}
\citation{craswell2020overview}
\citation{craswell2020overview2020}
\citation{nguyen2016ms}
\citation{gao2021unsupervised}
\citation{wu2022contextual}
\citation{craswell2020overview}
\citation{craswell2020overview2020}
\citation{gao2021unsupervised}
\citation{wu2022contextual}
\citation{gao2022tevatron}
\citation{ren2021rocketqav2}
\citation{santhanam2021colbertv2}
\citation{khattab2020colbert}
\citation{qu2020rocketqa}
\citation{nogueira2019doc2query}
\citation{dai2019context}
\citation{mao2020generation}
\citation{gao2021unsupervised}
\citation{liu2022retromae}
\citation{ren2021rocketqav2}
\citation{ma2022pre}
\citation{xiong2020approximate}
\citation{lu2021less}
\citation{hofstatter2021efficiently}
\citation{liu2022retromae}
\citation{wang2022simlm}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Fine-tuning}{7}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Baselines}{7}{subsection.4.3}\protected@file@percent }
\citation{gao2021unsupervised}
\citation{wu2022contextual}
\citation{dai2019context}
\citation{nogueira2019doc2query}
\citation{xiong2020approximate}
\citation{lu2021less}
\citation{hofstatter2021efficiently}
\citation{gao2021coil}
\citation{khattab2020colbert}
\citation{ma2022pre}
\citation{gao2021condenser}
\citation{qu2020rocketqa}
\citation{ren2021pair}
\citation{wang2022simlm}
\citation{liu2022retromae}
\citation{zhang2022led}
\citation{gao2021unsupervised}
\citation{wu2022contextual}
\citation{thakur2021beir}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Main Results}{8}{subsection.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Out-of-domain Evaluation}{8}{subsection.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Analyses}{8}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Impact of Generated Query Number}{8}{subsection.5.1}\protected@file@percent }
\citation{xiong2020approximate}
\citation{zhan2021optimizing}
\citation{khattab2020colbert}
\citation{hofstatter2021efficiently}
\citation{lin2021in}
\citation{santhanam2021colbertv2}
\citation{qu2020rocketqa}
\citation{ren2021rocketqav2}
\citation{zhang2022adversarial}
\citation{zhang2022hlatr}
\citation{lu2021less}
\citation{gao2021condenser}
\citation{liu2022retromae}
\citation{zhou2022master}
\citation{chang2020pretraining}
\citation{gao2021unsupervised}
\citation{ma2022pre}
\citation{gao2021unsupervised}
\citation{wu2022contextual}
\citation{gao2021unsupervised}
\citation{wu2022contextual}
\citation{nogueira2019doc2query}
\citation{mallia2021learning}
\citation{li2022learning}
\citation{ma2020zero}
\citation{wang2021gpl}
\citation{li2022learning}
\citation{li2022learning}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Impact of Mixed Context}{9}{subsection.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Related Works}{9}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusions}{9}{section.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8}Limitations}{9}{section.8}\protected@file@percent }
\bibstyle{acl_natbib}
\bibcite{bai2020sparterm}{Bai et al.2020}
\bibcite{chang2020pretraining}{Chang et al.2020}
\bibcite{craswell2020overview}{Craswell et al.2020a}
\bibcite{craswell2020overview2020}{Craswell et al.2020b}
\bibcite{dai2019context}{Dai and Callan2019}
\bibcite{devlin2018bert}{Devlin et al.2018}
\bibcite{fan2021pretraining}{Fan et al.2021}
\bibcite{formal2021spladev2}{Formal et al.2021a}
\bibcite{formal2021splade}{Formal et al.2021b}
\bibcite{gao2021condenser}{Gao and Callan2021a}
\bibcite{gao2021unsupervised}{Gao and Callan2021b}
\bibcite{gao2022unsupervised}{Gao and Callan2022}
\bibcite{gao2021coil}{Gao et al.2021}
\bibcite{gao2022neural}{Gao et al.2022a}
\bibcite{gao2022tevatron}{Gao et al.2022b}
\bibcite{guo2022semantic}{Guo et al.2022}
\bibcite{hofstatter2021efficiently}{Hofst√§tter et al.2021}
\bibcite{karpukhin2020dense}{Karpukhin et al.2020}
\bibcite{khattab2020colbert}{Khattab and Zaharia2020}
\bibcite{lee2020learning}{Lee et al.2020}
\bibcite{li2022learning}{Li et al.2022}
\bibcite{lin2021pretrained}{Lin et al.2021a}
\bibcite{lin2021in}{Lin et al.2021b}
\bibcite{liu2019roberta}{Liu et al.2019}
\bibcite{liu2022retromae}{Liu and Shao2022}
\bibcite{lu2020nprinc}{Lu et al.2020}
\bibcite{lu2021less}{Lu et al.2021}
\bibcite{ma2020zero}{Ma et al.2020}
\bibcite{ma2022pre}{Ma et al.2022}
\bibcite{mallia2021learning}{Mallia et al.2021}
\bibcite{mao2020generation}{Mao et al.2020}
\bibcite{nguyen2016ms}{Nguyen et al.2016}
\bibcite{nogueira2019doc2query}{Nogueira and Lin2019}
\bibcite{qu2020rocketqa}{Qu et al.2020}
\bibcite{ren2021pair}{Ren et al.2021a}
\bibcite{ren2021rocketqav2}{Ren et al.2021b}
\bibcite{santhanam2021colbertv2}{Santhanam et al.2021}
\bibcite{shen2022lexmae}{Shen et al.2022}
\bibcite{thakur2021beir}{Thakur et al.2021}
\bibcite{wang2021gpl}{Wang et al.2021}
\bibcite{wang2022simlm}{Wang et al.2022}
\bibcite{wu2022contextual}{Wu et al.2022}
\bibcite{xiong2020approximate}{Xiong et al.2020}
\bibcite{yu2021few}{Yu et al.2021}
\bibcite{zhan2021optimizing}{Zhan et al.2021}
\bibcite{zhang2021adversarial}{Zhang et al.2021}
\bibcite{zhang2022led}{Zhang et al.2022a}
\bibcite{zhang2022hlatr}{Zhang et al.2022b}
\bibcite{zhou2022master}{Zhou et al.2022}
\bibcite{zhu2021adaptive}{Zhu et al.2021}
\citation{gao2022unsupervised}
\citation{nogueira2019doc2query}
\@writefile{toc}{\contentsline {section}{\numberline {A}Statistically Analysis of Weakly Correlated Passages}{13}{appendix.A}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Main results on MS-MARCO passage ranking and TREC DL datasets. $\dagger $ denotes our reproduction using publicly available codes. The score that is better in comparison is marked in bold.}}{15}{table.caption.5}\protected@file@percent }
\newlabel{tab:main_results}{{1}{15}{Main results on MS-MARCO passage ranking and TREC DL datasets. $\dagger $ denotes our reproduction using publicly available codes. The score that is better in comparison is marked in bold}{table.caption.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Out-of-domain evaluation on BEIR benchmark. The score that is better in comparison is marked in bold.}}{16}{table.caption.6}\protected@file@percent }
\newlabel{tab:beir}{{2}{16}{Out-of-domain evaluation on BEIR benchmark. The score that is better in comparison is marked in bold}{table.caption.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Impact of the number of generated queries. The score that is better in comparison is marked in bold.}}{16}{table.caption.7}\protected@file@percent }
\newlabel{tab:query_num}{{3}{16}{Impact of the number of generated queries. The score that is better in comparison is marked in bold}{table.caption.7}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Effect of mixing passage-query and passage-passage pairs in pre-training.}}{16}{table.caption.8}\protected@file@percent }
\newlabel{tab:mixed}{{4}{16}{Effect of mixing passage-query and passage-passage pairs in pre-training}{table.caption.8}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Correlation statistics of human annotation results of different contextual pairs, each with 200 pairs. The score that is better in comparison is marked in bold.}}{16}{table.caption.11}\protected@file@percent }
\newlabel{tab:appendix}{{5}{16}{Correlation statistics of human annotation results of different contextual pairs, each with 200 pairs. The score that is better in comparison is marked in bold}{table.caption.11}{}}
\gdef \@abspage@last{16}
