\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{brown2020language}
\citation{shoeybi2020megatron}
\citation{xue2021mt5}
\citation{hoffmann2022training}
\citation{chowdhery2022palm}
\citation{zhang2022opt}
\citation{chung2022scaling}
\citation{workshop2023bloom}
\citation{touvron2023llama}
\citation{xue2021mt5}
\citation{nllb2022}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}\protected@file@percent }
\citation{sennrich2016improving}
\citation{fadaee2017data}
\citation{shu2019meta}
\citation{ren2019learning}
\citation{gu2018meta}
\citation{kocmi2017curriculum}
\citation{zhang2018empirical}
\citation{platanios2019competence}
\citation{zhang2019curriculum}
\citation{nllb2022}
\citation{brown2020language}
\citation{touvron2023llama}
\citation{petroni2019language}
\citation{radford2018improving}
\citation{radford2019language}
\citation{dong2019unified}
\citation{devlin2019bert}
\citation{lewis2019bart}
\citation{sun2019ernie}
\citation{liu2019roberta}
\citation{clark2020electra}
\citation{yang2020xlnet}
\citation{raffel2020exploring}
\citation{gao2021making}
\citation{schick2021exploiting}
\citation{liu2021pretrain}
\citation{xue2021mt5}
\citation{he2021deberta}
\citation{taori2023stanford}
\citation{shin2020autoprompt}
\citation{schick2020automatically}
\citation{li2021prefix}
\citation{hambardzumyan2021warp}
\citation{lester2021power}
\citation{zhong2021adapting}
\citation{wallace2021bertese}
\citation{haviv2021bertese}
\citation{jiang2020can}
\citation{chen2022knowprompt}
\citation{qin2021learning}
\citation{liu2021makes}
\citation{han2021ptr}
\citation{zhong2021factual}
\citation{lu2022fantastically}
\citation{ben2022pada}
\citation{wang2022iteratively}
\citation{zhou2023large}
\citation{chung2022scaling}
\citation{wei2023chain}
\citation{wang2023self}
\citation{li2023opt}
\citation{wei2022finetuned}
\citation{wang2022super}
\citation{gu2023pretraining}
\citation{wang2023selfinstr}
\citation{zhang2022automatic}
\citation{press2023measuring}
\citation{zhou2023least}
\citation{xue2021mt5}
\citation{papineni2002bleu}
\citation{loshchilov2019decoupled}
\citation{xue2021mt5}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related work}{3}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Experiments on a difficult single language pair translation task}{3}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Setup}{3}{subsection.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Task reformulations. Baseline: a direct translation pair. POSE: append a prefix of the target translation to the input translation. ParSE: append a parallel English translation to the input translation. MiPS: append a different parallel translation to both the input and output.}}{4}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:reformulations}{{1}{4}{Task reformulations. Baseline: a direct translation pair. POSE: append a prefix of the target translation to the input translation. ParSE: append a parallel English translation to the input translation. MiPS: append a different parallel translation to both the input and output}{figure.caption.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces POSE reformulation applied to the tib2eng translation task. Changes are highlighted in red.}}{4}{figure.caption.3}\protected@file@percent }
\newlabel{fig:pose_example}{{2}{4}{POSE reformulation applied to the tib2eng translation task. Changes are highlighted in red}{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Motivation}{4}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Modulating task difficulty}{4}{subsection.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Optimizing the curriculum}{4}{subsection.3.4}\protected@file@percent }
\citation{nllb2022}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Task difficulty experiment results on mT5 600M.}}{5}{table.caption.4}\protected@file@percent }
\newlabel{tab:difficulty}{{1}{5}{Task difficulty experiment results on mT5 600M}{table.caption.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Curriculum experiment results on mT5 600M.}}{5}{table.caption.5}\protected@file@percent }
\newlabel{tab:curriculum}{{2}{5}{Curriculum experiment results on mT5 600M}{table.caption.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Modulating scaffold substring}{5}{subsection.3.5}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Prefix+suffix experiment results on mT5 600M.}}{5}{table.caption.6}\protected@file@percent }
\newlabel{tab:substring}{{3}{5}{Prefix+suffix experiment results on mT5 600M}{table.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6}Matching the pretraining task}{5}{subsection.3.6}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Matching pretraining experiment results on mT5 600M with masking.}}{5}{table.caption.7}\protected@file@percent }
\newlabel{tab:masking}{{4}{5}{Matching pretraining experiment results on mT5 600M with masking}{table.caption.7}{}}
\citation{nllb2022}
\citation{goyal2021flores}
\citation{guzman2019flores}
\citation{popovic2015chrf}
\citation{nllb2022}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7}Final results and comparison to state-of-the-art}{6}{subsection.3.7}\protected@file@percent }
\newlabel{sec:final_results}{{3.7}{6}{Final results and comparison to state-of-the-art}{subsection.3.7}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Main results on the tib2eng translation task for mT5. Values shown are test set BLEU scores. The difference shown is the improvement gained by using the input finetuning reformulations. The NLLB column is the test set BLEU score for the corresponding sized NLLB model.}}{6}{table.caption.8}\protected@file@percent }
\newlabel{tab:tib2eng_results}{{5}{6}{Main results on the tib2eng translation task for mT5. Values shown are test set BLEU scores. The difference shown is the improvement gained by using the input finetuning reformulations. The NLLB column is the test set BLEU score for the corresponding sized NLLB model}{table.caption.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experiments on a massively multilingual translation task}{6}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Setup}{6}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Designing task reformulations}{6}{subsection.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Tib2eng translation task reformulation experiment results. These results compare the mT5 baseline (blue), mT5 POSE (orange), and the NLLB (green) experimental configurations. The solid lines and shaded areas are the mean and variance over learning rates, respectively. Left: 600M. Center: 1B. Right: 3B.}}{7}{figure.caption.9}\protected@file@percent }
\newlabel{fig:tib2eng_curves}{{3}{7}{Tib2eng translation task reformulation experiment results. These results compare the mT5 baseline (blue), mT5 POSE (orange), and the NLLB (green) experimental configurations. The solid lines and shaded areas are the mean and variance over learning rates, respectively. Left: 600M. Center: 1B. Right: 3B}{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Examples of the ParSE and MiPS input reformulations applied to the Flores200 translation task. The changes to the original input are highlighted in red.}}{7}{figure.caption.10}\protected@file@percent }
\newlabel{fig:flores_examples}{{4}{7}{Examples of the ParSE and MiPS input reformulations applied to the Flores200 translation task. The changes to the original input are highlighted in red}{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Results}{7}{subsection.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Analysis on mT5's pretraining dataset and Flores200}{7}{subsection.4.4}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Results on the Flores200 translation task for mT5. Values shown are test set chrF++ scores. The NLLB column is the task performance of a corresponding size NLLB model. For the NLLB score, we use the 200 xx-yy chrF++ scores listed here.}}{7}{table.caption.11}\protected@file@percent }
\newlabel{tab:flores_results}{{6}{7}{Results on the Flores200 translation task for mT5. Values shown are test set chrF++ scores. The NLLB column is the task performance of a corresponding size NLLB model. For the NLLB score, we use the 200 xx-yy chrF++ scores listed here}{table.caption.11}{}}
\citation{lample2019cross}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Flores200 translation task reformulation experiment results. These results compare the mT5 baseline (blue), mT5 ParSE (orange), and mT5 MiPS (green) experimental configurations. The solid lines and shaded areas are the mean and variance over learning rates, respectively. Left: 600M. Center: 1B. Right: 3B.}}{8}{figure.caption.12}\protected@file@percent }
\newlabel{fig:flores_curves}{{5}{8}{Flores200 translation task reformulation experiment results. These results compare the mT5 baseline (blue), mT5 ParSE (orange), and mT5 MiPS (green) experimental configurations. The solid lines and shaded areas are the mean and variance over learning rates, respectively. Left: 600M. Center: 1B. Right: 3B}{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Pretraining dataset sizes and Flores200 finetuning performance. The first row represents translation from a language in the pretraining set into other languages, including those not in the pretraining set. The second row represents translation from other languages into a language present in the pretraining set. Each dot represents one language and the value in the graph represents the corresponding chrF++ test set score for that language and model. Points shown only cover languages present in the mT5 pretraining set. The point corresponding to English is the rightmost point on all the graphs. Dataset sizes are calculated using the number of examples of each language present in the mC4 dataset. Dataset sizes range from 100k to 1B examples.}}{8}{figure.caption.13}\protected@file@percent }
\newlabel{fig:pretrain_analysis}{{6}{8}{Pretraining dataset sizes and Flores200 finetuning performance. The first row represents translation from a language in the pretraining set into other languages, including those not in the pretraining set. The second row represents translation from other languages into a language present in the pretraining set. Each dot represents one language and the value in the graph represents the corresponding chrF++ test set score for that language and model. Points shown only cover languages present in the mT5 pretraining set. The point corresponding to English is the rightmost point on all the graphs. Dataset sizes are calculated using the number of examples of each language present in the mC4 dataset. Dataset sizes range from 100k to 1B examples}{figure.caption.13}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{8}{section.5}\protected@file@percent }
\bibstyle{acl_natbib}
\bibcite{brown2020language}{Brown et al.2020}
\bibcite{chowdhery2022palm}{Chowdhery et al.2022}
\bibcite{chung2022scaling}{Chung et al.2022}
\bibcite{clark2020electra}{Clark et al.2020}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Breakdown of model and setup performance over different splits of the Flores200 dataset. ``In'' refers to a language that was found in the mT5 pretraining dataset and ``out'' refers to a language that was not. ``To Eng'' and ``From Eng'' is referred to as xx-eng and eng-xx in some other papers, respectively. Notably, the proposed techniques improve ``To Eng'' performance up to 4.2 chrF++ and ``From Eng'' performance up to 9.4 chrF++, in the 600M case. We hypothesize this difference in improvement is due to the finetuning task including more English examples in the input, helping with downstream English translations as well as other language translations.}}{9}{table.caption.14}\protected@file@percent }
\newlabel{tab:flores_breakdown}{{7}{9}{Breakdown of model and setup performance over different splits of the Flores200 dataset. ``In'' refers to a language that was found in the mT5 pretraining dataset and ``out'' refers to a language that was not. ``To Eng'' and ``From Eng'' is referred to as xx-eng and eng-xx in some other papers, respectively. Notably, the proposed techniques improve ``To Eng'' performance up to 4.2 chrF++ and ``From Eng'' performance up to 9.4 chrF++, in the 600M case. We hypothesize this difference in improvement is due to the finetuning task including more English examples in the input, helping with downstream English translations as well as other language translations}{table.caption.14}{}}
\bibcite{devlin2019bert}{Devlin et al.2019}
\bibcite{dong2019unified}{Dong et al.2019}
\bibcite{fadaee2017data}{Fadaee et al.2017}
\bibcite{gao2021making}{Gao et al.2021}
\bibcite{gu2018meta}{Gu et al.2018}
\bibcite{gu2023pretraining}{Gu et al.2023}
\bibcite{guzman2019flores}{Guzm\'an et al.2019}
\bibcite{goyal2021flores}{Goyal et al.2021}
\bibcite{hambardzumyan2021warp}{Hambardzumyan et al.2021}
\bibcite{han2021ptr}{Han et al.2021}
\bibcite{haviv2021bertese}{Haviv et al.2021}
\bibcite{he2021deberta}{He et al.2021}
\bibcite{hoffmann2022training}{Hoffmann et al.2022}
\bibcite{li2023opt}{Iyer et al.2023}
\bibcite{jiang2020can}{Jiang et al.2020}
\bibcite{kocmi2017curriculum}{Kocmi and Bojar2017}
\bibcite{lample2019cross}{Lample and Conneau2019}
\bibcite{lester2021power}{Lester et al.2021}
\bibcite{lewis2019bart}{Lewis et al.2019}
\bibcite{li2021prefix}{Li and Liang2021}
\bibcite{liu2019roberta}{Liu et al.2019}
\bibcite{liu2021makes}{Liu et al.2021}
\bibcite{loshchilov2019decoupled}{Loshchilov and Hutter2019}
\bibcite{lu2022fantastically}{Lu et al.2022}
\bibcite{wei2022finetuned}{Min et al.2022}
\bibcite{nllb2022}{NLLB Team et al.2022}
\bibcite{papineni2002bleu}{Papineni et al.2002}
\bibcite{petroni2019language}{Petroni et al.2019}
\bibcite{platanios2019competence}{Platanios et al.2019}
\bibcite{popovic2015chrf}{Popovi\'c2015}
\bibcite{press2023measuring}{Press et al.2023}
\bibcite{qin2021learning}{Qin and Eisner2021}
\bibcite{raffel2020exploring}{Raffel et al.2020}
\bibcite{radford2018improving}{Radford et al.2018}
\bibcite{radford2019language}{Radford et al.2019}
\bibcite{ren2019learning}{Ren et al.2019}
\bibcite{schick2021exploiting}{Schick and Sch\"utze2021}
\bibcite{schick2020automatically}{Schick et al.2020}
\bibcite{sennrich2016improving}{Sennrich et al.2016}
\bibcite{shin2020autoprompt}{Shin et al.2020}
\bibcite{shoeybi2020megatron}{Shoeybi et al.2020}
\bibcite{shu2019meta}{Shu et al.2019}
\bibcite{sun2019ernie}{Sun et al.2019}
\bibcite{taori2023stanford}{Taori et al.2023}
\bibcite{touvron2023llama}{Touvron et al.2023}
\bibcite{wallace2021bertese}{Wallace et al.2021}
\bibcite{wang2022iteratively}{Wang et al.2022a}
\bibcite{wang2022super}{Wang et al.2022b}
\bibcite{wang2023self}{Wang et al.2023a}
\bibcite{wang2023selfinstr}{Wang et al.2023b}
\bibcite{wei2022finetuned}{Wei et al.2022}
\bibcite{wei2023chain}{Wei et al.2023}
\bibcite{workshop2023bloom}{Workshop2023}
\bibcite{xue2021mt5}{Xue et al.2021}
\bibcite{yang2020xlnet}{Yang et al.2020}
\bibcite{zhang2018empirical}{Zhang et al.2018}
\bibcite{zhang2019curriculum}{Zhang et al.2019}
\bibcite{zhang2022opt}{Zhang et al.2022a}
\bibcite{zhang2022automatic}{Zhang et al.2022b}
\bibcite{zhong2021adapting}{Zhong et al.2021a}
\bibcite{zhong2021factual}{Zhong et al.2021b}
\bibcite{zhou2023least}{Zhou et al.2023a}
\bibcite{zhou2023large}{Zhou et al.2023b}
\@writefile{toc}{\contentsline {section}{\numberline {A}Flores200 in- and out- pretrain results}{14}{appendix.A}\protected@file@percent }
\newlabel{app:flores}{{A}{14}{Flores200 in- and out- pretrain results}{appendix.A}{}}
\gdef \@abspage@last{14}
