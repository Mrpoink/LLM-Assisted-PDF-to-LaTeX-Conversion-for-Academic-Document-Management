\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Background Work}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Character-level Sequence-to-Sequence Tasks}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Transfer Learning}{3}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces The 27 typologically diverse languages (Subsection 4.1) from the 2023 shared task, all of which are investigated in this work. We use some UD Treebanks for our analytical experiments in Subsection 6, the specific treebanks are listed in the final column.}}{4}{}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:languages}{{1}{4}{}{table.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Transfer Learning for Character-level Tasks}{4}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Data Diversity and Multi-task Learning}{4}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Architecture and Training}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Architecture}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Training Tasks}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Training Setups}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Data}{6}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Target-task Data}{6}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Extracted Data}{6}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}External Data}{6}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Experiments}{7}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Experimental Setup}{7}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Results}{7}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}When Does Denoising Hurt MTL?}{7}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The distribution of secondary task gradients between 20\% and 30\% training as in Bingel and S\o {}gaard (2017) for cases in which the target task gradients are $\geq 0$. A negative number indicates the model is still improving upon the secondary task.}}{8}{}\protected@file@percent }
\newlabel{fig:gradients}{{1}{8}{}{figure.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Results}{8}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Training Dynamics in MTL}{8}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion}{9}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8}Future Work}{9}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {9}Limitations}{9}{}\protected@file@percent }
\bibstyle{acl_natbib}
\bibcite{ahmadi2023revisiting}{Ahmadi and Mahmudi2023}
\bibcite{ashby2021results}{Ashby et al.2021}
\bibcite{batsuren2022unimorph}{Batsuren et al.2022}
\bibcite{bingel2017identifying}{Bingel and S\o {}gaard2017}
\bibcite{bjerva2019transductive}{Bjerva et al.2019}
\bibcite{caruana1997multitask}{Caruana1997}
\bibcite{cotterell2018conll}{Cotterell et al.2018}
\bibcite{cotterell2017conll}{Cotterell et al.2017}
\bibcite{cotterell2016sigmorphon}{Cotterell et al.2016}
\bibcite{devlin2019bert}{Devlin et al.2019}
\bibcite{dong2022neural}{Dong et al.2022}
\bibcite{falcon2019pytorch}{Falcon and The PyTorch Lightning team2019}
\bibcite{fifty2021efficiently}{Fifty et al.2021}
\bibcite{goldman2023sigmorphon}{Goldman et al.2023}
\bibcite{kann2016single}{Kann and Sch\"{u}tze2016}
\bibcite{kann2017unlabeled}{Kann and Sch\"{u}tze2017}
\bibcite{kirov2016very}{Kirov et al.2016}
\bibcite{kodner2022sigmorphon}{Kodner et al.2022}
\bibcite{kodner2023reality}{Kodner et al.2023}
\bibcite{krishna2023downstream}{Krishna et al.2023}
\bibcite{lewis2020bart}{Lewis et al.2020}
\bibcite{liu2019roberta}{Liu et al.2019}
\bibcite{luong2016multi}{Luong et al.2016}
\bibcite{martinezalonso2017when}{Mart\'{i}nez Alonso and Plank2017}
\bibcite{muradoglu2022eeny}{Muradoglu and Hulden2022}
\bibcite{nivre2020universal}{Nivre et al.2020}
\bibcite{peters2018deep}{Peters et al.2018}
\bibcite{phang2018sentence}{Phang et al.2018}
\bibcite{pimentel2021sigmorphon}{Pimentel et al.2021}
\bibcite{pruksachatkun2020intermediate}{Pruksachatkun et al.2020}
\bibcite{raffel2019exploring}{Raffel et al.2019}
\bibcite{vaswani2017attention}{Vaswani et al.2017}
\bibcite{vincent2010stacked}{Vincent et al.2010}
\bibcite{virtanen2020scipy}{Virtanen et al.2020}
\bibcite{vylomova2020sigmorphon}{Vylomova et al.2020}
\bibcite{wiemerslage2023investigation}{Wiemerslage et al.2023}
\bibcite{wu2021applying}{Wu et al.2021}
\bibcite{xue2022byt5}{Xue et al.2022}
\bibcite{zeman2023universal}{Zeman et al.2023}
\@writefile{toc}{\contentsline {section}{\numberline {A}Data details}{15}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}Limitations of UniMorph and SIGMORPHON}{15}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}Selection and Sampling}{15}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.3}Preparing Additional Data from UD Treebanks}{16}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {B}Models and Experimental Details}{16}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1}Implementation}{16}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {B.2}Compute and Infrastructure}{16}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {B.3}Reproducibility}{16}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {B.4}Morphological Inflection in Japanese}{16}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {C}Significance Testing}{16}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces The development and test accuracies of the 5 model variants, for all the 27 languages. For each language, the highest development accuracy is underlined and highest test accuracy is bolded.}}{17}{}\protected@file@percent }
\newlabel{tab:results}{{2}{17}{}{table.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Results for our models by language from the experiments with external data, reporting development and test accuracy. For each language, the highest development accuracy is underlined and highest test accuracy is bolded. Note: results for non `-UD' models are identical to Table~\ref {tab:results}.}}{18}{}\protected@file@percent }
\newlabel{tab:external_results}{{3}{18}{}{table.3}{}}
\gdef \@abspage@last{18}
