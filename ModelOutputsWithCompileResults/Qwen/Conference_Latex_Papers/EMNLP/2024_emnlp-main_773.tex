=====FILE: main.tex=====
\documentclass[10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{times}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{array}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{geometry}
\usepackage{natbib}
\usepackage{url}
\usepackage{hyperref}
\hypersetup{colorlinks=true, linkcolor=blue, citecolor=blue, urlcolor=blue}

\geometry{a4paper, margin=1in}

\title{Contribution of Linguistic Typology to Universal Dependency Parsing: An Empirical Investigation}
\author{Ali Basirat \\ Center for Language Technology \\ University of Copenhagen \\ \texttt{alib@hum.ku.dk} \and Navid Baradaran Hemmati \\ Certified Translation Agency No. 1141 \\ Mashhad, Khorasan, Iran \\ \texttt{navidbh@gmail.com}}

\begin{document}

\maketitle

\begin{abstract}
Universal Dependencies (UD) is a global initiative to create a standard annotation for the dependency syntax of human languages. Addressing its deviation from typological principles, this study presents an empirical investigation of a typologically motivated transformation of UD proposed by William Croft. Our findings underscore the significance of the transformations across diverse languages and highlight their advantages and limitations.
\end{abstract}

\section{Introduction}
Universal Dependencies (UD) \citep{Nivre2016,deMarneffe2021} is widely used as a standard for morphosyntactic annotations. Ever since its initial release in October 2014, however, the scheme has been criticized with respect to its adherence to typological principles \citep{Choi2021,Kanayama2020}. \citet{Croft2017} cite \citet{Nivre2015}'s argument that the NLP community has traditionally had little concern for language typology and linguistic universals. They maintain that the UD initiative, akin to prior parsing and tagging scheme proposals aimed at a universal description of the world's languages, fails to refer explicitly to the extensive typological literature on universals, which accounts for the language-specific annotations that it provides besides those that are actually universal in typological terms. Therefore, they continue to propose their own dependency annotation scheme, claiming to represent cross-linguistic variations more comprehensively based on the following four design principles.

The first principle distinguishes universal constructions from language-specific strategies and favors classification based on the former. For example, a copula strategy, used in English to realize a predicate nominal construction, may be represented by a different strategy in another language, so the separate relation in UD for copulas is absent in \citet{Croft2017}'s revision. The second principle emphasizes the use of the same labels for the same functions realized syntactically and morphologically.\footnote{In UD, the \texttt{case} label replaces earlier dependency relations for marking prepositional phrases, indicating a syntactic strategy, similar to how it represents a morphological strategy.} The third principle prioritizes information packaging over lexical semantics and contributes significantly to the provision of a more economic tag set, as in the substitution of the UD relations for different nominal modifiers with a single label, detailed in Section~\ref{sec:transformation}. The fourth principle emphasizes consideration of dependency structure ranks, including predicates, arguments, modifiers, and adverbs qualifying modifiers, representing a range of dependency levels. This can be instantiated by \citet{Croft2017}'s different treatments of complex sentences, complex predicates, and arguments, although they are all dependent on the predicate.

\citet{Croft2017} emphasize that the advantages brought about by their scheme may sacrifice the practical purposes pursued by UD, including achieving high parsing accuracy. This concern has restricted the scheme's application to instructional purposes despite its theoretical potential to address UD's typological gaps. This paper investigates the empirical impact of the scheme on parsing accuracy, aiming to enable its future use in UD revisions.

Our results on a typologically diverse set of languages confirm that it is more straightforward to parse treebanks with typologically informed UD annotation (referred to as TUD henceforth) than to parse ones with standard UD annotation. The results show significant but not necessarily fundamental improvement, as \citet{Croft2017}'s proposals address only the classification of dependency relations without affecting the overall tree structure.

\section{Related Work}
Incorporating knowledge of language diversity into NLP systems is widely regarded as a valuable strategy for enhancing language independence \citep{Bender2009}. In the context of Universal Dependencies, the literature addresses typological limitations through parsing architecture and annotation scheme considerations. \citet{Basirat2021} integrate the notion of syntactic nuclei into the UD parsing framework to cope with the typological differences of languages. Their experimentation demonstrates that nucleus composition consistently improves parsing accuracy. This idea is further explored by \citet{Nivre2022}, who find that the observed parsing improvement results from the greater capability of the enriched models of analyzing main predicates, nominal dependents, clausal dependents, and coordination structures.

Other proposals present alternative annotation schemes or revisions to UD. \citet{Gerdes2018} propose the Surface-Syntactic Universal Dependencies (SUD), claimed to be a richer and easier variant of UD. They argue that SUD treebanks enable cross-linguistic typological measures thanks to their distributional and functional criteria. \citet{Gerdes2019} recall the SUD's general principles, update its relation set, address annotation issues, and present an orthogonal layer of syntactic features. \citet{Gerdes2021} further suggest that a new treebank should initially be developed in SUD, even if a UD treebank is intended. The 2021 International Conference on Parsing Technologies \citep{Oepen2021} was dedicated to the additional structural layer of UD, known as Enhanced Universal Dependencies (EUD), to encode grammatical relations that can be represented more adequately using graphical rather than purely rooted trees.

This paper examines a typologically revised annotation scheme for UD, called TUD, based on \citet{Croft2017}'s proposal. Unlike SUD and EUD, which modify dependencies structurally, TUD affects only the dependency labels while preserving the dependency tree topology. Furthermore, it involves less radical dependency relation mappings and retains the majority of original UD labels regardless of the corresponding POS tags.

\section{Transformation}
\label{sec:transformation}
We devise a set of transformation rules in the form $x \rightarrow y$ to map a UD relation $x$ to a TUD relation $y$.

\citet{Croft2017} distinguish the subject relation from object and oblique. They label this relation \texttt{sbj} regardless of its categorization as a noun phrase or a clause, in line with their fourth principle. This is realized in our script via the consolidation rules \texttt{nsubj} $\rightarrow$ \texttt{sbj} and \texttt{csubj} $\rightarrow$ \texttt{sbj}. Furthermore, they find it redundant under the third principle to tag direct and indirect objects differently, so we consider consolidation rules \texttt{iobj} $\rightarrow$ \texttt{obj*} and \texttt{obj} $\rightarrow$ \texttt{obj*} to exclude \texttt{iobj}. The asterisk indicates that \texttt{obj} is already a UD relation, with the latter rule assumed to retain it throughout the conversion.

\citet{Croft2017} challenge the distinction made in UD between complements in terms of grammatical role, including obligatory and nonobligatory control. Our consolidation rules \texttt{ccomp} $\rightarrow$ \texttt{comp} and \texttt{xcomp} $\rightarrow$ \texttt{comp} serve to neutralize the distinction, conforming to the third principle. Moreover, they point out that UD treats resultatives as controlled complements, which it labels \texttt{xcomp}. They suggest that these complex predicate elements be labeled similarly to other secondary predicates and adverbs of manner, which are tagged \texttt{sec}. The rule \texttt{xcomp} $\rightarrow$ \texttt{sec} is included to realize this, complying with the fourth principle. Thus, the fragmentation rules \texttt{xcomp} $\rightarrow$ \texttt{comp} and \texttt{xcomp} $\rightarrow$ \texttt{sec} have the same UD relation on their left-hand sides. \texttt{xcomp} $\rightarrow$ \texttt{comp} is set to apply where the POS tag of the token with the \texttt{xcomp} dependency relation is VERB, which is assumed not to be the case for resultatives, where \texttt{xcomp} $\rightarrow$ \texttt{sec} is to apply instead.

UD treebanks optionally set the morphological feature \texttt{AdvType} with different values for adverbs of manner, location, time, quantity or degree, cause, and modal nature. On the other hand, \citet{Croft2017} propose in line with their fourth principle that the diversity of adverbs in semantics, syntactic distribution, and morphological form needs to be captured and suggest that adverbs of manner should be labeled \texttt{sec}, and ones expressing degree or hedging, aspect or modality, and location or time should be tagged \texttt{qlfy}, \texttt{aux}, and \texttt{obl}, respectively. Therefore, the fragmentation rules \texttt{advmod} $\rightarrow$ \texttt{sec} $|$ \texttt{qlfy} $|$ \texttt{aux*} $|$ \texttt{obl*} are there to convert \texttt{advmod} to each of the above relations if \texttt{AdvType} is set to the corresponding value. According to the UD documentation, the major values include \texttt{Man}, for adverb of manner, \texttt{Loc}, for adverb of location, \texttt{Tim}, for adverb of time, \texttt{Deg}, for adverb of quantity or degree, \texttt{Cau}, for adverb of cause, and \texttt{Mod}, for adverb of modal nature. Where a different or no setting exists, \texttt{advmod} $\rightarrow$ \texttt{obl*} will apply by default, as \citet{Croft2017} assert that the UD \texttt{advmod} relation should be excluded altogether.

\citet{Croft2017} analyze light verbs as complex predicates, tagged \texttt{cxp}, unlike in UD, where they are treated similarly to nominal compounds. Therefore, the rule \texttt{compound} $\rightarrow$ \texttt{cxp} is included in our script, in accordance with the fourth principle, to transform the UD \texttt{compound} relation to \texttt{cxp} where the token's parent is POS-tagged VERB, assumed to signal a light verb construction alongside the token's own \texttt{compound} dependency relation label. They also suggest that copulas should be treated as light verbs, hence the consolidation rule \texttt{cop} $\rightarrow$ \texttt{cxp} in our script, which conforms to the first principle. Furthermore, they suggest that \texttt{nummod}, \texttt{amod}, and \texttt{det} should all be tagged \texttt{mod}, as they involve the same type of information in general, conforming to the third principle. The consolidation rules \texttt{nummod} $\rightarrow$ \texttt{mod}, \texttt{amod} $\rightarrow$ \texttt{mod}, and \texttt{det} $\rightarrow$ \texttt{mod} are there to realize this simplification. Figure~\ref{fig:transformation} summarizes the transformations.

\begin{figure}[h]
\centering
\fbox{\parbox{0.9\textwidth}{\centering IMAGE NOT PROVIDED\\ Transformation rule summary diagram}}
\caption{A summary of the transformation rules. P1--P4 indicate the principles they conform to.}
\label{fig:transformation}
\end{figure}

It should be noted that the eventual aim of this paper is to pave the way for the creation of a totally typologically-based version of UD. The intended scheme will be applicable as a basis for the annotation of text from scratch, involving all the considerations made in \citet{Croft2017}. Since that would be a costly transformation, we need to ensure beforehand that it merits the cost. Therefore, we attempt a preliminary transformation phase, where we apply changes to the available UD treebanks under the limitations imposed by the UD guidelines. In other words, the treebanks resulting from the conversion procedure are intermediary means that enable empirical investigation rather than finalized corpora prepared for use by a corpus linguist. We provide a manual evaluation of the proposed transformation in the next section.

\section{Experiments and Results}
We evaluate the impact of the typological transformations based on their contribution to parsing performance. Our test benchmark consists of 20 treebanks from UD 2.12 belonging to diverse language families, inspired by \citet{Nivre2022}. In addition to language diversity, we consider the presence of labels needed for the maximal application of the transformation rules. For this purpose, we incorporate treebanks that include the annotations required for the transformation. As stated in Section~\ref{sec:transformation}, for instance, the morphological feature annotation on adverb types, required for our transformation of the \texttt{advmod} relation, is optional according to the UD guidelines. Therefore, we add some of the few languages that have included this information in order to cover that specific transformation. Table~\ref{tab:results} outlines the selected treebanks with statistics about their sizes and transformed token ratios (Col. IR).

Before proceeding with the parsing analysis, we first present our manual evaluation of the conversion rules in the following section.

\subsection{Manual Evaluation}
To inspect the performance of the conversion script, we attempt a manual annotation of sample sentences from two of the UD treebanks, where we have mastery over the languages. For that purpose, we randomly select 25 and 50 sentences from the development sets of Persian Seraji and English EWT, containing totals of 599 and 2001 sentences, at intervals of 24 and 40 sentences, respectively. Due to the wider variety of text types on the English side, leading to smaller-sized sentences on average, the two samples end up containing almost as many tokens: 689 and 687, respectively. Then, we manually annotate all the sentences in the two samples based on \citet{Croft2017}'s guidelines and compare the results to the corresponding outputs of the conversion script to spot the mismatches. For each mismatch, it is examined whether the conversion process is responsible. A summary of the manual annotation is provided in Appendix~\ref{app:manual}.

In the case of Persian, a total of 71 tokens are identified, 64 of which represented annotation differences that can be traced back to disagreements between our views and the original UD treebank annotators'. In other words, over 90\% of the observed incompatibility would be there also if the original UD scheme were adopted as the basis, and slightly more than 1\% of the examined tokens are labeled incorrectly due to failure on the part of the conversion script. The two major erroneous cases include one where the \texttt{advmod} relation could better be converted to \texttt{aux} than to \texttt{obl} and one where conversion from \texttt{compound} to \texttt{cxp} is blocked as the conditions set for the application of the relevant rule are not met. Furthermore, there are 5 tokens where conversions from \texttt{nmod} or \texttt{amod} to \texttt{obl} and/or from \texttt{obl} to \texttt{sec} would provide better descriptions, while the required rules are missing due to the absence of clues. These are also considered strictly as cases of script failure.

A few inter-annotator disagreements are also observed for English, which we prefer to ignore as nonnatives. However, the conversion script is responsible for a total of 14 tokens, i.e., slightly more than 2\%. Except for one token where the \texttt{compound} relation is incorrectly converted to \texttt{cxp}, they all represent the conversion, by default, of \texttt{advmod} $\rightarrow$ \texttt{obl} rather than \texttt{advmod} $\rightarrow$ \texttt{qlfy} (12 instances) or \texttt{advmod} $\rightarrow$ \texttt{sec} (1 instance).

\subsection{Parsing Performance}
To address \citet{Croft2017}'s concerns about TUD's practical and theoretical advantage, we base our analysis on the Labeled Attachment Score (LAS), as the typological conversion affects only the dependency labels, and the tree structures remain unchanged. Given that LAS accounts for both dependency labels and structures, it is a more appropriate metric for this analysis. The experiments are based on two primary dependency parsing architectures: transition-based \citep{Nivre2004} and graph-based parsing \citep{McDonald2005}.

We use UUParser \citep{deLhoneux2017} for the former and the Biaffine parser \citep{Dozat2017} for the latter with the settings outlined in Appendix~\ref{app:setup}. We apply the transformation rules on each treebank and independently train three parsing models, each with distinct random seeds, using both the original (UD) and transformed (TUD) treebanks. The average LASs on the development sets are reported in Cols. UD and TUD. Additionally, Col. Ora (oracle) represents the upper bound for parsing performance, achievable if the dependency relations of the transformed tokens are predicted correctly.

It might be argued that any improvement in accuracy resulting from the transformation lies in the simplifying nature of the proposed scheme, which involves plenty of consolidation rules. We maintain that not as much rise in parsing accuracy could be achieved through a random set of merging rules as brought about by our typologically-motivated rules. To demonstrate this, we conduct a randomization experiment, explained in Appendix~\ref{app:random}, with the results reported in the Cols. RND. To assess the significance of the differences between TUD and other baselines, we utilize McNemar's test, as detailed in Appendix~\ref{app:test}, and mark the significant differences ($p$-value $< .05$) with an asterisk.

The IR values indicate the importance of the typological transformation, applicable to almost 28\% of the tokens, and that, if predicted correctly (Col. Ora), it can improve the performance by 2.1 and 3.0 points for the transition and graph-based parsing, respectively. However, the parsers can only harness a small but statistically significant portion of this potential improvement, with transition-based achieving 0.21 points and graph-based achieving 0.48 points. Figure~\ref{fig:improvement} visualizes the absolute LAS improvement (or degradation) caused by the typological transformations. We can observe that, on most treebanks, the parsing models result in a better performance on typologically transformed treebanks and that, except for Latin, the negative results are statistically insignificant. These findings highlight the transformation's constructive role in enhancing parsing accuracy without introducing significant adverse effects.

\begin{figure}[h]
\centering
\fbox{\parbox{0.9\textwidth}{\centering IMAGE NOT PROVIDED\\ Absolute LAS improvement/degradation plot}}
\caption{Absolute LAS improvement (or degradation). Significant results with $p$-value $< 0.05$ are marked with $\star$.}
\label{fig:improvement}
\end{figure}

Earlier in this section, we emphasized the typological motivation behind the applied consolidation rules, hence their preference over random merging rules. In other words, we raise parsing performance while adhering to well-established typological principles. Following the third principle, for example, we merge all the dependency relations that package the same grammatical information into a single tag, thereby gaining both theoretical and practical benefits. Empirical evidence, summarized in Figure~\ref{fig:contribution}, demonstrates that the third principle is by far the most contributive to the rise in parsing accuracy, while the fourth principle, mainly corresponding to fragmentation rules, is the most detrimental. Moreover, the first principle, represented by only one rule, is rather neutral in this respect, and the second principle is not reflected in the transformations, as UD fully conforms to this principle already. For a detailed discussion of the contribution of the individual transformation rules, see Appendix~\ref{app:rules}.

\begin{figure}[h]
\centering
\fbox{\parbox{0.9\textwidth}{\centering IMAGE NOT PROVIDED\\ Rule contribution plot}}
\caption{The transformation rules' contribution (or detraction). The results with $p$-value $< 0.05$ are marked with $\star$.}
\label{fig:contribution}
\end{figure}

\begin{table}[h]
\centering
\caption{Average parsing accuracy (LAS) before (UD) and after (TUD) typological transformation. IR: Impact Rate (percentage of tokens transformed). Significant improvements over UD marked with $\star$.}
\label{tab:results}
\begin{tabular}{l l l r r r r r r r r}
\toprule
Language & Treebank & Family & Genus & Size & IR & \multicolumn{2}{c}{Transition-based} & \multicolumn{2}{c}{Graph-based} \\
\cmidrule(lr){7-8} \cmidrule(lr){9-10}
 & & & & (K) & (\%) & UD & TUD & UD & TUD \\
\midrule
Arabic & padt & Afro-Asiatic & Semitic & 254 & 20 & 77.83$\star$ & 78.10 & 78.49 & 78.50 \\
Armenian & armtdp & Indo-European & Indo-Iranian & 47 & 25 & 73.13 & 72.91 & 66.72 & 66.86 \\
Basque & bdt & Isolate & -- & 97 & 26 & 74.94 & 74.90 & 67.54$\star$ & 69.35 \\
Chinese & gsd & Sino-Tibetan & Sinitic & 111 & 23 & 70.05 & 69.90 & 66.77$\star$ & 67.11 \\
Cl-Chinese & kyoto & Sino-Tibetan & Sinitic & 406 & 31 & 75.33 & 75.51 & 74.81 & 75.00 \\
English & ewt & Indo-European & Germanic & 230 & 33 & 82.75 & 82.91 & 81.60$\star$ & 81.81 \\
Finnish & tdt & Uralic & Finno-Ugric & 181 & 29 & 78.15 & 78.10 & 72.04$\star$ & 72.81 \\
Hindi & hdtb & Indo-European & Indo-Iranian & 316 & 22 & 87.58$\star$ & 87.79 & 89.06$\star$ & 89.30 \\
Italian & isdt & Indo-European & Romance & 288 & 34 & 87.24$\star$ & 87.43 & 87.15 & 87.28 \\
Korean & gsd & Koreanic & Altaic & 69 & 23 & 72.53 & 72.88 & 67.49 & 67.21 \\
Latin & ittb & Indo-European & Italic & 421 & 33 & 83.26$\star$ & 82.95 & 85.53 & 85.54 \\
Latvian & lvtb & Indo-European & Baltic & 253 & 29 & 79.81 & 79.83 & 78.06$\star$ & 78.30 \\
Marathi & ufal & Indo-European & Indo-Iranian & 3 & 30 & 48.71 & 49.01 & 48.86 & 50.68 \\
Persian & seraji & Indo-European & Indo-Iranian & 137 & 26 & 81.26 & 81.27 & 78.76 & 78.66 \\
Russian & taiga & Indo-European & Slavic & 187 & 28 & 64.95$\star$ & 65.50 & 62.64$\star$ & 63.35 \\
Swedish & talbanken & Indo-European & Germanic & 76 & 34 & 76.02 & 76.40 & 70.79 & 71.05 \\
Turkish & imst & Turkic & Altaic & 48 & 28 & 54.74$\star$ & 55.56 & 48.52$\star$ & 50.32 \\
Urdu & udtb & Indo-European & Indo-Iranian & 123 & 24 & 76.19$\star$ & 76.87 & 75.76$\star$ & 76.69 \\
Vietnamese & vtb & Austroasiatic & Vietic & 46 & 31 & 48.62$\star$ & 49.04 & 47.34 & 47.12 \\
Wolof & wtb & Niger-Congo & Atlantic-Congo & 34 & 28 & 72.02 & 72.42 & 67.16$\star$ & 67.69 \\
\midrule
Average & & & & 166 & 28 & 73.26$\star$ & 73.47 & 70.75$\star$ & 71.23 \\
\bottomrule
\end{tabular}
\end{table}

\section{Conclusion}
The typological transformation of Universal Dependencies presents an advantage in terms of parsing performance. This benefit is observable across the two primary parsing approaches, namely the transition-based and the graph-based parsing, and in many languages. The positive impact on parsing performance can be attributed to the consolidation rules, which merge the dependency relations with similar typological properties. On the contrary, the parsing performance is slightly hindered by fragmentation rules, indicating their detrimental effect in the context of Universal Dependencies.

Our empirical results demonstrate that an annotation scheme resulting from the typological transformation does not sacrifice the practical aims of UD. Therefore, we suggest establishing such a scheme as an alternative basis for treebanking. Our manual evaluation highlights the importance of typological annotation from scratch or the use of more advanced automatic conversion from the existing UD scheme. In future work, we plan to improve the conversion method and explore the practical benefits of the proposed scheme in downstream tasks.

\section*{Acknowledgement}
We are grateful to the anonymous reviewers for their invaluable feedback on this work. We also extend our thanks to the Danish research center Diec for providing computing resources via UCloud, and to the Danish National Life Science Supercomputing Center for granting access to Computerome 2.0 through Project ku-00223.

\section*{Limitations}
A limitation of this study is that not all of \citet{Croft2017}'s suggested transformation rules are considered due to a lack of annotation in the benchmark. Besides the labels on the right-hand sides of the rules in Section~\ref{sec:transformation}, \citet{Croft2017} name two tags for independent elements indicating indexing or agreement and linkers: \texttt{idx} and \texttt{lnk}. They categorize the above relations as common strategies, implying that they are not regarded as universal constructions. We have decided to ignore the above phenomena at this stage in the absence of clear clues as to how they are marked in each of the treebanks that contain them as independent tokens. We make the same decision for cases where it would be extremely difficult to identify the conditions for applying a rule, as in the case of depictives that are closely similar in structure to adverbial clauses. While these are both marked in UD as \texttt{advcl}, \citet{Croft2017} suggest that the former should be labeled \texttt{sec}, similarly to resultatives and manner adverbs, transformed via the consolidation rules \texttt{xcomp} $\rightarrow$ \texttt{sec} and \texttt{advmod} $\rightarrow$ \texttt{sec}, respectively. Our script, however, leaves \texttt{advcl} tags unchanged, as one could hardly set proper conditions for an \texttt{advcl}-to-\texttt{sec} transformation to apply given the clues available on UD treebanks. In addition to these, our benchmark lacks any application for the rules \texttt{advmod} $\rightarrow$ \texttt{sec} and \texttt{advmod} $\rightarrow$ \texttt{aux*} due to the absence of the optional morphological feature settings on the relevant UD treebanks. Besides, the manual evaluation of the conversion is restricted to two languages due to our limited expertise in other languages.

\bibliographystyle{acl_natbib}
\bibliography{refs}

\appendix

\section{Manual Analysis}
\label{app:manual}
Table~\ref{tab:persian} and Table~\ref{tab:english} show the results of the manual analyses made for Persian and English, respectively. They list information on the tokens where the dependency relations assigned via the manual annotation process differ from those set after the conversion. Following the first two columns representing the sentence ID in the original development treebank and the token number in that sentence, respectively, the third to fifth columns indicate the dependency labels set through the original UD annotation, the automatic conversion, and the manual TUD annotation. We include a sixth column, named Our UD to represent the label that we would set based on UD rather than TUD to enable decision-making on the actual source of mismatch. The seventh column, named C, shows if the original UD relation, mentioned in the third column, is among those that (potentially) undergo conversion. The eighth column (HA) indicates whether the manual analysis changes the head as well as the dependency tag of the token. Finally, the ninth column (CR) shows if the conversion process is responsible for the mismatch observed between the contents of Columns four and five. Obvious cases of conversion failure are those where such a mismatch occurs while Columns three and six have the same contents, which means that the script fails to identify the context required for the expected conversion to take place. However, there are additional cases where the contents of the latter pair of columns also mismatch, which means that the script can still be improved to achieve more intelligent identification of more complicated such contexts, or more sophisticated techniques can be applied to further improve the script performance, particularly where this cooccurs with a negative content in Column seven, which means that the original UD label does not actually appear on the left-hand side of any of the current rules.

\begin{table}[h]
\centering
\caption{The manual analysis of Persian. C: Convertible; HA: Head Affected; CR: Conversion Responsible}
\label{tab:persian}
\begin{tabular}{l r l l l l c c c}
\toprule
Sent. ID & Tok. & Orig. UD & Auto. TUD & Manu. TUD & Our UD & C & HA & CR \\
\midrule
dev-s1 & 20 & nmod & nmod & obl & obl & no & yes & no \\
dev-s1 & 24 & xcomp & sec & sbj & nsubj & yes & no & no \\
dev-s1 & 29 & fixed & fixed & nmod & nmod & no & yes & no \\
dev-s1 & 30 & fixed & fixed & case & case & no & yes & no \\
dev-s25 & 7 & flat & flat & compound & compound & no & no & no \\
dev-s25 & 12 & flat & flat & compound & compound & no & no & no \\
dev-s25 & 13 & obj & obj & sec & advmod & no & no & no \\
dev-s25 & 19 & compound & compound & aux & aux & yes & no & no \\
dev-s50 & 6 & flat & flat & nmod & nmod & no & no & no \\
dev-s50 & 7 & mark & mark & cc & cc & no & no & no \\
dev-s50 & 14 & compound & compound & cxp & cop & no & no & no \\
dev-s75 & 5 & compound & cxp & acl & acl & yes & yes & no \\
dev-s75 & 6 & acl & acl & aux & aux & no & yes & no \\
dev-s75 & 7 & mark & mark & cc & cc & no & no & no \\
dev-s75 & 15 & obl & obl & nmod & nmod & no & yes & no \\
dev-s75 & 16 & compound & cxp & acl & acl & yes & yes & no \\
dev-s75 & 17 & ccomp & comp & cxp & cop & yes & yes & no \\
dev-s100 & 13 & nmod & nmod & mod & amod & no & yes & no \\
dev-s100 & 26 & advcl & advcl & obl & obl & no & no & no \\
dev-s100 & 32 & nmod & nmod & cxp & compound & no & yes & no \\
dev-s100 & 37 & conj & conj & obl & obl & no & yes & no \\
dev-s100 & 39 & nmod & nmod & obl & obl & no & yes & no \\
dev-s100 & 42 & compound & cxp & conj & conj & yes & yes & no \\
dev-s150 & 1 & advmod & obl & aux & advmod & yes & no & yes \\
dev-s200 & 31 & obl & obl & nmod & nmod & no & yes & no \\
dev-s275 & 4 & obl & obl & nmod & nmod & no & yes & no \\
dev-s350 & 16 & nummod & mod & compound & compound & no & no & no \\
dev-s350 & 24 & nmod & nmod & obl & obl & no & no & no \\
dev-s350 & 26 & nmod & nmod & obl & obl & no & no & no \\
dev-s350 & 31 & compound & compound & cxp & cop & yes & no & yes \\
dev-s350 & 34 & compound & compound & aux & aux & yes & no & no \\
dev-s375 & 7 & mark & mark & obl & advmod & no & no & no \\
dev-s375 & 29 & compound & compound & aux & aux & yes & no & no \\
dev-s400 & 8 & nmod & nmod & obl & obl & no & no & yes \\
dev-s400 & 17 & obl & obl & nmod & nmod & no & yes & no \\
dev-s425 & 6 & nsubj & sbj & appos & appos & yes & yes & no \\
dev-s450 & 2 & flat & flat & nmod & nmod & no & yes & no \\
dev-s450 & 3 & flat & flat & compound & compound & no & yes & no \\
dev-s450 & 11 & nmod & nmod & sbj & nsubj & no & yes & no \\
dev-s450 & 12 & nsubj & sbj & nmod & nmod & yes & yes & no \\
dev-s450 & 14 & obl & obl & nmod & nmod & no & yes & no \\
dev-s450 & 23 & obl & obl & sec & obl & no & no & yes \\
dev-s450 & 27 & nsubj & sbj & appos & appos & yes & yes & no \\
dev-s450 & 29 & fixed & fixed & nmod & nmod & no & yes & no \\
dev-s475 & 5 & nmod & nmod & fixed & fixed & no & no & no \\
dev-s475 & 7 & flat & flat & obl & nmod & no & yes & no \\
dev-s475 & 13 & compound & compound & aux & aux & yes & no & no \\
dev-s475 & 29 & obl & obl & fixed & fixed & no & yes & no \\
dev-s475 & 30 & amod & mod & sec & obl & yes & yes & yes \\
dev-s500 & 8 & obl & obl & fixed & fixed & no & yes & no \\
dev-s500 & 9 & nmod & nmod & sec & obl & no & yes & yes \\
dev-s525 & 8 & flat & flat & nmod & nmod & no & no & no \\
dev-s525 & 9 & punct & punct & flat & flat & no & yes & no \\
dev-s525 & 10 & nummod & mod & flat & flat & yes & yes & no \\
dev-s525 & 13 & nmod & nmod & obl & obl & no & yes & no \\
dev-s525 & 27 & nmod & nmod & fixed & fixed & no & yes & no \\
dev-s525 & 28 & nmod & nmod & obl & obl & no & yes & no \\
dev-s525 & 29 & advcl & advcl & aux & aux & no & no & no \\
dev-s525 & 32 & obl & obl & fixed & fixed & no & yes & no \\
dev-s525 & 33 & amod & mod & sec & obl & yes & no & yes \\
dev-s525 & 39 & conj & conj & advcl & advcl & no & no & no \\
dev-s550 & 7 & nmod & nmod & obl & obl & no & yes & no \\
dev-s575 & 13 & nmod & nmod & obl & obl & no & yes & no \\
dev-s575 & 15 & xcomp & sec & parataxis & parataxis & yes & yes & no \\
dev-s575 & 16 & conj & conj & cxp & cop & no & yes & no \\
dev-s575 & 18 & amod & mod & sbj & nsubj & yes & yes & no \\
dev-s575 & 19 & nsubj & sbj & nmod & nmod & yes & yes & no \\
dev-s575 & 29 & obl & obl & nmod & nmod & no & yes & no \\
dev-s575 & 45 & obl & obl & nmod & nmod & no & yes & no \\
dev-s599 & 12 & dislocated & dislocated & appos & appos & no & yes & no \\
dev-s599 & 21 & conj & conj & parataxis & parataxis & no & no & no \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[h]
\centering
\caption{The manual analysis of English. C: Convertible; HA: Head Affected; CR: Conversion Responsible}
\label{tab:english}
\begin{tabular}{l r l l l l c c c}
\toprule
Sent. ID & Tok. & Orig. UD & Auto. TUD & Manu. TUD & Our UD & C & HA & CR \\
\midrule
weblog-b...00-0001 & 24 & advmod & obl & qlfy & advmod & yes & no & yes \\
weblog-b...00-0001 & 32 & advmod & obl & qlfy & advmod & yes & no & yes \\
weblog-b...00-0001 & 38 & advmod & obl & qlfy & advmod & yes & no & yes \\
weblog-b...00-0001 & 46 & advmod & obl & qlfy & advmod & yes & no & yes \\
weblog-t...00-0021 & 12 & advmod & obl & qlfy & advmod & yes & no & yes \\
weblog-t...00-0037 & 2 & advmod & obl & qlfy & advmod & yes & no & yes \\
email-en...00-0030 & 2 & advmod & obl & sec & advmod & yes & no & yes \\
newsgro...00-0005 & 7 & compound & cxp & compound & compound & yes & no & yes \\
newsgro...00-0007 & 9 & advmod & obl & qlfy & advmod & yes & no & yes \\
newsgro...00-0012 & 22 & advmod & obl & qlfy & advmod & yes & no & yes \\
newsgro...00-0012 & 25 & advmod & obl & qlfy & advmod & yes & no & yes \\
answers...00-0008 & 29 & advmod & obl & qlfy & advmod & yes & no & yes \\
reviews-...00-0002 & 3 & advmod & obl & qlfy & advmod & yes & no & yes \\
reviews-...00-0003 & 2 & advmod & obl & qlfy & advmod & yes & no & yes \\
\bottomrule
\end{tabular}
\end{table}

\section{Parsing Setup}
\label{app:setup}
Our transition-based parsing experiments utilize the implementation from \citet{Basirat2021}, with the nucleus composition disabled.\footnote{\url{https://github.com/abasirat/uuparser}} For the graph-based experiments, we rely on the Biaffine module integrated into the SuPar parser.\footnote{\url{https://github.com/yzhangcs/parser}} In both parsers, we refrain from employing pre-trained embeddings, including both static and contextualized models, due to their inconsistent performance across different languages, which could potentially impact the research outcomes. Instead, we opt for a BiLSTM encoder in both scenarios to mitigate external influences and maintain result consistency. Similarly, we avoid using morphosyntactic features such as part-of-speech tags or morphological features due to their varying prediction performance at the test time, which could influence the analysis across languages \citep{Tiedemann2015}.

Both parsers are trained for 30 epochs with the word embedding size of 100 and the character embedding dimensions of 100 for UUParser and 50 for SuPar. The UUParser parameters are set to their default values as suggested by \citet{Nivre2022}. The arc and relation MLP projection sizes of SuPar are set to 500 and 300, respectively, and the other parameters are set to their default values. We disable the projective parsing in both parsers. The computational resource we use to train one transition-based model is a node of three CPUs and 5--10 GB memory in an HPC---however, the graph-based models, each consisting of 12M trainable parameters, are trained on NVIDIA Tesla V100 GPU.

\section{Hypothesis Testing}
\label{app:test}
We utilize McNemar's test to evaluate the significance of the parsing difference between the two schemes. The test enables us to measure the significance of the changes in parsing performance for each token before and after the typological transformation. More specifically, McNemar's test is a paired-sample t-test for a dichotomous variable that takes two values. In our study, the dichotomous dependent variable of the test indicates whether a token is correctly classified in a scheme or not. The variable takes a value of 1 if the dependency head and label of a token are predicted accurately and a value of 0 otherwise. The categorical independent variable of the test refers to the two dependency schemes, UD and TUD. We collect the value of the dependent variable for all tokens across the two schemes, resulting in two lists of the size of the number of tokens, with the values in each list determining whether the token is classified correctly in the corresponding scheme or not. From these lists, we build a contingency table, shown in Table~\ref{tab:mcnemar}, with the following description:
\begin{itemize}
\item A: the number of tokens predicted correctly in both schemes
\item B: the number of tokens predicted correctly in UD but incorrectly in TUD
\item C: the number of tokens mispredicted in UD but predicted correctly in TUD
\item D: the number of mispredicted tokens in both schemes.
\end{itemize}
With this setting, we estimate the $p$-value to reject the null hypothesis that the typological transformation does not impact parsing accuracy ($p_b = p_c$). We estimate the $p$-value based on the binomial distribution. To address the effect of randomness in the parsing models, we collect the statistics from the concatenation of the three runs with different random seeds.

\begin{table}[h]
\centering
\caption{The contingency table for McNemar's test}
\label{tab:mcnemar}
\begin{tabular}{c c c}
\toprule
 & \multicolumn{2}{c}{After (TUD)} \\
 & 1 & 0 \\
\midrule
Before (UD) & 1 & A \\
 & 0 & C \\
\midrule
 & B & D \\
\bottomrule
\end{tabular}
\end{table}

\section{Random Transformation}
\label{app:random}
To ensure that the parsing gain made by the typological transformation is not only due to the consolidation and fragmentation of the rules but also to the linguistic motivations behind them, we design a random transformation setup where the elements of a subset of dependency labels are randomly merged or expanded. To this aim, we search among all possible sets of consolidation and fragmentation rules and select one with an impact rate close to the average impact rate of the typological transformations (28\%), explained in Section 4.

In a minimal setup, the number of possible rule sets is proportionate to the number of partitions of the dependency labels set. In this setup, consolidation rules are formed by merging the subsets with at least two labels, and the fragmentation rules can be over some of the singleton subsets. Therefore, the size of the search space with $n$ dependency labels is in the scale of $e^{\sum_{k=0}^{\infty} k^n/k!}$, which is the $n$th element of the Bell series, and it is approximately $5.3 \times 10^{31}$ for $n = 37$ UD base dependency labels.

To make the problem more tractable and comparable with the \citet{Croft2017}'s typological transformation rules, we restrict the partitioning to subsets with at most two elements. In this setup, the consolidation rules in each partitioning are formed by merging the elements of subsets that include two elements (i.e., each subset $\{l_i, l_j\}$ of dependency labels introduces two rules $l_i \rightarrow l_{ij}$ and $l_j \rightarrow l_{ij}$), and the singleton subsets like $\{l_i\}$ either form identity rules with no impact ($l_i \rightarrow l_i$) or expand into three sub-labels ($l_i \rightarrow l_{ik}$, $k = 1, 2, 3$). When expanding, one of the $l_i \rightarrow l_{ik}$, $k = 1, 2, 3$ rules is randomly applied with a uniform probability. The impact rate of a consolidation rule $l_i \rightarrow l_{ij}$ is $n_i/N$, and the impact rate of an expansion rule $l_i \rightarrow l_{ik}$ is $n_i/(3N)$, where $n_i$ is the frequency of occurrence of the label $l_i$, and $N$ is the total number of tokens in the corpus. The total impact rate of a rule set is then the sum of the impact rates of its rules.

Even with these simplifications, the search space is fairly large, and a complete search requires significant computing resources to find a rule set with a desired impact rate. Therefore, we formulate it as a simulated annealing search that searches for a rule set with a total impact rate of 0.28, an initial temperature of 1.0, and a cooling rate of 0.99. To address the randomness effect, we perform the random transformation three times on each treebank, train a parsing model on the transformed treebanks, and report the average LAS in Table~\ref{tab:results}, Column RND.

\section{Rule Contribution}
\label{app:rules}
We present some statistics about the distribution of the transformation rules and numerical results of each rule's contribution to the tokens' dependency label prediction. For each rule, we gather all tokens that can undergo the transformation and calculate their LAS (Labeled Attachment Score) both before and after applying the rule. Table~\ref{tab:rules} shows the absolute improvement or degradation in LAS after applying the transformation rules (Column $\Delta$), along with the $p$-values from McNemar's significance test. It also represents the relative contribution of the rules with respect to their distribution, i.e., $\Delta \times P$, where $P$ is the relative frequency of the tokens undergoing each rule.

In summary, the results in Table~\ref{tab:rules} (Row SUM) show that the transformation rules contribute positively to the prediction of the dependency relations with both the transition-based and the graph-based parsers. Further investigation of the results reveals the varying contribution of the rules to the performance gain. The relative contribution of the rules represented in Column $\Delta \times P$ (and Figure~\ref{fig:contribution}) illustrates the enhancement achieved by each transformation in classifying tokens that undergo the respective transformation. We can see that most rules constructively impact parsing with similar ranks for the two parsers and that untransformed tokens ($x \rightarrow x$) are not influenced.

The most significant contribution arises from the consolidation rules. A crucial factor influencing their effectiveness is the inherent difficulty in distinguishing between source relations, often being misclassified as one another in UD, which is no longer an issue once they are merged in TUD. In particular, the effectiveness of the \texttt{iobj} $\rightarrow$ \texttt{obj*} rule is highlighted by the common misclassification scenario, where indirect objects (\texttt{iobj}) are mistakenly identified as direct objects (\texttt{obj}). Therefore, the unification of \texttt{iobj} and \texttt{obj} prevents the parser from misclassifying them as each other. We found an analogous explanation for other consolidation rules that unify the clausal complements \texttt{ccomp} and \texttt{xcomp} into \texttt{comp}, combine the subject relations \texttt{nsubj} and \texttt{csubj} into \texttt{sbj}, and merge the determiner \texttt{det} with modifiers \texttt{amod} and \texttt{nummod} into \texttt{mod}. The small improvement made by \texttt{cop} $\rightarrow$ \texttt{cxp} in the transition-based parser can also be attributed to the misclassification of copula as the compound, which is unified with copula in the typological scheme.

However, the fragmentation rules such as \texttt{xcomp} $\rightarrow$ \texttt{sec} and \texttt{advmod} $\rightarrow$ \texttt{qlfy} exhibit a negative influence. The detrimental impact of \texttt{advmod} $\rightarrow$ \texttt{qlfy} stems from the frequent mutual misclassification of adverbial and adjectival modifiers in UD, which persists even after typological transformation, manifested as mislabeling qualifying adverbs (\texttt{qlfy}) as modifiers (\texttt{mod}) in TUD, albeit at a higher rate, which is in turn because \texttt{mod} in TUD has a broader scope than \texttt{amod} in UD. In addition to the erroneous items present in both schemes, the rule introduces multiple frequent errors in TUD for tokens accurately classified in UD. The top four recurring errors include the misclassification of \texttt{qlfy} as \texttt{sbj} (13\%), \texttt{obl*} (12\%), \texttt{mod} (4\%), and \texttt{aux*} (4\%) for tokens correctly classified in UD as \texttt{advmod}. Similarly, the \texttt{xcomp} $\rightarrow$ \texttt{sec} rule negatively impacts parsing accuracy by misclassifying open clausal complements (\texttt{xcomp}) and objects (\texttt{obj}) in UD. This misclassification is due to their ambiguities and syntactic similarities, which persist between \texttt{sec} and \texttt{obj} in TUD, encompassing a large number of tokens, leading to increased errors. Putting it all together, we conclude that the fragmentation rules detract from parsing performance and that their degradation levels are proportional to the scales of their target relations.

\begin{table}[h]
\centering
\caption{Rules' contributions. $\Delta$: Absolute improvement (degradation) to tokens' dependency label prediction undergoing each transformation rule. $n$: total frequency. $P$: relative frequency.}
\label{tab:rules}
\begin{tabular}{l r r r r r r}
\toprule
Rule & $n$ & $P = n/N$ & \multicolumn{2}{c}{Transition-based} & \multicolumn{2}{c}{Graph-based} \\
\cmidrule(lr){4-5} \cmidrule(lr){6-7}
 & & & $\Delta$ & $\Delta \times P$ & $\Delta$ & $\Delta \times P$ \\
\midrule
advmod $\rightarrow$ qlfy & 32 & 0.01\% & $-14.14$ & $-0.0016$ & $-9.09$ & $-0.0010$ \\
xcomp $\rightarrow$ sec & 1,108 & 0.40\% & $-7.70$ & $-0.0306$ & $-9.90$ & $-0.0393$ \\
nsubj $\rightarrow$ sbj & 20,510 & 7.34\% & $-0.20$ & $-0.0146$ & $-0.46$ & $-0.0335$ \\
cop $\rightarrow$ cxp & 3,777 & 1.35\% & $-0.02$ & $-0.0003$ & $-0.32$ & $-0.0044$ \\
compound $\rightarrow$ cxp & 3,195 & 1.14\% & 0.25 & 0.0029 & $-0.56$ & $-0.0064$ \\
aux $\rightarrow$ aux* & 8,568 & 3.07\% & $-0.18$ & $-0.0056$ & 0.11 & 0.0034 \\
$x \rightarrow x$ & 181,148 & 64.85\% & 0.05 & 0.0319 & 0.08 & 0.0490 \\
advmod $\rightarrow$ obl & 11,853 & 4.24\% & 0.49 & 0.0208 & 0.59 & 0.0252 \\
amod $\rightarrow$ mod & 12,923 & 4.63\% & 0.51 & 0.0235 & 0.68 & 0.0314 \\
obj $\rightarrow$ obj* & 14,427 & 5.17\% & 0.19 & 0.0098 & 1.12 & 0.0580 \\
det $\rightarrow$ mod & 9,810 & 3.51\% & 0.76 & 0.0267 & 1.30 & 0.0458 \\
nummod $\rightarrow$ mod & 4,485 & 1.61\% & 0.42 & 0.0068 & 2.22 & 0.0357 \\
xcomp $\rightarrow$ comp & 2,391 & 0.86\% & 0.72 & 0.0061 & 2.29 & 0.0196 \\
csubj $\rightarrow$ sbj & 485 & 0.17\% & 2.63 & 0.0046 & 2.79 & 0.0048 \\
ccomp $\rightarrow$ comp & 2,965 & 1.06\% & 2.92 & 0.0310 & 3.46 & 0.0367 \\
iobj $\rightarrow$ obj* & 1,643 & 0.59\% & 8.39 & 0.0493 & 21.40 & 0.1259 \\
\midrule
SUM & 279,320 & 100\% & $-4.93$ & 0.1605 & 15.72 & 0.3509 \\
\bottomrule
\end{tabular}
\end{table}

\end{document}
=====END FILE=====

=====FILE: refs.bib=====
@inproceedings{Nivre2016,
  author = {Nivre, Joakim and de Marneffe, Marie-Catherine and Ginter, Filip and Goldberg, Yoav and Hajic, Jan and Manning, Christopher D. and McDonald, Ryan and Petrov, Slav and Pyysalo, Sampo and Silveira, Natalia and Tsarfaty, Reut and Zeman, Daniel},
  title = {Universal Dependencies v1: A Multilingual Treebank Collection},
  booktitle = {Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16)},
  year = {2016},
  pages = {1659--1666},
  address = {Portorož, Slovenia},
  publisher = {European Language Resources Association (ELRA)}
}

@article{deMarneffe2021,
  author = {de Marneffe, Marie-Catherine and Manning, Christopher D. and Nivre, Joakim and Zeman, Daniel},
  title = {Universal Dependencies},
  journal = {Computational Linguistics},
  volume = {47},
  number = {2},
  pages = {255--308},
  year = {2021}
}

@inproceedings{Choi2021,
  author = {Choi, Hee-Soo and Guillaume, Bruno and Fort, Karën},
  title = {Corpus-based Language Universals Analysis Using Universal Dependencies},
  booktitle = {Proceedings of the Second Workshop on Quantitative Syntax (Quasy, SyntaxFest 2021)},
  year = {2021},
  pages = {33--44},
  address = {Sofia, Bulgaria},
  publisher = {Association for Computational Linguistics}
}

@inproceedings{Kanayama2020,
  author = {Kanayama, Hiroshi and Iwamoto, Ran},
  title = {How Universal are Universal Dependencies? Exploiting Syntax for Multilingual Clause-level Sentiment Detection},
  booktitle = {Proceedings of the Twelfth Language Resources and Evaluation Conference},
  year = {2020},
  pages = {4063--4073},
  address = {Marseille, France},
  publisher = {European Language Resources Association}
}

@inproceedings{Croft2017,
  author = {Croft, William and Nordquist, Dawn and Regan, Michael and Looney, Katherine},
  title = {Linguistic Typology Meets Universal Dependencies},
  booktitle = {Proceedings of the 15th International Workshop on Treebanks and Linguistic Theories (TLT15)},
  year = {2017},
  pages = {63--75},
  address = {Bloomington, IN},
  publisher = {CEUR Workshop Proceedings}
}

@inproceedings{Nivre2015,
  author = {Nivre, Joakim},
  title = {Towards a Universal Grammar for Natural Language Processing},
  booktitle = {Computational Linguistics and Intelligent Text Processing},
  year = {2015},
  pages = {3--16},
  address = {Cham},
  publisher = {Springer International Publishing}
}

@inproceedings{Basirat2021,
  author = {Basirat, Ali and Nivre, Joakim},
  title = {Syntactic Nuclei in Dependency Parsing -- A Multilingual Exploration},
  booktitle = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},
  year = {2021},
  pages = {1376--1387},
  address = {Online},
  publisher = {Association for Computational Linguistics}
}

@inproceedings{Bender2009,
  author = {Bender, Emily M.},
  title = {Linguistically Naïve != Language Independent: Why NLP Needs Linguistic Typology},
  booktitle = {Proceedings of the EACL 2009 Workshop on the Interaction between Linguistics and Computational Linguistics: Virtuous, Vicious or Vacuous?},
  year = {2009},
  pages = {26--32},
  address = {Athens, Greece},
  publisher = {Association for Computational Linguistics}
}

@inproceedings{deLhoneux2017,
  author = {de Lhoneux, Miryam and Shao, Yan and Basirat, Ali and Kiperwasser, Eliyahu and Stymne, Sara and Goldberg, Yoav and Nivre, Joakim},
  title = {From Raw Text to Universal Dependencies -- Look, No Tags!},
  booktitle = {Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies},
  year = {2017},
  address = {Vancouver, Canada}
}

@inproceedings{Dozat2017,
  author = {Dozat, Timothy and Manning, Christopher D.},
  title = {Deep Biaffine Attention for Neural Dependency Parsing},
  booktitle = {International Conference on Learning Representations},
  year = {2017}
}

@inproceedings{Gerdes2018,
  author = {Gerdes, Kim and Guillaume, Bruno and Kahane, Sylvain and Perrier, Guy},
  title = {SUD or Surface-syntactic Universal Dependencies: An Annotation Scheme Near-isomorphic to UD},
  booktitle = {Proceedings of the Second Workshop on Universal Dependencies (UDW 2018)},
  year = {2018},
  pages = {66--74},
  address = {Brussels, Belgium},
  publisher = {Association for Computational Linguistics}
}

@inproceedings{Gerdes2019,
  author = {Gerdes, Kim and Guillaume, Bruno and Kahane, Sylvain and Perrier, Guy},
  title = {Improving Surface-syntactic Universal Dependencies (SUD): MWEs and Deep Syntactic Features},
  booktitle = {Proceedings of the 18th International Workshop on Treebanks and Linguistic Theories (TLT, SyntaxFest 2019)},
  year = {2019},
  pages = {126--132},
  address = {Paris, France},
  publisher = {Association for Computational Linguistics}
}

@inproceedings{Gerdes2021,
  author = {Gerdes, Kim and Guillaume, Bruno and Kahane, Sylvain and Perrier, Guy},
  title = {Starting a New Treebank? Go SUD!},
  booktitle = {Proceedings of the Sixth International Conference on Dependency Linguistics (Depling, SyntaxFest 2021)},
  year = {2021},
  pages = {35--46},
  address = {Sofia, Bulgaria},
  publisher = {Association for Computational Linguistics}
}

@inproceedings{Oepen2021,
  editor = {Oepen, Stephan and Sagae, Kenji and Tsarfaty, Reut and Bouma, Gosse and Seddah, Djamé and Zeman, Daniel},
  title = {Proceedings of the 17th International Conference on Parsing Technologies and the IWPT 2021 Shared Task on Parsing into Enhanced Universal Dependencies (IWPT 2021)},
  year = {2021},
  address = {Online},
  publisher = {Association for Computational Linguistics}
}

@inproceedings{Nivre2004,
  author = {Nivre, Joakim},
  title = {Incrementality in Deterministic Dependency Parsing},
  booktitle = {Proceedings of the Workshop on Incremental Parsing: Bringing Engineering and Cognition Together},
  year = {2004},
  pages = {50--57},
  address = {Barcelona, Spain},
  publisher = {Association for Computational Linguistics}
}

@article{Nivre2022,
  author = {Nivre, Joakim and Basirat, Ali and Dürlich, Luise and Moss, Adam},
  title = {Nucleus Composition in Transition-based Dependency Parsing},
  journal = {Computational Linguistics},
  volume = {48},
  number = {4},
  pages = {849--886},
  year = {2022}
}

@inproceedings{McDonald2005,
  author = {McDonald, Ryan and Pereira, Fernando and Ribarov, Kiril and Hajic, Jan},
  title = {Non-projective Dependency Parsing Using Spanning Tree Algorithms},
  booktitle = {Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing},
  year = {2005},
  pages = {523--530},
  address = {Vancouver, British Columbia, Canada},
  publisher = {Association for Computational Linguistics}
}

@inproceedings{Tiedemann2015,
  author = {Tiedemann, Jörg},
  title = {Cross-lingual Dependency Parsing with Universal Dependencies and Predicted PoS Labels},
  booktitle = {Proceedings of the Third International Conference on Dependency Linguistics (Depling 2015)},
  year = {2015},
  pages = {340--349},
  address = {Uppsala, Sweden},
  publisher = {Uppsala University}
}
=====END FILE=====