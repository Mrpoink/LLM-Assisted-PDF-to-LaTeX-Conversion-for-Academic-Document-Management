=====FILE: main.tex=====
\documentclass[11pt,conference]{article}
\usepackage[utf8]{inputenc}
\usepackage{times}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{url}
\usepackage{balance}

\title{MTA4DPR: Multi-Teaching-Assistants Based Iterative Knowledge Distillation for Dense Passage Retrieval}
\author{
Qixi Lu$^{1,2}$, Endong Xun$^{1}$, Gongbo Tang$^{2*}$ \\
$^{1}$Beijing Advanced Innovation Center for Language Resources, Beijing Language and Culture University, China \\
$^{2}$School of Information Science, Beijing Language and Culture University, China \\
\texttt{\{lqxaixxh@gmail.com,\{edxun,gongbo.tang\}@blcu.edu.cn\}} \\
$^{*}$Corresponding author
}

\begin{document}
\maketitle

\begin{abstract}
[ILLEGIBLE]
\end{abstract}

\section{Introduction}
[ILLEGIBLE]

\section{Related Work}
\subsection{Dense Retrieval}
[ILLEGIBLE]

\subsection{Knowledge Distillation}
[ILLEGIBLE]

\section{Methodology}
\subsection{Preliminary}
[ILLEGIBLE]

\subsection{The MTA4DPR Framework}
[ILLEGIBLE]

\section{Experiments and Analysis}
\subsection{Experimental Settings}
[ILLEGIBLE]

\subsection{Main Results}
[ILLEGIBLE]

\subsection{Ablation Study}
[ILLEGIBLE]

\subsection{Analysis}
[ILLEGIBLE]

\section{Conclusion}
[ILLEGIBLE]

\section*{Limitations}
[ILLEGIBLE]

\section*{Acknowledgements}
This work is supported by National Natural Science Foundation of China (NSFC) (No. 62076038).

\section*{Ethics Statement}
[ILLEGIBLE]

\section*{Licenses}
[ILLEGIBLE]

\bibliographystyle{acl_natbib}
\begin{thebibliography}{99}

\bibitem[Adriana et al.2015]{adriana2015fitnets}
Romero Adriana, Ballas Nicolas, K Samira Ebrahimi, Chassang Antoine, Gatta Carlo, and Bengio Yoshua. 2015. Fitnets: Hints for thin deep nets. \textit{Proc. ICLR}, 2(3):1.

\bibitem[Bengio et al.2009]{bengio2009curriculum}
Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston. 2009. Curriculum learning. In \textit{Proceedings of the 26th annual international conference on machine learning}, pages 41–48.

\bibitem[Beyer et al.2022]{beyer2022knowledge}
Lucas Beyer, Xiaohua Zhai, Amélie Royer, Larisa Markeeva, Rohan Anil, and Alexander Kolesnikov. 2022. Knowledge distillation: A good teacher is patient and consistent. In \textit{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 10925–10934.

\bibitem[Chen et al.2018]{chen2018darkrank}
Yuntao Chen, Naiyan Wang, and Zhaoxiang Zhang. 2018. Darkrank: Accelerating deep metric learning via cross sample similarities transfer. In \textit{Proceedings of the AAAI conference on artificial intelligence}, volume 32.

\bibitem[Cormack et al.2009]{cormack2009reciprocal}
Gordon V Cormack, Charles LA Clarke, and Stefan Buettcher. 2009. Reciprocal rank fusion outperforms condorcet and individual rank learning methods. In \textit{Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval}, pages 758–759.

\bibitem[Craswell et al.2020a]{craswell2020overview2020}
Nick Craswell, Bhaskar Mitra, Emine Yilmaz, and Daniel Campos. 2020a. Overview of the trec 2020 deep learning track. \textit{Text REtrieval Conference}.

\bibitem[Craswell et al.2020b]{craswell2020overview2019}
Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, and Ellen M Voorhees. 2020b. Overview of the trec 2019 deep learning track. \textit{arXiv preprint arXiv:2003.07820}.

\bibitem[Dai and Callan2019]{dai2019deeper}
Zhuyun Dai and Jamie Callan. 2019. Deeper text understanding for ir with contextual neural language modeling. In \textit{Proceedings of the 42nd international ACM SIGIR conference on research and development in information retrieval}, pages 985–988.

\bibitem[Formal et al.2021]{formal2021splade}
Thibault Formal, Benjamin Piwowarski, and Stéphane Clinchant. 2021. Splade: Sparse lexical and expansion model for first stage ranking. In \textit{Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval}, pages 2288–2292.

\bibitem[Gao and Callan2021a]{gao2021condenser}
Luyu Gao and Jamie Callan. 2021a. Condenser: a pre-training architecture for dense retrieval. In \textit{Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing}, pages 981–993.

\bibitem[Gao and Callan2021b]{gao2021language}
Luyu Gao and Jamie Callan. 2021b. Is your language model ready for dense representation fine-tuning? \textit{CoRR}, abs/2104.08253.

\bibitem[Gao et al.2021]{gao2021coil}
Luyu Gao, Zhuyun Dai, and Jamie Callan. 2021. Coil: Revisit exact lexical match in information retrieval with contextualized inverted list. In \textit{Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, pages 3030–3042.

\bibitem[Heo et al.2019]{heo2019knowledge}
Byeongho Heo, Minsik Lee, Sangdoo Yun, and Jin Young Choi. 2019. Knowledge transfer via distillation of activation boundaries formed by hidden neurons. In \textit{Proceedings of the AAAI conference on artificial intelligence}, volume 33, pages 3779–3787.

\bibitem[Hinton et al.2015]{hinton2015distilling}
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the knowledge in a neural network. \textit{arXiv preprint arXiv:1503.02531}.

\bibitem[Huang et al.2022]{huang2022knowledge}
Tao Huang, Shan You, Fei Wang, Chen Qian, and Chang Xu. 2022. Knowledge distillation from a stronger teacher. \textit{Advances in Neural Information Processing Systems}, 35:33716–33727.

\bibitem[Karpukhin et al.2020]{karpukhin2020dense}
Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for open-domain question answering. In \textit{Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)}, pages 6769–6781.

\bibitem[Kenton and Toutanova2019]{devlin2019bert}
Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In \textit{Proceedings of naacl-HLT}, volume 1, page 2. Minneapolis, Minnesota.

\bibitem[Kwiatkowski et al.2019]{kwiatkowski2019natural}
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: A benchmark for question answering research. \textit{Transactions of the Association for Computational Linguistics}, 7:452–466.

\bibitem[Lee et al.2024]{lee2024rethinking}
Jinhyuk Lee, Zhuyun Dai, Sai Meher Karthik Duddu, Tao Lei, Iftekhar Naim, Ming-Wei Chang, and Vincent Zhao. 2024. Rethinking the role of token retrieval in multi-vector retrieval. \textit{Advances in Neural Information Processing Systems}, 36.

\bibitem[Lin and Ma2021]{lin2021few}
Jimmy Lin and Xueguang Ma. 2021. A few brief notes on deepimpact, coil, and a conceptual framework for information retrieval techniques. \textit{arXiv preprint arXiv:2106.14807}.

\bibitem[Lin et al.2023]{lin2023prod}
Zhenghao Lin, Yeyun Gong, Xiao Liu, Hang Zhang, Chen Lin, Anlei Dong, Jian Jiao, Jingwen Lu, Daxin Jiang, Rangan Majumder, et al. 2023. Prod: Progressive distillation for dense retrieval. In \textit{Proceedings of the ACM Web Conference 2023}, pages 3299–3308.

\bibitem[Lu2024]{lu2024m2dpr}
Qixi Lu. 2024. M2DPR: A multi-task multi-view representation learning framework for dense passage retrieval. In \textit{NAACL Student Research Workshop 2024}.

\bibitem[Lu et al.2022]{lu2022ernie}
Yuxiang Lu, Yiding Liu, Jiaxiang Liu, Yunsheng Shi, Zhengjie Huang, Shikun Feng Yu Sun, Hao Tian, Hua Wu, Shuaiqiang Wang, Dawei Yin, et al. 2022. Ernie-search: Bridging cross-encoder with dual-encoder via self on-the-fly distillation for dense passage retrieval. \textit{arXiv preprint arXiv:2205.09153}.

\bibitem[Ma et al.2024]{ma2024fine}
Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin. 2024. Fine-tuning llama for multi-stage text retrieval. In \textit{Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval}, pages 2421–2425.

\bibitem[Mao et al.2021]{mao2021generation}
Yuning Mao, Pengcheng He, Xiaodong Liu, Yelong Shen, Jianfeng Gao, Jiawei Han, and Weizhu Chen. 2021. Generation-augmented retrieval for open-domain question answering. In \textit{Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)}, pages 4089–4100.

\bibitem[Mienye et al.2020]{mienye2020improved}
Ibomoiye Domor Mienye, Yanxia Sun, and Zenghui Wang. 2020. Improved predictive sparse decomposition method with densenet for prediction of lung cancer. \textit{Int. J. Comput}, 1:533–541.

\bibitem[Mirzadeh et al.2020]{mirzadeh2020improved}
Seyed Iman Mirzadeh, Mehrdad Farajtabar, Ang Li, Nir Levine, Akihiro Matsukawa, and Hassan Ghasemzadeh. 2020. Improved knowledge distillation via teacher assistant. In \textit{Proceedings of the AAAI conference on artificial intelligence}, volume 34, pages 5191–5198.

\bibitem[Ni et al.2022]{ni2022large}
Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernandez Abrego, Ji Ma, Vincent Zhao, Yi Luan, Keith Hall, Ming-Wei Chang, et al. 2022. Large dual encoders are generalizable retrievers. In \textit{Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing}, pages 9844–9855.

\bibitem[Nogueira et al.2019]{nogueira2019doc2query}
Rodrigo Nogueira, Jimmy Lin, and AI Epistemic. 2019. From doc2query to doctttttquery. \textit{Online preprint}, 6:2.

\bibitem[Peng et al.2019]{peng2019correlation}
Baoyun Peng, Xiao Jin, Jiaheng Liu, Dongsheng Li, Yichao Wu, Yu Liu, Shunfeng Zhou, and Zhaoning Zhang. 2019. Correlation congruence for knowledge distillation. In \textit{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 5007–5016.

\bibitem[Qin et al.2024]{qin2024large}
Zhen Qin, Rolf Jagerman, Kai Hui, Honglei Zhuang, Junru Wu, Le Yan, Jiaming Shen, Tianqi Liu, Jialu Liu, Donald Metzler, et al. 2024. Large language models are effective text rankers with pairwise ranking prompting. In \textit{Findings of the Association for Computational Linguistics: NAACL 2024}, pages 1504–1518.

\bibitem[Qu et al.2021]{qu2021rocketqa}
Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Wayne Xin Zhao, Daxiang Dong, Hua Wu, and Haifeng Wang. 2021. Rocketqa: An optimized training approach to dense passage retrieval for open-domain question answering. In \textit{Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, pages 5835–5847.

\bibitem[Ren et al.2021a]{ren2021pair}
Ruiyang Ren, Shangwen Lv, Yingqi Qu, Jing Liu, Wayne Xin Zhao, Qiaoqiao She, Hua Wu, Haifeng Wang, and Ji-Rong Wen. 2021a. Pair: Leveraging passage-centric similarity relation for improving dense passage retrieval. In \textit{Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021}, pages 2173–2183.

\bibitem[Ren et al.2021b]{ren2021rocketqav2}
Ruiyang Ren, Yingqi Qu, Jing Liu, Wayne Xin Zhao, Qiaoqiao She, Hua Wu, Haifeng Wang, and Ji-Rong Wen. 2021b. Rocketqav2: A joint training method for dense passage retrieval and passage re-ranking. In \textit{Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing}, pages 2825–2835.

\bibitem[Robertson et al.2009]{robertson2009probabilistic}
Stephen Robertson, Hugo Zaragoza, et al. 2009. The probabilistic relevance framework: Bm25 and beyond. \textit{Foundations and Trends\textregistered in Information Retrieval}, 3(4):333–389.

\bibitem[Son et al.2021]{son2021densely}
Wonchul Son, Jaemin Na, Junyong Choi, and Wonjun Hwang. 2021. Densely guided knowledge distillation using multiple teacher assistants. In \textit{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 9395–9404.

\bibitem[Sun et al.2024]{sun2024lead}
Hao Sun, Xiao Liu, Yeyun Gong, Anlei Dong, Jingwen Lu, Yan Zhang, Linjun Yang, Rangan Majumder, and Nan Duan. 2024. Lead: Liberal feature-based distillation for dense retrieval. In \textit{Proceedings of the 17th ACM International Conference on Web Search and Data Mining}, pages 655–664.

\bibitem[Wang et al.2023]{wang2023simlm}
Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. 2023. SimLM: Pre-training with representation bottleneck for dense passage retrieval. In \textit{Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 2244–2258, Toronto, Canada. Association for Computational Linguistics.

\bibitem[Wu et al.2021]{wu2021one}
Chuhan Wu, Fangzhao Wu, and Yongfeng Huang. 2021. One teacher is enough? pre-trained language model distillation from multiple teachers. In \textit{Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021}, pages 4408–4413.

\bibitem[Wu et al.2023]{wu2023contextual}
Xing Wu, Guangyuan Ma, Meng Lin, Zijia Lin, Zhongyuan Wang, and Songlin Hu. 2023. Contextual masked auto-encoder for dense passage retrieval. In \textit{Proceedings of the AAAI Conference on Artificial Intelligence}, volume 37, pages 4738–4746.

\bibitem[Xiao et al.2022]{xiao2022retromae}
Shitao Xiao, Zheng Liu, Yingxia Shao, and Zhao Cao. 2022. Retromae: Pre-training retrieval-oriented language models via masked auto-encoder. In \textit{Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing}, pages 538–548.

\bibitem[Xiong et al.]{xiong2021approximate}
Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul N Bennett, Junaid Ahmed, and Arnold Overwijk. Approximate nearest neighbor negative contrastive learning for dense text retrieval. In \textit{International Conference on Learning Representations}.

\bibitem[Yang et al.2022]{yang2022cross}
Chuanguang Yang, Helong Zhou, Zhulin An, Xue Jiang, Yongjun Xu, and Qian Zhang. 2022. Cross-image relational knowledge distillation for semantic segmentation. In \textit{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 12319–12328.

\bibitem[Yuan et al.2021]{yuan2021reinforced}
Fei Yuan, Linjun Shou, Jian Pei, Wutao Lin, Ming Gong, Yan Fu, and Daxin Jiang. 2021. Reinforced multi-teacher selection for knowledge distillation. In \textit{Proceedings of the AAAI Conference on Artificial Intelligence}, volume 35, pages 14284–14291.

\bibitem[Zeng et al.2022]{zeng2022curriculum}
Hansi Zeng, Hamed Zamani, and Vishwa Vinay. 2022. Curriculum learning for dense retrieval distillation. In \textit{Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval}, pages 1979–1983.

\end{thebibliography}

\appendix
\section{Algorithm 1}
\begin{algorithm}[h]
\caption{MTA4DPR Training Process}
\label{alg:mta4dpr}
\begin{algorithmic}[1]
\REQUIRE $T$: the teacher model; $TA$: the assistant models; $M_\theta$: the student model; $Q$: the query set; $P$: the passage set; $max\_iter$: maximum number of training iterations; $max\_steps$: maximum number of training steps; $\eta$: Learning rate;
\ENSURE $M_\theta$
\STATE $i \gets 0$
\WHILE{$i < max\_iter$}
\STATE $D_{train}, D_{eval} \gets GenDataset(T, TA, Q, P)$
\REPEAT
\STATE $id_{bestTA} \gets TASelect(D_{train})$
\STATE $\theta \gets \theta - \eta \nabla_\theta \mathcal{L}_{total}(D_{train}, M_\theta, id_{bestTA})$
\UNTIL{$max\_steps$ reached}
\STATE $outperformed\_TA \gets Compare(M_\theta, TA, D_{eval})$
\IF{$outperformed\_TA$}
\STATE remove $Worst(TA)$
\STATE add $M_\theta$ into $TA$
\ENDIF
\STATE $i \gets i + 1$
\ENDWHILE
\end{algorithmic}
\end{algorithm}

\end{document}
=====END FILE=====