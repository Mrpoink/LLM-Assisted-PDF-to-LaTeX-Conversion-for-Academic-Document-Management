\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Background and Definition}{3}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}The MATHTRAP Dataset}{3}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Dataset Composition}{3}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Evaluation Protocol}{3}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Results and Discussion}{3}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}The Compositionality of LLMs}{3}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}The Compositionality of Human}{4}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Mitigating LLMs' Failure on MathTrap}{4}{subsection.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusions}{4}{section.5}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Overview of the MATHTRAP Dataset. The first column represents the five introduced trap types and their percentages in the dataset. The yellow highlighted text emphasizes the difference in problem descriptions before and after introducing traps. Additionally, we annotate Conceptual Problems to test whether models possess traplated knowledge. We hope that if a model can accurately answer both the Original Problems and the Conceptual Problems, it will also be able to accurately answer the Trap Problems. Appendix section 3.1 provides definitions of the trap types, and Table 11 offers explanations for these 5 example traps. We have included GPT-4-0125- preview's responses to selected problem from the table in Appendix Tables 12-15.}}{7}{table.1}\protected@file@percent }
\newlabel{tab:dataset_overview}{{1}{7}{Overview of the MATHTRAP Dataset. The first column represents the five introduced trap types and their percentages in the dataset. The yellow highlighted text emphasizes the difference in problem descriptions before and after introducing traps. Additionally, we annotate Conceptual Problems to test whether models possess traplated knowledge. We hope that if a model can accurately answer both the Original Problems and the Conceptual Problems, it will also be able to accurately answer the Trap Problems. Appendix section 3.1 provides definitions of the trap types, and Table 11 offers explanations for these 5 example traps. We have included GPT-4-0125- preview's responses to selected problem from the table in Appendix Tables 12-15}{table.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Accuracy $(\%)$ of various models on three types of MATHTRAP problems. Conceptual' represents Conceptual problems, Original' refers to the original problems, and Trap' denotes the trap problems. Ratio' refers to the ratio of the accuracy on Trap problems to the accuracy on Original problems. It reflects the degree to which the performance is maintained when facing problems with traps, relative to the original problems.}}{8}{table.2}\protected@file@percent }
\newlabel{tab:model_results}{{2}{8}{Accuracy $(\%)$ of various models on three types of MATHTRAP problems. Conceptual' represents Conceptual problems, Original' refers to the original problems, and Trap' denotes the trap problems. Ratio' refers to the ratio of the accuracy on Trap problems to the accuracy on Original problems. It reflects the degree to which the performance is maintained when facing problems with traps, relative to the original problems}{table.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Human accuracy $(\%)$ on MATHTRAP. "Trap Problem (w/o Notice)" refers to the accuracy of human solutions when unaware that the problems contain traps. "Trap Problem (w/Notice)" indicates the accuracy of human solutions when informed that the problems contain traps. "Original Problem" refers to the accuracy of human solutions on the original problems.}}{8}{table.3}\protected@file@percent }
\newlabel{tab:human_results}{{3}{8}{Human accuracy $(\%)$ on MATHTRAP. "Trap Problem (w/o Notice)" refers to the accuracy of human solutions when unaware that the problems contain traps. "Trap Problem (w/Notice)" indicates the accuracy of human solutions when informed that the problems contain traps. "Original Problem" refers to the accuracy of human solutions on the original problems}{table.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces The impact of external intervention methods on the accuracy for original problems and trap problems. "w/o Notice" refers to the control experiment without any external intervention. "w/ Notice" indicates using a natural language prompt to inform the model that the problem description may contain traps. ICL (1/5-shot) refers to adding one or five demonstrations in the context to exemplify how to handle trap problems. The prompt templates employed are presented in Tables 8-10 in the Appendix.}}{9}{table.4}\protected@file@percent }
\newlabel{tab:intervention_results}{{4}{9}{The impact of external intervention methods on the accuracy for original problems and trap problems. "w/o Notice" refers to the control experiment without any external intervention. "w/ Notice" indicates using a natural language prompt to inform the model that the problem description may contain traps. ICL (1/5-shot) refers to adding one or five demonstrations in the context to exemplify how to handle trap problems. The prompt templates employed are presented in Tables 8-10 in the Appendix}{table.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces The impact of fine-tuning data configurations on the accuracy for original and trap problems. We use Llemma as the foundation model. The parentheses indicate the judge model used.}}{9}{table.5}\protected@file@percent }
\newlabel{tab:finetune_results}{{5}{9}{The impact of fine-tuning data configurations on the accuracy for original and trap problems. We use Llemma as the foundation model. The parentheses indicate the judge model used}{table.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Prompt template used for evaluating the Trap Problem across various Large Language Models (LLMs) using GPT- 4.}}{9}{table.6}\protected@file@percent }
\newlabel{tab:prompt_eval}{{6}{9}{Prompt template used for evaluating the Trap Problem across various Large Language Models (LLMs) using GPT- 4}{table.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Prompt template used for answer augmentation of the Trap Problem using GPT- 4.}}{9}{table.7}\protected@file@percent }
\newlabel{tab:prompt_aug}{{7}{9}{Prompt template used for answer augmentation of the Trap Problem using GPT- 4}{table.7}{}}
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces The prompt template used for directly suggesting to large language models (LLMs) that the problem might be unreasonable.}}{9}{table.8}\protected@file@percent }
\newlabel{tab:prompt_notice}{{8}{9}{The prompt template used for directly suggesting to large language models (LLMs) that the problem might be unreasonable}{table.8}{}}
\@writefile{lot}{\contentsline {table}{\numberline {9}{\ignorespaces The prompt template used under the 1- shot setting for in- context learning.}}{10}{table.9}\protected@file@percent }
\newlabel{tab:prompt_1shot}{{9}{10}{The prompt template used under the 1- shot setting for in- context learning}{table.9}{}}
\@writefile{lot}{\contentsline {table}{\numberline {10}{\ignorespaces The prompt template used under the 5- shot setting for in- context learning.}}{10}{table.10}\protected@file@percent }
\newlabel{tab:prompt_5shot}{{10}{10}{The prompt template used under the 5- shot setting for in- context learning}{table.10}{}}
\@writefile{lot}{\contentsline {table}{\numberline {11}{\ignorespaces Responses of GPT-4-0125- preview to Trap Problems. "Output(w/o Notice)" refers to the model's output when no additional prompt is provided, whereas "Output(w/ Notice)" denotes the outputs when the model is informed that the problem may be unreasonable. The sections highlighted in yellow delineate the distinction between original problems and trap problems. The green sections represent instances where the model's final answers are correct, while the red sections indicate where the model's final answers are incorrect.}}{11}{table.11}\protected@file@percent }
\newlabel{tab:gpt4_responses}{{11}{11}{Responses of GPT-4-0125- preview to Trap Problems. "Output(w/o Notice)" refers to the model's output when no additional prompt is provided, whereas "Output(w/ Notice)" denotes the outputs when the model is informed that the problem may be unreasonable. The sections highlighted in yellow delineate the distinction between original problems and trap problems. The green sections represent instances where the model's final answers are correct, while the red sections indicate where the model's final answers are incorrect}{table.11}{}}
\@writefile{lot}{\contentsline {table}{\numberline {12}{\ignorespaces Responses of GPT-4-0125-preview to both original and conceptual problems. The sections highlighted in yellow delineate the distinction between original problems and trap problems. The green sections represent instances where the model’s final answers are correct.}}{12}{table.12}\protected@file@percent }
\newlabel{tab:gpt4_orig_concept}{{12}{12}{Responses of GPT-4-0125-preview to both original and conceptual problems. The sections highlighted in yellow delineate the distinction between original problems and trap problems. The green sections represent instances where the model’s final answers are correct}{table.12}{}}
\bibcite{anil2022exploring}{1}
\bibcite{azerbayev2024llemma}{2}
\bibcite{bian2024chatgpt}{3}
\bibcite{bubeck2023sparks}{4}
\bibcite{cobbe2021training}{5}
\bibcite{dziri2023faith}{6}
\bibcite{fodor1988connectionism}{7}
\bibcite{gou2024tora}{8}
\bibcite{guo2024deepseekcoder}{9}
\bibcite{he2024selfdemos}{10}
\bibcite{hendrycks2021measuring}{11}
\bibcite{hilbert1922grundlagen}{12}
\bibcite{hosseini2022compositional}{13}
\bibcite{hu2024casebased}{14}
\bibcite{kazemi2023lambada}{15}
\bibcite{koralus2023humans}{16}
\bibcite{lake2023human}{17}
\bibcite{luo2023wizardmath}{18}
\bibcite{miao2020diverse}{19}
\bibcite{openai2024o1}{20}
\bibcite{patel2021nlpmodels}{21}
\bibcite{sanyal2022robustlr}{22}
\bibcite{tang2024paradox}{23}
\bibcite{toshniwal2024openmathinstruct}{24}
\bibcite{wu2024reasoning}{25}
\bibcite{xi2024training}{26}
\bibcite{yu2023metamath}{27}
\bibcite{zhang2023counterfactual}{28}
\bibcite{zheng2024opencodeinterpreter}{29}
\gdef \@abspage@last{14}
