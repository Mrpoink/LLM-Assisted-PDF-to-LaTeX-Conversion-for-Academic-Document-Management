\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Our Approach}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Experiments}{6}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Narrative Retrieval}{7}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}In-Task Performance}{7}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}In-Domain Adaptation: Movie Remake Dataset}{7}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.3}Retellings}{8}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.4}Segment Retrieval}{8}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Narrative Understanding: ROCStories}{9}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Results}{10}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Movie Remakes}{11}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Retellings}{12}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Scene Retrieval}{12}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Story Cloze: ROCStories}{13}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Approximate Attribution}{13}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Attribution scores on individual tokens in the final layer of our StoryEmb model are shown as a delta from the E5 model. Negative scores indicate less contribution to the similarity in the StoryEmb model. In the example, it seems clear that less emphasis is placed on named entities.}}{14}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Qualitative Exploration}{14}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8}Conclusion}{15}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {9}Future Work}{15}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Pair of narratively similar segment summaries.}}{16}{}\protected@file@percent }
\newlabel{fig:segments}{{2}{16}{}{figure.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {10}Limitations}{16}{}\protected@file@percent }
\bibstyle{plain}
\bibcite{BehnamGhader2024}{1}
\bibcite{Cer2017}{2}
\bibcite{Chambers2008}{3}
\bibcite{Chambers2009}{4}
\bibcite{Chaturvedi2018}{5}
\@writefile{toc}{\contentsline {section}{\numberline {11}Ethical Considerations}{17}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {12}Acknowledgements}{17}{}\protected@file@percent }
\bibcite{Chen2022a}{6}
\bibcite{Chen2022b}{7}
\bibcite{Gao2021}{8}
\bibcite{Glass2022}{9}
\bibcite{Goldman2023}{10}
\bibcite{GranrothWilding2016}{11}
\bibcite{Hatzel2023}{12}
\bibcite{Hatzel2024}{13}
\bibcite{Jiang2023a}{14}
\bibcite{Jiang2023b}{15}
\bibcite{Kukkonen2019}{16}
\bibcite{Lau2016}{17}
\bibcite{Le2014}{18}
\bibcite{Lee2020}{19}
\bibcite{Mann1947}{20}
\bibcite{Manning2008}{21}
\bibcite{Moeller2024}{22}
\bibcite{Mostafazadeh2016}{23}
\bibcite{Ni2022}{24}
\bibcite{Reimers2019}{25}
\bibcite{Springer2024}{26}
\bibcite{Sundararajan2017}{27}
\bibcite{Wang2024}{28}
\bibcite{Wei2022}{29}
\@writefile{toc}{\contentsline {section}{\numberline {A}LLM Judge}{21}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {B}Retelling Dataset}{21}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {C}Retelling Dataset Results}{21}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {D}Data \& Code Availability}{22}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Retrieval performance on the Tell-Me-Again test set by Hatzel and Biemann (2024), with and without their anonymization strategy.}}{22}{}\protected@file@percent }
\newlabel{tab:table1}{{1}{22}{}{table.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Test set retrieval performance on the dataset by Chaturvedi et al. (2018), with and without the anonymization strategy by Hatzel and Biemann (2024) applied to the dataset. \(^{**}\) + 2 steps" denotes two additional steps of training.}}{22}{}\protected@file@percent }
\newlabel{tab:table2}{{2}{22}{}{table.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Retrieval performance on retelling dataset introduced in Section 4.1.3, optionally with the movie remakes added as distractors.}}{22}{}\protected@file@percent }
\newlabel{tab:table3}{{3}{22}{}{table.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Mean narrative similarity score on a scale of 1- 10 in top vs. bottom ranked scenes in terms of similarity as judged by an LLM judge or an annotator, after removing obvious duplicates. The first author performed the annotations.}}{23}{}\protected@file@percent }
\newlabel{tab:table4}{{4}{23}{}{table.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces We list the accuracy at picking the correct story ending from two options on the ROCStories dataset. The superscript \(\Delta \) denotes that the embedding distance approach outlined in Section 4.2 is used and evaluated on the development set. The GPT-3 and FLAN results are taken from Wei et al. (2022), and the supervised RoBERTa result is taken from Jiang et al. (2023b).}}{23}{}\protected@file@percent }
\newlabel{tab:table5}{{5}{23}{}{table.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces The average contribution to sentence similarity of selected named-entity and parts-of-speech tags was analyzed on layer 31 of the E5 and StoryEmb models. The statistics exclude our task prefix.}}{23}{}\protected@file@percent }
\newlabel{tab:table6}{{6}{23}{}{table.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Retrieval performance on retelling dataset introduced in Section 4.1.3}}{23}{}\protected@file@percent }
\newlabel{tab:table7}{{7}{23}{}{table.7}{}}
\gdef \@abspage@last{23}
