=====FILE: main.tex=====
\documentclass[11pt]{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{multirow}
\usepackage{array}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{url}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
}
\usepackage{geometry}
\geometry{a4paper, margin=1in}

\title{Back to School: Translation Using Grammar Books}
\author{Jonathan Hus$^\alpha$ \and Antonios Anastasopoulos$^{\alpha,\beta}$ \\
$^\alpha$ Department of Computer Science, George Mason University, VA, USA \\
$^\beta$ Archimedes AI Unit, Athena Research Center, Athens, Greece \\
\{jhus,antonis\}@gmu.edu}
\date{}

\begin{document}

\maketitle

\begin{abstract}
Machine translation systems for high resource languages perform exceptionally well and produce high quality translations. Unfortunately, the vast majority of languages lack the quantity of parallel sentences needed to train such systems. These under- represented languages are not entirely without resources, as bilingual dictionaries and grammar books may be available as linguistic reference material. With current large language models (LLMs) supporting near book- length contexts, we can use the available material to ensure advancements are shared among all of the world's languages. In this paper, we use dictionaries and grammar books to improve machine translation. We evaluate on 16 typologically diverse low- resource languages, showing encouraging improvements.
\end{abstract}

\section{Introduction}
Machine translation systems have progressed remarkably, but they require massive amounts of parallel sentences (Bapna et al., 2022). More recently, instruction- tuned large language models (LLMs) have also proven capable of performing machine translation. However, their performance is best when translating among high- resource languages that were most likely seen during training. Current transformer- based state- of- the- art large language models and multilingual translation models are trained on huge web- scraped corpora, with data in the order of trillions of tokens.

text[[115, 758, 486, 869], [511, 251, 881, 444]]
While the web is a vast resource of good training data, the web is also mainly comprised of just a handful of languages. There are an estimated 7000 languages in the world, but just 10 languages cover \(84\%\) of the web content, with English covering more than \(50\%\) . Therefore, low- resource languages are not well- represented in the training data for the large language models (Joshi et al., 2020), leading to systematic performance disparities across languages (Blasi et al., 2022). More importantly, language translation systems rely on a large number of parallel sentences, providing examples of sentences in the source and target languages. Therefore, the sheer magnitude of data that current translation systems require is simply not available for low resource languages. Given these constraints, the compelling question is: how can we create well- performing translation systems for low resource languages?

One approach to enabling machine translation for low- resource languages is to collect many parallel sentences. However, this is laborious, expensive, and time- consuming, requiring the skills of linguists and native speakers. Another approach would be to incorporate language reference material into the translation process of the LLM. The advantage of this approach is that a good number of dictionaries and grammar books have been created over decades (and longer) and require little additional effort to use them.

In this work, we push the frontier using the latter approach to improve on the ability of LLMs to perform machine translation of low- resource languages by utilizing available linguistic reference materials. We incorporate dictionaries, grammar books, and a small number of parallel sentences into the prompt of a state- of- the- art LLM. We evaluate on 16 typologically diverse low- resource languages, performing analyses using different combinations of reference materials.

\section{Related Work}
While tens of high- resource languages have enjoyed the recent advances in machine translation, many of the world's \(7000+\) languages have been unable to partake in the success.

The current state of the art in multilingual and low- resource translation is the No Language Left

\noindent
 Behind model (NLLB Team et al., 2022), relying on a mined and curated corpus of parallel sentences for 200 languages, including many low- resource ones. A large multilingual encoder- decoder translation model was then trained on this data to create a machine translation system for these languages.

On the other end of the spectrum, Tanzer et al. (2023) incorporated dictionaries, sentences, and grammar books to perform machine translation in a zero- shot setting, i.e., in a language without any other data available, akin to how a documentary linguist or any second- language learner might learn a new language ("Machine Translation from One Book (MTOB)"). This paper inspired our own work, as it provides a framework for using LLMs to perform translation of resource- scarce languages. However, they were limited in the size of the context for the models they chose, and therefore, were only able to extract smaller chunks of the grammar book for inclusion. Here, we explore this paradigm in a much larger scale, with 15 more languages, performing additional necessary analyses.

Last, Zhang et al. (2024) explored a similar path utilizing grammar books. They were also limited by the size of the model context, but they additionally used a morphological analyzer on the grammar books to extract linguistic features to assist in translation. Such tools are unfortunately unavailable for all languages, making this approach not feasible for scaling to thousands of languages.

\section{Preliminaries and Problem Definition}
A traditional neural MT system models \(p_{\mathrm{MT}}(\mathbf{y}|\mathbf{x})\) learned over source- target sentence pairs \(\langle \mathbf{x},\mathbf{y}\rangle\) .At inference time, given a new source sentence, we sample a high- probability output from the learned distributions. A SOTA LLM, however, is first pre- trained to model \(p_{\mathrm{LM}}(x)\) and then instructiontuned on \(p_{\mathrm{LM - ins}}(\mathbf{y}|\pi)\) over prompt- target text pairs \(\langle \pi ,\mathbf{y}\rangle\) covering multiple downstream tasks (often including MT). At inference time, with a similar prompt we sample outputs from the final model.

A translation prompt \(\pi (\cdot)\) at a minimum needs to include the task definition t (e.g. "Please translate the following sentence to French:") and the source sentence \(\mathbf{x}\) .. \(\pi (\mathbf{x},\mathbf{t})\) For learning to translate an entirely unseen language, Tanzer et al. (2023) crafted prompts \(\pi (\mathbf{x},\mathbf{t},\mathbf{d},\mathbf{s},\mathbf{g})\) that additionally included:

word- level translations d obtained from a bilingual dictionary \(\mathcal{D}\) selected for their similarity to

the words of the given source sentence, a few parallel sentence examples s, selected from a small collection of parallel sentences \(\mathcal{S}\) for their similarity to the given source sentence, and excerpts g from a grammar book \(\mathcal{G}\) , also selected for similarity to the source sentence using longest common substring distance.

\section{Experiments}
Languages We focus on 16 largely under- served low- resource languages, chosen for geographical and typological diversity, as well as resource (dictionary, grammars) and evaluation data availability. Specifically, we work with: Chokwe, Chuvash, Dinka, Dogri, Gitskan, Guarani, Ilokano, Kabuverdianu, Kachin, Kalamang, Kimbundu, Latgalian, Minangkabau, Mizo, Natugu, and Wolof. We evaluate translation both into and out of English.

Dictionaries We obtain dictionaries from PanLex for all our languages. Note that, in cases where the number of words in the dictionary was less than 100 we do not include them in the prompt. The size of each dictionary is included in Appendix B.

Parallel Sentences For the parallel sentences that are part of the prompts as translation examples, we use the dev portion of the FLORES- 200 dataset.4 Gitskan and Natugu are not represented in FLORES and instead we use the data that Zhang et al. (2024) provided.

Grammar Books The DReaM corpus (Virk et al., 2020) contains digitized versions of thousands of linguistic documents, including grammar books and sketches, for many languages. The source of these documents is often in paper format, and due to the scanning/OCR quality, the digitized versions often contain scanning artifacts. We select one grammar document for each of our languages (concrete details in Appendix B). We perform slight manual cleanup to remove some items (e.g., scanning artifacts, table of contents) and to ensure that the grammar would fit in the LLM's context size.

Evaluation We use the devtest portion of FLORES- 200 as our evaluation set. For Gitskan and Natugu, we use the test sets from the SIGMORPHON 2023 shared task (Ginn et al., 2023).

\subsection{Model}
We use the GPT- 4- turbo model for our experiments. In addition to being the latest offering from OpenAI (and presumably its most capable, at the time of writing), it has an input context size of 128K. This large context enables book- length text to be included in the prompt. The grammar books we use range from tens of pages to a couple hundred pages in length, which equates to roughly 40K to 120K tokens. Models with such capacity have only recently been made available, which affords us the opportunity to use full- length grammar books as opposed to smaller heuristically- selected excerpts.

text[[115, 805, 485, 916], [511, 537, 881, 616]]
Prompt Format Our prompts largely follow the MTOB framework, using complete prompts \(\pi (\mathbf{x}, \mathbf{t}, \mathbf{d}, \mathbf{s}, \mathbf{g})\) with task instructions and source sentence (provided in the prompt beginning and repeated at the end), as well as word pairs from the dictionary, example sentences, and the language's grammar. We perform ablations removing compo nents from the prompt to establish their contributions, e.g. repeating all experiments without incorporating the grammar book, i.e. using \(\pi (\mathbf{x}, \mathbf{t}, \mathbf{d}, \mathbf{s})\) . We provide specific details as well as an example prompt in Appendix C.

\section{Results}
Table 1 shows the results for the experiments. We report results on both translation directions, with different prompt configurations as discussed above. We report two comparison models: Baseline corresponds to 0- shot LLM translation performance i.e., only with prompt \(\pi (\mathbf{x}, \mathbf{t})\) , and the "skyline" performance of NLLB, the current SOTA multilingual MT model. We also report results by adding words \((\mathbf{W}: \pi (\mathbf{x}, \mathbf{t}, \mathbf{d}))\) , sentences \((\mathbf{W} + \mathbf{S}: \pi (\mathbf{x}, \mathbf{t}, \mathbf{d}, \mathbf{s})\) , and grammars \((\mathbf{W} + \mathbf{S} + \mathbf{G}: \pi (\mathbf{x}, \mathbf{t}, \mathbf{d}, \mathbf{s}, \mathbf{g}))\) to the prompt.

For each language and direction, we have four systems that we compare. We compute all evaluation metrics using SacreBLEU \(^5\) (Post, 2018) and we also report statistical significance using paired bootstrap resampling, comparing our best

\noindent
 performing system to the other systems. In most cases, we find that the difference is statistically significant, indicating that the translation performance is dependent on the selected prompt content.

\subsection{Comparison to SOTA MT}
We compare the best results we achieved with the chrF++ scores from NLLB, for the languages supported by NLLB. Note that these are languages with at least some online presence. In general the NLLB scores were better, but there were a few instances where our approach outperformed NLLB. When going from English to a target language, including words and sentences in the prompt for Kabuverdianu and Kimbundu provided the best results. For Kabuverdianu, including the grammar book also surpassed the NLLB score. When translating Kabuverdianu into English, the baseline model (0- shot) with no reference material was best. Kabuverdianu, as a Portuguese- based Creole, has many similarities to Portuguese, a high resource language. This might explain this result and it could be reflective of GPT- 4's capabilities.

\subsection{Sentences or Grammar Books?}
The results of our experiments show that the inclusion of grammar books does not always lead to the best score (see bottom rows of Table 1). In fact, when translating from English, using only words and sentences yields the highest score for 12 of the languages. When translating into English, the combination of words, sentences, and grammar books had the highest score for six of the languages. However, including no reference material at all was the best approach for six languages as well.

To explore the reasons behind these results, we perform a linear regression that aims to predict the score of the \(W + S + G\) combination given the baseline score and the following features:

Number of words in the reference dictionary Number of sentences available in corpora as reported in OPUS (Tiedemann, 2009)6 Perplexity of the grammar book Length of the grammar book in tokens

The features regarding words and sentences correspond directly to data availability, with the assumption that more data is better. The grammar book features are proxies for the quality and the completeness of the documented grammar. For perplexity, we used a GPT- 2 model and passed the

\begin{center}
\begin{figure}[h]
\centering
\fbox{\parbox{0.8\linewidth}{\centering IMAGE NOT PROVIDED}}
\caption{Using grammars is particularly beneficial for extremely low-resource languages. Simple promptbased MT (zero-shot) is best for high-resource ones.}
\end{figure}
\end{center}

\noindent
grammar book as input to the model. LM perplexity is then measured using a sliding window strategy.

The \(R^2\) values for these regressions are listed in Table 2. Put simply, the \(R^2\) value denotes the quality of the model fit, and can help us determine the percentage of variance in the dependent variable (downstream performance, in our case) that can be explained by the independent variable.

We find that the number of dictionary words and the length of the grammar books have a positive influence on the score, while the perplexity has a negative impact. While this aligns with our expectations, a finding that is seemingly surprising is that the number of available sentences has a negative impact on the score compared to the baseline. This necessitates further research to actually confirm, but we suspect that this is because GPT- 4 has already been pre- trained on data from these languages and, consequently, it can perform better on them. This is most pronounced when translating into English, where the top 5 languages (by number of sentences) all perform best under the baseline setting i.e., no additional reference material. All languages that are best translated using no reference material appear before all of the languages that are best translated using the combination of dictionaries, parallel sentences, and grammar books. This suggests that using grammars might be best suited to extremely low- resource languages with less than \(10^3\) parallel sentences.

\section{Conclusion}
In this paper, we showed that utilizing reference material such as dictionaries and grammar books in the prompt of an LLM can improve the performance of machine translation for low- resource languages. We evaluated the performance on 16 languages and showed that the improvement is especially pronounced for languages that have minimal presence on the web. Our work shows that this approach has the potential to address the gap for extremely low- resource languages and identifies a concrete path for improving MT for more than 2,000 languages.

\section{Limitations}
A primary contribution of this paper is the use of full- length grammar books in the input prompt in order to "teach" a model how to translate into a given language. However, there are some limitations with this approach. First, high quality grammar books are difficult to obtain for many languages. The DReaM corpus does an admirable job of curating and digitizing many linguistic references, but the output is not perfect. Multi- column text documents and tables lose information that is conveyed by the location of text relative to other text on the page. The LLMs, therefore, are most likely not taking full advantage of that information. Additionally, scanning artifacts like headers and page numbers add unnecessary clutter to the reference material.

At the time of this writing, GPT- 4- turbo was the only available model with the desired context length of 128K. Running the experiments using a set of models would indicate whether the reference material is improving translations or whether the model itself (and its associated training) is responsible for the performance.

The sizes of the bilingual dictionaries were inconsistent, with a handful having less than 20 words. We removed these low- volume dictionaries from our experiments. However, larger dictionaries of similar magnitudes would most likely improve the translations and would allow translation performance across the various languages to be better compared.

Finally, these experiments are not cheap. We estimate that all these experiments cost around \$15,000 USD using the standard pricing tier under the Azure Open AI Studio. This could significantly hinder the reproducibility of our results.

\section{Ethics Statement}
We do not anticipate any ethical issues arising from our work.

\section{Acknowledgements}
We are thankful to the reviewers and meta- reviewer for their constructive feedback. This work was generously supported by the National Science Foundation under grant IIS- 2327143. It has also benefited from resources provided through the Microsoft Accelerate Foundation Models Research (AFMR) grant program. This work was partially supported by resources provided by the Office of Research Computing at George Mason University (URL: https://orc.gmu.edu) and funded in part by grants from the National Science Foundation (Award Number 2018631).

\section*{References}
\begin{thebibliography}{00}

\bibitem{bapna2022} Ankur Bapna, Isaac Caswell, Julia Kreutzer, Orhan Firat, Daan van Esch, Aditya Siddhant, Mengmeng Niu, Pallavi Baljekar, Xavier Garcia, Wolfgang Macherey, Theresa Breiner, Vera Axelrod, Jason Riesa, Yuan Cao, Mia Xu Chen, Klaus Macherey, Maxim Krikun, Pidong Wang, Alexander Gutkin, Apurva Shah, Yanping Huang, Zhifeng Chen, Yonghui Wu, and Macduff Hughes. 2022. Building machine translation systems for the next thousand languages.

\bibitem{blasi2022} Damian Blasi, Antonios Anastasopoulos, and Graham Neubig. 2022. Systematic inequalities in language technology performance across the world's languages. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5486- 5505, Dublin, Ireland. Association for Computational Linguistics.

\bibitem{ginn2023} Michael Ginn, Sarah Moeller, Alexis Palmer, Anna Stacey, Garrett Nicolai, Mans Hulden, and Miikka Silfverberg. 2023. Findings of the SIGMORPHON 2023 shared task on interlinear glossing. In Proceedings of the 20th SIGMORPHON workshop on Computational Research in Phonetics, Phonology, and Morphology, pages 186- 201, Toronto, Canada. Association for Computational Linguistics.

\bibitem{joshi2020} Pratik Joshi, Sebastian Santy, Amar Budhiraja, Kalika Bali, and Monojit Choudhury. 2020. The state and fate of linguistic diversity and inclusion in the NLP world. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6282- 6293, Online. Association for Computational Linguistics.

\bibitem{kamholz2014} David Kamholz, Jonathan Pool, and Susan Colowick. 2014. PanLex: Building a resource for panlingual lexical translation. In Proceedings of the Ninth International Conference on Language Resources and

\noindent
 Evaluation (LREC'14), pages 3145-3150, Reykjavik, Iceland. European Language Resources Association (ELRA).

\bibitem{nllb2022} NLLB Team, Marta R. Costa- jussa, James Cross, Onur Celebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loic Barrault, Gabriel Mejia- Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzman, Philipp Koehn, Alexandre Mourachko, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, and Jeff Wang. 2022. No language left behind: Scaling humancentered machine translation.

\bibitem{popovic2017} Maja Popovic. 2017. chrF++: words helping character n- grams. In Proceedings of the Second Conference on Machine Translation, pages 612- 618, Copenhagen, Denmark. Association for Computational Linguistics.

\bibitem{post2018} Matt Post. 2018. A call for clarity in reporting BLEU scores. In Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186- 191, Belgium, Brussels. Association for Computational Linguistics.

\bibitem{tanzer2023} Garrett Tanzer, Mirac Suzgun, Eline Visser, Dan Jurafsky, and Luke Melas- Kyriazi. 2023. A benchmark for learning to translate a new language from one grammar book. In Arxiv.

\bibitem{tiedemann2009} Jorg Tiedemann. 2009. News from OPUS - A Collection of Multilingual Parallel Corpora with Tools and Interfaces, volume V, pages 237- 248.

\bibitem{virk2020} Shafqat Mumtaz Virk, Harald Hammarstrom, Markus Forsberg, and Soren Wichmann. 2020. The DReaM corpus: A multilingual annotated corpus of grammars for the world's languages. In Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 878- 884, Marseille, France. European Language Resources Association.

\bibitem{zhang2024} Kexun Zhang, Yee Man Choi, Zhenqiao Song, Taiqi He, William Yang Wang, and Lei Li. 2024. Hire a linguist!: Learning endangered languages with incontext linguistic descriptions.

\end{thebibliography}

\appendix

\section{Additional Experimental Results}
Table 3 shows the best performing system for each language and direction, sorted in descending order by number of available sentences as reported by OPUS.

Table 4 and Table 5 show the results from our paired significance tests. The best performing system for a given language and direction is compared to each of the other systems, with statistically significant differences indicated with an asterisk.

The main paper uses chrF++ scores to evaluate translations, which is the metric used by NLLB. We also calculate BLEU scores for all of our experiments, which are provided in Table 6.

\section{Resources}
For our experiments, we gathered dictionaries, parallel sentences, and grammar books to use in the prompts. Dictionaries were obtained from PanLex (Kamholz et al., 2014) and converted into the format required by the code. The dictionary used in MTOB included part of speech tags for each word, which is unavailable in PanLex. Therefore, we did not include this feature in our dictionaries. The sizes of the dictionaries are shown in Table 8. Kala

\noindent
mang is not available in PanLex, and we instead used the version from the MTOB paper.

For sentences, we used the FLORES dataset, originally released by Meta as FLORES- 2007 and now maintained by the Open Language Data Initiative (OLDI) as FLORES+8. For each language in the dataset, the dev split has 997 sentences and the devtest split has 1012 sentences. We used dev sentences as sample sentences in the prompts, while devtest sentences are used as translation tasks for our system on which performance was measured. For Dogri and Chuvash only the dev split is available. We therefore randomly split the dev split into dev and devtest with 497 and 500 sentences, respectively. Gitskan and Natugu are not represented in FLORES and we obtain sentences from the SIGMORPHON 2023 Shared Task on Interlinear Glossing,9 which has dev, train, and test splits. These were combined to form dev and devtest splits. For Kalamang, the train and test splits as provided in the original paper were used unaltered. Table 8 lists the sizes of the train and test splits for each of the languages.

Grammar books were obtained from the DReaM corpus, which contains digitized versions of numerous linguistic reference materials. When selecting the specific grammar book or sketch to use for each language, we searched for documents that provided a well- rounded description, appeared to have been well- processed by optical character recognition, and would fit within the context of GPT- 4. For each document we performed limited formatting, such as removing the table of contents, in order to reduce the token count. Table 7 lists the source documents used for the grammar books as well as the number of tokens for each document. Perplexity was measured using a GPT- 2 model in order to provide a coarse assessment of the quality of the document. For Kalamang, we used the grammar book provided in MTOB. Specifically, we use the "long" version, which is a manually curated subset of Visser's grammar, that they tested on a Claude 2 model.

The authors of MTOB and the maintainers of FLORES+ explicitly request that this reference data, and the parallel sentences in particular, are not publicly hosted as plain text. This is to ensure that the resources are not web- scraped where they could

\noindent
potentially be included in the training data of future models, which would taint results of MT tests. In accordance with their requests, and with the same spirit in mind, we have password encrypted all reference material that we have posted and request that any users of our data do the same.

\section{Prompt Format}
Each sentence to be translated is formatted into a prompt for GPT- 4. The prompt has five components: prefix, words, sentences, grammar book, and suffix. The experiment configuration determines whether words (W), sentences (S), or grammar books (G) are included in the prompt. The prefix and suffix are always included in the prompt. In the following sections, we show the format of the prompt by example, using an Ilokano- to- English translation task. We heavily used the code provided by the authors of "Machine Translation from One Book" to generate the prompts.

\subsection{Prefix}
The prefix provides the task to perform (translation), the source and target languages, and the sentence to translate.

You are an expert translator. Translate the following sentence from Ilokano to English: Adu pay ti babbabassit a klase ti pusa ngem kadakuada a mangmangan iti babbabassit a klase ti ayup a kas iti kuneho, antelope, ken ugsa.

\subsection{Words}
For words, we attempt to retrieve the item from the bilingual dictionary. For each word in the source sentence, the top two matching words from the dictionary, as measured by LCS, are included in the prompt.

To help with the translation, here is one of the closest entries to Adu in the bilingual dictionary: Ilokano word: Adams English translation: Adams

To help with the translation, here is one of the closest entries to Adu in the bilingual dictionary: Ilokano word: adu English translation: many; lots of; majority; many; much

To help with the translation, here is one of the closest entries to pay in the bilingual dictionary: Ilokano word: payo English translation: correct; right

To help with the translation, here is one of the closest entries to pay in the bilingual dictionary: Ilokano word: pay English translation: just; please; again; still; yet; also

Additional word- level translations are provided for the remaining words of the source sentence.

\subsection{Sentences}
For sentences, we attempt to retrieve similar samples from our small corpus of parallel sentences. For each word in the source sentence, we find sentences that contain that word, as measured by LCS, and include the top two matches in the prompt.

To help with the translation, here is a translated sentence with words similar to "Adu" in a list of translated reference sentences:

Ilokano sentence: Adu dagti restaurant iti aglawlaw ti hardin, ket no iti malem ken rabii masansan nga adda dagiti libre a konsiero iti akintengnga a gazebo.

English translation: There are a number of restaurants surrounding the garden, and in the afternoons and evening there free concerts are often given from the central gazebo.

To help with the translation, here is a translated sentence with words similar to "Adu" in a list of translated reference sentences:

Ilokano sentence: Adu a gobierno ti mangsupul ti bakuna para iti nadumaduma a sakit para kadagiti sangaili a sumrek, wenno dagiti residente a rumuar iti pagilianda.

English translation: Many governments require visitors entering, or residents leaving, their countries to be vaccinated for a range of diseases.

Additional sentence- level translations are provided for the remaining words of the source sentence.

\subsection{Grammar Book}
We include the full grammar book in the prompt.

To help with the translation, here is the full text of a bilingual grammar book:

\#\# FULL BOOK INSERTED HERE \#\#

This is the end of the bilingual grammar book.

\subsection{Suffix}
The suffix reiterates the task and prompts for the appropriate translation.

Now write the translation.

Ilokano: Adu pay ti babbabassit a klase ti pusa ngem kadakuada a mangmangan iti babbabassit a klase ti ayup a kas iti kuneho, antelope, ken ugsa.

English translation:

\end{document}
=====END FILE=====