\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}WURA Dataset}{3}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Auditing and Cleaning mC4}{3}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}Language Contamination}{3}{subsubsection.2.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2}mC4 is a Great Source!}{3}{subsubsection.2.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Combination with Existing Language Resources and Non-African Languages}{3}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Experimental Setup}{3}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Model}{3}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Downstream Tasks}{4}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Cross-lingual Question Answering}{4}{subsubsection.3.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Machine Translation}{4}{subsubsection.3.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3}Summarization}{4}{subsubsection.3.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.4}Text Classification}{4}{subsubsection.3.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Baseline Models}{4}{subsection.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Result and Discussion}{4}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Downstream Performance}{4}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Cross-lingual Question Answering:}{4}{subsubsection.4.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}Machine Translation}{4}{subsubsection.4.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.3}Summarization}{4}{subsubsection.4.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.4}Text Classification}{4}{subsubsection.4.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Discussion}{5}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Results for Nigerian Pidgin}{5}{subsubsection.4.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}Impact of Data Quality on LMs}{5}{subsubsection.4.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}AfriTeVa V2 Large Model}{5}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Related Work}{5}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion}{5}{section.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8}Limitations}{5}{section.8}\protected@file@percent }
\bibstyle{unsrt}
\bibcite{abadji2022}{1}
\bibcite{adebara2022}{2}
\bibcite{adelani2022}{3}
\bibcite{adelani2023}{4}
\bibcite{agic2019}{5}
\bibcite{ahia2023}{6}
\bibcite{alabi2020}{7}
\bibcite{alabi2022}{8}
\bibcite{bapna2022}{9}
\@writefile{toc}{\contentsline {section}{\numberline {9}Acknowledgements}{6}{section.9}\protected@file@percent }
\bibcite{caswell2020}{10}
\bibcite{chung2022}{11}
\bibcite{conneau2019}{12}
\bibcite{conneau2020}{13}
\bibcite{devlin2019}{14}
\bibcite{dione2023}{15}
\bibcite{lample2019}{16}
\bibcite{hasan2021}{17}
\bibcite{hernandez2022}{18}
\bibcite{kreutzer2022}{19}
\bibcite{kudo2018}{20}
\bibcite{leong2022}{21}
\bibcite{nllb2022}{22}
\bibcite{ogueji2021}{23}
\bibcite{ogundepo2023}{24}
\bibcite{ogundepo2022}{25}
\bibcite{ortiz2019}{26}
\bibcite{palen2022}{27}
\bibcite{petrov2023}{28}
\bibcite{rae2021}{29}
\bibcite{raffel2020}{30}
\bibcite{rajpurkar2016}{31}
\bibcite{resnik1999}{32}
\bibcite{roberts2022}{33}
\bibcite{shazeer2020}{34}
\bibcite{xue2022}{35}
\bibcite{xue2021}{36}
\bibcite{acs2019}{37}
\@writefile{toc}{\contentsline {section}{\numberline {A}Data}{9}{appendix.A}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}mC4 Audit}{9}{subsection.A.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}Web Crawling}{9}{subsection.A.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {B}Tokenization}{9}{appendix.B}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {C}AfriTeVa V2 Large}{9}{appendix.C}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces MasakhaNews classification results: Evaluation is done using the weighted F1 score and the scores presented are averaged across 3 seeds. AfriTeVa V2 surpasses mT5-base by up to 10 points. The average scores excluding languages not in the mC4 corpus are also provided in AVGSL.}}{11}{table.1}\protected@file@percent }
\newlabel{tab:news_class_results}{{1}{11}{MasakhaNews classification results: Evaluation is done using the weighted F1 score and the scores presented are averaged across 3 seeds. AfriTeVa V2 surpasses mT5-base by up to 10 points. The average scores excluding languages not in the mC4 corpus are also provided in AVGSL}{table.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces MAFAND-MT results: Evaluation is done using the BLEU score and we obtain significantly better performance on average across all languages in both the en-xx and xx-en directions, except for ibo and pcm.}}{11}{table.2}\protected@file@percent }
\newlabel{tab:mt_results}{{2}{11}{MAFAND-MT results: Evaluation is done using the BLEU score and we obtain significantly better performance on average across all languages in both the en-xx and xx-en directions, except for ibo and pcm}{table.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces XL-SUM results: Performance based on Rouge-1, Rouge-2 and Rouge-L. AfriTeVa V2 is generally more effective than mT5.}}{11}{table.3}\protected@file@percent }
\newlabel{tab:xl_sum_results}{{3}{11}{XL-SUM results: Performance based on Rouge-1, Rouge-2 and Rouge-L. AfriTeVa V2 is generally more effective than mT5}{table.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Cross-lingual Question Answering Results: F1 and Exact Match (EM) Accuracy scores on the test set of AfriQA (Onguendo et al., 2023). For both metrics, AfriTeVa V2 outperforms mT5 except for twi.}}{12}{table.4}\protected@file@percent }
\newlabel{tab:qa_results}{{4}{12}{Cross-lingual Question Answering Results: F1 and Exact Match (EM) Accuracy scores on the test set of AfriQA (Onguendo et al., 2023). For both metrics, AfriTeVa V2 outperforms mT5 except for twi}{table.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Tokenizer Fertilities: We measure the fertilities of our tokenizers with varying vocabulary sizes using the MasakhanePOS dataset. The 150k tokenizer gives the best trade-off in size and fertility scores across all languages, especially in the second sampling configuration.}}{12}{table.5}\protected@file@percent }
\newlabel{tab:tokenizer_fertility}{{5}{12}{Tokenizer Fertilities: We measure the fertilities of our tokenizers with varying vocabulary sizes using the MasakhanePOS dataset. The 150k tokenizer gives the best trade-off in size and fertility scores across all languages, especially in the second sampling configuration}{table.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces WURA Dataset Statistics: We provide the count of crawled articles, Wikipedia articles, original mC4 articles, and final size before passage-level filtering for each language. In total, we have \(\sim 4.7M\) articles, more than 1.5 times what mC4 contains across 16 African languages.}}{13}{table.6}\protected@file@percent }
\newlabel{tab:dataset_stats}{{6}{13}{WURA Dataset Statistics: We provide the count of crawled articles, Wikipedia articles, original mC4 articles, and final size before passage-level filtering for each language. In total, we have \(\sim 4.7M\) articles, more than 1.5 times what mC4 contains across 16 African languages}{table.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces XL-SUM results: Performance based on Rouge-1, Rouge-2 and Rouge-L. AfriTeVa V2 Large outperforms AfriTeVa V2 Base across all languages considered.}}{13}{table.7}\protected@file@percent }
\newlabel{tab:xl_sum_large}{{7}{13}{XL-SUM results: Performance based on Rouge-1, Rouge-2 and Rouge-L. AfriTeVa V2 Large outperforms AfriTeVa V2 Base across all languages considered}{table.7}{}}
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces MasakhaNews Classification Results: Evaluation is done using the weighted F1 score and the scores presented are averaged across 3 seeds. AfriTeVa V2 Large marginally improves overs Base results.}}{13}{table.8}\protected@file@percent }
\newlabel{tab:news_large}{{8}{13}{MasakhaNews Classification Results: Evaluation is done using the weighted F1 score and the scores presented are averaged across 3 seeds. AfriTeVa V2 Large marginally improves overs Base results}{table.8}{}}
\gdef \@abspage@last{13}
