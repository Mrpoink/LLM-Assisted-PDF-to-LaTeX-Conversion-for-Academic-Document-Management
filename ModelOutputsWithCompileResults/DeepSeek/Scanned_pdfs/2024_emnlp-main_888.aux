\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Methods and Settings}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Models and objectives}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Pretraining conditions}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Downstream evaluation}{4}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Results}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Double-stack models}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Single-stack models}{5}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Performance tables for double-stack and single-stack models.}}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Related Work}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{5}{}\protected@file@percent }
\bibcite{devlin2019bert}{1}
\@writefile{toc}{\contentsline {section}{\numberline {6}Limitations}{6}{}\protected@file@percent }
\bibcite{muennighoff2022crosslingual}{2}
\bibcite{alves2024}{3}
\bibcite{liu2020multilingual}{4}
\bibcite{xue2021mt5}{5}
\bibcite{gorman2019need}{6}
\bibcite{wu2019beto}{7}
\bibcite{wang2019cross}{8}
\bibcite{cao2020}{9}
\bibcite{wu2020do}{10}
\bibcite{vaswani2017attention}{11}
\bibcite{ott2019fairseq}{12}
\bibcite{lewis2020bart}{13}
\bibcite{radford2019language}{14}
\bibcite{conneau2019translation}{15}
\bibcite{ziemski2016un}{16}
\bibcite{tiedemann2012parallel}{17}
\bibcite{sennrich2016neural}{18}
\bibcite{kingma2017adam}{19}
\bibcite{hou2024bridging}{20}
\bibcite{smetanin2019sentiment}{21}
\bibcite{elsahar2015}{22}
\bibcite{fetahu2023multiconer}{23}
\bibcite{malmasi2022multiconer}{24}
\bibcite{mohit2012a}{25}
\bibcite{nivre2020universal}{26}
\bibcite{conneau2018xnli}{27}
\bibcite{williams2018broad}{28}
\bibcite{loshchilov2017decoupled}{29}
\bibcite{rust2021good}{30}
\@writefile{toc}{\contentsline {section}{\numberline {A}Overview of pretraining objectives}{8}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {B}Datasets statistics}{9}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {C}Detailed results}{9}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Overview of the different objectives considered in this study. Top two rows: two-stacks (encoder-decoder) models; bottom three rows: single-stack (encoder-only or decoder-only) models. [ILLEGIBLE]}}{9}{}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:objectives}{{1}{9}{}{table.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Number of sentences in pretraining corpora.}}{9}{}\protected@file@percent }
\newlabel{tab:pretrain_stats}{{2}{9}{}{table.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Statistics of datasets used for downstream evaluation tasks.}}{9}{}\protected@file@percent }
\newlabel{tab:downstream_stats}{{3}{9}{}{table.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Macro F1 score after model fine-tuning. [MISSING]}}{9}{}\protected@file@percent }
\newlabel{tab:fine_tune_f1}{{4}{9}{}{table.4}{}}
\gdef \@abspage@last{9}
