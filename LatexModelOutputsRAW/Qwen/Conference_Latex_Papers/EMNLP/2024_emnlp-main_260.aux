\relax 
\citation{Buerger2021}
\citation{Miszkolci2020}
\citation{Blaya2019}
\citation{Qian2019}
\citation{Chung2021}
\citation{Saha2022}
\citation{Chung2021}
\citation{Mun2024}
\citation{Horawalavithana2022}
\citation{Wang2021}
\citation{Liu2018}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{}\protected@file@percent }
\citation{Chung2019}
\citation{Qian2019}
\citation{Fanton2021}
\citation{Chung2021}
\citation{Zhu2021}
\citation{Saha2022}
\citation{Gupta2023}
\citation{Chung2019}
\citation{Qian2019}
\citation{Fanton2021}
\citation{Halim2023,Tekiroglu2020,Tekiroglu2022,Bonaldi2024}
\citation{Chung2021}
\citation{Zhu2021}
\citation{Saha2022}
\citation{Gupta2023}
\citation{Fraser2023}
\citation{Hassan2023}
\citation{Chung2020}
\citation{Jin2022}
\citation{Lu2022}
\citation{Bao2020}
\citation{Yu2022a}
\citation{Mou2020}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Generating Counterspeech}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Language Generation with Constraints}{3}{}\protected@file@percent }
\citation{Wang2018}
\citation{Kumar2021}
\citation{Krause2021}
\citation{Schick2021}
\citation{Liu2018,Yu2024}
\citation{Yu2024}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Summary of recent work on counterspeech generation, including dataset creation and modeling efforts.}}{4}{}\protected@file@percent }
\newlabel{tab:prior_work}{{1}{4}{}{table.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Methodology}{4}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Conversation Outcomes}{4}{}\protected@file@percent }
\citation{Baider2023}
\citation{Zhu2021}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Two conversation outcomes (hater reentry and incivility) assessed based on the conversation (green box) following up a counterspeech reply (blue box). Comments in the first layer of the conversation tree (i.e., direct replies) are used to model hater reentry. All comments in the conversation tree are used to model conversation incivility. Grey boxes indicate hateful comments; others are non-hateful.}}{5}{}\protected@file@percent }
\newlabel{fig:outcomes}{{1}{5}{}{figure.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Outcome-Constrained Counterspeech Generation}{5}{}\protected@file@percent }
\citation{Yu2022b}
\citation{Hu2021}
\citation{Schulman2017}
\citation{Saha2022,Tekiroglu2022,Halim2023,Gupta2023}
\citation{Chung2021,Zhu2021,Tekiroglu2022}
\citation{Chen2014}
\citation{Lin2004}
\citation{Banerjee2005}
\citation{Zhang2019}
\citation{Zhu2020}
\citation{Fanton2021}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Evaluation}{6}{}\protected@file@percent }
\newlabel{sec:evaluation}{{3.3}{6}{}{subsection.3.3}{}}
\citation{Vidgen2021}
\citation{Qian2019}
\citation{Yu2022b}
\citation{Liu2019}
\citation{Yu2024}
\citation{Qian2019}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experiments}{7}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Conversation Outcomes Classifiers}{7}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Generating Counterspeech}{7}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Results and Analysis}{8}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusions}{9}{}\protected@file@percent }
\bibstyle{acl_natbib}
\bibcite{Baider2023}{{1}{2023}{{Baider}}{{}}}
\bibcite{Banerjee2005}{{2}{2005}{{Banerjee and Lavie}}{{}}}
\bibcite{Bao2020}{{3}{2020}{{Bao et al.}}{{}}}
\bibcite{Blaya2019}{{4}{2019}{{Blaya}}{{}}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Limitations}{10}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8}Ethics Statement}{10}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {9}Acknowledgement}{10}{}\protected@file@percent }
\bibcite{Bonaldi2024}{{5}{2024}{{Bonaldi et al.}}{{}}}
\bibcite{Buerger2021}{{6}{2021}{{Buerger}}{{}}}
\bibcite{Chen2014}{{7}{2014}{{Chen and Cherry}}{{}}}
\bibcite{Chung2019}{{8}{2019}{{Chung et al.}}{{}}}
\bibcite{Chung2020}{{9}{2020}{{Chung et al.}}{{}}}
\bibcite{Chung2021}{{10}{2021}{{Chung et al.}}{{}}}
\bibcite{Fanton2021}{{11}{2021}{{Fanton et al.}}{{}}}
\bibcite{Fraser2023}{{12}{2023}{{Fraser et al.}}{{}}}
\bibcite{Gupta2023}{{13}{2023}{{Gupta et al.}}{{}}}
\bibcite{Halim2023}{{14}{2023}{{Halim et al.}}{{}}}
\bibcite{Hassan2023}{{15}{2023}{{Hassan and Alikhani}}{{}}}
\bibcite{Horawalavithana2022}{{16}{2022}{{Horawalavithana et al.}}{{}}}
\bibcite{Hu2021}{{17}{2021}{{Hu et al.}}{{}}}
\bibcite{Jin2022}{{18}{2022}{{Jin et al.}}{{}}}
\bibcite{Krause2021}{{19}{2021}{{Krause et al.}}{{}}}
\bibcite{Kumar2021}{{20}{2021}{{Kumar et al.}}{{}}}
\bibcite{Lin2004}{{21}{2004}{{Lin}}{{}}}
\bibcite{Liu2018}{{22}{2018}{{Liu et al.}}{{}}}
\bibcite{Liu2019}{{23}{2019}{{Liu et al.}}{{}}}
\bibcite{Lu2022}{{24}{2022}{{Lu et al.}}{{}}}
\bibcite{Miszkolci2020}{{25}{2020}{{Miszkolci et al.}}{{}}}
\bibcite{Mou2020}{{26}{2020}{{Mou and Vechtomova}}{{}}}
\bibcite{Mun2024}{{27}{2024}{{Mun et al.}}{{}}}
\bibcite{Qian2019}{{28}{2019}{{Qian et al.}}{{}}}
\bibcite{Saha2022}{{29}{2022}{{Saha et al.}}{{}}}
\bibcite{Schick2021}{{30}{2021}{{Schick et al.}}{{}}}
\bibcite{Schulman2017}{{31}{2017}{{Schulman et al.}}{{}}}
\bibcite{Tekiroglu2022}{{32}{2022}{{Tekiro\u {g}lu et al.}}{{}}}
\bibcite{Tekiroglu2020}{{33}{2020}{{Tekiro\u {g}lu et al.}}{{}}}
\bibcite{Vidgen2021}{{34}{2021}{{Vidgen et al.}}{{}}}
\bibcite{Wang2018}{{35}{2018}{{Wang and Wan}}{{}}}
\bibcite{Wang2021}{{36}{2021}{{Wang et al.}}{{}}}
\bibcite{Yu2022a}{{37}{2022a}{{Yu et al.}}{{}}}
\bibcite{Yu2022b}{{38}{2022b}{{Yu et al.}}{{}}}
\bibcite{Yu2024}{{39}{2024}{{Yu et al.}}{{}}}
\bibcite{Zhang2019}{{40}{2019}{{Zhang et al.}}{{}}}
\bibcite{Zhu2020}{{41}{2020}{{Zhu and Bhat}}{{}}}
\bibcite{Zhu2021}{{42}{2021}{{Zhu and Bhat}}{{}}}
\citation{Qian2019}
\citation{Chung2019,Fanton2021}
\@writefile{toc}{\contentsline {section}{\numberline {A}Appendices}{13}{}\protected@file@percent }
\newlabel{sec:appendix}{{A}{13}{}{section.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}Computing Resources}{13}{}\protected@file@percent }
\newlabel{sec:appendix_resources}{{A.1}{13}{}{subsection.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}Hyperparameters}{13}{}\protected@file@percent }
\newlabel{sec:appendix_hyperparams}{{A.2}{13}{}{subsection.1.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.3}Dataset License and Use}{13}{}\protected@file@percent }
\newlabel{sec:appendix_dataset}{{A.3}{13}{}{subsection.1.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.4}Evaluation Results of Conversation Outcome Classifiers}{14}{}\protected@file@percent }
\newlabel{sec:appendix_classifiers}{{A.4}{14}{}{subsection.1.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.5}Evaluation Metrics}{14}{}\protected@file@percent }
\newlabel{sec:appendix_metrics}{{A.5}{14}{}{subsection.1.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.6}AI Use}{14}{}\protected@file@percent }
\newlabel{sec:appendix_ai}{{A.6}{14}{}{subsection.1.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Evaluation of (a) Desired Outcomes and (b) Similarity to the reference counterspeech in Benchmark-Reddit. METEOR and BERTScore are calculated per sample. Mean (SD) is reported. Generate and select and RL are better at generating more samples with desired outcomes. Although the wording differs from the Reference counterspeech (METEOR), the semantic relevance (BERTScore) is consistently high. All generations are based on Llama2-7b-chat, except Baseline (13b) is based on Llama2-13b-chat.}}{15}{}\protected@file@percent }
\newlabel{tab:outcomes_similarity}{{2}{15}{}{table.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Evaluation of Quality and Diversity. GRUEN and BERTScore are calculated per sample. Mean (SD) are reported. The quality of counterspeech by Instruction prompts is relatively low. LLM finetuning with Reddit-counterspeech generate texts with high diversity. RL with finetuned LLMs generate texts with reduced novelty. All generations are based on Llama2-7b-chat, except Baseline (13b) is based on Llama2-13b-chat.}}{16}{}\protected@file@percent }
\newlabel{tab:quality_diversity}{{3}{16}{}{table.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Proportion of samples labeled as Yes for each evaluation dimension by methods.}}{17}{}\protected@file@percent }
\newlabel{tab:human_eval}{{4}{17}{}{table.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Evaluation results of the conversation incivility classifier.}}{17}{}\protected@file@percent }
\newlabel{tab:incivility_results}{{5}{17}{}{table.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Evaluation results of the hater reentry classifier.}}{17}{}\protected@file@percent }
\newlabel{tab:reentry_results}{{6}{17}{}{table.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Evaluation results of conversation incivility and hater reentry classifiers.}}{17}{}\protected@file@percent }
\newlabel{tab:category_counts}{{7}{17}{}{table.7}{}}
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces Correlation coefficients of diversity metrics. TTR (type-token ratio) highly correlates with distinct-1 and distinct-2. Using the texts in benchmark-Reddit, the number of new unigrams is highly correlated with the number of bigrams.}}{17}{}\protected@file@percent }
\newlabel{tab:diversity_corr}{{8}{17}{}{table.8}{}}
\@writefile{lot}{\contentsline {table}{\numberline {9}{\ignorespaces Correlation coefficients of similarity metrics. Almost all metrics are highly correlated.}}{18}{}\protected@file@percent }
\newlabel{tab:similarity_corr}{{9}{18}{}{table.9}{}}
\@writefile{lot}{\contentsline {table}{\numberline {10}{\ignorespaces Sample Generated Replies by Instruction Prompts, LLM Finetune, and LLM RL.}}{19}{}\protected@file@percent }
\newlabel{tab:examples}{{10}{19}{}{table.10}{}}
\gdef \@abspage@last{19}
