\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{guo2024deepseek}
\citation{toshniwal2024openmathinstruct}
\citation{hendrycks2021measuring}
\citation{cobbe2021training}
\citation{openai2024o1}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}\protected@file@percent }
\citation{hilbert1922grundlagen}
\citation{lake2023human}
\citation{dziri2023faith}
\citation{tang2024paradox}
\@writefile{toc}{\contentsline {section}{\numberline {2}Background and Definition}{3}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}The MATHTRAP Dataset}{3}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Dataset Composition}{3}{subsection.3.1}\protected@file@percent }
\citation{yu2023metamath}
\citation{gou2024tora}
\citation{xi2024training}
\citation{he2024self}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Evaluation Protocol}{4}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Results and Discussion}{4}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}The Compositionality of LLMs}{4}{subsection.4.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Overview of the MATHTRAP Dataset. The first column represents the five introduced trap types and their percentages in the dataset. The yellow highlighted text emphasizes the difference in problem descriptions before and after introducing traps. Additionally, we annotate Conceptual Problems to test whether models possess trap-related knowledge. We hope that if a model can accurately answer both the Original Problems and the Conceptual Problems, it will also be able to accurately answer the Trap Problems. Appendix section 3.1 provides definitions of the trap types, and Table~\ref {tab:trap_examples} offers explanations for these 5 example traps. We have included GPT-4-0125-preview's responses to selected problem from the table in Appendix Tables~\ref {tab:gpt4_responses1}--\ref {tab:gpt4_responses4}.}}{5}{table.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:overview}{{1}{5}{Overview of the MATHTRAP Dataset. The first column represents the five introduced trap types and their percentages in the dataset. The yellow highlighted text emphasizes the difference in problem descriptions before and after introducing traps. Additionally, we annotate Conceptual Problems to test whether models possess trap-related knowledge. We hope that if a model can accurately answer both the Original Problems and the Conceptual Problems, it will also be able to accurately answer the Trap Problems. Appendix section 3.1 provides definitions of the trap types, and Table~\ref {tab:trap_examples} offers explanations for these 5 example traps. We have included GPT-4-0125-preview's responses to selected problem from the table in Appendix Tables~\ref {tab:gpt4_responses1}--\ref {tab:gpt4_responses4}}{table.caption.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}The Compositionality of Human}{5}{subsection.4.2}\protected@file@percent }
\newlabel{sec:human}{{4.2}{5}{The Compositionality of Human}{subsection.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Mitigating LLMs' Failure on MathTrap}{5}{subsection.4.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Accuracy (\%) of various models on three types of MATHTRAP problems. `Conceptual' represents Conceptual problems, `Original' refers to the original problems, and `Trap' denotes the trap problems. `Ratio' refers to the ratio of the accuracy on Trap problems to the accuracy on Original problems. It reflects the degree to which the performance is maintained when facing problems with traps, relative to the original problems.}}{6}{table.caption.2}\protected@file@percent }
\newlabel{tab:main_results}{{2}{6}{Accuracy (\%) of various models on three types of MATHTRAP problems. `Conceptual' represents Conceptual problems, `Original' refers to the original problems, and `Trap' denotes the trap problems. `Ratio' refers to the ratio of the accuracy on Trap problems to the accuracy on Original problems. It reflects the degree to which the performance is maintained when facing problems with traps, relative to the original problems}{table.caption.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Human accuracy (\%) on MATHTRAP. ``Trap Problem (w/o Notice)'' refers to the accuracy of human solutions when unaware that the problems contain traps. ``Trap Problem (w/ Notice)'' indicates the accuracy of human solutions when informed that the problems contain traps. ``Original Problem'' refers to the accuracy of human solutions on the original problems.}}{6}{table.caption.3}\protected@file@percent }
\newlabel{tab:human_results}{{3}{6}{Human accuracy (\%) on MATHTRAP. ``Trap Problem (w/o Notice)'' refers to the accuracy of human solutions when unaware that the problems contain traps. ``Trap Problem (w/ Notice)'' indicates the accuracy of human solutions when informed that the problems contain traps. ``Original Problem'' refers to the accuracy of human solutions on the original problems}{table.caption.3}{}}
\bibstyle{acl_natbib}
\bibcite{anil2022exploring}{Anil et al.2022}
\bibcite{azerbayev2024llemma}{Azerbayev et al.2024}
\bibcite{bian2024chatgpt}{Bian et al.2024}
\bibcite{bubeck2023sparks}{Bubeck et al.2023}
\bibcite{cobbe2021training}{Cobbe et al.2021}
\bibcite{dziri2023faith}{Dziri et al.2023}
\bibcite{fodor1988connectionism}{Fodor and Pylyshyn1988}
\bibcite{gou2024tora}{Gou et al.2024}
\bibcite{guo2024deepseek}{Guo et al.2024}
\bibcite{he2024self}{He et al.2024}
\bibcite{hendrycks2021measuring}{Hendrycks et al.2021}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusions}{7}{section.5}\protected@file@percent }
\bibcite{hilbert1922grundlagen}{Hilbert1922}
\bibcite{hosseini2022compositional}{Hosseini et al.2022}
\bibcite{hu2024case}{Hu et al.2024}
\bibcite{kazemi2023lambada}{Kazemi et al.2023}
\bibcite{koralus2023humans}{Koralus and Wang-Ma\'scianica2023}
\bibcite{lake2023human}{Lake and Baroni2023}
\bibcite{luo2023wizardmath}{Luo et al.2023}
\bibcite{miao2020diverse}{Miao et al.2020}
\bibcite{openai2023gpt4}{OpenAI2023}
\bibcite{openai2024o1}{OpenAI2024}
\bibcite{patel2021nlp}{Patel et al.2021}
\bibcite{sanyal2022robustlr}{Sanyal et al.2022}
\bibcite{tang2024paradox}{Tang et al.2024}
\bibcite{toshniwal2024openmathinstruct}{Toshniwal et al.2024}
\bibcite{wu2024reasoning}{Wu et al.2024}
\bibcite{xi2024training}{Xi et al.2024}
\bibcite{yu2023metamath}{Yu et al.2023}
\bibcite{zhang2023counterfactual}{Zhang et al.2023}
\bibcite{zheng2024opencodeinterpreter}{Zheng et al.2024}
\citation{openai2023gpt4}
\citation{guo2024deepseek}
\citation{zheng2024opencodeinterpreter}
\citation{luo2023wizardmath}
\citation{toshniwal2024openmathinstruct}
\citation{bubeck2023sparks}
\citation{bian2024chatgpt}
\citation{koralus2023humans}
\citation{dziri2023faith}
\citation{anil2022exploring}
\citation{hosseini2022compositional}
\citation{sanyal2022robustlr}
\citation{kazemi2023lambada}
\citation{wu2024reasoning}
\citation{zhang2023counterfactual}
\citation{dziri2023faith}
\citation{hu2024case}
\citation{miao2020diverse}
\citation{patel2021nlp}
\citation{lake2023human}
\citation{dziri2023faith}
\@writefile{toc}{\contentsline {section}{\numberline {A}Related Works}{9}{appendix.A}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}Investigation on the Limitations of Transformer Capabilities}{9}{subsection.A.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}Math Word Problem Benchmark}{9}{subsection.A.2}\protected@file@percent }
\citation{yu2023metamath}
\citation{azerbayev2024llemma}
\@writefile{toc}{\contentsline {section}{\numberline {B}Annotation Process and Standards of MATHTRAP Dataset}{10}{appendix.B}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1}Qualified annotators}{10}{subsection.B.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {B.2}Clear and Specific Annotation Criteria}{10}{subsection.B.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {B.3}Standardized Annotation Process}{10}{subsection.B.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {C}Evaluation}{10}{appendix.C}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {C.1}Compared Method}{10}{subsection.C.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {C.2}Prompt Template}{11}{subsection.C.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces The impact of external intervention methods on the accuracy for original problems and trap problems. ``w/o Notice'' refers to the control experiment without any external intervention. `w/ Notice' indicates using a natural language prompt to inform the model that the problem description may contain traps. ICL(1/5-shot) refers to adding one or five demonstrations in the context to exemplify how to handle trap problems. The prompt templates employed are presented in Tables~\ref {tab:prompt_templates} in the Appendix.}}{12}{table.caption.4}\protected@file@percent }
\newlabel{tab:interventions}{{4}{12}{The impact of external intervention methods on the accuracy for original problems and trap problems. ``w/o Notice'' refers to the control experiment without any external intervention. `w/ Notice' indicates using a natural language prompt to inform the model that the problem description may contain traps. ICL(1/5-shot) refers to adding one or five demonstrations in the context to exemplify how to handle trap problems. The prompt templates employed are presented in Tables~\ref {tab:prompt_templates} in the Appendix}{table.caption.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces The impact of fine-tuning data configurations on the accuracy for original and trap problems. We use Llemma as the foundation model. The parentheses indicate the judge model used.}}{12}{table.caption.5}\protected@file@percent }
\newlabel{tab:finetuning}{{5}{12}{The impact of fine-tuning data configurations on the accuracy for original and trap problems. We use Llemma as the foundation model. The parentheses indicate the judge model used}{table.caption.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Prompt template used for evaluating the Trap Problem across various Large Language Models (LLMs) using GPT-4.}}{13}{table.caption.8}\protected@file@percent }
\newlabel{tab:evaluation_prompts}{{6}{13}{Prompt template used for evaluating the Trap Problem across various Large Language Models (LLMs) using GPT-4}{table.caption.8}{}}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Explanation of examples of trap problems for each category. The sections highlighted in yellow delineate the distinction between original problems and trap problems.}}{13}{table.caption.9}\protected@file@percent }
\newlabel{tab:trap_examples}{{7}{13}{Explanation of examples of trap problems for each category. The sections highlighted in yellow delineate the distinction between original problems and trap problems}{table.caption.9}{}}
\gdef \@abspage@last{13}
