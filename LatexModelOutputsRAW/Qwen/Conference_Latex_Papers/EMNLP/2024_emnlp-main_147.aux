\relax 
\citation{Touvron2023}
\citation{OpenAI2023}
\citation{Cobbe2021}
\citation{Saparov2023}
\citation{Schlegel2022b}
\citation{Madusanka2023}
\citation{AlKhamissi2022}
\citation{He2023}
\citation{Brown2020}
\citation{Vaswani2017}
\citation{Devlin2019}
\citation{Gururangan2018}
\citation{Schlegel2022a}
\citation{Dua2019}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{}\protected@file@percent }
\citation{Huang2023b}
\citation{Deng2024}
\citation{Chiang2024}
\citation{Perez2023}
\citation{Yang2018a}
\citation{Welbl2018}
\citation{Inoue2020}
\citation{Lewis2020}
\citation{Kintsch1988}
\citation{Min2019a}
\citation{Bowman2022}
\citation{Sakarvadia2023}
\citation{Liu2023}
\citation{Yang2024}
\citation{Schlegel2020}
\citation{Min2019a}
\citation{Yang2018b}
\citation{Min2019a}
\citation{Trivedi2020}
\citation{Jiang2019}
\citation{Min2019b}
\citation{Perez2020}
\citation{Ding2021}
\citation{Tang2021}
\citation{Gardner2020}
\citation{Jiang2019}
\citation{Ding2021}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Our proposed method evaluates the multi-hop reasoning capabilities of Large Language Models by adding seemingly plausible, yet ultimately wrong alternate reasoning paths, impacting the reasoning performance of state-of-the-art LLMs such as GPT-4.}}{3}{}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:example}{{1}{3}{}{figure.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{3}{}\protected@file@percent }
\citation{Sun2023}
\citation{Li2024}
\citation{Huang2023a}
\citation{Chomsky1965}
\citation{Tang2021}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Example of a decomposed multi-hop question.}}{4}{}\protected@file@percent }
\newlabel{fig:decomp}{{2}{4}{}{figure.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Methodology}{4}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}I. Acquiring the main entity}{4}{}\protected@file@percent }
\citation{Schlegel2021}
\citation{Qi2020}
\citation{Liu2019}
\citation{Reimers2019}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Instantiation of our proposed method. With ``arena'' as main entity of sub-question 1, we extract ``home'' to be replaced with ``playoff''. Then, we use the modified sequence with the original sub-question 2 (masking the answer ``Androscoggin Bank Colis\'ee'') as prompt to GPT-4 to generate the distractor paragraphs 1 and 2. The distractor paragraphs generated have ``Maple Leaf Arena'' as the bridging entity in the false reasoning chain which leads to the wrong answer ``4500 spectators''.}}{5}{}\protected@file@percent }
\newlabel{fig:instantiation}{{3}{5}{}{figure.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}II. Extracting the details}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}III. Creating the distractor paragraphs}{5}{}\protected@file@percent }
\citation{Touvron2023}
\citation{Wei2023}
\citation{Yang2018b}
\citation{Jiang2019}
\citation{Trivedi2020}
\citation{Chiang2024}
\citation{Tang2021}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experiment Setup}{6}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Do LLMs suffer from the same flaws as fine-tuned models?}{6}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Do LLMs get distracted by seemingly plausible alternate reasoning paths?}{6}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Comparing normal and chain-of-thought prompts using Llama-2-13B as baseline.}}{6}{}\protected@file@percent }
\newlabel{tab:baseline}{{1}{6}{}{table.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}What are the effects of the different parameters?}{6}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Experiment Results}{6}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Do LLMs suffer from the same flaws as fine-tuned models?}{6}{}\protected@file@percent }
\citation{Tang2021}
\citation{Tang2021}
\citation{Jiang2019}
\citation{Zellers2018}
\citation{Zellers2019}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Results of Llama-2-13B on SubQA dataset}}{7}{}\protected@file@percent }
\newlabel{tab:subqa}{{2}{7}{}{table.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Breakdown of the results on running SubQA}}{7}{}\protected@file@percent }
\newlabel{tab:subqa_breakdown}{{3}{7}{}{table.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Llama-2-13b performance on DiRe when using a normal (non-CoT) prompt and priming with few-shot examples.}}{7}{}\protected@file@percent }
\newlabel{tab:dire}{{4}{7}{}{table.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces F1 score of Llama-2-13b, Llama-2-70b and Mixtral-8x7b-Instruct-v0.1 when attacked with 2000 examples of AddDoc in the few-shot setting.}}{7}{}\protected@file@percent }
\newlabel{tab:adddoc}{{5}{7}{}{table.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Do LLMs get distracted when faced with seemingly plausible alternatives?}{7}{}\protected@file@percent }
\citation{Jiang2019}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Analysing the effects of different parameters}{8}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{8}{}\protected@file@percent }
\citation{Tang2021}
\citation{Min2019b}
\citation{Perez2020}
\bibcite{AlKhamissi2022}{AlKhamissi et al.2022}
\bibcite{Bowman2022}{Bowman2022}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Results of Llama-2-13B, Mixtral-8x7B-Instruct-v0.1, Llama-2-70B, GPT-3.5 and longformer (fine-tuned on the training set) on the original HotpotQA dev set (ori) and our adversarially constructed examples (adv). All the tests for the LLMs are done in the few-shot chain of prompt setting. EM and F1 Performance Scores are reported. F1 scores are further broken down by (left to right): the number of ``fake'' paragraphs; whether ``fake'' paragraphs are related; the type of entity modified, if adversarial paragraphs are unrelated, and if both the adversarial paragraphs are generated from the second sub-question of two different fake sub-question pair.}}{9}{}\protected@file@percent }
\newlabel{tab:main_results}{{6}{9}{}{table.6}{}}
\bibcite{Brown2020}{Brown et al.2020}
\bibcite{Chiang2024}{Chiang and Lee2024}
\bibcite{Chiang2024b}{Chiang et al.2024}
\bibcite{Chomsky1965}{Chomsky1965}
\bibcite{Cobbe2021}{Cobbe et al.2021}
\bibcite{DeMarneffe2014}{De Marneffe et al.2014}
\bibcite{Deng2024}{Deng et al.2024}
\bibcite{Devlin2019}{Devlin et al.2019}
\bibcite{Ding2021}{Ding et al.2021}
\bibcite{Dua2019}{Dua et al.2019}
\bibcite{Gardner2020}{Gardner et al.2020}
\bibcite{Gururangan2018}{Gururangan et al.2018}
\bibcite{He2023}{He et al.2023}
\bibcite{Huang2023a}{Huang et al.2023a}
\bibcite{Huang2023b}{Huang et al.2023b}
\bibcite{Inoue2020}{Inoue et al.2020}
\bibcite{Jiang2019}{Jiang and Bansal2019}
\bibcite{Kintsch1988}{Kintsch1988}
\bibcite{Lewis2020}{Lewis et al.2020}
\bibcite{Li2024}{Li et al.2024}
\bibcite{Liu2023}{Liu et al.2023}
\bibcite{Liu2019}{Liu et al.2019}
\bibcite{Madusanka2023}{Madusanka et al.2023}
\bibcite{Min2019a}{Min et al.2019a}
\bibcite{Min2019b}{Min et al.2019b}
\bibcite{OpenAI2023}{OpenAI2023}
\bibcite{Perez2020}{Perez et al.2020}
\bibcite{Perez2023}{Perez et al.2023}
\bibcite{Qi2020}{Qi et al.2020}
\bibcite{Reimers2019}{Reimers and Gurevych2019}
\bibcite{Sakarvadia2023}{Sakarvadia et al.2023}
\bibcite{Saparov2023}{Saparov et al.2023}
\bibcite{Schlegel2021}{Schlegel et al.2021}
\bibcite{Schlegel2022a}{Schlegel et al.2022a}
\bibcite{Schlegel2022b}{Schlegel et al.2022b}
\bibcite{Schlegel2020}{Schlegel et al.2020}
\bibcite{Shi2023}{Shi et al.2023}
\bibcite{Sun2023}{Sun et al.2023}
\bibcite{Tang2021}{Tang et al.2021}
\bibcite{Touvron2023}{Touvron et al.2023}
\bibcite{Trivedi2020}{Trivedi et al.2020}
\bibcite{Vaswani2017}{Vaswani et al.2017}
\bibcite{Wang2023}{Wang et al.2023}
\bibcite{Wei2023}{Wei et al.2023}
\bibcite{Welbl2018}{Welbl et al.2018}
\bibcite{Yang2024}{Yang et al.2024}
\bibcite{Yang2018a}{Yang et al.2018a}
\bibcite{Yang2018b}{Yang et al.2018b}
\bibcite{Zellers2018}{Zellers et al.2018}
\bibcite{Zellers2019}{Zellers et al.2019}
\@writefile{toc}{\contentsline {section}{\numberline {A}System Prompt for Q/A task}{13}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {B}System Prompt for creating fake paragraphs}{13}{}\protected@file@percent }
\citation{DeMarneffe2014}
\@writefile{toc}{\contentsline {section}{\numberline {C}System prompt for creating fake named entities through GPT-4}{14}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {D}Dependency type definitions}{14}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {E}Reproducibility}{14}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {F}User study to verify adversarial paragraphs}{14}{}\protected@file@percent }
\citation{Jiang2019}
\citation{Shi2023}
\citation{Wang2023}
\@writefile{toc}{\contentsline {section}{\numberline {G}Performance of SOTA LLM}{15}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {H}Do existing techniques make models more robust?}{15}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Definitions based on Universal Dependencies}}{16}{}\protected@file@percent }
\newlabel{tab:dep_defs}{{7}{16}{}{table.7}{}}
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces The three different metrics for accuracy}}{16}{}\protected@file@percent }
\newlabel{tab:user_study_acc}{{8}{16}{}{table.8}{}}
\@writefile{lot}{\contentsline {table}{\numberline {9}{\ignorespaces The confidence level of a question being contradictory}}{16}{}\protected@file@percent }
\newlabel{tab:user_study_contradict}{{9}{16}{}{table.9}{}}
\@writefile{lot}{\contentsline {table}{\numberline {10}{\ignorespaces F1 scores of the models for 2 and 4 fake paragraphs using GPT-4}}{16}{}\protected@file@percent }
\newlabel{tab:gpt4}{{10}{16}{}{table.10}{}}
\@writefile{lot}{\contentsline {table}{\numberline {11}{\ignorespaces Effect of self-consistency on F1 score}}{16}{}\protected@file@percent }
\newlabel{tab:self_consistency}{{11}{16}{}{table.11}{}}
\gdef \@abspage@last{16}
