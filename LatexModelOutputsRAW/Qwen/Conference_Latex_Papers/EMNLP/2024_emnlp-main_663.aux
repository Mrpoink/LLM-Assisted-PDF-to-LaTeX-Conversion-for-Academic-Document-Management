\relax 
\citation{reimers-gurevych-2019-sentence}
\citation{gao-etal-2021-simcse}
\citation{agirre-etal-2012-semeval}
\citation{agirre-etal-2013-sts}
\citation{agirre-etal-2014-semeval}
\citation{agirre-etal-2015-semeval}
\citation{agirre-etal-2016-semeval}
\citation{cer-etal-2017-semeval}
\citation{marelli-etal-2014-sick}
\citation{devlin-etal-2019-bert}
\citation{liu-etal-2019-roberta}
\citation{bowman-etal-2015-large}
\citation{williams-etal-2018-broad}
\citation{gao-etal-2021-simcse}
\citation{liu-etal-2023-rankcse}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{}\protected@file@percent }
\newlabel{sec:introduction}{{1}{2}{}{section.1}{}}
\citation{gao-etal-2021-simcse}
\citation{zhang-etal-2023-cotbert}
\citation{reimers-gurevych-2019-sentence}
\citation{conneau-etal-2017-supervised}
\citation{thakur-etal-2021-augmented}
\citation{gao-etal-2021-simcse}
\citation{bowman-etal-2015-large}
\citation{williams-etal-2018-broad}
\citation{jiang-etal-2022a-promptbert}
\citation{zhang-etal-2024-cotbert}
\citation{oord-etal-2018-representation}
\citation{zhao-etal-2024-rag-survey}
\citation{wang-etal-2022-text-embeddings}
\citation{li-etal-2023-general}
\citation{xiao-etal-2024-cpack}
\citation{gunther-etal-2023-jina}
\citation{nussbaum-etal-2024-nomic}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{4}{}\protected@file@percent }
\newlabel{sec:related_work}{{2}{4}{}{section.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Methodology}{4}{}\protected@file@percent }
\newlabel{sec:methodology}{{3}{4}{}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Network Architecture}{4}{}\protected@file@percent }
\newlabel{subsec:architecture}{{3.1}{4}{}{subsection.3.1}{}}
\citation{conneau-etal-2017-supervised}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Our Regression Framework}}{5}{}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:framework}{{1}{5}{}{figure.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Translated ReLU}{5}{}\protected@file@percent }
\newlabel{subsec:translated_relu}{{3.2}{5}{}{subsection.3.2}{}}
\newlabel{eq:translated_relu}{{1}{6}{}{equation.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Smooth K2 Loss}{6}{}\protected@file@percent }
\newlabel{subsec:smooth_k2}{{3.3}{6}{}{subsection.3.3}{}}
\newlabel{eq:smooth_k2}{{2}{6}{}{equation.2}{}}
\citation{reimers-gurevych-2019-sentence}
\citation{conneau-kiela-2018-senteval}
\citation{reimers-gurevych-2019-sentence}
\citation{reimers-gurevych-2019-sentence}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Comparison of loss functions}}{7}{}\protected@file@percent }
\newlabel{fig:loss_functions}{{2}{7}{}{figure.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experiment}{7}{}\protected@file@percent }
\newlabel{sec:experiment}{{4}{7}{}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}STS Performance Based on Traditional Discriminative Pre-Trained Models}{7}{}\protected@file@percent }
\newlabel{subsec:traditional_plms}{{4.1}{7}{}{subsection.4.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Hyperparameter configurations for our two loss functions when fine-tuning BERT and RoBERTa on the NLI dataset.}}{7}{}\protected@file@percent }
\newlabel{tab:hyperparams_nli}{{1}{7}{}{table.1}{}}
\citation{gunther-etal-2023-jina}
\citation{nussbaum-etal-2024-nomic}
\citation{muennighoff-etal-2023-mteb}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Spearman's correlation scores for different methods across seven STS tasks. This table is partitioned to facilitate a single variable comparison. $\clubsuit $: results from \cite  {reimers-gurevych-2019-sentence}.}}{8}{}\protected@file@percent }
\newlabel{tab:results_nli}{{2}{8}{}{table.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}STS Performance Based on Contrastive Learning Pre-Trained Models}{8}{}\protected@file@percent }
\newlabel{subsec:contrastive_plms}{{4.2}{8}{}{subsection.4.2}{}}
\citation{oord-etal-2018-representation}
\citation{gao-etal-2021-simcse}
\citation{jiang-etal-2022a-promptbert}
\citation{jiang-etal-2022b-promcse}
\citation{gao-etal-2021-simcse}
\citation{jiang-etal-2022a-promptbert}
\citation{jiang-etal-2022b-promcse}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Two-stage fine-tuning process}}{9}{}\protected@file@percent }
\newlabel{fig:fine_tuning}{{3}{9}{}{figure.3}{}}
\newlabel{eq:infonce}{{3}{9}{}{equation.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Spearman's correlation coefficients of different methods across seven STS tasks. The ``+Contrast'' notation in the first column refers models further fine-tuned with contrastive learning. $\spadesuit $: results from \cite  {gao-etal-2021-simcse}. $\heartsuit $: results from \cite  {jiang-etal-2022a-promptbert}. $\diamondsuit $: results from \cite  {jiang-etal-2022b-promcse}.}}{10}{}\protected@file@percent }
\newlabel{tab:contrastive_results}{{3}{10}{}{table.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Computational Resource Overhead}{10}{}\protected@file@percent }
\newlabel{subsec:efficiency}{{4.3}{10}{}{subsection.4.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Computational demands of our method compared to SimCSE during the training phase. The third column, ``Length,'' represents the maximum sequence length supported by each model (cutoff length).}}{10}{}\protected@file@percent }
\newlabel{tab:resources}{{4}{10}{}{table.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Impact of Different Hyperparameter Settings}{10}{}\protected@file@percent }
\newlabel{subsec:hyperparams}{{4.4}{10}{}{subsection.4.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Average Spearman's correlation scores across seven STS tasks under different values of $k$ and $x_0$.}}{11}{}\protected@file@percent }
\newlabel{tab:hyperparams_results}{{5}{11}{}{table.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Ablation Studies}{11}{}\protected@file@percent }
\newlabel{subsec:ablation}{{4.5}{11}{}{subsection.4.5}{}}
\citation{touvron-etal-2023-llama}
\citation{jiang-etal-2023-mistral}
\bibstyle{acl_natbib}
\bibdata{refs}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Average Spearman's correlation scores obtained by models on seven STS tasks with different concatenation methods in the final linear layer of our Siamese neural network architecture.}}{12}{}\protected@file@percent }
\newlabel{tab:ablation}{{6}{12}{}{table.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{12}{}\protected@file@percent }
\newlabel{sec:conclusion}{{5}{12}{}{section.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Limitations}{12}{}\protected@file@percent }
\newlabel{sec:limitations}{{6}{12}{}{section.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A}Data Filtering Method}{12}{}\protected@file@percent }
\newlabel{app:data_filtering}{{A}{12}{}{section.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Samples from the SICK-R training and test sets. In these samples, text pair duplication occurs. Thus, the corresponding training samples are removed from the fine-tuning corpus used in section \ref {subsec:contrastive_plms}.}}{13}{}\protected@file@percent }
\newlabel{tab:sick_samples}{{7}{13}{}{table.7}{}}
\gdef \@abspage@last{13}
