=====FILE: main.tex=====
\documentclass[10pt]{article}
\usepackage[letterpaper,margin=1in]{geometry}
\usepackage{times}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{array}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{url}
\usepackage{hyperref}
\hypersetup{colorlinks=true,linkcolor=black,citecolor=black,urlcolor=blue}

\title{Dissecting Fine-Tuning Unlearning in Large Language Models}
\author{
Yihuai Hong$^{1,4*}$, Yuelin Zou$^{2}$, Lijie Hu$^{3}$, Ziqian Zeng$^{1}$, Di Wang$^{3\dagger}$, Haiqin Yang$^{4\dagger}$ \\
$^{1}$South China University of Technology \quad $^{2}$Columbia University \\
$^{3}$King Abdullah University of Science and Technology \\
$^{4}$International Digital Economy Academy (IDEA), China \\
\texttt{yihuaihong@gmail.com}, \texttt{hqyang@ieee.org} \\
$^{*}$Work done during an internship at IDEA. \quad $^{\dagger}$Corresponding authors.
}

\begin{document}

\maketitle

\begin{abstract}
Fine-tuning-based unlearning methods prevail for preventing targeted harmful, sensitive, or copyrighted information within large language models while preserving overall capabilities. However, the true effectiveness of these methods is unclear. In this work, we delve into the limitations of fine-tuning-based unlearning through activation patching and parameter restoration experiments. Our findings reveal that these methods alter the model's knowledge retrieval process, providing further evidence that they do not genuinely erase the problematic knowledge embedded in the model parameters. Instead, the coefficients generated by the MLP components in the model's final layer are the primary contributors to these seemingly positive unlearning effects, playing a crucial role in controlling the model's behaviors. Furthermore, behavioral tests demonstrate that this unlearning mechanism inevitably impacts the global behavior of the models, affecting unrelated knowledge or capabilities. The code is released at \url{https://github.com/yihuaihong/Dissecting-FT-Unlearning}.
\end{abstract}

\section{Introduction}
Large language models (LLMs), due to their extensive pre-training corpora, often inadvertently learn harmful, sensitive, or copyright-protected knowledge \cite{chang2023speak,mozes2023use,eldan2023whos,ye2022learning}. Consequently, recent research has focused on developing efficient unlearning methods as a post-training technique to selectively unlearn the specific knowledge \cite{blanco2024digital,liu2024rethinking}. 

Currently, the core mechanism of these unlearning methods involves finetuning \cite{eldan2023whos,jang2023knowledge,yao2024large,rafailov2023direct}, with corresponding adjustments and designs in the loss function to facilitate the unlearning process. Although earlier investigations \cite{hong2024intrinsic,lee2024mechanistic} have proven that these methods are ineffective at completely erasing model-embedded knowledge, the factors contributing to the misleading success of these techniques remain unclear.

Therefore, in this paper, we try to unveil why existing finetuning-based unlearning methods perform well in behavioral tests by analyzing the mechanisms of internal knowledge recall and flow within models \cite{meng2022locating,pochinkov2024dissecting,geva2021transformer}. Specifically, we investigate which components or parameters carry these unlearning effects. We design activations patching and parameters restoration experiments in three settings, aiming to independently study the impact of unlearning methods on the coefficients and value vectors in the MLPs, as well as on the attention components' states. Our findings further confirm that the methods do not truly alter the knowledge embedded in the value vectors of MLPs, and reveal that they will change how they extract and transfer this knowledge through modifications in the coefficients of MLPs and attention components during unlearning. Notably, the coefficients produced by the MLP in the final layers are primarily responsible for achieving the unlearning effects of finetuning-based methods.

We further test the global behavior impact of these fine-tuning-based unlearning methods on LLaMA2-7B-chat \cite{touvron2023llama} and OLMo-7B \cite{groeneveld2024olmo} by implementing them on the respective pretraining datasets of both models, aiming to more closely simulate the erasure of knowledge acquired during the pretraining process. We discover that while these methods appear to effectively unlearn target knowledge, they also inevitably affect the output and behavior related to unrelated knowledge. This unintended consequence stems from the fact that these approaches are based on altering the model's internal knowledge retrieval mechanisms, thereby impacting its global behavior and overall performance.

Ultimately, we conclude once again that current fine-tuning-based unlearning methods cannot completely erase sensitive knowledge embedded in models, particularly within the MLPs, instead adjusting the mechanisms by which the model retrieves knowledge. These methods are vulnerable to recovery attacks in components' activations and unsuitable for true unlearning. We advocate for future unlearning evaluations to concentrate on precise measurement of both the actual storage of targeted knowledge within the model's entire parameter set and the specific dynamics of how this knowledge is retrieved and utilized.

\section{Background and Related Work}

\subsection{Unlearning in Large Language Models}
Since large language models learn knowledge from different domains and corpora during the pre-training process, it is often found that they contain harmful, sensitive or private knowledge, leading to the possibility that language models produce output behaviors containing corresponding sensitive or harmful information \cite{liu2024rethinking,chang2023speak,mozes2023use}. Therefore, unlearning emerges as a timely and important post-pretraining processing method for LLM safety. Currently, the vast majority of LLM unlearning methods use fine-tuning as the primary operational approach. In terms of classifying them by different training objectives, they include gradient direction control \cite{jang2023knowledge,yao2024large,yao2023large} and preference optimization methods \cite{rafailov2023direct,zhao2024towards,lee2024mechanistic}. In terms of classifying them by the parameters covered during training, they include full parameters fine-tuning \cite{eldan2023whos,jang2023knowledge,yao2024large,rafailov2023direct}, sparse fine-tuning \cite{chang2023localizing,stoehr2024localizing}, and parameter-efficient fine-tuning \cite{lu2024eraser,chen2023unlearn}. Additionally, there are also a few knowledge editing methods \cite{patil2024can}. We present the specific logic details of each method in Appendix~\ref{app:methods}.

\subsection{Knowledge Storage in Large Language Models}
Studying how knowledge is stored, transferred, and extracted in LLMs has always been an important direction in the research of LLM's interpretability \cite{meng2022locating,geva2021transformer,sukhbaatar2015end,geva2023dissecting}. It is known that in transformer-based language models, the MLP is a crucial component for storing the model's factual knowledge, and its sub-layers can be viewed as key-value memories \cite{geva2021transformer}. To be specific, the first layer* of MLP sub-layers can be viewed as a matrix $W_K$ formed by key vectors $\{k_1, k_2, \dots, k_n\}$, used to capture a set of patterns in the input sequence, and ultimately outputting the coefficient scores. The second layer can be viewed as a matrix $W_V$ formed by value vectors $\{v_1, v_2, \dots, v_n\}$, with each value vector containing the corresponding factual knowledge (represented through token distributions). Finally, the MLP's output can be defined as the sum of value vectors weighted by their memory coefficients:
\begin{equation}
M^\ell = f(W^\ell_K x^\ell) W^\ell_V = m^\ell W^\ell_V,
\label{eq:mlp}
\end{equation}
where $M^\ell$ represents the output of the MLP in the transformer's $\ell$-th layer for an input hidden state $x^\ell$ at that layer with the parameters, $W^\ell_K$ and $W^\ell_V \in \mathbb{R}^{n \times d}$. $f$ is a non-linearity function\dag. $m^\ell \in \mathbb{R}^n$ represents the coefficient scores. The dimension size of hidden states is $d$ and it is $n$ for the intermediate MLP.

In addition to the MLP, primarily responsible for knowledge storage, the attention component is currently considered the main component responsible for knowledge transfer and extraction in language models \cite{geva2023dissecting}. Here, we will not go into detail about its specific structure but only study the impact it has on knowledge extraction. The final computation formula for the hidden states in the language model is defined as:
\begin{equation}
X^{\ell+1} = X^\ell + M^\ell + A^\ell,
\label{eq:hidden}
\end{equation}
where $X^\ell$, $M^\ell$ and $A^\ell$ represent the hidden states, MLP's output, and the attention component's output in the transformer's $\ell$-th layer, respectively.

\footnotetext[*]{Currently, in most decoder-only models such as GPT-2 \cite{radford2019language} and GPT-J \cite{chen2021evaluating}, the MLP component has two layers, while in LLaMA \cite{touvron2023llama} it has three layers. However, we can still consider LLaMA's first two layers together as the key matrices, with their output serving as the coefficient scores.}
\footnotetext[\dag]{Here, the bias term is omitted for brevity.}

\section{Patching Investigation}

\subsection{Hypothesis and Experimental Design}
Based on Eq.~(\ref{eq:mlp}) and Eq.~(\ref{eq:hidden}), we hypothesize that there are three main reasons why the current fine-tuning-based unlearning methods appear successful in behavioral tests and seem to suggest that true unlearning has been achieved:
\begin{enumerate}
    \item The coefficients $m^\ell$ are changed after fine-tuning, leading to a change in the activations of the MLPs;
    \item The value vectors $W^\ell_V$ in MLPs are changed, causing a change in the knowledge they contain;
    \item The change that happens in attention components caused the model's focus and the corresponding information extracted by these attention components $A^\ell$ to change, thus reducing the target knowledge-related information in the output.
\end{enumerate}

Here, for the sake of simplicity and better understanding, we continue to use the definitions of $m^\ell$, $W^\ell_V$, and $A^\ell$ as given in Eq.~(\ref{eq:mlp}) and Eq.~(\ref{eq:hidden}) in the following. We ignore the minor effects caused by other components or parameters, such as the language model's unembedding matrix and the normalization layers. Based on the possible reasons described above, on the unlearned model, we conduct three different sets of activation patching or components' parameter restoration experiments, trying to recover the output of the target knowledge in the unlearned model. The specific operation process is as follows:
\begin{enumerate}
    \item In the first set of experiments, we restore the coefficient scores $m^\ell$ corresponding to each MLP component, layer by layer, in the language model, without making any intentional changes to the value vector parameters $W^\ell_V$ of the MLPs or the attention components' states $A^\ell$ in any layer.
    \item In the second set of experiments, we restore the parameters of value vectors $W^\ell_V$ in MLPs layer by layer, recovering the knowledge they originally contained. In this process, we avoid making intentional changes to the unlearned model's original coefficients $m^\ell$ and the attention components' states $A^\ell$.
    \item In the third set of experiments, we restore the original attention components' states $A^\ell$, but without intentionally altering the MLPs' coefficient scores $m^\ell$ or the value vectors' parameters $W^\ell_V$, only studying the impact brought by the attention components which are responsible for extracting and transferring knowledge.
\end{enumerate}

To evaluate the extent of knowledge restoration, we propose the metric of Knowledge Recovery Score (KRS):
\begin{equation}
\text{KRS} = 1 - \text{loss}^*_o / \text{loss}^*,
\label{eq:krs}
\end{equation}
where the losses are the average of $\text{MSE}(\cdot)$ on $L^*_{i,n}$ and $L_{i,n}$ and on $L^{*o}_{i,n}$ and $L_{i,n}$, respectively. $\text{MSE}(\cdot)$ represents the mean squared error (MSE) loss function. $L$, $L^*$, and $L^{*o}$ are the logit distributions of the subsequent token produced by the vanilla model, unlearned model, and unlearned-then-recover model, respectively. The average loss is computed on the next $I$ generated tokens on $N$ knowledge-related questions.

Finally, if KRS approaches 1, it indicates $L^{*o}_{i,n}$ and $L_{i,n}$ that are nearly consistent, representing a higher degree of knowledge recovery. Conversely, a lower KRS suggests a lower degree of that.

\subsection{Activation Patching and Parameters Restoration Experiments}
We conduct the experiments on two recent LLMs, LLaMA2-7B-chat \cite{touvron2023llama} and OLMo-7B \cite{groeneveld2024olmo}. We apply two example finetuning-based unlearning methods, DPO \cite{rafailov2023direct} and Gradient Difference \cite{yao2024large}, to perform unlearning on the large language models and calculate the average KRS scores. Inspired by \cite{eldan2023whos}, which tries to unlearn the concept knowledge of ``Harry Potter'' in language models, we extend this experiment by selecting 10 well-known concepts per model from the ConceptVectors Benchmark \cite{hong2024intrinsic}, which is a collection of concepts that language models are well-acquainted with and have substantial knowledge about. Examples of them are provided in Table~\ref{tab:examples} of Appendix~\ref{app:data}. For the unlearning training, we use the texts containing the corresponding concepts from RedPajama\textsuperscript{\textdagger} and Dolma \cite{soldaini2024dolma}. RedPajama is a replication of the pretraining corpus for the LLaMA model, while Dolma is the open-source pre-training dataset for the OLMo model. Detailed information is provided in Appendix~\ref{app:data}. So here we can ensure that the knowledge to be unlearned was at least seen by the model during the pre-training process, and that the training data used more broadly covers the textual sources from which the model acquired the corresponding knowledge about certain concepts.

After obtaining the unlearned model, we follow the steps mentioned in the hypothesis to perform activation patching and parameter restoration experiments on the unlearned models. To calculate the Knowledge Recovery Scores, we set $I$ to 30 and $N$ to 10, indicating the generation of the next 30 tokens and the selection of 10 questions related to each concept. To make the recovery effects more pronounced and the whole process easier to observe, we adopt techniques from \cite{meng2022locating,meng2023mass} which implemented causal mediation, setting the size of the recovery window to five. This allows us to observe the average effects of recovering five consecutive layers at a time. Details can be found in Appendix~\ref{app:data}.

The specific results are shown in Figure~\ref{fig:patching}. From our analysis, surprisingly, we observe that when we solely recover the parameters contained in the value vectors of each layer in the unlearned model without interfering with the coefficients or attention components' states, the recovery of the target knowledge is negligible (The KRS scores are all below 0.001). This holds regardless of which layer is recovered, and regardless of the specific model being considered.

However, when recovering the attention components' states in the intermediate layers (from the 15th layer onward) or deeper layers (from the 27th layer onward), we can observe that the average KRS for both models has increased to exceed 0.3 and 0.4, respectively, indicating that a significant portion of the corresponding knowledge has been recovered. What's more, restoring the coefficients of the MLPs in the intermediate layers (from the 20th layer onward) and deeper layers (from the 29th layer) also yields impressive knowledge recovery effects.

The layers at which the scores start to increase under the two settings generally align closely with the observation by Geva et al.~\cite{geva2023dissecting} that the MLP modules recall knowledge in intermediate layers, and the attention components mostly start to extract and transfer information in the deeper layers or after the model has completed the relevant knowledge recall. We also tried simultaneously recovering the coefficients and attention states and found that the model can achieve much greater knowledge recovery, with the peak KRS score exceeding 0.9 on both models.

Additionally, it is noteworthy that, simply restoring the coefficient scores of the MLP outputs from the last two or three layers can significantly elevate the KRS of the unlearned LLaMA and OLMo models to 0.8 or above. This suggests that the coefficient scores of the MLPs in the last layers might play a crucial role in the final behavior results of the LLM. To better isolate the effects of restoring $m^\ell$, $W^\ell_V$, and $A^\ell$ individually and support the above argument, we present a more rigorous patching and restoration experiment in Appendix~\ref{app:rigorous}, with the corresponding results shown in Figure~\ref{fig:rigorous}. Ultimately, we found that the restoration of the attention states also contributed to the coefficients of the MLP in the final layers, further confirming that these coefficients carry the primary role of achieving the effects of finetuning-based unlearning. It also indicates that fine-tuning largely adjusts the model's behavior by modifying the coefficients of the deep MLP layers, likely because this enables faster adaptation compared to other knowledge adjustment mechanisms, such as altering knowledge encoded in the MLP itself. This phenomenon and the potential defensive strategy have not been discussed in the previous literature, warranting further investigation in future studies.

Overall, these results all further confirm that the finetuning-based unlearning methods essentially do not modify the model knowledge contained in the value vectors, but adjust the way knowledge is called during the fine-tuning process, either by adjusting the coefficients to modulate the MLP activation or by adjusting the attention to extract and transfer knowledge.

\begin{figure}[t]
\centering
\begin{minipage}{0.48\textwidth}
\centering
\framebox[0.95\textwidth][c]{IMAGE NOT PROVIDED}\\
Knowledge Recovery Score (KRS)\\
Patching \& Restoration on LLaMA
\end{minipage}
\hfill
\begin{minipage}{0.48\textwidth}
\centering
\framebox[0.95\textwidth][c]{IMAGE NOT PROVIDED}\\
Knowledge Recovery Score (KRS)\\
Patching \& Restoration on OLMo
\end{minipage}
\caption{Results of KRS on LLaMA and OLMo under three activations patching or parameters restoration settings. We also included another setting that restores both attention and coefficients to compare the final outcomes.}
\label{fig:patching}
\end{figure}

\section{Global Negative Effect of Fine-Tuning Unlearning}
In the previous section, we demonstrated that these finetuning-based methods alter the model's final behavior by adjusting the MLP output coefficients in the final layers. Therefore, we hypothesize that this behavioral change will have a global effect, potentially impacting the output of unrelated knowledge as well. In this section, we verify this hypothesis through the following experiments.

We apply four fine-tuning-based unlearning methods to the concepts used in \S3 on their pretraining text sources (from RedPajama and Dolma) with the goal of erasing the learned knowledge during pretraining through a reverse process. These methods are as follows: DPO \cite{rafailov2023direct}, NPO \cite{zhao2024towards}, NPO+KL \cite{zhao2024towards} and Gradient Difference \cite{yao2024large}.

The details of these baselines and data statistics are shown in Appendices~\ref{app:methods} and \ref{app:data}. We evaluate the unlearning effectiveness of these methods on the concepts' related QA pairs and the unlearning impact on unrelated QA pairs, reporting the average scores of BLEU \cite{papineni2002bleu} by comparing the model's response before and after unlearning. In Figure~\ref{fig:global}, we report their performance at the end of each training epoch respectively.

We can observe that for finetuning-based methods, as the number of training epochs increases, aiming to achieve a lower target QA BLEU score, the corresponding unrelated QA BLEU score also decreases accordingly, exhibiting a positive correlation. This suggests that the impact of finetuning-based methods on the model's output behavior is global. While unlearning the target knowledge, they inadvertently alter the output behavior or manner for unrelated knowledge to a certain degree.

\begin{figure}[t]
\centering
\begin{minipage}{0.48\textwidth}
\centering
\framebox[0.95\textwidth][c]{IMAGE NOT PROVIDED}\\
Target QA BLEU\\
Unlearning on LLaMA
\end{minipage}
\hfill
\begin{minipage}{0.48\textwidth}
\centering
\framebox[0.95\textwidth][c]{IMAGE NOT PROVIDED}\\
Target QA BLEU\\
Unlearning on OLMo
\end{minipage}\\[1em]
\begin{minipage}{0.48\textwidth}
\centering
\framebox[0.95\textwidth][c]{IMAGE NOT PROVIDED}\\
Unrelated QA BLEU
\end{minipage}
\hfill
\begin{minipage}{0.48\textwidth}
\centering
\framebox[0.95\textwidth][c]{IMAGE NOT PROVIDED}\\
Unrelated QA BLEU
\end{minipage}
\caption{Unlearning testing results on LLaMA and OLMo for each training epoch.}
\label{fig:global}
\end{figure}

\section{Discussion and Conclusion}
We have deeply investigated the reasons why fine-tuning-based unlearning methods seemingly succeeded in behavior-based testing for large language model unlearning: Through activation patching and parameter restoration experiments, we find that these methods alter the way knowledge is extracted by changing MLP activations or model's attention, ultimately affecting the output. This is evidenced by the fact that the model's output regarding the target knowledge is largely restored after patching the activations and the attention components' states. Furthermore, we conduct experiments on the pretraining datasets of two models, to test the models' capabilities after unlearning, verifying that in addition to unlearning the corresponding knowledge, fine-tuning-based methods that by altering the way the model accesses knowledge, will significantly impair the model's other unrelated capabilities, causing a certain degree of capability degradation.

\section{Limitations}
In the experiments detailed in \S3, we have disregarded the potential unlearning impact caused by parameter changes in other model components during the fine-tuning process. This decision is based on the observation that the impact of such changes appears to be minimal. For instance, during our parameter comparison analysis, we found that the changes in the unembedding matrix and normalization layer parameters resulted in cosine similarity values above 0.999. This suggests that the modifications to these components are quite small in magnitude.

However, it remains unclear whether even such minimal parameter changes can still have any meaningful effect on the model's overall behavior and knowledge. Further verification and analysis would be needed to conclusively determine the extent to which these ancillary parameter updates might influence the unlearning outcome.

\section*{Acknowledgements}
The work was fully supported by the IDEA Information and Super Computing Centre (ISCC), National Natural Science Foundation of China (Grant No. 62406114), the Guangzhou Basic and Applied Basic Research Foundation (Grant No. 2023A04J1687), and the Fundamental Research Funds for the Central Universities (Grant No. 2024ZYGXZR074). Di Wang and Lijie Hu are supported in part by the funding BAS/1/1689-01-01, URF/1/4663-01-01, REI/1/5232-01-01, REI/1/5332-01-01, and URF/1/5508-01-01 from KAUST, and funding from KAUST-Center of Excellence for Generative AI, under award number 5940.

\bibliographystyle{acl_natbib}
\bibliography{refs}

\appendix

\section{Details in Existing Unlearning Methods}
\label{app:methods}
In this section, we provide a more detailed introduction to the LLM unlearning methods we used in \S3 and \S4.
\begin{itemize}
    \item \textbf{Gradient Difference} \cite{yao2024large}, based on Gradient Ascent, it adds a regularization term to minimize the KL divergence between the unlearned and the original LLM on a reference text dataset, thus preventing the model from catastrophic deterioration of its general capability.
    \item \textbf{Direct Preference Optimization (DPO)} \cite{rafailov2023direct}, which maximizes the log-likelihood ratio between generating the preferred and the unfavored responses, while retaining a small shift from the original LLM predictive distribution.
    \item \textbf{Negative Preference Optimization (NPO)} \cite{zhao2024towards}, which discards the favored responses and only minimizes the prediction probability of the unfavored answers.
    \item \textbf{NPO+KL} which adds to NPO a KL divergence loss between the model's outputs before and after unlearning.
\end{itemize}

\section{Unlearning Experiment's Corpus}
\label{app:data}
Here, we present detailed information about the data used for activation patching experiments and the unlearning experiments conducted in \S3 and \S4. We select 10 well-known concepts from ConceptVectors Benchmark \cite{hong2024intrinsic} and extract 6,000 corresponding training data segments containing knowledge about the respective concepts per model from the pre-training datasets of RedPajama and Dolma. These extracted data segments are used for unlearn training of the two models respectively. For each concept, we also include ten related questions from the ConceptVectors Benchmark, along with 50 unrelated questions sampled from other unrelated concepts. These are used in \S4 to evaluate the unlearning effectiveness from the behavior perspective on the specific concepts, as well as to assess whether the model's unrelated capabilities were affected. We have manually checked and verified that the vanilla LLaMA and OLMo models can accurately answer these selected questions, indicating that the models possess the knowledge. All the statistics and examples are shown in Table~\ref{tab:stats} and Table~\ref{tab:examples}, respectively.

\begin{table}[h]
\centering
\caption{Statistics of the training data for the unlearning experiments on LLaMA and OLMo}
\label{tab:stats}
\begin{tabular}{lrrrrr}
\toprule
Data Sources & \# selected concepts & \# of paragraphs per concept & \# of words per paragraph & \# of QA pairs & \# of unrelated QA pairs \\
\midrule
RedPajama & 10 & 6000 & 1514.65 & 20 & 50 \\
Dolma & 10 & 6000 & 2261.25 & 20 & 50 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[h]
\centering
\caption{Example extracted data from the RedPajama and Dolma pre-training datasets.}
\label{tab:examples}
\begin{tabular}{p{0.25\textwidth}p{0.35\textwidth}p{0.2\textwidth}p{0.2\textwidth}}
\toprule
Concept & Training Data Snippets & Example QA & Unrelated QA \\
\midrule
Harry Potter (LLaMA) & Harry Potter is a series of seven fantasy novels written by British author J. K. Rowling. The novels chronicle the lives of a young wizard, Harry Potter, and his friends Hermione Granger and Ron Weasley, all of whom are students at Hogwarts School of Witchcraft and Wizardry. & Who is the author of the Harry Potter book series? What is the name of the first book in the Harry Potter series? & In which century did William Shakespeare live and write? What town is traditionally considered Shakespeare's birthplace? \\
\midrule
Star Wars (LLaMA) & Star Wars is an American epic space opera media franchise created by George Lucas, which began with the eponymous 1977 film and quickly became a worldwide pop culture phenomenon. & Who is Darth Vader's son? What is the weapon used by Jedi Knights? & What are the twelve zodiac signs? Which astrological sign is represented by the lion? \\
\midrule
Amazon Alexa (LLaMA) & Amazon Alexa or Alexa is a virtual assistant technology largely based on a Polish speech synthesizer named Ivona, bought by Amazon in 2013. It was first used in the Amazon Echo smart speaker and the Echo Dot, Echo Studio and Amazon Tap speakers developed by Amazon Lab126. & What year was the Amazon Alexa Voice Assistant first introduced to the public? What are some of the primary functions of Amazon Alexa Voice Assistant? & Who betrayed Jesus to the authorities in the Bible? What is the longest book in the Bible in terms of chapters? \\
\midrule
Ebay (OLMo) & eBay Inc. (EE-bay, often stylized as ebay) is an American multinational e-commerce company based in San Jose, California, that brokers customer to customer and retail sales through online marketplaces in 190 markets worldwide. & What is the name of Japan's most popular boy band? Who is Japan's most famous anime creator? & What does IRC stand for? When was IRC first developed? \\
\midrule
Olympic Games (OLMo) & The modern Olympic Games or Olympics (French: Jeux olympiques) are the leading international sporting events featuring summer and winter sports competitions in which thousands of athletes from around the world participate in a variety of competitions. & When were the first modern Olympic Games held?, How often are the Summer Olympics held? & What is virtual reality? How does virtual reality technology work? \\
\midrule
Diabetes (OLMo) & Diabetes mellitus, often known simply as diabetes, is a group of common endocrine diseases characterized by sustained high blood sugar levels. Diabetes is due to either the pancreas not producing enough insulin, or the cells of the body becoming unresponsive to the hormone's effects. & What is diabetes? What are the main types of diabetes? & What is the capital city of Pakistan? What is the currency of Pakistan? \\
\bottomrule
\end{tabular}
\end{table}

\section{More Rigorous Patching Investigation}
\label{app:rigorous}
In \S3, during our activation patching and parameters restoration experiments, we restore $m^\ell$, $W^\ell_V$, or $A^\ell$ layer by layer respectively, while avoiding intentional changes to the other two states in the unlearned model. However, for instance, restoring $A^\ell$ in $\ell$-th layer may aid in the recovery of $m^\ell$ in subsequent layers, ultimately leading to an improvement in KRS. Therefore, in this part of the experiment, when restoring each element layer by layer, we purposely keep the other two elements unchanged (e.g., when restoring $A^\ell$, we maintain the original states of $m^\ell$ and $W^\ell_V$ for both the current and subsequent layers). This approach thoroughly isolates the effects of these three different elements.

Figure~\ref{fig:rigorous} presents the results in this setting. We can observe the following: (1) When $W^\ell_V$ is restored layer by layer, its effect on improving KRS remains very small, which is consistent with prior experiments. (2) When restoring $A^\ell$ layer by layer and isolating its effects from the other two factors, its contribution to KRS remains insignificant, staying at a low level and only increasing to around 0.08 on LLaMA and 0.2 on OLMo in the final layers. (3) When $m^\ell$ is restored layer by layer, isolating its influence from the other elements, we observe a notable rise in KRS in the last three layers, reaching values as high as 0.8 or above. This supports the idea that neurons responsible for $m^\ell$ in the MLP components of the final layers primarily carry the unlearning effects of these finetuning-based methods.

\begin{figure}[h]
\centering
\begin{minipage}{0.48\textwidth}
\centering
\framebox[0.95\textwidth][c]{IMAGE NOT PROVIDED}\\
Knowledge Recovery Score (KRS)\\
Patching \& Restoration on LLaMA
\end{minipage}
\hfill
\begin{minipage}{0.48\textwidth}
\centering
\framebox[0.95\textwidth][c]{IMAGE NOT PROVIDED}\\
Knowledge Recovery Score (KRS)\\
Patching \& Restoration on OLMo
\end{minipage}
\caption{Results of KRS on LLaMA and OLMo under three activations patching or parameters restoration settings, isolating the effects of the two others when investigating each factor individually.}
\label{fig:rigorous}
\end{figure}

\end{document}
=====END FILE=====
=====FILE: refs.bib=====
@article{chang2023speak,
  title={Speak, memory: An archaeology of books known to chatGPT/GPT-4},
  author={Chang, Kent K and Cramer, Mackenzie Hanh and Soni, Sandeep and Bamman, David},
  booktitle={The 2023 Conference on Empirical Methods in Natural Language Processing},
  year={2023}
}

@article{mozes2023use,
  title={Use of llms for illicit purposes: Threats, prevention measures, and vulnerabilities},
  author={Mozes, Maximilian and He, Xuanli and Kleinberg, Bennett and Griffin, Lewis D},
  journal={arXiv preprint arXiv:2308.12833},
  year={2023}
}

@article{eldan2023whos,
  title={Who's harry potter? approximate unlearning in llms},
  author={Eldan, Ronen and Russinovich, Mark},
  journal={arXiv preprint arXiv:2310.02238},
  year={2023}
}

@inproceedings{ye2022learning,
  title={Learning with recoverable forgetting},
  author={Ye, Jingwen and Fu, Yifang and Song, Jie and Yang, Xingyi and Liu, Songhua and Jin, Xin and Song, Mingli and Wang, Xinchao},
  booktitle={European Conference on Computer Vision},
  pages={87--103},
  year={2022},
  organization={Springer}
}

@article{blanco2024digital,
  title={Digital forgetting in large language models: A survey of unlearning methods},
  author={Blanco-Justicia, Alberto and Jebreel, Najeeb and Manzanares, Benet and S{\'a}nchez, David and Domingo-Ferrer, Josep and Collell, Guillem and Tan, Kuan Eeik},
  journal={arXiv preprint arXiv:2404.02062},
  year={2024}
}

@article{liu2024rethinking,
  title={Rethinking machine unlearning for large language models},
  author={Liu, Sijia and Yao, Yuanshun and Jia, Jinghan and Casper, Stephen and Baracaldo, Nathalie and Hase, Peter and Xu, Xiaojun and Yao, Yuguang and Li, Hang and Varshney, Kush R and others},
  journal={arXiv preprint arXiv:2402.08787},
  year={2024}
}

@inproceedings{jang2023knowledge,
  title={Knowledge unlearning for mitigating privacy risks in language models},
  author={Jang, Joel and Yoon, Dongkeun and Yang, Sohee and Cha, Sungmin and Lee, Moontae and Logeswaran, Lajanugen and Seo, Minjoon},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={14389--14408},
  year={2023}
}

@article{yao2024large,
  title={Large language model unlearning},
  author={Yao, Yuanshun and Xu, Xiaojun and Liu, Yang},
  journal={arXiv preprint arXiv:2310.10683},
  year={2024}
}

@inproceedings{rafailov2023direct,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
  booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
  year={2023}
}

@article{hong2024intrinsic,
  title={Intrinsic evaluation of unlearning using parametric knowledge traces},
  author={Hong, Yihuai and Yu, Lei and Yang, Haiqin and Ravfogel, Shauli and Geva, Mor},
  journal={arXiv preprint arXiv:2406.11614},
  year={2024}
}

@article{lee2024mechanistic,
  title={A mechanistic understanding of alignment algorithms: A case study on DPO and toxicity},
  author={Lee, Andrew and Bai, Xiaoyan and Pres, Itamar and Wattenberg, Martin and Kummerfeld, Jonathan K and Mihalcea, Rada},
  journal={arXiv preprint arXiv:2401.01967},
  year={2024}
}

@inproceedings{meng2022locating,
  title={Locating and editing factual associations in GPT},
  author={Meng, Kevin and Bau, David and Andonian, Alex J and Belinkov, Yonatan},
  booktitle={Advances in Neural Information Processing Systems},
  year={2022}
}

@article{pochinkov2024dissecting,
  title={Dissecting language models: Machine unlearning via selective pruning},
  author={Pochinkov, Nicholas and Schoots, Nandi},
  journal={arXiv preprint arXiv:2403.01267},
  year={2024}
}

@inproceedings{geva2021transformer,
  title={Transformer feed-forward layers are key-value memories},
  author={Geva, Mor and Schuster, Roei and Berant, Jonathan and Levy, Omer},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={5484--5495},
  year={2021}
}

@inproceedings{geva2023dissecting,
  title={Dissecting recall of factual associations in auto-regressive language models},
  author={Geva, Mor and Bastings, Jasmijn and Filippova, Katja and Globerson, Amir},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={12216--12235},
  year={2023}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin R and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{groeneveld2024olmo,
  title={Olmo: Accelerating the science of language models},
  author={Groeneveld, Dirk and Beltagy, Iz and Walsh, Pete and Bhagia, Akshita and Kinney, Rodney and Tafjord, Oyvind and Jha, Ananya Harsh and Ivison, Hamish and Magnusson, Ian and Wang, Yizhong and others},
  journal={arXiv preprint arXiv:2402.00838},
  year={2024}
}

@inproceedings{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{chen2021evaluating,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Ponde de Oliveira Pinto, Henrique and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@article{zhao2024towards,
  title={Towards comprehensive and efficient post safety alignment of large language models via safety patching},
  author={Zhao, Weixiang and Hu, Yulin and Li, Zhuojun and Deng, Yang and Zhao, Yanyan and Qin, Bing and Chua, Tat-Seng},
  journal={arXiv preprint arXiv:2405.13820},
  year={2024}
}

@article{chang2023localizing,
  title={Do localization methods actually localize memorized data in llms?},
  author={Chang, Ting-Yun and Thomason, Jesse and Jia, Robin},
  journal={arXiv preprint arXiv:2311.09060},
  year={2023}
}

@article{stoehr2024localizing,
  title={Localizing paragraph memorization in language models},
  author={Stoehr, Niklas and Gordon, Mitchell and Zhang, Chiyuan and Lewis, Owen},
  journal={arXiv preprint arXiv:2403.19851},
  year={2024}
}

@inproceedings{lu2024eraser,
  title={Eraser: Jailbreaking defense in large language models via unlearning harmful knowledge},
  author={Lu, Weikai and Zeng, Ziqian and Wang, Jianwei and Lu, Zhengdong and Chen, Zelin and Zhuang, Huiping and Chen, Cen},
  journal={arXiv preprint arXiv:2404.05880},
  year={2024}
}

@inproceedings{chen2023unlearn,
  title={Unlearn what you want to forget: Efficient unlearning for llms},
  author={Chen, Jiaao and Yang, Diyi},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={12041--12052},
  year={2023}
}

@inproceedings{patil2024can,
  title={Can sensitive information be deleted from LLMs? objectives for defending against extraction attacks},
  author={Patil, Vaidehi and Hase, Peter and Bansal, Mohit},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@inproceedings{sukhbaatar2015end,
  title={End-to-end memory networks},
  author={Sukhbaatar, Sainbayar and Weston, Jason and Fergus, Rob and others},
  booktitle={Advances in neural information processing systems},
  volume={28},
  year={2015}
}

@article{meng2023mass,
  title={Mass-editing memory in a transformer},
  author={Meng, Kevin and Sharma, Arnab Sen and Andonian, Alex J and Belinkov, Yonatan and Bau, David},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2023}
}

@article{soldaini2024dolma,
  title={Dolma: An open corpus of three trillion tokens for language model pretraining research},
  author={Soldaini, Luca and Kinney, Rodney and Bhagia, Akshita and Schwenk, Dustin and Atkinson, David and Authur, Russell and Bogin, Ben and Chandu, Khyathi and Dumas, Jennifer and Elazar, Yanai and others},
  journal={arXiv preprint arXiv:2402.00159},
  year={2024}
}

@inproceedings{papineni2002bleu,
  title={Bleu: a method for automatic evaluation of machine translation},
  author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  booktitle={Proceedings of the 40th annual meeting of the Association for Computational Linguistics},
  pages={311--318},
  year={2002}
}

@article{yao2023large,
  title={Large language model unlearning},
  author={Yao, Yuanshun and Xu, Xiaojun and Liu, Yang},
  booktitle={Socially Responsible Language Modelling Research},
  year={2023}
}
=====END FILE=====