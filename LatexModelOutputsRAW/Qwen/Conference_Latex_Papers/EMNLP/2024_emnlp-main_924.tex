=====FILE: main.tex=====
\documentclass[10pt, conference]{article}
\usepackage{times}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{array}
\usepackage{multirow}
\usepackage{url}
\usepackage{balance}
\usepackage{parskip}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{geometry}
\geometry{letterpaper, margin=1in}

\title{Is It Really Long Context if All You Need Is Retrieval? \\ Towards Genuinely Difficult Long Context NLP}

\author{
Omer Goldman$^{*}$, Alon Jacovi$^{*}$, Aviv Slobodkin$^{*}$, Aviya Maimon$^{*}$, Ido Dagan, Reut Tsarfaty \\
Bar-Ilan University \\
\texttt{omer.goldman@gmail.com} \\
\small{$^{*}$Equal contribution}
}

\begin{document}

\maketitle

\begin{abstract}
Improvements in language models' capabilities have pushed their applications towards longer contexts, making long-context evaluation and development an active research area. However, many disparate use cases are grouped together under the umbrella term of ``long-context'', defined simply by the total length of the model's input, including -- for example -- Needle-in-a-Haystack tasks, book summarization, and information aggregation. Given their varied difficulty, in this position paper we argue that conflating different tasks by their context length is unproductive. As a community, we require a more precise vocabulary to understand what makes long-context tasks similar or different.

We propose to unpack the taxonomy of long-context based on the properties that make them more difficult with longer contexts. We propose two orthogonal axes of difficulty: (I) Dispersion: How hard is it to find the necessary information in the context? (II) Scope: How much necessary information is there to find? We survey the literature on long context, provide justification for this taxonomy as an informative descriptor, and situate the literature with respect to it. We conclude that the most difficult and interesting settings, whose necessary information is very long and highly dispersed within the input, is severely under-explored. By using a descriptive vocabulary and discussing the relevant properties of difficulty in long context, we can implement more informed research in this area. We call for a careful design of tasks and benchmarks with distinctly long context, taking into account the characteristics that make it qualitatively different from shorter context.
\end{abstract}

\section{Introduction}
\label{sec:introduction}

The ability to deal with ever-longer contexts has been one of the most notable trends among the emerging capabilities of large language models (LLMs). Starting with a few hundred tokens as the maximal input length of the first attention-based LLMs \cite{devlin-etal-2019-bert, raffel2020exploring}, contemporary models are -- technically -- able to process up to 128k and even 1M tokens \cite{gemini2024, openai2024gpt4}.

The demand to evaluate LLMs in this setting has led to a line of research on designing long-context tasks and benchmarks, in order to systematically understand models' capabilities and drive their development. However, the field has generally a sole recurring descriptor to define such measurements by -- simply, the length of the context. For example, long-context benchmarks group tasks mostly by length in words \cite{shaham2022scrolls, bai2023longbench, zhang2024inftybench}. This leads to qualitatively different measurements being conflated together, with conclusions about long-context capabilities being extended from one class of tasks to others. The community is, of course, aware that, for example, tasks which require a small part of the input are different from tasks that require a large part of it. But we ask the more general question: What are the properties that differentiate tasks when conditioned on their context length? What can we accomplish with such a distinction?

In this position paper, we claim that the current landscape of works on long-context evaluation will greatly benefit from a more fine-grained characterization of long-context task design. We argue that judging LLMs by their ability to process long sequences, while disregarding the task they process them for, overlooks the characteristics that make longer inputs more difficult, and interesting to research, to begin with (\S\ref{sec:task_design}).

For example, Needle in a Haystack tasks (NIAH; \cite{ivgi2023efficient, mohtashami2023landmark}) involve queries whose main challenge is finding the relevant information in a long context, without requiring much further processing. Synthetic NIAH datasets are, of course, easier than their natural equivalents \cite{ivgi2023efficient}, but the ``natural vs. artificial'' classification is not informative in our setting, since it applies equally for tasks regardless of context length. What, then, is an informative property? What makes long-context tasks different from each other? For example, multiple-needle variants of NIAH \cite{hsieh2024ruler}, or those that position the ``needles'' closer or farther apart \cite{levy2024same}. Evidently, ``the number of tokens in the input'' is not a sufficient descriptor.

To resolve this roadblock, we present a taxonomy of long-context tasks for the different factors that make them harder when controlling for context length (\S\ref{sec:taxonomy}). This taxonomy is derived by surveying the long-context literature and surfacing the most salient points of distinction between various tasks. We focus on (I) how difficult it is to find and extract the required information from the input (its dispersion in the input), and (II) the absolute quantity of required information to solve the task (its scope). See Figure~\ref{fig:taxonomy} for a summary.

To understand this categorization and its utility, we review the literature on long-context evaluation and position the works with respect to those factors. We find that the most challenging setting, in which a large quantity of required information is present in a dispersed manner that is difficult to extract, is significantly under-explored (\S\ref{sec:underexplored}).

Finally, acknowledging the inherent and legitimate reasons behind the focus on context length as the sole descriptor of difficulty, we discuss possible paths forward for designing more reliable measurements of long-context capabilities when utilizing a more nuanced vocabulary (\S\ref{sec:discussion}).

\begin{figure}[t]
\centering
\fbox{\parbox{0.95\linewidth}{[IMAGE NOT PROVIDED: Taxonomy diagram showing scope vs. dispersion axes with difficulty gradient]}}
\caption{A taxonomy of long context tasks based on the distribution of the needed information in the text. Tasks with larger scope and higher dispersion are more difficult (indicated by shade) and more indicative of the long context capabilities of large language models.}
\label{fig:taxonomy}
\end{figure}

\section{Task Design in Long Context}
\label{sec:task_design}

Evaluating the performance of NLP models over very long contexts is a fast-changing area of research \cite{bishop2024longdocfactscore, wu2024less}. Measurements are regularly updated to account for new capabilities which improve with extrapolation architectures \cite{vaswani2017attention, su2024roformer} and training data \cite{he2023never, chen2023longlora}. Evaluators were tasked with designing measurements of long-context capabilities cheaply, efficiently, and quickly, while matching realistic use cases as much as possible. The most common way of differentiating long-context tasks, besides the context's length, is whether they are naturally-constructed or synthetically-constructed \cite{tay2020long, bai2023longbench, hsieh2024ruler}.

\paragraph{Natural construction.} A simple yet effective way of ``moving the goalpost'' for context length is by modeling long-context tasks based on short-context tasks. This was done, for example, with QA \cite{ko2018narrativeqa, cf. dunn2017searchqa}, summarization \cite{huang2021a, cf. narayan2018don}, and NLI \cite{koreeda2021a, cf. williams2018broad}. Specialized domains like legal \cite{bruno2022lawngnli, nguyen2024captain} and literature \cite{wang2022squality, kryscinski2022booksum} often involve longer texts, turning typically short-context tasks such as QA and NLI into long-context scenarios. Another more native methodology is to create new tasks which inherently require a long context, such as multi-document summarization \cite{fabbri2019multi, angelidis2021extractive}, survey generation \cite{gao2024large}, and structured data aggregation \cite{caciularu2024tact}. Both methodologies share the constraint that, due to their natural construction (i.e., using natural text), once created, they are difficult to modify for longer contexts as models' long-context capabilities improve.

\paragraph{Synthetic construction.} A more flexible approach, sacrificing natural construction for length control, is to use distractors to synthetically increase the context length. This method allows for cheap and efficient (in terms of task construction cost) evaluation of models' full context length capabilities, with difficulty adjusted by controlling the distractors. Tasks like Needle-in-a-Haystack (NIAH; \cite{ivgi2023efficient, kamradt2023needle}) and PassKey retrieval \cite{mohtashami2023landmark} were created to evaluate a model's ability to pin-point specific information amid lengthy distractors. Flexible and effective against existing models, they became standard benchmarks for evaluating new long-context models \cite{glm2024, jiang2024mixtral}. Followup studies have complicated these tasks by increasing the number of critical details to locate \cite{arora2023zoology, liu2024a} and changing their position within the input \cite{liu2024b, levy2024same}.

\paragraph{Limitations of the status quo.} NIAH-like tasks aim to assess information retrieval capabilities, yet many ``naturally constructed'' QA and reading-comprehension tasks with trivial questions about a long context accomplish the same goal. At the same time, ``multiple needles'' NIAH can increase difficulty not by increasing the quantity of needles or length of input, but by adding distractors between needles \cite{levy2024same}. What can systematically explain the different variables at play, in order to inform better task design in the future?

Clearly, there are underlying qualitative differences that discriminate between these various tasks besides their natural and synthetic constructions, and besides their actual context length. Therefore, we require a more informative vocabulary to discuss the goals of each task design, what it accomplishes, and what it does not, in terms of measuring long-context capabilities.

\section{What Makes Long Context More than Retrieval?}
\label{sec:taxonomy}

We require a taxonomy to capture task difficulty variations beyond mere ``number of tokens''. We focus on the information that is canonically required to solve the task as the conditioning variable. Our classification can be summarized via the following two questions, when asked about a given task: (I) How difficult is it to find and extract the required information? (II) How much information is needed to be found?

Assuming that some highlighting of the relevant information is needed to solve the task (see Figure~\ref{fig:taxonomy}), the latter question asks how much text is highlighted, while the former addresses the challenge of locating the relevant text for highlighting.

For instance, consider the task of summarizing a book, in comparison to a NIAH task of identifying a numerical detail in a long financial report (e.g., ``how much did the company earn in 2015?''). Although both tasks involve long texts, the information required and its accessibility vary significantly. The NIAH task focuses on localized, identifiable information, while summarization requires extracting key details dispersed throughout the text, tangled together with irrelevant content. Therefore, we can say that the book summarization task is more difficult on both axes (I) and (II).

Below we give more formal descriptions of the two axes characterized by the questions above.

\paragraph{(I) Dispersion.} Although the question above intuitively defines ``difficulty of information finding'', we offer a more concrete description. Between two similar tasks, we consider the information harder to find in one task compared to another if: (1) it is more obscured (e.g., linguistically, semantically, contextually, etc); (2) it is more sparse, such that it is interspersed with non-required information; (3) its indicators are less redundant, such that there are fewer places in the document where the same information is available.

\paragraph{(II) Scope.} The property of scope is simpler, and refers to the minimal quantity of information needed to solve the task. Importantly, we are not concerned with precise metric for ``quantity of information'' at this stage -- it can refer to quantity of tokens, sentences, relations, cells in a table, etc. Any metric that reliably captures difficulty for an established solver is sufficient for our purposes.

\paragraph{Illustrative example.} To illustrate, consider the Wikipedia entry for New York City and a simple question: ``What is the estimated population of the city?'' Since the answer needs a small snippet of information, we say that the task has small scope. And since it is easily accessible, we say that it has low dispersion. Consider, instead, the question ``how many syllables are in this document?'' -- since this question requires the entire document to answer, we say that it has large scope, but if we consider counting syllables as straightforward, then we say its dispersion is still low. Finally, with the question ``Was the city's mayor elected before or after the city was affected by Hurricane Sandy?'' -- since it requires snippets from at least two different areas of the text, we can say that when compared to the question about the city's population, the dispersion is higher, but not as high as for the question ``What makes the city a prominent place on the world stage?'' which poses a challenge on both axes.

\begin{figure}[t]
\centering
\fbox{\parbox{0.95\linewidth}{[IMAGE NOT PROVIDED: Scatter plot showing benchmarks distributed across scope-dispersion quadrants with color gradient indicating difficulty]}}
\caption{This figure illustrates our subjective judgment on the distribution of long-context benchmarks for each task, categorized by their scope and dispersion characteristics, with the four quadrants being marked by the dashed lines. Difficulty is expressed by shade, where red is more difficult and green is easier. Notably, some tasks, like Question-answering (QA), appear in multiple quadrants, as different benchmarks demand varying levels of scope and dispersion (e.g., a single fact versus multiple facts spread across a document). For a detailed breakdown of benchmarks and their task associations, refer to Appendix A.}
\label{fig:benchmarks}
\end{figure}

\section{Challenging Long Context Is Under-Explored}
\label{sec:underexplored}

Revisiting the works surveyed in \S\ref{sec:task_design}, they clearly differ with respect to both scope and dispersion.

\paragraph{With respect to dispersion.} The information needed for tasks ranges from easily accessible to highly dispersed and difficult to detect. On low dispersion we have NIAH \cite{kamradt2023needle, mohtashami2023landmark} and a myriad of factual single-hop QA datasets \cite{tseng2016towards, ko2017narrativeqa, kwiatkowski2019natural, dasigi2021dataset, inter alia} in which the answer is relatively accessible. Adding more snippets of information separated by distractors, either in the form of several needles \cite{arora2023zoology, hsieh2024ruler} or of hops in a multi-hop question \cite{trivedi2022musique, zhao2022multihiertt}, complicates the information detection due to the need to find at least two snippets \cite{levy2024same}, thereby increasing dispersion. Dispersion can also be increased by making the detection of the information less straightforward \cite{pang2022quality} or requiring aggregation \cite{shaham2023zeroscrolls}. Lastly, summarization tasks are of a very high dispersion \cite{huang2021a, wang2022squality}, as they require the non-trivial detection of salient facts that are interwoven with the irrelevant text.

\paragraph{With respect to scope.} Tasks overwhelmingly target relatively small scope. In addition to the aforementioned NIAH tasks and their variants, many QA datasets apply as well \cite{li2023loogle, zhao2023docmatheval, reddy2024docfinqa, inter alia}. A somewhat higher scope is achieved by datasets for query-based summarization \cite{zhong2021qmsum, wang2022squality}, and QA datasets with more obfuscated answers that require reading the text surrounding the answer for its verification \cite{an2023leval, he2023never}. Although much higher on the scope ladder, book summarization is still limited in its scope as datasets include texts that are only of up to 20k tokens \cite{huang2021a, chen2022a, shaham2023zeroscrolls}. Currently, tasks with the highest scope, requiring information across the entire input length, are artificial and of low dispersion, like common words extraction \cite{hsieh2024ruler}.

\paragraph{Conclusion.} Figure~\ref{fig:benchmarks} summarizes the above classification of tasks and datasets. Note that without a concrete definition of dispersion and scope, the plot is only an illustration that involves a good deal of subjective judgements. However, we conclude that (1) the majority of tasks designed to challenge LLMs in a long-context setting target either scope or dispersion, such that (2) tasks that push current models' capabilities on both axes are under-represented in the current landscape.

\section{Discussion: Towards Genuinely Difficult Long-Context Task Design}
\label{sec:discussion}

\paragraph{Challenges.} Designing meaningful long-context tasks amidst rapid model progress is profoundly challenging, making the deficiency in tasks representing difficulty on both the dispersion and scope axes unsurprising. One source of this challenge is the lack of diverse, coherent long texts, as models' context windows can now be comparable to the length of the New Testament\textsuperscript{1} and the Odyssey\textsuperscript{2}. The methodologies discussed in \S\ref{sec:task_design} for creating long context tasks -- lengthening short context tasks and synthetically creating length-adjustable tasks -- are preferred for their straightforward definition and the incremental adjustments they require for existing data. They rely on the common understanding of machine comprehension as formulated with short context in mind \cite{dunietz2020test}, and therefore they are intuitive and easy to comprehend for NLP researchers without domain expertise (e.g., in law or biomedical fields that have long contexts).

\paragraph{Future work.} The goals laid forward in this work are clear: For more durable and robust measurement of long-context capabilities, we must design tasks that explicitly target both the dispersion and scope capabilities of models. How can this be achieved? As mentioned, one possible avenue is to rely more on tasks that require domain expertise, such as legal documents \cite{bruno2022lawngnli}, financial reports \cite{reddy2024docfinqa}, biomedical publications \cite{stylianou2021improved}, and so on. In specialized domains, it is common that dispersion will be naturally higher \cite{zhao2022multihiertt}. Tasks that involve implicit aggregations over structured data, such as table manipulation \cite{caciularu2024tact}, are possible avenues for increasing both scope and dispersion synthetically by leveraging the data structure. In this work, we argue that an explicit vocabulary for such properties of difficulty is what can enable more informed techniques to design difficult tasks in the future.

\section{Conclusions}

We present a taxonomy of factors that make long-context tasks more challenging compared to short ones. This is in contrast with the existing literature that refers only to the length of the input as the hallmark of long context, and as a result ends up conflating tasks of different character when assessing the ability of models to understand longer text. We reviewed works on evaluation in a long-context setting and found that the most challenging setting, in which the information needed is of large scope and is highly dispersed within the input, is under-explored. Finally, we suggested some leads for future work to tackle this imbalance towards a more informative long context evaluation.

\section{Limitations}

\paragraph{Formality.} In the context of this work, we have deliberately adhered to a taxonomy based on an intuitive description, in the interest of utility to a wide diversity of research and flexibility for future work. Difficulty in searching for and extracting information, and quantity of information, are both vague terms that can only be grounded in the context of a specific family of tasks and use-cases. We intend for this work to serve as a call to action and a tool for a shared vocabulary in the interest of more informed long-context task design in the future, rather than to anchor the taxonomy to a specific and fragile point in time.

\paragraph{Retrieval is still interesting.} Although we argue that small scope and low dispersion tasks are the least indicative of the model's ability to long-context capabilities, tasks that are well-served by implicit retrieval or by traditional retrieval-based pipelines are certainly relevant and useful in a variety of common use-cases \cite{stylianou2021improved, bruno2022lawngnli, gao2023rarr}.

\paragraph{Other uses for a long-context window.} This paper deals only with long inputs that serve as inputs to a task. The long context of course can have other purposes as well, like containing many in-context examples \cite{bertsch2024incontext} or containing other modalities and structures \cite{jiang2023structgpt}.

\section*{Acknowledgments}

The authors would like to thank Gabriel Stanovsky for the fruitful discussions.

\bibliographystyle{acl_natbib}
\begin{thebibliography}{100}

\bibitem[Amar et al.2023]{amar2023openasp}
Shmuel Amar, Liat Schiff, Ori Ernst, Asi Shefer, Ori Shapira, and Ido Dagan. 2023. OpenAsp: A benchmark for multi-document open aspect-based summarization. In \textit{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing}, pages 1967--1991, Singapore. Association for Computational Linguistics.

\bibitem[An et al.2023]{an2023leval}
Chenxin An, Shansan Gong, Ming Zhong, Xingjian Zhao, Mukai Li, Jun Zhang, Lingpeng Kong, and Xipeng Qiu. 2023. L-eval: Instituting standardized evaluation for long context language models. Preprint, arXiv:2307.11088.

\bibitem[Angelidis et al.2021]{angelidis2021extractive}
Stefanos Angelidis, Reinald Kim Amplayo, Yoshihiko Suhara, Xiaolan Wang, and Mirella Lapata. 2021. Extractive opinion summarization in quantized transformer spaces. \textit{Transactions of the Association for Computational Linguistics}, 9:277--293.

\bibitem[Arora et al.2023]{arora2023zoology}
Simran Arora, Sabri Eyuboglu, Aman Timalsina, Isys Johnson, Michael Poli, James Zou, Atri Rudra, and Christopher R{\'e}. 2023. Zoology: Measuring and improving recall in efficient language models. arXiv preprint arXiv:2312.04927.

\bibitem[Aumiller and Gertz2022]{aumiller2022klexikon}
Dennis Aumiller and Michael Gertz. 2022. Klexikon: A German dataset for joint summarization and simplification. In \textit{Proceedings of the Thirteenth Language Resources and Evaluation Conference}, pages 2693--2701, Marseille, France. European Language Resources Association.

\bibitem[Bai et al.2023]{bai2023longbench}
Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. 2023. Longbench: A bilingual, multitask benchmark for long context understanding. Preprint, arXiv:2308.14508.

\bibitem[Bertsch et al.2024]{bertsch2024incontext}
Amanda Bertsch, Maor Ivgi, Uri Alon, Jonathan Berant, Matthew R. Gormley, and Graham Neubig. 2024. In-context learning with long-context models: An in-depth exploration. Preprint, arXiv:2405.00200.

\bibitem[Bishop et al.2024]{bishop2024longdocfactscore}
Jennifer A Bishop, Qianqian Xie, and Sophia Ananiadou. 2024. Longdocfactscore: Evaluating the factuality of long document abstractive summarisation. Preprint, arXiv:2309.12455.

\bibitem[Boni et al.2021]{boni2021howsumm}
Odellia Boni, Guy Feigenblat, Guy Lev, Michal Shmueli-Scheuer, Benjamin Sznajder, and David Konopnicki. 2021. Howsumm: A multi-document summarization dataset derived from wikihow articles. Preprint, arXiv:2110.03179.

\bibitem[Bruno and Roth2022]{bruno2022lawngnli}
William Bruno and Dan Roth. 2022. Lawngnli: A long-premise benchmark for in-domain generalization from short to long contexts and for implication-based retrieval. Preprint, arXiv:2212.03222.

\bibitem[Caciularu et al.2024]{caciularu2024tact}
Avi Caciularu, Alon Jacovi, Eyal Ben-David, Sasha Goldshtein, Tal Schuster, Jonathan Herzig, Gal Elidan, and Amir Globerson. 2024. Tact: Advancing complex aggregative reasoning with information extraction tools. Preprint, arXiv:2406.03618.

\bibitem[Chen et al.2022a]{chen2022a}
Mingda Chen, Zewei Chu, Sam Wiseman, and Kevin Gimpel. 2022a. SummScreen: A dataset for abstractive screenplay summarization. In \textit{Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 8602--8615, Dublin, Ireland. Association for Computational Linguistics.

\bibitem[Chen et al.2022b]{chen2022b}
Mingda Chen, Zewei Chu, Sam Wiseman, and Kevin Gimpel. 2022b. Summscreen: A dataset for abstractive screenplay summarization. Preprint, arXiv:2104.07091.

\bibitem[Chen et al.2023]{chen2023longlora}
Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. 2023. Longlora: Efficient fine-tuning of long-context large language models. ArXiv, abs/2309.12307.

\bibitem[Cohan et al.2018]{cohan2018discourse}
Arman Cohan, Franck Dernoncourt, Doo Soon Kim, Trung Bui, Seokhwan Kim, Walter Chang, and Nazli Goharian. 2018. A discourse-aware attention model for abstractive summarization of long documents. Preprint, arXiv:1804.05685.

\bibitem[Dasigi et al.2021]{dasigi2021dataset}
Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A. Smith, and Matt Gardner. 2021. A dataset of information-seeking questions and answers anchored in research papers. In \textit{Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, pages 4599--4610, Online. Association for Computational Linguistics.

\bibitem[Devlin et al.2019]{devlin-etal-2019-bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In \textit{Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)}, pages 4171--4186, Minneapolis, Minnesota. Association for Computational Linguistics.

\bibitem[Dong et al.2024]{dong2024bamboo}
Zican Dong, Tianyi Tang, Junyi Li, Wayne Xin Zhao, and Ji-Rong Wen. 2024. BAMBOO: A comprehensive benchmark for evaluating long text modeling capacities of large language models. In \textit{Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)}, pages 2086--2099, Torino, Italia. ELRA and ICCL.

\bibitem[Dunietz et al.2020]{dunietz2020test}
Jesse Dunietz, Greg Burnham, Akash Bharadwaj, Owen Rambow, Jennifer Chu-Carroll, and Dave Ferrucci. 2020. To test machine comprehension, start by defining comprehension. In \textit{Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics}, pages 7839--7859, Online. Association for Computational Linguistics.

\bibitem[Dunn et al.2017]{dunn2017searchqa}
Matthew Dunn, Levent Sagun, Mike Higgins, V Ugur Guney, Volkan Cirik, and Kyunghyun Cho. 2017. Searchqa: A new q\&a dataset augmented with context from a search engine. arXiv preprint arXiv:1704.05179.

\bibitem[Fabbri et al.2019]{fabbri2019multi}
Alexander R. Fabbri, Irene Li, Tianwei She, Suyi Li, and Dragomir R. Radev. 2019. Multi-news: a large-scale multi-document summarization dataset and abstractive hierarchical model. Preprint, arXiv:1906.01749.

\bibitem[Feng et al.2021]{feng2021multidoc2dial}
Song Feng, Siva Sankalp Patel, Hui Wan, and Sachindra Joshi. 2021. MultiDoc2Dial: Modeling dialogues grounded in multiple documents. In \textit{Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing}, pages 6162--6176, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.

\bibitem[Gao et al.2024]{gao2024large}
Fan Gao, Hang Jiang, Rui Yang, Qingcheng Zeng, Jinghui Lu, Moritz Blum, Dairui Liu, Tianwei She, Yuang Jiang, and Irene Li. 2024. Large language models on wikipedia-style survey generation: an evaluation in nlp concepts. Preprint, arXiv:2308.10410.

\bibitem[Gao et al.2023]{gao2023rarr}
Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun Tejasvi Chaganty, Yicheng Fan, Vincent Y. Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan, and Kelvin Guu. 2023. Rarr: Researching and revising what language models say, using language models. Preprint, arXiv:2210.08726.

\bibitem[Gemini Team Google2024]{gemini2024}
Gemini Team Google. 2024. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. Preprint, arXiv:2403.05530.

\bibitem[GLM Team2024]{glm2024}
GLM Team. 2024. GLM-4-9b-chat technical report.

\bibitem[Guo et al.2023]{guo2023longcoder}
Daya Guo, Canwen Xu, Nan Duan, Jian Yin, and Julian McAuley. 2023. Longcoder: A long-range pre-trained language model for code completion. Preprint, arXiv:2306.14893.

\bibitem[He et al.2023]{he2023never}
Junqing He, Kunhao Pan, Xiaoqun Dong, Zhuoyang Song, Yibo Liu, Yuxin Liang, Hao Wang, Qianguo Sun, Songxin Zhang, Zejian Xie, and Jiaxing Zhang. 2023. Never lost in the middle: Improving large language models via attention strengthening question answering. Preprint, arXiv:2311.09198.

\bibitem[Hendrycks et al.2021]{hendrycks2021cuad}
Dan Hendrycks, Collin Burns, Anya Chen, and Spencer Ball. 2021. Cuad: An expert-annotated nlp dataset for legal contract review. Preprint, arXiv:2103.06268.

\bibitem[Ho et al.2020]{ho2020constructing}
Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. 2020. Constructing a multi-hop QA dataset for comprehensive evaluation of reasoning steps. In \textit{Proceedings of the 28th International Conference on Computational Linguistics}, pages 6609--6625, Barcelona, Spain (Online). International Committee on Computational Linguistics.

\bibitem[Hsieh et al.2024]{hsieh2024ruler}
Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, Yang Zhang, and Boris Ginsburg. 2024. Ruler: What's the real context size of your long-context language models? Preprint, arXiv:2404.06654.

\bibitem[Hu et al.2023]{hu2023meetingbank}
Yebowen Hu, Timothy Ganter, Hanieh Deilamsalehy, Franck Dernoncourt, Hassan Foroosh, and Fei Liu. 2023. MeetingBank: A benchmark dataset for meeting summarization. In \textit{Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 16409--16423, Toronto, Canada. Association for Computational Linguistics.

\bibitem[Huang et al.2021a]{huang2021a}
Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu Wang. 2021a. Efficient attentions for long document summarization. In \textit{Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, pages 1419--1436, Online. Association for Computational Linguistics.

\bibitem[Huang et al.2021b]{huang2021b}
Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu Wang. 2021b. Efficient attentions for long document summarization. Preprint, arXiv:2104.02112.

\bibitem[Ivgi et al.2023]{ivgi2023efficient}
Maor Ivgi, Uri Shaham, and Jonathan Berant. 2023. Efficient long-text understanding with short-text models. \textit{Transactions of the Association for Computational Linguistics}, 11:284--299.

\bibitem[Jiang et al.2024]{jiang2024mixtral}
Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, L{\'e}lio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Th{\'e}ophile Gervet, Thibaut Lavril, Thomas Wang, Timoth{\'e}e Lacroix, and William El Sayed. 2024. Mixtral of experts. Preprint, arXiv:2401.04088.

\bibitem[Jiang et al.2023]{jiang2023structgpt}
Jinhao Jiang, Kun Zhou, Zican Dong, Keming Ye, Xin Zhao, and Ji-Rong Wen. 2023. StructGPT: A general framework for large language model to reason over structured data. In \textit{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing}, pages 9237--9251, Singapore. Association for Computational Linguistics.

\bibitem[Kamradt2023]{kamradt2023needle}
Gregory Kamradt. 2023. Needle in a haystack- pressure testing LLMs. GitHub.

\bibitem[Kocisky et al.2017]{ko2017narrativeqa}
Tom{\'a}{\v{s}} Ko{\v{c}}isk{\'y}, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, G{\'a}bor Melis, and Edward Grefenstette. 2017. The narrativaqa reading comprehension challenge. Preprint, arXiv:1712.07040.

\bibitem[Kocisky et al.2018]{ko2018narrativeqa}
Tom{\'a}{\v{s}} Ko{\v{c}}isk{\'y}, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, G{\'a}bor Melis, and Edward Grefenstette. 2018. The NarrativeQA Reading Comprehension Challenge. \textit{Transactions of the Association for Computational Linguistics}, 6:317--328.

\bibitem[Koreeda and Manning2021a]{koreeda2021a}
Yuta Koreeda and Christopher Manning. 2021a. ContractNLI: A dataset for document-level natural language inference for contracts. In \textit{Findings of the Association for Computational Linguistics: EMNLP 2021}, pages 1907--1919, Punta Cana, Dominican Republic. Association for Computational Linguistics.

\bibitem[Koreeda and Manning2021b]{koreeda2021b}
Yuta Koreeda and Christopher D. Manning. 2021b. Contractnli: A dataset for document-level natural language inference for contracts. Preprint, arXiv:2110.01799.

\bibitem[Kornilova and Eidelman2019]{kornilova2019billsum}
Anastassia Kornilova and Vladimir Eidelman. 2019. BillSum: A corpus for automatic summarization of US legislation. In \textit{Proceedings of the 2nd Workshop on New Frontiers in Summarization}, pages 48--56, Hong Kong, China. Association for Computational Linguistics.

\bibitem[Kryscinski et al.2022]{kryscinski2022booksum}
Wojciech Kryscinski, Nazneen Rajani, Divyansh Agarwal, Caiming Xiong, and Dragomir Radev. 2022. BOOKSUM: A collection of datasets for long-form narrative summarization. In \textit{Findings of the Association for Computational Linguistics: EMNLP 2022}, pages 6536--6558, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.

\bibitem[Kry\'scinski et al.2022]{kryscinski2022booksum}
Wojciech Kry{\'s}ci{\'n}ski, Nazneen Rajani, Divyansh Agarwal, Caiming Xiong, and Dragomir Radev. 2022. Booksum: A collection of datasets for long-form narrative summarization. Preprint, arXiv:2105.08209.

\bibitem[Kulkarni et al.2020]{kulkarni2020aquamuse}
Sayali Kulkarni, Sheide Chammas, Wan Zhu, Fei Sha, and Eugene Ie. 2020. Aquamuse: Automatically generating datasets for query-based multi-document summarization. Preprint, arXiv:2010.12694.

\bibitem[Kuratov et al.2024]{kuratov2024search}
Yuri Kuratov, Aydar Bulatov, Petr Anokhin, Dmitry Sorokin, Artyom Sorokin, and Mikhail Burtsev. 2024. In search of needles in a 11m haystack: Recurrent memory finds what llms miss. Preprint, arXiv:2402.10790.

\bibitem[Kwiatkowski et al.2019]{kwiatkowski2019natural}
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: A benchmark for question answering research. \textit{Transactions of the Association for Computational Linguistics}, 7:452--466.

\bibitem[Levy et al.2024]{levy2024same}
Mosh Levy, Alon Jacoby, and Yoav Goldberg. 2024. Same task, more tokens: the impact of input length on the reasoning performance of large language models. Preprint, arXiv:2402.14848.

\bibitem[Li et al.2023]{li2023loogle}
Jiaqi Li, Mengmeng Wang, Zilong Zheng, and Muhan Zhang. 2023. Loogle: Can long-context language models understand long contexts? Preprint, arXiv:2311.04939.

\bibitem[Liu et al.2024a]{liu2024a}
Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. 2024a. World model on million-length video and language with ringattention. arXiv preprint arXiv:2402.08268.

\bibitem[Liu et al.2024b]{liu2024b}
Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2024b. Lost in the middle: How language models use long contexts. \textit{Transactions of the Association for Computational Linguistics}, 12:157--173.

\bibitem[Liu et al.2023a]{liu2023a}
Shuaiqi Liu, Jiannong Cao, Ruosong Yang, and Zhiyuan Wen. 2023a. Long text and multi-table summarization: Dataset and method. Preprint, arXiv:2302.03815.

\bibitem[Liu et al.2023b]{liu2023b}
Tianyang Liu, Canwen Xu, and Julian McAuley. 2023b. Repobench: Benchmarking repository-level code auto-completion systems. Preprint, arXiv:2306.03091.

\bibitem[Malaviya et al.2024]{malaviya2024expertqa}
Chaitanya Malaviya, Subin Lee, Sihao Chen, Elizabeth Sieber, Mark Yatskar, and Dan Roth. 2024. ExpertQA: Expert-curated questions and attributed answers. In \textit{Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)}, pages 3025--3045, Mexico City, Mexico. Association for Computational Linguistics.

\bibitem[Mohtashami and Jaggi2023]{mohtashami2023landmark}
Amirkeivan Mohtashami and Martin Jaggi. 2023. Landmark attention: Random-access infinite context length for transformers. In \textit{Workshop on Efficient Systems for Foundation Models@ ICML2023}.

\bibitem[Narayan et al.2018]{narayan2018don}
Shashi Narayan, Shay B. Cohen, and Mirella Lapata. 2018. Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. In \textit{Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing}, pages 1797--1807, Brussels, Belgium. Association for Computational Linguistics.

\bibitem[Nguyen et al.2024]{nguyen2024captain}
Chau Nguyen, Phuong Nguyen, Thanh Tran, Dat Nguyen, An Trieu, Tin Pham, Anh Dang, and Le-Minh Nguyen. 2024. Captain at coliee 2023: Efficient methods for legal information retrieval and entailment tasks. Preprint, arXiv:2401.03551.

\bibitem[OpenAI2024]{openai2024gpt4}
OpenAI. 2024. GPT-4 technical report. Preprint, arXiv:2303.08774.

\bibitem[Pal et al.2023]{pal2023giraffe}
Arka Pal, Deep Karkhanis, Manley Roberts, Samuel Dooley, Arvind Sundararajan, and Siddartha Naidu. 2023. Giraffe: Adventures in expanding context lengths in llms. Preprint, arXiv:2308.10882.

\bibitem[Pang et al.2022]{pang2022quality}
Richard Yuanzhe Pang, Alicia Parrish, Nitish Joshi, Nikita Nangia, Jason Phang, Angelica Chen, Vishakh Padmakumar, Johnny Ma, Jana Thompson, He He, and Samuel Bowman. 2022. QuALITY: Question answering with long input texts, yes! In \textit{Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, pages 5336--5358, Seattle, United States. Association for Computational Linguistics.

\bibitem[Prasad et al.2023]{prasad2023meetingqa}
Archiki Prasad, Trung Bui, Seunghyun Yoon, Hanieh Deilamsalehy, Franck Dernoncourt, and Mohit Bansal. 2023. MeetingQA: Extractive question-answering on meeting transcripts. In \textit{Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 15000--15025, Toronto, Canada. Association for Computational Linguistics.

\bibitem[Raffel et al.2020]{raffel2020exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. \textit{Journal of machine learning research}, 21(140):1--67.

\bibitem[Reddy et al.2024]{reddy2024docfinqa}
Varshini Reddy, Rik Koncel-Kedziorski, Viet Dac Lai, Michael Krumdick, Charles Lovering, and Chris Tanner. 2024. Docfinqa: A long-context financial reasoning dataset. Preprint, arXiv:2401.06915.

\bibitem[Saunders et al.2022]{saunders2022self}
William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan Leike. 2022. Self-critiquing models for assisting human evaluators. Preprint, arXiv:2206.05802.

\bibitem[Shaham et al.2023]{shaham2023zeroscrolls}
Uri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant, and Omer Levy. 2023. ZeroSCROLLS: A zero-shot benchmark for long text understanding. In \textit{Findings of the Association for Computational Linguistics: EMNLP 2023}, pages 7977--7989, Singapore. Association for Computational Linguistics.

\bibitem[Shaham et al.2022]{shaham2022scrolls}
Uri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong, Mor Geva, Jonathan Berant, and Omer Levy. 2022. SCROLLS: Standardized CompaRison over long language sequences. In \textit{Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing}, pages 12007--12021, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.

\bibitem[Sharma et al.2019]{sharma2019big}
Eva Sharma, Chen Li, and Lu Wang. 2019. BIG-PATENT: A large-scale dataset for abstractive and coherent summarization. In \textit{Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics}, pages 2204--2213, Florence, Italy. Association for Computational Linguistics.

\bibitem[Stylianou et al.2021]{stylianou2021improved}
Nikolaos Stylianou, Panagiotis Kosmoliaptsis, and Ioannis Vlahavas. 2021. Improved biomedical entity recognition via longer context modeling. In \textit{Artificial Intelligence Applications and Innovations: 17th IFIP WG 12.5 International Conference, AIAI 2021, Hersonissos, Crete, Greece, June 25--27, 2021, Proceedings 17}, pages 45--56. Springer.

\bibitem[Su et al.2024]{su2024roformer}
Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. 2024. Roformer: Enhanced transformer with rotary position embedding. \textit{Neurocomputing}, 568:127063.

\bibitem[Takeshita et al.2024]{takeshita2024aclsum}
Sotaro Takeshita, Tommaso Green, Ines Reinig, Kai Eckert, and Simone Ponzetto. 2024. ACLSum: A new dataset for aspect-based summarization of scientific publications. In \textit{Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)}, pages 6660--6675, Mexico City, Mexico. Association for Computational Linguistics.

\bibitem[Tay et al.2020]{tay2020long}
Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. 2020. Long range arena: A benchmark for efficient transformers. Preprint, arXiv:2011.04006.

\bibitem[Trivedi et al.2022]{trivedi2022musique}
Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2022. Musique: Multi-hop questions via single-hop question composition. Preprint, arXiv:2108.00573.

\bibitem[Tseng et al.2016]{tseng2016towards}
Bo-Hsiang Tseng, Sheng-Syun Shen, Hung-Yi Lee, and Lin-Shan Lee. 2016. Towards machine comprehension of spoken content: Initial toefl listening comprehension test by machine.

\bibitem[Vaswani et al.2017]{vaswani2017attention}
Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In \textit{Neural Information Processing Systems}.

\bibitem[Wang et al.2022]{wang2022squality}
Alex Wang, Richard Yuanzhe Pang, Angelica Chen, Jason Phang, and Samuel R. Bowman. 2022. SQuALITY: Building a long-document summarization dataset the hard way. In \textit{Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing}, pages 1139--1156, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.

\bibitem[Williams et al.2018]{williams2018broad}
Adina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In \textit{Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)}, pages 1112--1122, New Orleans, Louisiana. Association for Computational Linguistics.

\bibitem[Wu et al.2024]{wu2024less}
Yunshu Wu, Hayate Iso, Pouya Pezeshkpour, Nikita Bhutani, and Estevam Hruschka. 2024. Less is more for long document summary evaluation by llms. Preprint, arXiv:2309.07382.

\bibitem[Yang et al.2018]{yang2018hotpotqa}
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. Preprint, arXiv:1809.09600.

\bibitem[Zhang et al.2024a]{zhang2024a}
Jiebin Zhang, Eugene J. Yu, Qinyu Chen, Chen-hao Xiong, Dawei Zhu, Han Qian, Mingbo Song, Xiaoguang Li, Qun Liu, and Sujian Li. 2024a. Retrieval-based full-length wikipedia generation for emergent events. Preprint, arXiv:2402.18264.

\bibitem[Zhang et al.2024b]{zhang2024inftybench}
Xinrong Zhang, Yingfa Chen, Shengding Hu, Zi-hang Xu, Junhao Chen, Moo Khai Hao, Xu Han, Zhen Leng Thai, Shuo Wang, Zhiyuan Liu, and Maosong Sun. 2024b. $\infty$bench: Extending long context evaluation beyond 100k tokens. Preprint, arXiv:2402.13718.

\bibitem[Zhao et al.2022]{zhao2022multihiertt}
Yilun Zhao, Yunxiang Li, Chenying Li, and Rui Zhang. 2022. MultiHiertt: Numerical reasoning over multi hierarchical tabular and textual data. In \textit{Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 6588--6600, Dublin, Ireland. Association for Computational Linguistics.

\bibitem[Zhao et al.2023]{zhao2023docmatheval}
Yilun Zhao, Yitao Long, Hongjun Liu, Linyong Nan, Lyuhao Chen, Ryo Kamoi, Yixin Liu, Xiangru Tang, Rui Zhang, and Arman Cohan. 2023. Docmath-eval: Evaluating numerical reasoning capabilities of llms in understanding long documents with tabular data. ArXiv, abs/2311.09805.

\bibitem[Zhong et al.2021]{zhong2021qmsum}
Ming Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia Mutuma, Rahul Jha, Ahmed Hassan Awadallah, Asli Celikyilmaz, Yang Liu, Xipeng Qiu, and Dragomir Radev. 2021. QMSum: A new benchmark for query-based multi-domain meeting summarization. In \textit{Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, pages 5905--5921, Online. Association for Computational Linguistics.

\bibitem[Zhou et al.2023]{zhou2023odsum}
Yijie Zhou, Kejian Shi, Wencai Zhang, Yixin Liu, Yilun Zhao, and Arman Cohan. 2023. Odsum: New benchmarks for open domain multi-document summarization. Preprint, arXiv:2309.08960.

\end{thebibliography}

\appendix
\section{Benchmark Scope-Dispersion Classification}
\label{app:classification}

In Table~\ref{tab:classification} we delineate the different long-context benchmarks, as well as classify them according to how challenging they are scope-wise and dispersion-wise.

\begin{table*}[ht]
\centering
\caption{Classification of long-context benchmarks in terms of Scope and Dispersion.}
\label{tab:classification}
\begin{tabular}{p{0.45\linewidth}p{0.45\linewidth}}
\toprule
\textbf{LOW SCOPE} & \textbf{HIGH SCOPE} \\
\midrule
\textbf{LOW DISPERSION} & \\
QA & QBS \\
Qasper \cite{dasigi2021dataset} & QMSum \cite{zhong2021qmsum} \\
NarrativeQA \cite{ko2018narrativeqa} & SQuALITY \cite{wang2022squality} \\
Short-dependency QA \cite{li2023loogle} & Related Work Summarization \cite{an2023leval} \\
MultiFieldQA \cite{bai2023longbench} & SPACE \cite{angelidis2021extractive} \\
LitM(QA) \cite{liu2024b} & WebBrain-G \cite{qian2023webbrain} \\
L-eval(MC QA) \cite{an2023leval} & AquaMuse \cite{kulkarni2020aquamuse} \\
NQ \cite{kwiatkowski2019natural} & FINDSum-Liquidity \cite{liu2023a} \\
RULER(single-hop QA) \cite{hsieh2024ruler} & \\
MeetingQA \cite{prasad2023meetingqa} & Aggregation \\
BABILong(tasks 1,4-6,9-10) \cite{kuratov2024search} & ZeroSCROLLS(SpaceDigest\& BookSumSort) \cite{shaham2023zeroscrolls} \\
Giraffe(2 tasks) \cite{pal2023giraffe} & PassageCount \cite{bai2023longbench} \\
 & FINDSum-ROO \cite{liu2023a} \\
Retrieval & \\
LitM(Key-value Retrieval) \cite{liu2024b} & Aspect-based Summarization \\
MultiDoc2Dial(GSP) \cite{feng2021multidoc2dial} & ACLSum \cite{takeshita2024aclsum} \\
TopicRet \cite{dacheng2023} & OpenAsp \cite{amar2023openasp} \\
Wiki-GenBen \cite{zhang2024a} & \\
RULER(S-NIAH\& MK-NIAH) \cite{hsieh2024ruler} & Text Sorting \\
LongChat-Lines \cite{pal2023giraffe} & Bamboo(ShowsSort\& ReportSumSort) \cite{dong2024bamboo} \\
 & \\
NLI & Retrieval \\
LawngNLI \cite{bruno2022lawngnli} & PassageRetrieval \cite{bai2023longbench} \\
ContractNLI \cite{koreeda2021b} & \\
Hallucination Detection \cite{dong2024bamboo} & LFQA \\
FLenQA(3 tasks) \cite{levy2024same} & LongFQA \cite{an2023leval} \\
 & \\
Fill-mask & NLI \\
Cloze \cite{li2023loogle} & Legal Case Entailment \cite{nguyen2024captain} \\
 & \\
NLG & \\
MultiDoc2Dial(ARG) \cite{feng2021multidoc2dial} & \\
\midrule
\textbf{HIGH DISPERSION} & \\
QA & Summarization \\
QuALITY \cite{pang2022quality} & GovReport \cite{huang2021b} \\
Long-dependency QA \cite{li2023loogle} & SummScreenFD \cite{chen2022b} \\
DuReader \cite{bai2023longbench} & Loogle(Summarization) \cite{li2023loogle} \\
SFiction QA \cite{an2023leval} & VCSUM \cite{bai2023longbench} \\
ExpertQA \cite{malaviya2024expertqa} & Self-critiquing \cite{saunders2022self} \\
DocFinQA \cite{reddy2024docfinqa} & Abstract Generation \cite{an2023leval} \\
BABILong(tasks 2-3,12) \cite{kuratov2024search} & Multi-News \cite{fabbri2019multi} \\
Bamboo(QA) \cite{dong2024bamboo} & BigPatent \cite{sharma2019big} \\
 & Scientific Summarization \cite{cohan2018discourse} \\
Multi-hop QA & BillSum \cite{kornilova2019billsum} \\
MuSiQue \cite{trivedi2022musique} & HowSumm \cite{boni2021howsumm} \\
HotpotQA \cite{yang2018hotpotqa} & ODSum \cite{zhou2023odsum} \\
Multi-hop Tracing \cite{hsieh2024ruler} & Klexikon(Summarization) \cite{aumiller2022klexikon} \\
RULER(multi-hop QA) \cite{hsieh2024ruler} & Booksum \cite{kryscinski2022booksum} \\
2WikiMultihopQA \cite{ho2020constructing} & MeetingBank \cite{hu2023meetingbank} \\
 & \\
NLI & Text Simplification \\
FLenQA(3 rand. placement tasks) \cite{levy2024same} & Klexikon(Simplification) \cite{aumiller2022klexikon} \\
Legal Textual Entailment \cite{nguyen2024captain} & \\
 & Reasoning \\
Code Understanding & Long ListOps \cite{tay2020long} \\
LCC \cite{guo2023longcoder} & \\
RepoBench-P \cite{liu2023b} & Retrieval \\
CodeU \cite{an2023leval} & LRA(task 3) \cite{tay2020long} \\
PrivateEval \cite{dong2024bamboo} & \\
 & Next Token Prediction \\
Classification & PG-19 \cite{rae2019compressive} \\
LRA(tasks 2,4-6) \cite{tay2020long} & Bamboo(LM) \cite{dong2024bamboo} \\
 & \\
Retrieval & Reasoning \\
COLIEE(tasks 1,3,4) \cite{nguyen2024captain} & DocMath-Eval \cite{zhao2023docmatheval} \\
RULER(MV-NIAH\& MQ-NIAH) \cite{hsieh2024ruler} & BABILong(tasks 14-20) \cite{kuratov2024search} \\
 & \\
 & Aggregation \\
 & RULER(2 Aggr. tasks) \cite{hsieh2024ruler} \\
 & BABILong(tasks 7-8) \cite{kuratov2024search} \\
 & \\
 & NLU \\
 & Academic Feedback Generation \cite{an2023leval} \\
 & CUAD \cite{hendrycks2021cuad} \\
 & \\
 & Factuality Evaluation \\
 & LongSciVerify \cite{bishop2024longdocfactscore} \\
 & \\
 & Coreference Resolution \\
 & BABILong(tasks 11,13) \cite{kuratov2024search} \\
\bottomrule
\end{tabular}
\end{table*}

\end{document}
=====END FILE=====