\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{raffel2020exploring}
\citation{devlin2019bert}
\citation{conneau2019unsupervised}
\citation{xue2021mt5}
\citation{suarez2019oscar}
\citation{abadji2022towards}
\citation{kreutzer2022quality}
\citation{ogueji2021small}
\citation{conneau2020unsupervised}
\citation{adelani2022few}
\citation{alabi2022adapting}
\citation{adebara2022serengeti}
\citation{kreutzer2022quality}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}\protected@file@percent }
\citation{kreutzer2022quality}
\citation{caswell2020language}
\citation{nllb2022no}
\citation{kreutzer2022quality}
\citation{xue2021mt5}
\citation{alabi2022adapting}
\citation{adebara2022serengeti}
\@writefile{toc}{\contentsline {section}{\numberline {2}WURA Dataset}{3}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Auditing and Cleaning mC4}{3}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}Language Contamination}{3}{subsubsection.2.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2}mC4 is a Great Source!}{3}{subsubsection.2.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Combination with Existing Language Resources and Non-African Languages}{3}{subsection.2.2}\protected@file@percent }
\citation{roberts2022scaling}
\citation{shazeer2020glu}
\citation{raffel2020exploring}
\citation{ogundepo2023afriqa}
\citation{adelani2022few}
\citation{hasan2021xl}
\citation{adelani2023masakhanews}
\citation{xue2021mt5}
\citation{xue2022byt5}
\citation{chung2022scaling}
\citation{ogundepo2022afriteva}
\citation{alabi2022adapting}
\citation{rajpurkar2016squad}
\@writefile{toc}{\contentsline {section}{\numberline {3}Experimental Setup}{4}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Model}{4}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Downstream Tasks}{4}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Cross-lingual Question Answering}{4}{subsubsection.3.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Machine Translation}{4}{subsubsection.3.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3}Summarization}{4}{subsubsection.3.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.4}Text Classification}{4}{subsubsection.3.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Baseline Models}{4}{subsection.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Result and Discussion}{4}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Downstream Performance}{4}{subsection.4.1}\protected@file@percent }
\citation{rae2021scaling}
\citation{kreutzer2022quality}
\citation{hernandez2022scaling}
\citation{alabi2022adapting}
\citation{resnik1999bible}
\citation{agic2019jw300}
\citation{kreutzer2022quality}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Cross-lingual Question Answering}{5}{subsubsection.4.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}Machine Translation}{5}{subsubsection.4.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.3}Summarization}{5}{subsubsection.4.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.4}Text Classification}{5}{subsubsection.4.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Discussion}{5}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Results for Nigerian Pidgin}{5}{subsubsection.4.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}Impact of Data Quality on LMs}{5}{subsubsection.4.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}AfriTeVa V2 Large Model}{5}{section.5}\protected@file@percent }
\citation{alabi2020massive}
\citation{conneau2020unsupervised}
\citation{ortiz2019asynchronous}
\citation{xue2021mt5}
\citation{bapna2022building}
\citation{ogueji2021small}
\citation{leong2022bloom}
\citation{palen2022multilingual}
\citation{alabi2022adapting}
\citation{adebara2022serengeti}
\bibstyle{acl_natbib}
\bibdata{refs}
\citation{kreutzer2022quality}
\@writefile{toc}{\contentsline {section}{\numberline {6}Related Work}{6}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion}{6}{section.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8}Limitations}{6}{section.8}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {A}mC4 Audit and Web Crawling}{6}{appendix.A}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}mC4 Audit}{6}{subsection.A.1}\protected@file@percent }
\citation{petrov2023language}
\citation{ahia2023all}
\citation{acs2019exploring}
\citation{dione2023masakhapos}
\citation{kudo2018sentencepiece}
\citation{lample2019cross}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}Web Crawling}{7}{subsection.A.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {B}Tokenization}{7}{appendix.B}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {C}AfriTeVa V2 Large}{7}{appendix.C}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Tokenizer Fertilities: We measure the fertilities of our tokenizers with varying vocabulary sizes using the MasakhanePOS dataset. The 150k tokenizer gives the best trade-off in size and fertility scores across all languages, especially in the second sampling configuration.}}{8}{table.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:fertility}{{1}{8}{Tokenizer Fertilities: We measure the fertilities of our tokenizers with varying vocabulary sizes using the MasakhanePOS dataset. The 150k tokenizer gives the best trade-off in size and fertility scores across all languages, especially in the second sampling configuration}{table.caption.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces XL-SUM results: Performance based on Rouge-1, Rouge-2 and Rouge-L. AfriTeVa V2 Large outperforms AfriTeVa V2 Base across all languages considered.}}{8}{table.caption.3}\protected@file@percent }
\newlabel{tab:summarization_large}{{2}{8}{XL-SUM results: Performance based on Rouge-1, Rouge-2 and Rouge-L. AfriTeVa V2 Large outperforms AfriTeVa V2 Base across all languages considered}{table.caption.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces MasakhaNews Classification Results: Evaluation is done using the weighted F1 score and the scores presented are averaged across 3 seeds. AfriTeVa V2 Large marginally improves overs Base results.}}{8}{table.caption.4}\protected@file@percent }
\newlabel{tab:classification_large}{{3}{8}{MasakhaNews Classification Results: Evaluation is done using the weighted F1 score and the scores presented are averaged across 3 seeds. AfriTeVa V2 Large marginally improves overs Base results}{table.caption.4}{}}
\gdef \@abspage@last{8}
