=====FILE: main.tex=====
\documentclass[11pt]{article}
\usepackage{times}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{multirow}
\usepackage{array}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{url}
\usepackage{float}

\geometry{a4paper, margin=1in}

% Custom commands for the paper
\newcommand{\plm}{PLM}
\newcommand{\sts}{STS}

\title{Advancing Semantic Textual Similarity Modeling: A Regression Framework with Translated ReLU and Smooth K2 Loss}
\author{Bowen Zhang and Chunping Li \\ School of Software, Tsinghua University \\ \texttt{zbw23@mails.tsinghua.edu.cn, cli@tsinghua.edu.cn}}

\begin{document}

\maketitle

\begin{abstract}
Since the introduction of BERT and RoBERTa, research on Semantic Textual Similarity (STS) has made groundbreaking progress. Particularly, the adoption of contrastive learning has substantially elevated state-of-the-art performance across various STS benchmarks. However, contrastive learning categorizes text pairs as either semantically similar or dissimilar, failing to leverage fine-grained annotated information and necessitating large batch sizes to prevent model collapse. These constraints pose challenges for researchers engaged in STS tasks that involve nuanced similarity levels or those with limited computational resources, compelling them to explore alternatives like Sentence-BERT. Despite its efficiency, Sentence-BERT tackles STS tasks from a classification perspective, overlooking the progressive nature of semantic relationships, which results in suboptimal performance. To bridge this gap, this paper presents an innovative regression framework and proposes two simple yet effective loss functions: Translated ReLU and Smooth K2 Loss. Experimental results demonstrate that our method achieves convincing performance across seven established STS benchmarks and offers the potential for further optimization of contrastive learning pre-trained models.\footnote{Our code and checkpoints are available at \url{https://github.com/ZBWpro/STS-Regression}.}
\end{abstract}

\section{Introduction}
\label{sec:introduction}

Semantic Textual Similarity (STS) constitutes a fundamental task in natural language processing, wielding significant influence across a multitude of applications, including text clustering, information retrieval, and recommendation systems. Despite the remarkable precision obtained by interactive architectures within these tasks, their inability to support offline computation limits their viability in large-scale text analysis scenarios. In response, the seminal work of Sentence-BERT \cite{reimers-gurevych-2019-sentence} introduces a dual-tower architecture to encode the sentences within a pair separately, thereby facilitating the derivation of independent embeddings. This approach showcases superior efficacy and has rapidly gained widespread acceptance, now serving as a cornerstone for various downstream tasks. Consequently, further improvements to Sentence-BERT hold significant research interest and practical value.

Nevertheless, the advent of contrastive learning methods, exemplified by SimCSE \cite{gao-etal-2021-simcse}, has led to more pronounced enhancements on renowned English STS benchmarks, such as STS12-16 \cite{agirre-etal-2012-semeval,agirre-etal-2013-sts,agirre-etal-2014-semeval,agirre-etal-2015-semeval,agirre-etal-2016-semeval}, STS-B \cite{cer-etal-2017-semeval}, and SICK-R \cite{marelli-etal-2014-sick}. This has shifted the research focus in recent years towards integrating contrastive learning techniques with pre-trained language models (PLMs) like BERT \cite{devlin-etal-2019-bert} and RoBERTa \cite{liu-etal-2019-roberta}. An intuitive comparison is that, when employing the NLI dataset \cite{bowman-etal-2015-large,williams-etal-2018-broad} as a training corpus, SimCSE-RoBERTa$_{\text{base}}$ attains an average Spearman's correlation score of 82.52 across these STS tasks, hugely surpassing the 74.21 achieved by Sentence-RoBERTa$_{\text{base}}$.

Such discernible performance disparity has inadvertently overshadowed the advantages of Sentence-BERT, especially in terms of data utilization efficiency and computational resource demands. Contrastive learning, by its self-supervised nature, predominantly recognizes text pairs as either similar or dissimilar. This binary categorization restricts contrastive learning methods to training on triplet-form data composed of an anchor sentence, a positive instance, and a hard negative instance in supervised settings \cite{gao-etal-2021-simcse}. Many practical scenarios, however, tend to provide more finely grained labeled data (e.g., highly relevant, moderately relevant, relevant, and irrelevant) \cite{liu-etal-2023-rankcse}, where contrastive learning approaches can usually only exploit text pairs whose similarity indicators are at the endpoints.

Furthermore, since contrastive learning enhances model discriminability by treating other samples within the same batch as negative instances, it requires large batch sizes, thereby consuming substantial computational resources. For example, SimCSE's supervised learning settings include a batch size of 512 and 3 epochs. To accommodate this configuration on consumer-grade GPUs, SimCSE limits the maximum input length to 32 tokens \cite{gao-etal-2021-simcse}. In contrast, Sentence-BERT and our proposed methodology necessitate a mere batch size of 16 and 1 epoch to reach convergence. Additionally, our default maximum input length is 256, significantly longer than SimCSE's.

The aforementioned drawbacks highlight the difficulty in completely replacing Sentence-BERT with contrastive learning methods. Hence, some cutting-edge works \cite{zhang-etal-2023-cotbert} continue to rely on Sentence-BERT for sentence embedding derivation. Nonetheless, given that STS tasks typically categorize text pairs by degrees of semantic similarity, and Sentence-BERT approaches these tasks from a classification standpoint, neglecting the progressive relationships between categories, there exists a clear opportunity for improvement.

As an illustration, consider an STS task with five categories, labeled consecutively from 1 to 5. Traditional classification strategies would yield identical loss for a sample scored at 2, irrespective of its prediction as 3 or 4, an approach evidently suboptimal.

To rectify such deficiency, this paper proposes a novel framework that converts multi-class STS tasks into regression problems, thus effectively capturing the progressive relationships between categories. For a given dataset, we first map its original labels to evenly spaced numerical values, ensuring that samples with higher similarity scores are assigned correspondingly greater values. Then, we set the number of nodes in the output layer to one, thereby enabling the model to produce a continuous prediction. Finally, the model parameters are updated according to the difference between predicted and actual scores.

Distinct from standard regression tasks, the ground truth within our transformed multi-category STS tasks manifest as a series of discrete points along the numerical axis. Therefore, instead of requiring precise matches to the target values, the floating-point predictions just need to be sufficiently close to get correctly classified. To accommodate this characteristic, we introduce a zero-gradient buffer zone to widely utilized L1 Loss and MSE Loss, unveiling two innovative loss functions: Translated ReLU and Smooth K2 Loss.

Comprehensive evaluations across seven STS benchmarks substantiate that our regression framework surpasses traditional classification strategies in handling multi-category STS tasks. Additionally, we find that our approach can further refine the performance of contrastive learning pre-trained models by utilizing filtered STS-B and SICK-R training sets. These findings highlight the effectiveness of our method and underscore the importance of harnessing task-specific data, an aspect often neglected in contrastive learning paradigms.

The main contributions of this study are outlined as follows:
\begin{itemize}
    \item Building upon the foundation of Sentence-BERT, we develop a regression framework adept at modeling the progressive relationships between categories in multi-class STS tasks. This not only enhances performance but also, due to regression's intrinsic properties, simplifies the prediction process for $K$-category problems to require only a single output node, significantly minimizing the model's output layer parameter count.
    \item We propose two novel loss functions, Translated ReLU and Smooth K2 Loss, specifically tailored to address classification problems involving progressive relationships between categories.
    \item Through empirical evidence, we demonstrate that our strategy can be combined with leading contrastive learning pre-trained models, leveraging fine-grained annotated data to further improve their performance. This offers a new perspective for current research in STS and sentence embeddings.
\end{itemize}

\section{Related Work}
\label{sec:related_work}

In this section, we primarily review three types of STS solutions that are directly relevant to our work:

\textbf{Siamese Neural Network Architectures:} These approaches \cite{reimers-gurevych-2019-sentence,conneau-etal-2017-supervised,thakur-etal-2021-augmented}, proposed relatively earlier in the field, have been widely applied across various domains owing to their effectiveness on annotated corpus. Although their performance on the seven STS benchmarks (STS 12-16, STS-B, SICK-R) is generally inferior to contemporary contrastive learning methods, this disparity largely stems from the absence of task-specific training data. Thus, models have the flexibility to opt for alternative sources, such as Wikipedia \cite{gao-etal-2021-simcse} or NLI datasets \cite{bowman-etal-2015-large,williams-etal-2018-broad}, which adapt readily to triplet format. Given our goal of tackling multi-category STS tasks, our model architecture remains rooted in the Siamese network. However, in contrast to preceding efforts, we introduce an innovative regression framework specifically designed to capture the progressive relationships between categories.

\textbf{Contrastive Learning Fine-Tuning Methods:} Contrastive learning is currently the mainstream paradigm for addressing STS tasks, with substantial research exploring its integration with the fine-tuning of PLMs \cite{jiang-etal-2022a-promptbert,zhang-etal-2024-cotbert}. However, contrastive learning loss functions, epitomized by InfoNCE Loss \cite{oord-etal-2018-representation}, concentrate exclusively on binary semantic categorization and are unable to fully utilize fine-grained labeled texts. Additionally, the necessity for large batch sizes to ensure negative sample diversity and prevent model collapse imposes significant computational demands. These two limitations are inherently difficult to overcome within contrastive learning itself, yet they are precisely the strengths of Sentence-BERT-style dual-tower models. Therefore, a primary objective of this paper is to investigate whether the performance of contrastive learning models can be further enhanced by incorporating traditional Siamese neural network architectures.

\textbf{Contrastive Learning Pre-Trained Models:} With the growing importance of embeddings in retrieval-augmented generation \cite{zhao-etal-2024-rag-survey} and other application scenarios, more companies and institutions are dedicating efforts to developing specialized text representation models. These approaches generally adopt multi-stage contrastive learning strategies for network pre-training \cite{wang-etal-2022-text-embeddings,li-etal-2023-general, xiao-etal-2024-cpack}. Additionally, compared to large-scale generative PLMs, lightweight discriminative models that capture bidirectional dependencies are often more preferred. In our experiments section, we employ two state-of-the-art contrastive learning pre-trained models, Jina Embeddings v2 \cite{gunther-etal-2023-jina} and Nomic Embed \cite{nussbaum-etal-2024-nomic}. Both are BERT-based encoder architectures with a parameter size of 137 million.

\section{Methodology}
\label{sec:methodology}

This section presents our methodological framework, beginning with a detailed exposition of the network architecture and its operational workflow in subsection \ref{subsec:architecture}. Then, in subsections \ref{subsec:translated_relu} and \ref{subsec:smooth_k2}, we introduce the two novel loss functions proposed in this study.

\subsection{Network Architecture}
\label{subsec:architecture}

As illustrated in Figure \ref{fig:framework}, we utilize a Siamese neural network with shared parameters for encoding input sentences via BERT to obtain corresponding word embedding matrices. Subsequently, sentence embeddings, denoted as $\mathbf{u}$ and $\mathbf{v}$ for paired sentences A and B, are derived through average pooling. These embeddings, both vectors of the hidden dimension, are then concatenated alongside their element-wise difference $|\mathbf{u}-\mathbf{v}|$ and passed through a fully connected layer with parameters sized at $3 \times \text{hidden\_dimension} \times 1$ to produce the model's predicted similarity score.

\begin{figure}[H]
\centering
\fbox{\parbox{0.95\textwidth}{%
\centering
IMAGE NOT PROVIDED\\
\textit{Caption: Our Regression Framework. Here, the two BERT models share same parameters, with ``dim'' representing the embedding dimensions of $\mathbf{u}$ and $\mathbf{v}$.}
}}
\caption{Our Regression Framework}
\label{fig:framework}
\end{figure}

Our method diverges from the original dual-tower structures employed by Sentence-BERT and InferSent \cite{conneau-etal-2017-supervised} in three critical aspects:

\begin{enumerate}
    \item We model STS tasks, characterized by a progressive relationship between categories, as regression problems. This is achieved by mapping labels from the original dataset to a sequence of incrementing numbers reflective of their similarity relations, thus conveying to the model that categories are not independent but progressively related.
    
    \item Building on this, we streamline the output node count in the final fully connected layer to one, thereby enabling the model to directly yield a similarity score rather than a categorical probability distribution. Through this adjustment, for STS tasks with $K$ categories, we effectively reduce the parameter size of the output layer from $3 \times \text{hidden\_dimension} \times K$ to $3 \times \text{hidden\_dimension} \times 1$. In light of the expanding hidden layer dimensions in modern PLMs, this optimization can save considerable computational resources.
    
    \item Unlike the classification-based approach of InferSent and Sentence-BERT, which assigns target classes for sentence pairs according to the highest probability, our regression framework categorizes based on the closeness between the predicted and actual values.
\end{enumerate}

To better understand this process, consider an STS task with four categories: ``highly relevant,'' ``moderately relevant,'' ``slightly relevant,'' and ``irrelevant.'' After clarifying the progressive relationship between these categories, we would map them to four consecutive numbers 0, 1, 2, 3, respectively, ranging from ``irrelevant'' to ``highly relevant.'' This mapping strategy is flexible, allowing for task-specific adjustments in both numerical nodes and intervals. Furthermore, the mapped nodes do not necessarily have to be integers. Subsequently, we encode the paired sentences and compute their semantic similarity, resulting in a floating-point prediction. By rounding this value, it can be converted into the corresponding label. For instance, a prediction of 2.875 for a sample pair would be classified as ``highly relevant,'' as it is closest to the boundary point of 3. Similarly, if a sample receives a predicted value of 1.333, it would be approximated to 1 and thus classified as ``slightly relevant,'' because 1.333 is closer to 1 among the four boundary points 0, 1, 2, 3.

Extending from the above examples, it can be seen that if the original labels are mapped to nodes spaced by $d$, as long as the difference between the model's prediction and the ground truth is less than $\frac{d}{2}$, the sample will be correctly classified. Specifically, for consecutive natural numbers, $d$ is equal to 1. However, conventional regression loss functions, represented by L1 Loss and MSE Loss, always enforce the model to exactly match the true value, a requirement that is unnecessary for our task scenario. Thus, we introduce a zero-gradient buffer zone into both functions, unveiling two new loss functions: Translated ReLU and Smooth K2 Loss.

\subsection{Translated ReLU}
\label{subsec:translated_relu}

We first present Translated ReLU, mathematically formulated in Equation \ref{eq:translated_relu}. Herein, $d$ represents the interval between mapped category labels. As previously discussed, when the difference between the model's predicted value and the ground truth is less than $\frac{d}{2}$, it signifies a correct classification of the sample. Traditional regression loss functions, however, mandate absolute congruence between predictions and true values, applying a penalty for any deviation. This stringent requirement to some extent diverts the model's focus from difficult samples that have not yet been correctly classified and ignores the inherent variability within classes.

\begin{equation}
\begin{aligned}
x &\rightarrow |\text{prediction} - \text{label}| \geq 0 \\
f(x) &= 
\begin{cases}
0 & x < x_0 \leq \frac{d}{2} \\
k(x - x_0) & x_0 \leq x
\end{cases} \\
f(x) &= \max(0, k(x - x_0))
\end{aligned}
\label{eq:translated_relu}
\end{equation}

To circumvent this limitation, we introduce an adjustable threshold hyperparameter $x_0$, and set the loss function to zero for values within $[0, x_0]$. This modification posits that a divergence less than $x_0$ between prediction and ground truth is deemed sufficiently precise, thus exempt from penalty or gradient update. For disparities exceeding $x_0$, Translated ReLU imposes a linear penalty. To maintain accurate classification, $x_0$ must not exceed $\frac{d}{2}$, with the interval between $x_0$ and $\frac{d}{2}$ acting as a margin akin to that in Hinge Loss. This margin can enhance model robustness by penalizing correctly predicted samples that lack adequate confidence. Additionally, a parameter $k$ is specified to control the slope of the function.

The graphical depiction of Translated ReLU is exhibited on the left side of Figure \ref{fig:loss_functions}, with parameters set to $k=2$ and $x_0=0.25$. This configuration resembles the ReLU activation function, albeit with a rightward translation. Our study employs Translated ReLU as a loss function and will compare its effects with those of L1 Loss in ensuing sections to demonstrate the significance of zero-gradient buffer zone for augmenting model performance.

\subsection{Smooth K2 Loss}
\label{subsec:smooth_k2}

Translated ReLU is characterized by its simplicity and efficacy. Nonetheless, we acknowledge its limitation pertaining to the abrupt lack of smoothness at the demarcation point $x = x_0$, alongside a constant gradient that fails to accommodate varying strengths of updates based on the distance between predictions and actual values. To address these concerns, we introduce another loss function termed Smooth K2 Loss to provide a smoother transition and a gradient that dynamically adjusts in accordance with the magnitude of discrepancy from the ground truth. The formulation and the derivative of Smooth K2 Loss are specified as follows:

\begin{equation}
\begin{aligned}
x &\rightarrow |\text{prediction} - \text{label}| \geq 0 \\
f(x) &= 
\begin{cases}
0 & x < x_0 \leq \frac{d}{2} \\
k(x^2 - 2x_0x + x_0^2) & x_0 \leq x
\end{cases} \\
\frac{\partial f(x)}{\partial x} &= 
\begin{cases}
0 & x < x_0 \leq \frac{d}{2} \\
2k(x - x_0) & x_0 \leq x
\end{cases}
\end{aligned}
\label{eq:smooth_k2}
\end{equation}

Echoing the design of Translated ReLU, Smooth K2 Loss also incorporates a zero-gradient buffer zone, but exhibits a quadratic function for $x \geq x_0$, as illustrated on the right side of Figure \ref{fig:loss_functions}. Given the differential mathematical underpinnings of these two loss functions, Smooth K2 Loss is recommended for scenarios with high-quality data and strong credibility. In contrast, when dealing with datasets that contain considerable noise, Translated ReLU may be a more suitable choice.

Additionally, prior to the application of Translated ReLU and Smooth K2 Loss, it is advisable to consider reassigning prediction values that transcend the defined category range to the nearest boundary. For instance, in a classification task where the category labels can be sequentially converted to 0, 1, 2 and 3, if the model predicts a value of 3.57 for a sample with an actual label of 3, this might be deemed acceptable and potentially obviate the need for a loss adjustment. This rationale stems from the observation that, despite the prediction's deviation exceeding $\frac{d}{2}=0.5$, the absence of subsequent boundary points beyond 3 warrants a relaxation of this criterion.

\begin{figure}[H]
\centering
\fbox{\parbox{0.95\textwidth}{%
\centering
IMAGE NOT PROVIDED\\
\textit{Caption: Comparison of Translated ReLU and Smooth K2 Loss, both with $k=2$, $x_0=0.25$.}
}}
\caption{Comparison of loss functions}
\label{fig:loss_functions}
\end{figure}

\section{Experiment}
\label{sec:experiment}

This section provides empirical validation of our regression framework and two innovative loss functions. We commence by comparing the performance of different modeling strategies for multi-category STS tasks and various loss functions (subsection \ref{subsec:traditional_plms}). Next, we demonstrate that, when supplemented with fine-grained training data, our Siamese neural network can effectively enhance the performance of contrastive learning PLMs (subsection \ref{subsec:contrastive_plms}). Following this, we highlight the computational efficiency of our methodology (subsection \ref{subsec:efficiency}) and explore the influence of varying hyperparameter settings on model performance (subsection \ref{subsec:hyperparams}). Finally, subsection \ref{subsec:ablation} presents ablation studies on our network architecture.

\subsection{STS Performance Based on Traditional Discriminative Pre-Trained Models}
\label{subsec:traditional_plms}

Our experimental setup here closely mirrors that of Sentence-BERT, leveraging fine-tuning on BERT or RoBERTa with a composite corpus derived from the SNLI and MNLI datasets. These NLI datasets categorize sentence pairs into three distinct classes: contradiction, neutral, and entailment. Sentence-BERT maps these classes to 0, 2, and 1, respectively, and employs a classification strategy for training \cite{reimers-gurevych-2019-sentence}. In contrast, our method sequentially maps contradiction, neutral, and entailment to 0, 1 and 2. This mapping reflects the natural order of semantic similarity, from least to most similar, thereby enabling our regression framework to better capture the progressive relationships between categories.

For computational efficiency, we uniformly set the batch size to 16 and limit training to a single epoch, with model checkpoints saved based on performance metrics on the STS-B development set. The specific hyperparameter settings for Translated ReLU and Smooth K2 Loss are detailed in Table \ref{tab:hyperparams_nli}. During evaluation, we assess the model's average Spearman correlation across seven STS tasks via the SentEval toolkit \cite{conneau-kiela-2018-senteval}.

\begin{table}[H]
\centering
\caption{Hyperparameter configurations for our two loss functions when fine-tuning BERT and RoBERTa on the NLI dataset.}
\label{tab:hyperparams_nli}
\begin{tabular}{lcc}
\toprule
PLM & Loss & $k$ & $x_0$ \\
\midrule
BERT$_{\text{base}}$ & Translated ReLU & 2.5 & 0.25 \\
BERT$_{\text{base}}$ & Smooth K2 Loss & 2 & 0.25 \\
RoBERTa$_{\text{base}}$ & Translated ReLU & 1 & 0.25 \\
RoBERTa$_{\text{base}}$ & Smooth K2 Loss & 3 & 0.25 \\
\bottomrule
\end{tabular}
\end{table}

The results of these experiments are summarized in Table \ref{tab:results_nli}, from which we distill insights along three pivotal aspects:

\begin{table}[H]
\centering
\caption{Spearman's correlation scores for different methods across seven STS tasks. This table is partitioned to facilitate a single variable comparison. $\clubsuit$: results from \cite{reimers-gurevych-2019-sentence}.}
\label{tab:results_nli}
\begin{tabular}{lccccccccl}
\toprule
Models & STS-12 & STS-13 & STS-14 & STS-15 & STS-16 & STS-B & SICK-R & Avg. \\
\midrule
\multicolumn{9}{c}{Implementation on BERT$_{\text{base}}$} \\
Sentence-BERT$_{\text{base}}$ $\clubsuit$ & 70.97 & 76.53 & 73.19 & 79.09 & 74.30 & 77.03 & 72.91 & 74.89 \\
BERT$_{\text{base}}$ + Cross Entropy & 70.01 & 71.18 & 70.10 & 78.37 & 72.92 & 74.88 & 73.58 & 73.01 \\
BERT$_{\text{base}}$ + L1 Loss & 69.76 & 69.56 & 68.13 & 76.33 & 70.96 & 73.61 & 70.28 & 71.23 \\
BERT$_{\text{base}}$ + Translated ReLU & 72.51 & 75.46 & 72.34 & 78.46 & 72.64 & 76.54 & 72.02 & 74.28 \\
BERT$_{\text{base}}$ + MSE Loss & 72.38 & 76.47 & 74.35 & 78.71 & 72.95 & 77.91 & 70.67 & 74.78 \\
BERT$_{\text{base}}$ + Smooth K2 Loss & 72.39 & 78.33 & 75.28 & 80.26 & 74.52 & 78.78 & 72.65 & 76.03 \\
\midrule
\multicolumn{9}{c}{Implementation on RoBERTa$_{\text{base}}$} \\
Sentence-RoBERTa$_{\text{base}}$ $\clubsuit$ & 71.54 & 72.49 & 70.80 & 78.74 & 73.69 & 77.77 & 74.46 & 74.21 \\
RoBERTa$_{\text{base}}$ + Cross Entropy & 71.15 & 74.29 & 72.66 & 79.44 & 74.12 & 76.56 & 73.02 & 74.46 \\
RoBERTa$_{\text{base}}$ + L1 Loss & 68.12 & 62.27 & 64.20 & 72.80 & 67.28 & 72.44 & 66.82 & 67.70 \\
RoBERTa$_{\text{base}}$ + Translated ReLU & 71.13 & 76.07 & 72.18 & 78.13 & 73.94 & 77.59 & 70.94 & 74.28 \\
RoBERTa$_{\text{base}}$ + MSE Loss & 72.67 & 77.09 & 72.93 & 79.52 & 74.12 & 77.88 & 69.85 & 74.87 \\
RoBERTa$_{\text{base}}$ + Smooth K2 Loss & 72.53 & 78.28 & 73.88 & 80.88 & 75.35 & 77.44 & 73.94 & 76.04 \\
\bottomrule
\end{tabular}
\end{table}

\begin{enumerate}
    \item \textbf{Classification Strategy vs. Regression Strategy:} Our regression framework, particularly when utilizing Smooth K2 Loss, yields an average Spearman correlation of 76.03 for BERT$_{\text{base}}$ and 76.04 for RoBERTa$_{\text{base}}$. These figures significantly outstrip those attained through Sentence-BERT and the classification strategy with Cross-Entropy Loss, highlighting the regression-based modeling's superiority in both reducing the output layer's parameter size and enhancing semantic discrimination in multi-category STS tasks.
    
    \item \textbf{Efficacy of the Zero-Gradient Buffer Zone:} The adoption of Translated ReLU improves performance for both BERT and RoBERTa beyond what is achieved with L1 Loss. Similarly, employing Smooth K2 Loss surpasses MSE Loss on both PLMs. These comparisons underline the benefit of incorporating a zero-gradient buffer zone, which helps balance the model's attention across diverse samples in regression-modeled multi-category classification tasks.
    
    \item \textbf{Adaptive Gradients Aligned with Prediction Errors:} Models trained with Smooth K2 Loss outperform those utilizing Translated ReLU, and models employing MSE Loss exceed those with L1 Loss. This evidences the advantages of dispensing differentiated gradients in line with prediction-ground truth deviations, especially when leveraging high-quality datasets like NLI.
\end{enumerate}

Collectively, these findings substantiate the merit of (1) adopting a regression framework for multi-class STS tasks and (2) enhancing traditional regression loss functions with a zero-gradient buffer zone to optimize model performance.

\subsection{STS Performance Based on Contrastive Learning Pre-Trained Models}
\label{subsec:contrastive_plms}

While the Siamese neural network, augmented by our regression framework and innovative loss functions, has exhibited significant performance improvements, a gap remains when compared to leading contrastive learning methods. To address this, we exploit the strengths of Siamese architectures in fully utilizing annotated data and explore whether it can be combined with top-performing contrastive learning models.

Jina Embeddings v2 \cite{gunther-etal-2023-jina} and Nomic Embed \cite{nussbaum-etal-2024-nomic} are two recently released embedding models that employ multi-stage contrastive learning strategies during pre-training, combining supervised and unsupervised approaches to optimize the networks. Both have achieved state-of-the-art results on the MTEB leaderboard \cite{muennighoff-etal-2023-mteb}. Therefore, if our method can further enhance the performance of these models, it would provide valuable insights for future research.

Among the seven STS benchmarks (STS12-16, STS-B, and SICK-R), STS-B and SICK-R come with their own training datasets. Specifically, STS-B contains sentence pairs with similarity scores ranging from 0 to 5, while SICK-R includes pairs with scores from 1 to 5. To ensure accurate evaluation, we performed strict data filtering to remove any training text pairs that appeared in the test sets. Details of this filtering process are provided in Appendix \ref{app:data_filtering}. We then applied a linear transformation, $\frac{5 \times \text{label}(z) - 1}{4}$, to convert all SICK-R training labels to the range $[0, 5]$ and merged them with the filtered STS-B training set. This procedure resulted in a fine-grained, task-specific corpus containing 5,398 sentence pairs.

Since Jina Embeddings v2 and Nomic Embed have undergone pre-training on massive texts, their model parameters have favorable initial distributions. In contrast, our newly introduced linear layer is randomly initialized (Figure \ref{fig:framework}). To facilitate effective joint training, we first freeze the entire PLM and only update the linear layer using the NLI dataset described in section \ref{subsec:traditional_plms}. After completing this step, we optimize both the PLM and the linear layer with the filtered STS training data. A schematic diagram of this workflow is shown in Figure \ref{fig:fine_tuning}. Throughout the entire procedure, Smooth K2 Loss is employed as the loss function.

\begin{figure}[H]
\centering
\fbox{\parbox{0.95\textwidth}{%
\centering
IMAGE NOT PROVIDED\\
\textit{Caption: Our two-stage fine-tuning process for contrastive learning pre-trained models. In the figure, modules highlighted in red are active during training and undergo backpropagation, while modules in blue are frozen and do not carry out updates.}
}}
\caption{Two-stage fine-tuning process}
\label{fig:fine_tuning}
\end{figure}

The performance of Nomic Embed and Jina Embeddings v2 on the seven STS tasks before and after fine-tuning is presented in Table \ref{tab:contrastive_results}. The results demonstrate that our network framework effectively enhances the performance of both models and surpasses BERT-based methods with comparable parameter sizes. Notably, we also test the impact of further updating the PLM using contrastive learning, which requires additional processing of the 5,398 training samples obtained earlier. To illustrate this, we take InfoNCE Loss \cite{oord-etal-2018-representation}, the most widely adopted contrastive learning loss function, as an example.

For any input sentence $x_i$, InfoNCE Loss computes the similarity between its encoding $f(x_i)$ and that of its positive instance $f(x_i^+)$ in the numerator, while aggregating the similarity calculations between $f(x_i)$ and other samples within the same batch in the denominator. This formulation aims to bring similar samples closer and push dissimilar ones apart. Equation \ref{eq:infonce} presents the standard expression of InfoNCE Loss, where $N$ represents the batch size and $\tau$ denotes a temperature hyperparameter.

\begin{equation}
\ell_i = -\log \frac{e^{\cos(f(x_i), f(x_i^+))/\tau}}{\sum_{j=1}^{N} e^{\cos(f(x_i), f(x_j^+))/\tau}}
\label{eq:infonce}
\end{equation}

As indicated by Equation \ref{eq:infonce}, the only component of InfoNCE Loss that can be filled with labeled data is the similarity calculation between positive samples in the numerator. Consequently, contrastive learning is limited to utilizing only text pairs with the highest similarity ratings. To work within this constraint, we selected 1,543 samples from the 5,398 training pairs by adopting a threshold of 4.0 to filter out positive sample pairs. As it can be observed in Table \ref{tab:contrastive_results}, after discarding such a large portion of annotation information, contrastive learning yields little improvement and may even lead to model collapse, causing performance degradation. In contexts where more detailed, domain-specific data is available, the shortcomings of contrastive learning in not being able to effectively harness multi-level label information, only performing coarse semantic distinctions, becomes more pronounced.

\begin{table}[H]
\centering
\caption{Spearman's correlation coefficients of different methods across seven STS tasks. The ``+Contrast'' notation in the first column refers models further fine-tuned with contrastive learning. $\spadesuit$: results from \cite{gao-etal-2021-simcse}. $\heartsuit$: results from \cite{jiang-etal-2022a-promptbert}. $\diamondsuit$: results from \cite{jiang-etal-2022b-promcse}.}
\label{tab:contrastive_results}
\begin{tabular}{lccccccccl}
\toprule
Models & STS-12 & STS-13 & STS-14 & STS-15 & STS-16 & STS-B & SICK-R & Avg. \\
\midrule
CT-SBERT$_{\text{base}}$ $\spadesuit$ & 74.84 & 83.20 & 78.07 & 83.84 & 77.93 & 81.46 & 76.42 & 79.39 \\
SimCSE-BERT$_{\text{base}}$ $\spadesuit$ & 75.30 & 84.67 & 80.19 & 85.40 & 80.82 & 84.25 & 80.39 & 81.57 \\
PromptBERT$_{\text{base}}$ $\heartsuit$ & 75.48 & 85.59 & 80.57 & 85.99 & 81.08 & 84.56 & 80.52 & 81.97 \\
PromCSE-BERT$_{\text{base}}$ $\diamondsuit$ & 75.58 & 84.33 & 79.67 & 85.79 & 81.24 & 84.25 & 80.79 & 81.81 \\
Nomic Embed Text v1 & 73.75 & 85.03 & 80.52 & 87.40 & 83.55 & 83.90 & 76.52 & 81.52 \\
Nomic Embed Text v1 + Contrast & 76.10 & 85.79 & 80.58 & 87.35 & 83.54 & 85.16 & 72.33 & 81.55 \\
Nomic Embed Text v1 + Ours & 73.06 & 86.63 & 81.06 & 87.67 & 83.43 & 85.18 & 82.75 & 82.83 \\
Jina Embeddings v2 & 74.28 & 84.18 & 78.81 & 87.55 & 85.35 & 84.85 & 78.98 & 82.00 \\
Jina Embeddings v2 + Contrast & 76.04 & 86.37 & 80.16 & 86.53 & 85.24 & 84.31 & 74.18 & 81.83 \\
Jina Embeddings v2 + Ours & 75.17 & 86.10 & 79.96 & 88.44 & 85.01 & 86.83 & 83.34 & 83.55 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Computational Resource Overhead}
\label{subsec:efficiency}

In addition to its inability to fully leverage fine-grained annotated data, the high memory requirements of contrastive learning also pose a challenge for many researchers. In this section, we compare the computational resource consumption of our method with that of SimCSE during training, based on four 24GB NVIDIA GPUs. The results are summarized in Table \ref{tab:resources}, where both BERT and RoBERTa are the base versions.

Despite setting the maximum sequence length for SimCSE to approximately 40\% of our method's default configuration, its memory usage remains significantly higher, reaching an astonishing 81GB. Thus, overall, our Siamese neural network strategy is more suitable for resource-constrained environments.

\begin{table}[H]
\centering
\caption{Computational demands of our method compared to SimCSE during the training phase. The third column, ``Length,'' represents the maximum sequence length supported by each model (cutoff length).}
\label{tab:resources}
\begin{tabular}{lccc}
\toprule
PLMs & Method & Length & Memory \\
\midrule
BERT & SimCSE & 100 & 81.30 GB \\
     & Ours   & 256 & 41.27 GB \\
\midrule
RoBERTa & SimCSE & 100 & 81.61 GB \\
        & Ours   & 256 & 42.33 GB \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Impact of Different Hyperparameter Settings}
\label{subsec:hyperparams}

In this study, we introduce two novel loss functions, Translated ReLU and Smooth K2 Loss, each characterized by two critical hyperparameters: $k$ and $x_0$. The parameter $k$ primarily controls the gradient of the loss function, while $x_0$ defines the tolerance threshold for model predictions. To discern the influence of these hyperparameters on model performance, we conducted a series of experiments across both traditional discriminative PLMs (BERT, RoBERTa) and the latest contrastive learning PLMs (Nomic Embed v1, Jina Embeddings v2).

The outcomes of these investigations are consolidated in Table \ref{tab:hyperparams_results}. Rather than executing an exhaustive grid search, initial values were selected based on our preliminary insights, followed by incremental adjustments. This implies that there may still be room for further improvement in our model's performance.

The experimental results from Table \ref{tab:hyperparams_results} reveal minor fluctuations in model performance across diverse hyperparameter configurations, which affirms the resilience and robustness of our proposed methodology. This stability highlights the inherent adaptability of our regression framework as well as loss functions, suggesting their applicability to a wide range of modeling scenarios without necessitating extensive hyperparameter optimization.

\begin{table}[H]
\centering
\caption{Average Spearman's correlation scores across seven STS tasks under different values of $k$ and $x_0$.}
\label{tab:hyperparams_results}
\begin{tabular}{lcccc}
\toprule
PLM & Loss & $k$ & $x_0$ & Performance \\
\midrule
\multicolumn{5}{c}{Implementation on Traditional Discriminative PLMs} \\
BERT$_{\text{base}}$ & Translated ReLU & 1.5 & 0.25 & 74.21 \\
BERT$_{\text{base}}$ & Translated ReLU & 2 & 0.25 & 74.21 \\
BERT$_{\text{base}}$ & Translated ReLU & 2.5 & 0.25 & 74.28 \\
BERT$_{\text{base}}$ & Smooth K2 Loss & 3 & 0.25 & 75.75 \\
BERT$_{\text{base}}$ & Smooth K2 Loss & 2.5 & 0.25 & 75.89 \\
BERT$_{\text{base}}$ & Smooth K2 Loss & 2 & 0.25 & 76.03 \\
RoBERTa$_{\text{base}}$ & Translated ReLU & 2 & 0.25 & 74.00 \\
RoBERTa$_{\text{base}}$ & Translated ReLU & 1.5 & 0.25 & 74.11 \\
RoBERTa$_{\text{base}}$ & Translated ReLU & 1 & 0.25 & 74.28 \\
RoBERTa$_{\text{base}}$ & Smooth K2 Loss & 2.5 & 0.25 & 75.89 \\
RoBERTa$_{\text{base}}$ & Smooth K2 Loss & 3 & 0.2 & 75.90 \\
RoBERTa$_{\text{base}}$ & Smooth K2 Loss & 3 & 0.25 & 76.04 \\
\midrule
\multicolumn{5}{c}{Implementation on Contrastive Learning PLMs} \\
Nomic v1 & Smooth K2 Loss & 3.5 & 0.2 & 82.76 \\
Nomic v1 & Smooth K2 Loss & 2.5 & 0.2 & 82.79 \\
Nomic v1 & Smooth K2 Loss & 2 & 0.2 & 82.82 \\
Nomic v1 & Smooth K2 Loss & 3 & 0.2 & 82.83 \\
Jina v2 & Smooth K2 Loss & 3 & 0.15 & 83.51 \\
Jina v2 & Smooth K2 Loss & 3 & 0.2 & 83.54 \\
Jina v2 & Smooth K2 Loss & 3.5 & 0.2 & 83.55 \\
Jina v2 & Smooth K2 Loss & 4 & 0.2 & 83.55 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Ablation Studies}
\label{subsec:ablation}

In section \ref{subsec:traditional_plms}, we initially demonstrated the effectiveness of our regression framework by comparing the performance of models utilizing both classification-based and regression-based strategies for multi-category STS tasks. Then, we elucidated the significance of zero-gradient buffer zones by evaluating the performance of models when selecting either Translated ReLU or L1 Loss, and Smooth K2 Loss or MSE Loss as the loss function. These comparisons directly align with the three core innovations of this paper and fulfill the role of ablation experiments.

Here, we extend our ablation study by evaluating our network architecture, as depicted in Figure \ref{fig:framework}. Specifically, we seek to determine the necessity of concatenating $\mathbf{u}$, $\mathbf{v}$, and their element-wise difference $|\mathbf{u} - \mathbf{v}|$ in the final linear layer of the model. To this end, we employ both BERT and RoBERTa under the same experimental conditions outlined in section \ref{subsec:traditional_plms}, with the results presented in Table \ref{tab:ablation}.

The findings indicate that the concatenation method $(\mathbf{u}, \mathbf{v}, |\mathbf{u} - \mathbf{v}|)$ is the most effective for both PLMs, thus further validating the rationale behind our proposed scheme.

\begin{table}[H]
\centering
\caption{Average Spearman's correlation scores obtained by models on seven STS tasks with different concatenation methods in the final linear layer of our Siamese neural network architecture.}
\label{tab:ablation}
\begin{tabular}{lcc}
\toprule
PLM & Concatenation & Spearman \\
\midrule
BERT$_{\text{base}}$ & $(\mathbf{u}, \mathbf{v})$ & 53.30 \\
BERT$_{\text{base}}$ & $(|\mathbf{u} - \mathbf{v}|)$ & 54.84 \\
BERT$_{\text{base}}$ & $(\mathbf{u}, \mathbf{v}, |\mathbf{u} - \mathbf{v}|)$ & 76.03 \\
RoBERTa$_{\text{base}}$ & $(\mathbf{u}, \mathbf{v})$ & 60.99 \\
RoBERTa$_{\text{base}}$ & $(|\mathbf{u} - \mathbf{v}|)$ & 59.10 \\
RoBERTa$_{\text{base}}$ & $(\mathbf{u}, \mathbf{v}, |\mathbf{u} - \mathbf{v}|)$ & 76.04 \\
\bottomrule
\end{tabular}
\end{table}

\section{Conclusion}
\label{sec:conclusion}

In this paper, we propose an innovative regression framework and develop two simple yet efficacious loss functions: Translated ReLU and Smooth K2 Loss, to address multi-class STS tasks. Compared to traditional classification approaches, our regression modeling strategy effectively captures the progressive relationships between categories, thereby achieving superior performance while reducing the parameter count in the model's output layer.

Further empirical evidence demonstrates that our method can also be combined with leading contrastive learning models, leveraging fine-grained annotated data to further enhance their performance. Moreover, this approach proves to be more advantageous than continued fine-tuning through contrastive learning, both in terms of performance gains and computational efficiency.

To support further research, we have made our code and model checkpoints publicly available.

\section{Limitations}
\label{sec:limitations}

Due to the lack of baselines and computational resource constraints, the experiments in this paper primarily focus on encoder-only discriminative models, rather than recently advanced generative pre-trained models (e.g., LLaMA \cite{touvron-etal-2023-llama}, Mistral \cite{jiang-etal-2023-mistral}). However, it is important to emphasize that, compared to mainstream generative PLMs, the models we selected---BERT, RoBERTa, Jina Embeddings v2, and Nomic Embed v1---have significantly fewer parameters. This results in higher inference efficiency, which is particularly advantageous in large-scale information retrieval and text clustering scenarios.

\section*{Acknowledgments}
We thank the anonymous reviewers for their insightful comments and suggestions. This work was supported by the National Key R\&D Program of China (No. 2022YFB3103100).

\bibliographystyle{acl_natbib}
\bibliography{refs}

\appendix

\section{Data Filtering Method}
\label{app:data_filtering}

As mentioned in section \ref{subsec:contrastive_plms}, before applying the STS-B and SICK-R training sets for model updates, we implemented strict data filtering to ensure that no sentence pairs present in the test sets would appear in the fine-tuning corpus.

To elaborate on this process, we first take the SICK-R dataset as an example to illustrate the standard format of STS datasets. As shown in Table \ref{tab:sick_samples}, each sample consists of two text strings, ``sentence 1'' and ``sentence 2,'' along with a floating-point number ``score'' that indicates the semantic similarity between them. We denote these as ``s1,'' ``s2,'' and ``r,'' respectively.

Then, for any sentence pair $(s1_i, s2_i, r_i)$ within the STS-B or SICK-R training set, if a sample $(s1_j, s2_j, r_j)$ exists in the test sets of STS12-16, STS-B, or SICK-R such that $s1_i = s1_j$ and $s2_i = s2_j$, or $s1_i = s2_j$ and $s2_i = s1_j$, we treat them as duplicates and remove the corresponding sentence pair from the training data. It should be noted that the entire process is conducted without any modifications to the test sets.

This filtering mechanism is stringent, as we do not take into account whether $r_i$ and $r_j$ are equal. In other words, as long as a sentence pair appears in both the training and test sets, it is removed from the training corpus, regardless of whether the similarity scores are identical. Under this protocol, even within the SICK-R dataset itself, there are instances where samples from the training and test sets overlap. Examples in Table \ref{tab:sick_samples} illustrate such cases. The goal of this approach is to maximize the model's generalization ability.

\begin{table}[H]
\centering
\caption{Samples from the SICK-R training and test sets. In these samples, text pair duplication occurs. Thus, the corresponding training samples are removed from the fine-tuning corpus used in section \ref{subsec:contrastive_plms}.}
\label{tab:sick_samples}
\begin{tabular}{p{0.45\textwidth}p{0.45\textwidth}c}
\toprule
sentence 1 & sentence 2 & score \\
\midrule
\multicolumn{3}{c}{Sentence pairs in the SICK-R training set} \\
A man in a blue jumpsuit is courageously performing a wheelie on a motorcycle & The man is doing a wheelie with a motorcycle on ground which is mostly barren & 4.1 \\
The tan dog is watching a brown dog that is swimming in a pond & A pet dog is standing on the bank and is looking at another brown dog in the pond & 4.3 \\
\midrule
\multicolumn{3}{c}{Sentence pairs in the SICK-R test set} \\
The man is doing a wheelie with a motorcycle on ground which is mostly barren & A man in a blue jumpsuit is courageously performing a wheelie on a motorcycle & 3.7 \\
A pet dog is standing on the bank and is looking at another brown dog in the pond & The tan dog is watching a brown dog that is swimming in a pond & 3.6 \\
\bottomrule
\end{tabular}
\end{table}

\end{document}
=====END FILE=====

=====FILE: refs.bib=====
@inproceedings{agirre-etal-2012-semeval,
    title = "{S}em{E}val-2012 Task 6: A Pilot on Semantic Textual Similarity",
    author = "Agirre, Eneko  and
      Cer, Daniel  and
      Diab, Mona  and
      Gonzalez-Agirre, Aitor",
    booktitle = "{*}{SEM} 2012: The First Joint Conference on Lexical and Computational Semantics -- Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation ({S}em{E}val 2012)",
    month = jun,
    year = "2012",
    address = "Montreal, Canada",
    publisher = "Association for Computational Linguistics",
    pages = "385--393",
}

@inproceedings{agirre-etal-2013-sts,
    title = "{*}{SEM} 2013 Shared Task: Semantic Textual Similarity",
    author = "Agirre, Eneko  and
      Cer, Daniel  and
      Diab, Mona  and
      Gonzalez-Agirre, Aitor  and
      Guo, Weiwei",
    booktitle = "Second Joint Conference on Lexical and Computational Semantics ({*}{SEM}), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity",
    month = jun,
    year = "2013",
    address = "Atlanta, Georgia",
    publisher = "Association for Computational Linguistics",
    pages = "32--43",
}

@inproceedings{agirre-etal-2014-semeval,
    title = "{S}em{E}val-2014 Task 10: Multilingual Semantic Textual Similarity",
    author = "Agirre, Eneko  and
      Banea, Carmen  and
      Cardie, Claire  and
      Cer, Daniel  and
      Diab, Mona  and
      Gonzalez-Agirre, Aitor  and
      Guo, Weiwei  and
      Lopez-Gazpio, I{\~n}igo  and
      Maritxalar, Montse  and
      Mihalcea, Rada  and
      Rigau, German  and
      Uria, Larraitz  and
      Wiebe, Janyce",
    booktitle = "Proceedings of the 8th International Workshop on Semantic Evaluation ({S}em{E}val 2014)",
    month = aug,
    year = "2014",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics and Dublin City University",
    pages = "81--91",
}

@inproceedings{agirre-etal-2015-semeval,
    title = "{S}em{E}val-2015 Task 2: Semantic Textual Similarity, English, Spanish and Pilot on Interpretability",
    author = "Agirre, Eneko  and
      Banea, Carmen  and
      Cardie, Claire  and
      Cer, Daniel  and
      Diab, Mona  and
      Gonzalez-Agirre, Aitor  and
      Guo, Weiwei  and
      Lopez-Gazpio, I{\~n}igo  and
      Maritxalar, Montse  and
      Mihalcea, Rada  and
      Rigau, German  and
      Uria, Larraitz  and
      Wiebe, Janyce",
    booktitle = "Proceedings of the 9th International Workshop on Semantic Evaluation ({S}em{E}val 2015)",
    month = jun,
    year = "2015",
    address = "Denver, Colorado",
    publisher = "Association for Computational Linguistics",
    pages = "252--263",
}

@inproceedings{agirre-etal-2016-semeval,
    title = "{S}em{E}val-2016 Task 1: Semantic Textual Similarity, Monolingual and Cross-lingual Evaluation",
    author = "Agirre, Eneko  and
      Banea, Carmen  and
      Cer, Daniel  and
      Diab, Mona  and
      Gonzalez-Agirre, Aitor  and
      Mihalcea, Rada  and
      Rigau, German  and
      Wiebe, Janyce",
    booktitle = "Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016)",
    month = jun,
    year = "2016",
    address = "San Diego, California",
    publisher = "Association for Computational Linguistics",
    pages = "497--511",
}

@inproceedings{bowman-etal-2015-large,
    title = "A Large Annotated Corpus for Learning Natural Language Inference",
    author = "Bowman, Samuel R.  and
      Angeli, Gabor  and
      Potts, Christopher  and
      Manning, Christopher D.",
    booktitle = "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2015",
    address = "Lisbon, Portugal",
    publisher = "Association for Computational Linguistics",
    pages = "632--642",
}

@inproceedings{cer-etal-2017-semeval,
    title = "{S}em{E}val-2017 Task 1: Semantic Textual Similarity Multilingual and Cross-lingual Focused Evaluation",
    author = "Cer, Daniel  and
      Diab, Mona  and
      Agirre, Eneko  and
      Lopez-Gazpio, I{\~n}igo  and
      Specia, Lucia",
    booktitle = "Proceedings of the 11th International Workshop on Semantic Evaluation ({S}em{E}val-2017)",
    month = aug,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    pages = "1--14",
}

@inproceedings{conneau-kiela-2018-senteval,
    title = "{S}ent{E}val: An Evaluation Toolkit for Universal Sentence Representations",
    author = "Conneau, Alexis  and
      Kiela, Douwe",
    booktitle = "Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018)",
    month = may,
    year = "2018",
    address = "Miyazaki, Japan",
    publisher = "European Language Resources Association (ELRA)",
}

@inproceedings{conneau-etal-2017-supervised,
    title = "Supervised Learning of Universal Sentence Representations from Natural Language Inference Data",
    author = "Conneau, Alexis  and
      Kiela, Douwe  and
      Schwenk, Holger  and
      Barrault, Lo{\"\i}c  and
      Bordes, Antoine",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    pages = "670--680",
}

@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    pages = "4171--4186",
}

@inproceedings{gao-etal-2021-simcse,
    title = "{S}im{CSE}: Simple Contrastive Learning of Sentence Embeddings",
    author = "Gao, Tianyu  and
      Yao, Xingcheng  and
      Chen, Danqi",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    pages = "6894--6910",
}

@article{gunther-etal-2023-jina,
    title = "{J}ina {E}mbeddings 2: 8192-token General-purpose Text Embeddings for Long Documents",
    author = "G{\"u}nther, Michael  and
      Ong, Jackmin  and
      Mohr, Isabelle  and
      Abdessalem, Alaeddine  and
      Abel, Tanguy  and
      Akram, Mohammad Kalim  and
      Guzman, Susana  and
      Mastrapas, Georgios  and
      Sturua, Saba  and
      Wang, Bo  and others",
    journal = "arXiv preprint arXiv:2310.19923",
    year = "2023",
}

@article{jiang-etal-2023-mistral,
    title = "{M}istral 7b",
    author = "Jiang, Albert Q  and
      Sablayrolles, Alexandre  and
      Mensch, Arthur  and
      Bamford, Chris  and
      Chaplot, Devendra Singh  and
      Casas, Diego de las  and
      Bressand, Florian  and
      Lengyel, Gianna  and
      Lample, Guillaume  and
      Saulnier, Lucile  and others",
    journal = "arXiv preprint arXiv:2310.06825",
    year = "2023",
}

@inproceedings{jiang-etal-2022a-promptbert,
    title = "{P}rompt-{BERT}: Improving {BERT} Sentence Embeddings with Prompts",
    author = "Jiang, Ting  and
      Jiao, Jian  and
      Huang, Shaohan  and
      Zhang, Zihan  and
      Wang, Deqing  and
      Zhuang, Fuzhen  and
      Wei, Furu  and
      Huang, Haizhen  and
      Deng, Denvy  and
      Zhang, Qi",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    pages = "8826--8837",
}

@inproceedings{jiang-etal-2022b-promcse,
    title = "Improved Universal Sentence Embeddings with Prompt-based Contrastive Learning and Energy-based Learning",
    author = "Jiang, Yuxin  and
      Zhang, Linhan  and
      Wang, Wei",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    pages = "3021--3035",
}

@article{li-etal-2023-general,
    title = "Towards General Text Embeddings with Multi-stage Contrastive Learning",
    author = "Li, Zehan  and
      Zhang, Xin  and
      Zhang, Yanzhao  and
      Long, Dingkun  and
      Xie, Pengjun  and
      Zhang, Meishan",
    journal = "arXiv preprint arXiv:2308.03281",
    year = "2023",
}

@inproceedings{liu-etal-2023-rankcse,
    title = "{R}ank{CSE}: Unsupervised Sentence Representations Learning via Learning to Rank",
    author = "Liu, Jiduan  and
      Liu, Jiahao  and
      Wang, Qifan  and
      Wang, Jingang  and
      Wu, Wei  and
      Xian, Yunsen  and
      Zhao, Dongyan  and
      Chen, Kai  and
      Yan, Rui",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    pages = "13785--13802",
}

@article{liu-etal-2019-roberta,
    title = "{R}o{B}erta: A Robustly Optimized {BERT} Pretraining Approach",
    author = "Liu, Yinhan  and
      Ott, Myle  and
      Goyal, Naman  and
      Du, Jingfei  and
      Joshi, Mandar  and
      Chen, Danqi  and
      Levy, Omer  and
      Lewis, Mike  and
      Zettlemoyer, Luke  and
      Stoyanov, Veselin",
    journal = "arXiv preprint arXiv:1907.11692",
    year = "2019",
}

@inproceedings{marelli-etal-2014-sick,
    title = "A {SICK} Cure for the Evaluation of Compositional Distributional Semantic Models",
    author = "Marelli, Marco  and
      Menini, Stefano  and
      Baroni, Marco  and
      Bentivogli, Luisa  and
      Bernardi, Raffaella  and
      Zamparelli, Roberto",
    booktitle = "Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14)",
    month = may,
    year = "2014",
    address = "Reykjavik, Iceland",
    publisher = "European Language Resources Association (ELRA)",
    pages = "216--223",
}

@inproceedings{muennighoff-etal-2023-mteb,
    title = "{MTEB}: Massive Text Embedding Benchmark",
    author = "Muennighoff, Niklas  and
      Tazi, Nouamane  and
      Magne, Loic  and
      Reimers, Nils",
    booktitle = "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics",
    month = may,
    year = "2023",
    address = "Dubrovnik, Croatia",
    publisher = "Association for Computational Linguistics",
    pages = "2014--2037",
}

@article{nussbaum-etal-2024-nomic,
    title = "{N}omic Embed: Training a Reproducible Long Context Text Embedder",
    author = "Nussbaum, Zach  and
      Morris, John X  and
      Duderstadt, Brandon  and
      Mulyar, Andriy",
    journal = "arXiv preprint arXiv:2402.01613",
    year = "2024",
}

@article{oord-etal-2018-representation,
    title = "Representation Learning with Contrastive Predictive Coding",
    author = "van den Oord, Aaron  and
      Li, Yazhe  and
      Vinyals, Oriol",
    journal = "arXiv preprint arXiv:1807.03748",
    year = "2018",
}

@inproceedings{reimers-gurevych-2019-sentence,
    title = "Sentence-{BERT}: Sentence Embeddings using Siamese {BERT}-Networks",
    author = "Reimers, Nils  and
      Gurevych, Iryna",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ({EMNLP}-{IJCNLP})",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    pages = "3982--3992",
}

@inproceedings{thakur-etal-2021-augmented,
    title = "Augmented {SBERT}: Data Augmentation Method for Improving Bi-Encoders for Pairwise Sentence Scoring Tasks",
    author = "Thakur, Nandan  and
      Reimers, Nils  and
      Daxenberger, Johannes  and
      Gurevych, Iryna",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    pages = "296--310",
}

@article{touvron-etal-2023-llama,
    title = "{LLaMA}: Open and Efficient Foundation Language Models",
    author = "Touvron, Hugo  and
      Lavril, Thibaut  and
      Izacard, Gautier  and
      Martinet, Xavier  and
      Lachaux, Marie-Anne  and
      Lacroix, Timoth{\'e}e  and
      Rozi{\`e}re, Baptiste  and
      Goyal, Naman  and
      Hambro, Eric  and
      Azhar, Faisal  and others",
    journal = "arXiv preprint arXiv:2302.13971",
    year = "2023",
}

@article{wang-etal-2022-text-embeddings,
    title = "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
    author = "Wang, Liang  and
      Yang, Nan  and
      Huang, Xiaolong  and
      Jiao, Binxing  and
      Yang, Linjun  and
      Jiang, Daxin  and
      Majumder, Rangan  and
      Wei, Furu",
    journal = "arXiv preprint arXiv:2212.03533",
    year = "2022",
}

@inproceedings{williams-etal-2018-broad,
    title = "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference",
    author = "Williams, Adina  and
      Nangia, Nikita  and
      Bowman, Samuel",
    booktitle = "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    pages = "1112--1122",
}

@inproceedings{xiao-etal-2024-cpack,
    title = "{C}-pack: Packed Resources for General {C}hinese Embeddings",
    author = "Xiao, Shitao  and
      Liu, Zheng  and
      Zhang, Peitian  and
      Muennighoff, Niklas  and
      Lian, Defu  and
      Nie, Jian-Yun",
    booktitle = "Proceedings of the 47th International {ACM} {SIGIR} Conference on Research and Development in Information Retrieval",
    year = "2024",
    pages = "641--649",
}

@inproceedings{zhang-etal-2023-cotbert,
    title = "{C}o{T}-bert: Enhancing Unsupervised Sentence Representation through Chain-of-Thought",
    author = "Zhang, Bowen  and
      Chang, Kehua  and
      Li, Chunping",
    booktitle = "International Conference on Artificial Neural Networks",
    year = "2023",
    pages = "148--163",
    publisher = "Springer",
}

@inproceedings{zhang-etal-2024-cotbert,
    title = "Automatic Chain of Thought Prompting in Large Language Models",
    author = "Zhang, Zhuosheng  and
      Zhang, Aston  and
      Li, Mu  and
      Smola, Alex",
    booktitle = "The Eleventh International Conference on Learning Representations, {ICLR} 2023",
    year = "2023",
    address = "Kigali, Rwanda",
}

@article{zhao-etal-2024-rag-survey,
    title = "Retrieval-Augmented Generation for {AI}-Generated Content: A Survey",
    author = "Zhao, Penghao  and
      Zhang, Hailin  and
      Yu, Qinhan  and
      Wang, Zhengren  and
      Geng, Yunteng  and
      Fu, Fangcheng  and
      Yang, Ling  and
      Zhang, Wentao  and
      Cui, Bin",
    journal = "arXiv preprint arXiv:2402.19473",
    year = "2024",
}
=====END FILE=====