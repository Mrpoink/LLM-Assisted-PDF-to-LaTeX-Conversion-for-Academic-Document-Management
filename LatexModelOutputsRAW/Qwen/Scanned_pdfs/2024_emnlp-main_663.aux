\relax 
\citation{reimers-gurevych-2019-sentence}
\citation{gao-etal-2021-simcse}
\citation{agirre-etal-2012-semeval}
\citation{agirre-etal-2013-semeval}
\citation{agirre-etal-2014-semeval}
\citation{agirre-etal-2015-semeval}
\citation{agirre-etal-2016-semeval}
\citation{cer-etal-2017-semeval}
\citation{marelli-etal-2014-sick}
\citation{devlin-etal-2019-bert}
\citation{liu-etal-2019-roberta}
\citation{bowman-etal-2015-large}
\citation{williams-etal-2018-broad}
\citation{gao-etal-2021-simcse}
\citation{liu-etal-2023-rankcse}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{}\protected@file@percent }
\citation{gao-etal-2021-simcse}
\citation{zhang-etal-2023-cotbert}
\citation{reimers-gurevych-2019-sentence}
\citation{conneau-etal-2017-supervised}
\citation{thakur-etal-2021-augmented}
\citation{gao-etal-2021-simcse}
\citation{bowman-etal-2015-large}
\citation{williams-etal-2018-broad}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Siamese Neural Network Architectures}{3}{}\protected@file@percent }
\citation{jiang-etal-2022a-promptbert}
\citation{zhang-etal-2024-cotbert}
\citation{oord-etal-2018-representation}
\citation{zhao-etal-2024-retrieval}
\citation{wang-etal-2022-text}
\citation{li-etal-2023-towards}
\citation{xiao-etal-2024-cpack}
\citation{gunther-etal-2023-jina}
\citation{nussbaum-etal-2024-nomic}
\citation{conneau-etal-2017-supervised}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Contrastive Learning Fine-Tuning Methods}{4}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Contrastive Learning Pre-Trained Models}{4}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Methodology}{4}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Network Architecture}{4}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Our Regression Framework}}{4}{}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:framework}{{1}{4}{}{figure.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Translated ReLU}{5}{}\protected@file@percent }
\newlabel{eq:translated_relu}{{1}{5}{}{equation.1}{}}
\citation{reimers-gurevych-2019-sentence}
\citation{conneau-kiela-2018-senteval}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Smooth K2 Loss}{6}{}\protected@file@percent }
\newlabel{eq:smooth_k2}{{2}{6}{}{equation.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Comparison of loss functions}}{6}{}\protected@file@percent }
\newlabel{fig:losses}{{2}{6}{}{figure.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experiment}{6}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}STS Performance Based on Traditional Discriminative Pre-trained Models}{6}{}\protected@file@percent }
\citation{reimers-gurevych-2019-sentence}
\citation{reimers-gurevych-2019-sentence}
\citation{gunther-etal-2023-jina}
\citation{nussbaum-etal-2024-nomic}
\citation{muennighoff-etal-2023-mteb}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Hyperparameter configurations for our two loss functions when fine-tuning BERT and RoBERTa on the NLI dataset.}}{7}{}\protected@file@percent }
\newlabel{tab:hyperparams}{{1}{7}{}{table.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}STS Performance Based on Contrastive Learning Pre-Trained Models}{7}{}\protected@file@percent }
\citation{oord-etal-2018-representation}
\citation{gao-etal-2021-simcse}
\citation{jiang-etal-2022a-promptbert}
\citation{jiang-etal-2022b-improved}
\citation{gao-etal-2021-simcse}
\citation{jiang-etal-2022a-promptbert}
\citation{jiang-etal-2022b-improved}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Spearman's correlation scores for different methods across seven STS tasks. This table is partitioned to facilitate a single variable comparison. $^\dagger $: results from \cite  {reimers-gurevych-2019-sentence}.}}{8}{}\protected@file@percent }
\newlabel{tab:main_results}{{2}{8}{}{table.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Two-stage fine-tuning process}}{8}{}\protected@file@percent }
\newlabel{fig:fine_tuning}{{3}{8}{}{figure.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Spearman's correlation coefficients of different methods across seven STS tasks. The ``+Contrast'' notation in the first column refers to models further fine-tuned with contrastive learning. $^\ddagger $: results from \cite  {gao-etal-2021-simcse}. $^\S  $: results from \cite  {jiang-etal-2022a-promptbert}. $^\P  $: results from \cite  {jiang-etal-2022b-improved}.}}{9}{}\protected@file@percent }
\newlabel{tab:contrastive_models}{{3}{9}{}{table.3}{}}
\newlabel{eq:infonce}{{3}{9}{}{equation.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Computational Resource Overhead}{9}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Computational demands of our method compared to SimCSE during the training phase. The third column, ``Length,'' represents the maximum sequence length supported by each model (cutoff length).}}{10}{}\protected@file@percent }
\newlabel{tab:resources}{{4}{10}{}{table.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Impact of Different Hyperparameter Settings}{10}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Ablation Studies}{10}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{10}{}\protected@file@percent }
\citation{touvron-etal-2023-llama}
\citation{jiang-etal-2023-mistral}
\bibstyle{acl_natbib}
\bibcite{agirre-etal-2012-semeval}{Agirre et al.2012}
\bibcite{agirre-etal-2013-semeval}{Agirre et al.2013}
\bibcite{agirre-etal-2014-semeval}{Agirre et al.2014}
\bibcite{agirre-etal-2015-semeval}{Agirre et al.2015}
\bibcite{agirre-etal-2016-semeval}{Agirre et al.2016}
\bibcite{bowman-etal-2015-large}{Bowman et al.2015}
\bibcite{cer-etal-2017-semeval}{Cer et al.2017}
\bibcite{conneau-kiela-2018-senteval}{Conneau and Kiela2018}
\bibcite{conneau-etal-2017-supervised}{Conneau et al.2017}
\bibcite{devlin-etal-2019-bert}{Devlin et al.2019}
\bibcite{gao-etal-2021-simcse}{Gao et al.2021}
\bibcite{gunther-etal-2023-jina}{G\"unther et al.2023}
\bibcite{jiang-etal-2022a-promptbert}{Jiang et al.2022a}
\bibcite{jiang-etal-2022b-improved}{Jiang et al.2022b}
\bibcite{jiang-etal-2023-mistral}{Jiang et al.2023}
\bibcite{li-etal-2023-towards}{Li et al.2023}
\bibcite{liu-etal-2019-roberta}{Liu et al.2019}
\bibcite{liu-etal-2023-rankcse}{Liu et al.2023}
\bibcite{marelli-etal-2014-sick}{Marelli et al.2014}
\bibcite{muennighoff-etal-2023-mteb}{Muennighoff et al.2023}
\bibcite{nussbaum-etal-2024-nomic}{Nussbaum et al.2024}
\bibcite{oord-etal-2018-representation}{Oord et al.2018}
\bibcite{reimers-gurevych-2019-sentence}{Reimers and Gurevych2019}
\bibcite{thakur-etal-2021-augmented}{Thakur et al.2021}
\bibcite{touvron-etal-2023-llama}{Touvron et al.2023}
\bibcite{wang-etal-2022-text}{Wang et al.2022}
\bibcite{williams-etal-2018-broad}{Williams et al.2018}
\bibcite{xiao-etal-2024-cpack}{Xiao et al.2024}
\bibcite{zhang-etal-2023-cotbert}{Zhang et al.2023}
\bibcite{zhang-etal-2024-cotbert}{Zhang et al.2024}
\bibcite{zhao-etal-2024-retrieval}{Zhao et al.2024}
\@writefile{toc}{\contentsline {section}{\numberline {A}Data Filtering Method}{13}{}\protected@file@percent }
\newlabel{app:filtering}{{A}{13}{}{section.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Average Spearman's correlation scores across seven STS tasks under different values of $k$ and $r_0$.}}{14}{}\protected@file@percent }
\newlabel{tab:hyperparam_study}{{5}{14}{}{table.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Average Spearman's correlation scores obtained by models on seven STS tasks with different concatenation methods in the final linear layer of our Siamese neural network architecture.}}{15}{}\protected@file@percent }
\newlabel{tab:ablation}{{6}{15}{}{table.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Samples from the SICK-R training and test sets. In these samples, text pair duplication occurs. Thus, the corresponding training samples are removed from the fine-tuning corpus used in section 4.2.}}{15}{}\protected@file@percent }
\newlabel{tab:sick_samples}{{7}{15}{}{table.7}{}}
\gdef \@abspage@last{15}
