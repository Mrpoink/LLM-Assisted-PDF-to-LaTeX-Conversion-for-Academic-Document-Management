\relax 
\citation{fan2021pretraining}
\citation{guo2022semantic}
\citation{lin2021pretrained}
\citation{karpukhin2020dense}
\citation{lee2020learning}
\citation{zhu2021improving}
\citation{gao2022neural}
\citation{yu2021few}
\citation{devlin2018bert}
\citation{liu2019roberta}
\citation{xiong2020approximate}
\citation{lu2021less}
\citation{hofstatter2021efficiently}
\citation{gao2021unsupervised}
\citation{ren2021rocketqav2}
\citation{ma2022pre}
\citation{liu2022retromae}
\citation{wu2022contextual}
\citation{wang2022simlm}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{}\protected@file@percent }
\citation{gao2021unsupervised}
\citation{wu2022contextual}
\citation{nguyen2016ms}
\citation{gao2022unsupervised}
\citation{nogueira2019doc2query}
\citation{dai2019context}
\citation{mao2021generation}
\citation{bai2020sparterm}
\citation{formal2021spladev2}
\citation{formal2021splade}
\citation{mallia2021learning}
\citation{shen2022lexmae}
\citation{nogueira2019doc2query}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces An example of low-relevance passages within a document from the MS-MARCO corpus. The two passages are weakly correlated in content.}}{3}{}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:example}{{1}{3}{}{figure.1}{}}
\citation{nguyen2016ms}
\citation{craswell2020overview}
\citation{craswell2020trec}
\citation{thakur2021beir}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces A comparison of context-supervised pre-training and query-as-context pre-training.}}{4}{}\protected@file@percent }
\newlabel{fig:comparison}{{2}{4}{}{figure.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Preliminary: Context-supervised Pre-training}{4}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Pre-training Corpus}{4}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Masked Language Modeling (MLM)}{4}{}\protected@file@percent }
\citation{gao2021unsupervised}
\citation{gao2021unsupervised}
\citation{wu2022contextual}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}coCondenser}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}CoT-MAE}{5}{}\protected@file@percent }
\citation{wu2022contextual}
\citation{nogueira2019doc2query}
\citation{gao2021unsupervised}
\citation{wu2022contextual}
\citation{gao2022tevatron}
\@writefile{toc}{\contentsline {section}{\numberline {3}Query-as-context Pre-training}{6}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Pre-training}{6}{}\protected@file@percent }
\citation{thakur2021beir}
\citation{gao2021unsupervised}
\citation{wu2022contextual}
\citation{wu2022contextual}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Illustration of the fine-tuning pipeline. The query-as-context pre-trained model is used to initialize the dual-encoder retrievers.}}{7}{}\protected@file@percent }
\newlabel{fig:fine-tuning}{{3}{7}{}{figure.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Fine-tuning}{7}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Experiment}{7}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Pre-training}{7}{}\protected@file@percent }
\citation{gao2021unsupervised}
\citation{gao2021condenser}
\citation{nguyen2016ms}
\citation{craswell2020overview}
\citation{craswell2020trec}
\citation{nguyen2016ms}
\citation{gao2021unsupervised}
\citation{wu2022contextual}
\citation{craswell2020overview}
\citation{craswell2020trec}
\citation{gao2021unsupervised}
\citation{wu2022contextual}
\citation{gao2022tevatron}
\citation{ren2021rocketqav2}
\citation{santhanam2021colbertv2}
\citation{khattab2020colbert}
\citation{qu2020rocketqa}
\citation{nogueira2019doc2query}
\citation{dai2019context}
\citation{mao2021generation}
\citation{gao2021unsupervised}
\citation{liu2022retromae}
\citation{ren2021rocketqav2}
\citation{ma2022pre}
\citation{xiong2020approximate}
\citation{lu2021less}
\citation{hofstatter2021efficiently}
\citation{liu2022retromae}
\citation{wang2022simlm}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Fine-tuning}{8}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Baselines}{8}{}\protected@file@percent }
\citation{gao2021unsupervised}
\citation{wu2022contextual}
\citation{dai2019context}
\citation{nogueira2019doc2query}
\citation{xiong2020approximate}
\citation{lu2021less}
\citation{hofstatter2021efficiently}
\citation{gao2021coil}
\citation{khattab2020colbert}
\citation{ma2022pre}
\citation{gao2021condenser}
\citation{qu2020rocketqa}
\citation{ren2021pair}
\citation{wang2022simlm}
\citation{liu2022retromae}
\citation{zhang2022led}
\citation{gao2021unsupervised}
\citation{wu2022contextual}
\citation{thakur2021beir}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Main Results}{9}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Out-of-domain Evaluation}{9}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Analyses}{9}{}\protected@file@percent }
\citation{xiong2020approximate}
\citation{zhan2021optimizing}
\citation{khattab2020colbert}
\citation{hofstatter2021efficiently}
\citation{lin2021in}
\citation{santhanam2021colbertv2}
\citation{qu2020rocketqa}
\citation{ren2021rocketqav2}
\citation{zhang2022adversarial}
\citation{zhang2022hlatr}
\citation{lu2021less}
\citation{gao2021condenser}
\citation{liu2022retromae}
\citation{zhong2022pre}
\citation{chang2020pre}
\citation{gao2021unsupervised}
\citation{ma2022pre}
\citation{gao2021unsupervised}
\citation{wu2022contextual}
\citation{gao2021unsupervised}
\citation{wu2022contextual}
\citation{nogueira2019doc2query}
\citation{mallia2021learning}
\citation{li2022query2doc}
\citation{ma2020zero}
\citation{wang2021gpl}
\citation{li2022query2doc}
\citation{li2022query2doc}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Impact of Generated Query Number}{10}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Impact of Mixed Context}{10}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Related Works}{10}{}\protected@file@percent }
\bibstyle{acl_natbib}
\bibcite{bai2020sparterm}{Bai et~al.(2020)}
\bibcite{chang2020pre}{Chang et~al.(2020)}
\bibcite{craswell2020overview}{Craswell et~al.(2020a)}
\bibcite{craswell2020trec}{Craswell et~al.(2020b)}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusions}{11}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8}Limitations}{11}{}\protected@file@percent }
\bibcite{dai2019context}{Dai and Callan(2019)}
\bibcite{devlin2018bert}{Devlin et~al.(2018)}
\bibcite{fan2021pretraining}{Fan et~al.(2021)}
\bibcite{formal2021spladev2}{Formal et~al.(2021a)}
\bibcite{formal2021splade}{Formal et~al.(2021b)}
\bibcite{gao2022neural}{Gao et~al.(2022a)}
\bibcite{gao2021condenser}{Gao and Callan(2021a)}
\bibcite{gao2021unsupervised}{Gao and Callan(2021b)}
\bibcite{gao2022unsupervised}{Gao and Callan(2022)}
\bibcite{gao2021coil}{Gao et~al.(2021)}
\bibcite{gao2022tevatron}{Gao et~al.(2022b)}
\bibcite{guo2022semantic}{Guo et~al.(2022)}
\bibcite{hofstatter2021efficiently}{Hofst\"atter et~al.(2021)}
\bibcite{karpukhin2020dense}{Karpukhin et~al.(2020)}
\bibcite{khattab2020colbert}{Khattab and Zaharia(2020)}
\bibcite{lee2020learning}{Lee et~al.(2020)}
\bibcite{li2022query2doc}{Li et~al.(2022)}
\bibcite{lin2021pretrained}{Lin et~al.(2021a)}
\bibcite{lin2021in}{Lin et~al.(2021b)}
\bibcite{liu2019roberta}{Liu et~al.(2019)}
\bibcite{liu2022retromae}{Liu and Shao(2022)}
\bibcite{lu2020neural}{Lu et~al.(2020)}
\bibcite{lu2021less}{Lu et~al.(2021)}
\bibcite{ma2020zero}{Ma et~al.(2020)}
\bibcite{ma2022pre}{Ma et~al.(2022)}
\bibcite{mallia2021learning}{Mallia et~al.(2021)}
\bibcite{mao2021generation}{Mao et~al.(2021)}
\bibcite{nguyen2016ms}{Nguyen et~al.(2016)}
\bibcite{nogueira2019doc2query}{Nogueira and Lin(2019)}
\bibcite{qu2020rocketqa}{Qu et~al.(2020)}
\bibcite{ren2021pair}{Ren et~al.(2021a)}
\bibcite{ren2021rocketqav2}{Ren et~al.(2021b)}
\bibcite{santhanam2021colbertv2}{Santhanam et~al.(2021)}
\bibcite{shen2022lexmae}{Shen et~al.(2022)}
\bibcite{thakur2021beir}{Thakur et~al.(2021)}
\bibcite{wang2021gpl}{Wang et~al.(2021)}
\bibcite{wang2022simlm}{Wang et~al.(2022)}
\bibcite{wu2022contextual}{Wu et~al.(2022)}
\bibcite{xiong2020approximate}{Xiong et~al.(2020)}
\bibcite{yu2021few}{Yu et~al.(2021)}
\bibcite{zhan2021optimizing}{Zhan et~al.(2021)}
\bibcite{zhang2021adversarial}{Zhang et~al.(2021)}
\bibcite{zhang2022led}{Zhang et~al.(2022a)}
\bibcite{zhang2022hlatr}{Zhang et~al.(2022b)}
\bibcite{zhou2022master}{Zhou et~al.(2022)}
\bibcite{zhu2021improving}{Zhu et~al.(2021)}
\bibcite{zhong2022pre}{Zhong et~al.(2022)}
\citation{gao2022unsupervised}
\citation{nogueira2019doc2query}
\@writefile{toc}{\contentsline {section}{\numberline {A}Statistically Analysis of Weakly Correlated Passages}{16}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Main results on MS-MARCO passage ranking and TREC DL datasets. $\dagger $ denotes our reproduction using publicly available codes. The score that is better in comparison is marked in bold.}}{17}{}\protected@file@percent }
\newlabel{tab:main_results}{{1}{17}{}{table.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Out-of-domain evaluation on BEIR benchmark. The score that is better in comparison is marked in bold.}}{18}{}\protected@file@percent }
\newlabel{tab:beir}{{2}{18}{}{table.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Impact of the number of generated queries. The score that is better in comparison is marked in bold.}}{19}{}\protected@file@percent }
\newlabel{tab:query_num}{{3}{19}{}{table.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Effect of mixing passage-query and passage-passage pairs in pre-training.}}{19}{}\protected@file@percent }
\newlabel{tab:mixed}{{4}{19}{}{table.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Correlation statistics of human annotation results of different contextual pairs, each with 200 pairs. The score that is better in comparison is marked in bold.}}{20}{}\protected@file@percent }
\newlabel{tab:correlation}{{5}{20}{}{table.5}{}}
\gdef \@abspage@last{20}
