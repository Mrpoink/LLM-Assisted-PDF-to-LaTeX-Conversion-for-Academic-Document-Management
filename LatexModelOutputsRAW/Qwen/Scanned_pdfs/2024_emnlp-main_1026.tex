=====FILE: main.tex=====
\documentclass[10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{times}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{array}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{url}
\usepackage{footnote}
\usepackage[margin=1in]{geometry}
\usepackage{indentfirst}
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\rhead{Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing}
\lhead{Pages 18463--18475}
\rfoot{Page \thepage}

\title{CasiMedicos-Arg: A Medical Question Answering Dataset \\ Annotated with Explanatory Argumentative Structures}
\author{Ekaterina Sviridova$^{1*}$ \quad Anar Yeginbergen$^{2*}$ \quad Ainara Estarrona$^{2}$ \quad Elena Cabrio$^{1}$ \quad Serena Villata$^{1}$ \quad Rodrigo Agerri$^{2}$ \\
$^{1}$Universit\'e C\^ote d'Azur, CNRS, Inria, I3S, France \\
$^{2}$HiTZ Center--Ixa, University of the Basque Country UPV/EHU \\
\texttt{\{sviridova, cabrio, villata\}@i3s.unice.fr} \\
\texttt{\{anar.yeginbergen, ainara.estarrona, rodrigo.agerri\}@ehu.eus} \\
$^{*}$Equal contribution}

\begin{document}

\maketitle

\begin{abstract}
Explaining Artificial Intelligence (AI) decisions is a major challenge nowadays in AI, in particular when applied to sensitive scenarios like medicine and law. However, the need to explain the rationale behind decisions is a main issue also for human-based deliberation as it is important to justify why a certain decision has been taken. Resident medical doctors for instance are required not only to provide a (possibly correct) diagnosis, but also to explain how they reached a certain conclusion. Developing new tools to aid residents to train their explanation skills is therefore a central objective of AI in education. In this paper, we follow this direction, and we present, to the best of our knowledge, the first multilingual dataset for Medical Question Answering where correct and incorrect diagnoses for a clinical case are enriched with a natural language explanation written by doctors. These explanations have been manually annotated with argument components (i.e., premise, claim) and argument relations (i.e., attack, support). The Multilingual CasiMedicos-Arg dataset consists of 558 clinical cases in four languages (English, Spanish, French, Italian) with explanations, where we annotated 5021 claims, 2313 premises, 2431 support relations, and 1106 attack relations. We conclude by showing how competitive baselines perform over this challenging dataset for the argument mining task.
\end{abstract}

\section{Introduction}
\label{sec:introduction}

There is an increasingly large body of research on AI applied to the medical domain with the objective of developing technology to assist and support medical doctors in explaining their decisions or how they have reached a certain conclusion. For example, resident medical doctors preparing for licensing exams may get AI support to explain what and why is the treatment or diagnosis correct given some background information \cite{safranek2023role,goenaga2024explanatory}.

A prominent example of this is the recent proliferation of Medical Question Answering (QA) datasets and benchmarks, in which the task often involves processing and acquiring relevant specialized medical knowledge to be able to answer a medical question based on the context provided by a clinical case \cite{singhal2023large,nori2023capabilities,xiong2024benchmarking}.

The development of Large Language Models (LLMs), both general purpose and specialized in the medical domain, has enabled rapid progress in Medical QA tasks which has led in turn to claims about LLMs being able to pass official medical exams such as the United States Medical Licensing Examination (USMLE) \cite{singhal2023towards,nori2023capabilities}. Thus, publicly available LLMs such as LLaMA \cite{touvron2023llama} or Mistral \cite{jiang2023mistral} and their respective medical-specific versions PMC-LLaMA \cite{wu2024pmc} and BioMistral \cite{labrak2024biomistral}, or proprietary models such as MedPaLM \cite{singhal2023large} and GPT-4 \cite{nori2023capabilities}, to name but a few, have been reporting high-accuracy scores in a variety of Medical QA benchmarks\footnote{\url{https://huggingface.co/spaces/leaderboard-medical-llm}}.

While these results constitute impressive progress, currently the Medical QA research field still presents a number of shortcomings. First, experimentation has been mostly focused on providing the correct answer in medical exams, usually in a multiple-choice setting. However, as doctors are also required to explain and argue about their predictions, research on Medical QA should also address the generation of argumentative explanations. Unfortunately, and to the best of our knowledge, no Medical QA dataset, that currently exists, includes correct and incorrect diagnoses enriched with natural language explanations written by medical doctors. Second, the large majority of Medical QA benchmarks are available only in English \cite{singhal2023large,xiong2024benchmarking}, which makes it impossible to know the ability of current LLMs for Medical QA in other languages.

In this paper, we address these issues by presenting CasiMedicos-Arg, the first Multilingual (English, French, Italian, Spanish) dataset for Medical QA with manually annotated gold explanatory argumentation about incorrect and correct predictions written by medical doctors. More specifically, the corpus consists of 558 documents with reference gold doctors' explanations which are enriched with manual annotations for argument components (5021 claims and 2313 premises) and relations (2431 support and 1106 attack). This new resource will make it possible, for the first time, to research not only on Argument Mining but also on generative techniques to argue about and explain predictions in Medical QA settings.

Finally, strong baselines on argument component detection, a challenging sequence labelling task, using encoder \cite{devlin2019bert,he2021debertav3}, encoder-decoder \cite{garcia2024medical} and decoder-only LLMs \cite{jiang2023mistral,touvron2023llama} demonstrate the validity of our annotated resource. Data, code and fine-tuned models are publicly available (\url{https://github.com/ixa-ehu/antidote-casimedicos}).

\section{Related Work}
\label{sec:related_work}

In this section, we will focus on reviewing datasets for Medical QA and on Explanatory Argumentation, the two main features of our main contribution, CasiMedicos-Arg.

\subsection{Medical Question Answering}
\label{subsec:medical_qa}

Several of the most popular Medical QA datasets \cite{jin2019pubmedqa,abacha2019medicationqa,abacha2019overview,jin2021disease,pal2022medmcqa} have been grouped into three multi-task English benchmarks, namely, MultiMedQA \cite{singhal2023large}, MIRAGE \cite{xiong2024benchmarking}, and the Open Medical-LLM Leaderboard \cite{pal2024open}, with the aim of providing comprehensive experimental evaluation benchmarks of LLMs for Medical QA.

MultiMedQA includes MedQA \cite{jin2021disease}, MedMCQA \cite{pal2022medmcqa}, PubMedQA \cite{jin2019pubmedqa}, LiveQA \cite{abacha2019overview}, MedicationQA \cite{abacha2019medicationqa}, MMLU clinical topics \cite{hendrycks2020measuring} and HealthSearchQA \cite{singhal2023large}. Except for the last one, all of them consist of a multiple-choice format and MedQA, MedMCQA and MMLU's source data come from licensing medical exams. In terms of size, MedQA includes almost 15K questions, MedMCQA 187K while the rest of them are of more moderate sizes, namely, 500 QA pairs in PubMedQA, around 1200 in MMLU, 738 in LiveQA and 674 in MedicationQA.

While every dataset except MedQA and HealthSearchQA includes long form correct answers, they are not considered really usable for benchmarking LLMs because they were not optimally constructed as a ground-truth by medical doctors or professional clinicians \cite{singhal2023large}.

The Open Medical-LLM Leaderboard also includes MedQA, MedMCQA, PubMedQA and MMLU clinical topics. General purpose LLMs such as GPT-4 \cite{nori2023capabilities}, PaLM \cite{chowdhery2022palm}, LLaMA \cite{touvron2023llama} or Mistral \cite{jiang2023mistral} report high-accuracy scores on these Medical QA benchmarks, although recently a number of specialized LLMs for the medical domain sometimes appear with even stronger performances. Some popular models include MedPaLM \cite{singhal2023large}, MedPaLM-2 \cite{singhal2023towards}, PMC-LLaMA \cite{wu2024pmc}, and more recently, BioMistral \cite{labrak2024biomistral}.

The MIRAGE benchmark includes subsets of MedQA, MedMCQA, PubMedQA, MMLU clinical topics and adds the BioASQ-YN dataset \cite{tsatsaronis2015overview} with the aim of evaluating Retrieval Augmented Generation (RAG) techniques for LLMs in Medical QA tasks. According to the authors, their MEnRAG method not only helps to address the problem of hallucinated content by grounding the generation on specific contexts, but it also provides relevant up-to-date knowledge that may not be encoded in the LLM \cite{xiong2024benchmarking}. By employing MEnRAG, they are able to clearly improve the zero-shot results of some of the tested LLMs, although the results for others are rather mixed.

To summarize, no Medical QA dataset currently provides reference gold argumentative explanations regarding the incorrect and correct predictions. Furthermore, and with the exception of \cite{vilares2019head}, they have been mostly developed for English, leaving a huge gap regarding the evaluation of LLMs in Medical QA for other languages. Motivated by this we present CasiMedicos-Arg, the first Medical QA dataset including gold reference explanations which has been manually annotated with argumentative structures, including argument components (premises and claims) and their relations (support and attack).

\subsection{Explanatory Argumentation in the Medical Domain}
\label{subsec:explanatory_argumentation}

Explanatory argumentation in natural language refers to the process of generating or analyzing explanations within argumentative texts. In recent years, natural language explanation generation has gained significant attention due to the advancements of generative models that are leveraged to develop specialized explanatory systems. The need for explanation generation is also driven by the predominant use of non-transparent algorithms which lack interpretability, thus being unsuitable for sensitive domains such as medical.

Camburu et al. \cite{camburu2018snli} tackle the task of explanation generation by introducing an extension of the Stanford Natural Language Inference (SNLI) dataset \cite{bowman2015large}, which includes a new layer of annotations providing explanations for the entailment, neutrality, or contradiction labels. The generation of these explanations is addressed with a bi-LSTM encoder trained on the new e-SNLI dataset. e-SNLI \cite{camburu2018snli} is also exploited to generate explanations for a NLI method, which first generates possible explanations for predicted labels (Label-specific Explanations) and then takes a final label decision \cite{kumar2020nile}. The authors use GPT-2 \cite{radford2019language} for label-specific generation and classify explanations with RoBERTa \cite{liu2019roberta}.

Narang et al. \cite{narang2020wt5} focus on generating complete explanations in natural language following a prediction step, utilizing a T5 model. The model is trained to predict both the label and the explanation. Li et al. \cite{li2021you} also propose to generate explanations along with predicting NLI labels. The generation step is leveraged for the question-answering task exploiting domain-specific or commonsense knowledge, while the NLI step allows to predict relations between a premise and a hypothesis. Kotonya and Toni \cite{kotonya2024towards} propose a framework to rationalize explanations taking into account not only free-form explanations, but also argumentative explanations. Furthermore, authors provide metrics for explanation evaluation.

In the medical domain, Molinet et al. \cite{molinet2024explanatory} propose generating template-based explanations for medical QA tasks. Their system incorporates medical knowledge from the Human Phenotype Ontology, making the explanations more verifiable and sound for the medical domain. At the same time, quality assessment of medical explanations remains challenging, as the process of decision-making is not transparent. In this regard, Marro et al. \cite{marro2023automatic} propose a new methodology to evaluate reasons of explanations in clinical texts.

Despite the extensive research proposing various approaches to generate explanations, these approaches are not grounded on any argumentation model. This is particularly important in sensitive domains like medicine, where sound and well-founded explanations are essential to justify the taken decision. Moreover, medical explanations require verified medical knowledge at their core, which the described methods lack, as discussed in \cite{molinet2024explanatory}.

\section{CasiMedicos-Arg Annotation}
\label{sec:annotation}

The Spanish Ministry of Health yearly publishes the Resident Medical or M\'edico Interno Residente (MIR) licensing exams including the correct answer. Every year the CasiMedicos MIR Project 2.0\footnote{\url{https://www.casimedicos.com/mir-2-0/}} takes the published exams by the ministry and provide gold explanatory arguments written by volunteer Spanish medical doctors to reason about the correct and incorrect options in the exam.

The Antidote CasiMedicos corpus consists of the original Spanish commented exams by the CasiMedicos doctors which were cleaned, structured and freely released for research purposes \cite{agerri2023hitz}. The original Spanish data was automatically translated and manually revised into English, French, and Italian. The corpus includes 622 documents each with a short clinical case, the multiple-choice questions and the explanations written by medical doctors\footnote{\url{https://huggingface.co/datasets/HiTZ/casimedicos-exp}}.

In the rest of this section we describe the process of manually annotating argumentative structures in the raw Antidote CasiMedicos dataset.

\subsection{Argumentation Annotation Guidelines}
\label{subsec:guidelines}

In line with the guidelines proposed by Mayer et al. \cite{mayer2021enhancing} for Randomized Controlled Trials (RCT) annotation, we identify two main argument components: Claims and Premises, and their relations, Support and Attack. Furthermore, we also propose to annotate Markers and labels specific to the medical domain, namely, Disease, Treatment and Diagnostics. In the following, we define and describe the annotation of each label.

\textbf{Claim} is a concluding statement made by the author about the outcome of the study \cite{mayer2021enhancing}:
\begin{enumerate}
    \item The patient's presenting picture is presumably erythema nodosum. (CasiMedicos)
    \item We propose immunotherapy with thymoglobulin and cyclosporine as a proper treatment. (CasiMedicos)
\end{enumerate}

\textbf{Premise} corresponds to an observation or measurement in the study, which supports or attacks another argument component, usually a claim. It is important that they are observed facts, therefore, credible without further evidence \cite{mayer2021enhancing}:
\begin{enumerate}
    \setcounter{enumi}{2}
    \item In addition, pancytopenia is not observed. (CasiMedicos)
    \item What is important is that the eye that has received the blow does not go up, and therefore there is double vision in the superior gaze. (CasiMedicos)
\end{enumerate}

Analyzing the CasiMedicos dataset, we found certain ambiguity between claims and premises. Thus, statements representing general medical knowledge about a disease, symptoms, or treatments must be annotated as claims. Although these statements may support or attack the main claim, they are not premises since they do not involve case-specific evidence but represent medical facts:
\begin{enumerate}
    \setcounter{enumi}{4}
    \item[\textbf{[}]The patient's presenting picture is presumably erythema nodosum.\textbf{]} \textbf{[}About 10\% of cases of erythema nodosum are associated with inflammatory bowel disease, both ulcerative colitis and Crohn's disease\textbf{]}. \textbf{[}As mentioned, in most cases, erythema nodosum has a self-limited course\textbf{]}. \textbf{[}When associated with inflammatory bowel disease, erythema nodosum usually resolves with treatment of the intestinal flare, and recurs with disease recurrences. Local measures include elevation of the legs and bed rest\textbf{]}. (CasiMedicos)
\end{enumerate}
Here the first statement in square brackets represents a claim that asserts the patient's diagnosis (erythema nodosum). The following ones represent information about the diagnosis, its symptoms and its possible treatment. They are not based on the evidences given in the case, but on general medical knowledge available to the doctor. Therefore, these examples should be annotated as Claims.

Additionally, long statements with multiple self-contained pieces of evidence must be divided into single premises to differentiate their relations to specific claims. For example, a given evidence in a sentence may support a claim while others may attack it. To preserve these distinctions, such sentences should be split into independent premises.

As well as Claims and Premises we annotate \textbf{Markers} -- discourse markers that are relevant for arguments as they help to identify the spans of argument components and the type of argumentative relations. In the following examples markers are written in bold:
\begin{enumerate}
    \setcounter{enumi}{5}
    \item Other causes related to this picture are autoimmune diseases leading to transverse myelitis (Beh\c{c}et's, FAS, SLE,\ldots) or inflammatory diseases such as sarcoidosis, \textbf{although} our patient does not seem to meet the criteria for them. (CasiMedicos)
    \item \textbf{Although} this usually gives a subacute or chronic picture. (CasiMedicos)
\end{enumerate}

The possible answers proposed in the CasiMedicos multiple-choice options correspond to predicting a Disease, a Treatment or a Diagnosis. We decided to also annotate them as they help to identify the type of doctor's arguments (whether to look justification of a diagnosis or about a possible treatment) and the type of argumentative relations.

For advanced reasoning comprehension, we need to explore argumentative relations connecting argument components (claims and premises) and forming a structure of an argument \cite{mayer2021enhancing}. Here we provide the definitions of support and attack relations, as well as real examples illustrating them.

\textbf{Support}. All statements or observations justifying the proposition of a target argument component are considered as supportive \cite{mayer2021enhancing}:
\begin{enumerate}
    \setcounter{enumi}{7}
    \item \textit{In the examination there is a clear dissociation with thermoalgesic anesthesia and preservation of arthrokinetic and vibratory.} [1] \textit{Reflexes are normal, neither abolished nor exalted.} [2] \textit{In addition, the rest of the examination is strictly normal.} [3] \textbf{With all this I believe that the correct answer is 5, that is a syringomyelic lesion, whose initial characteristic is the sensitive dissociation with anesthesia for the thermoalgesic and conservation of the posterior chordal.} (CasiMedicos)
\end{enumerate}
This example provides premises (in italics) that justify a claim (bold) which they are related to. The supportive nature is highlighted by the marker \textit{With all this I believe\ldots}.

\textbf{Attack}. An argument component is attacking another one if (i) it contradicts the proposition of a target component or (ii) it undercuts its implicit assumption of significance or relevance, for example, stating that the observations related to a target component are not significant or not relevant \cite{mayer2021enhancing}:
\begin{enumerate}
    \setcounter{enumi}{8}
    \item \textit{It might be tempting to answer 3 Fracture of the superior wall of the orbit with entrapment of the superior rectus muscle.} \textbf{However, muscles trapped in a fracture do not automatically lose their muscular action.} (CasiMedicos)
    \item \textit{The palpebral hematoma and hyposphagma (subconjunctival hemorrhage) does not give us the key data.} (CasiMedicos)
\end{enumerate}
These examples represent premises (in italics) which either contradict their claims (bold) in Example 9 or which are not considered significant to justify or reject target components (Example 10).

\subsection{CasiMedicos Real Case Example}
\label{subsec:real_case}

In this section we demonstrate a real CasiMedicos case annotated with argument components -- Premises (in square brackets in italics) and Claims (in square brackets in bold), as well as Markers (M). We consider this case to be exemplary because its explanation includes reasons on why the correct answer is correct and why the incorrect answers are incorrect. We do not include argumentative relations for the sake of space and clarity.

\textbf{QUESTION TYPE: PEDIATRICS}

\textbf{CLINICAL CASE}

[A woman comes to the office with her 3 year old daughter because she has detected a slight mammary development since 3 months without taking any medication or any relevant history.] Indeed, [\textit{the physical examination shows a Tanner stage II with no growth of pubic or axillary hair.}] [\textit{The external genitalia are normal.}] [\textit{Ultrasonography reveals a small uterus and radiology reveals a bone age of 3 years.}] What attitude should be adopted?

\begin{enumerate}
    \item [\textbf{[}Follow-up every 3--4 months, as this is a temporary condition that often resolves on its own.\textbf{]}]
    \item [\textbf{[}Breast biopsy.\textbf{]}]
    \item [\textbf{[}Mammography.\textbf{]}]
    \item [\textbf{[}Administration of GnRH analogues.\textbf{]}]
\end{enumerate}

\textbf{CORRECT ANSWER: 1}

[\textbf{It seems that they want to present us with precocious puberty (or premature telarche)}] (M) but [\textit{they do not provide any analytical data}] and [\textit{the ultrasound data are ambiguous}] ([\textit{we should assume that by a small uterus they are referring to a prepubertal uterus}], (M) but [\textit{they do not provide any data on ovarian size}]). [\textbf{We are presented with the case of a three-year-old girl with advanced mammary development, in principle without any associated cause}] ([\textit{in principle she does not take drugs that can increase the level of estrogen in the blood}], [\textit{she does not seem to use body creams or eat a lot of chicken meat}]). [\textbf{If we follow the diagnostic scheme for a premature telarche or suspicion of precocious puberty, we request bone age and abdominal ultrasound}] ([\textit{the bone age is not advanced as in precocious puberty, and we assume that with a small uterus they mean a prepubertal uterus}]); [\textbf{according to the complementary examinations that we are given, it does not seem to be precocious puberty, except for the clinical (Tanner II)}]. [\textbf{Strictly speaking, without analytical hormonal data, it seems that we could mark option 1, being necessary to follow the girl closely.}] [\textbf{If we take all the above data for granted, we could} (M) \textbf{rule out option 4, which would be the treatment of a central precocious puberty.}] [\textbf{Regarding the option of mammography, breast ultrasound is used in pediatrics, and in this case it would be indicated if we were told that there is breast asymmetry}] ([\textit{we discard option 3}]). [\textbf{Regarding breast biopsy, it would only be indicated if there are warning signs.}]

\subsection{Annotation Process and Results}
\label{subsec:annotation_process}

The annotation process consisted of three stages: training, reconciliation, and complete dataset annotation. During training, annotators worked on 10 CasiMedicos cases. We then calculated the inter-annotator agreement (IAA) results of the training phase to highlight weak spots, guideline flaws, and any issues in the dataset needing further analysis.

At the reconciliation phase, the descriptions of Claim and Premise labels were discussed and agreed upon. After this, we started the complete dataset annotation. As mentioned earlier, the original CasiMedicos dataset included 622 medical cases, but 64 cases were excluded during the annotation phase. Some of them did not have gold explanations while others were cases with confusing relations: the correct answer is a wrong disease, treatment, or diagnosis as asked in a question, thus, it is attacked by its premises instead of being supported. Therefore, the final number of annotated cases is 558. In the following subsections, we present the IAA of the entire dataset (\ref{subsec:iaa}), annotation results and their description (\ref{subsec:annotation_results}).

\subsection{Inter-Annotator Agreement (IAA)}
\label{subsec:iaa}

The IAA is calculated over a random batch of 100 CasiMedicos cases. Since one instance (e.g. a claim) is usually an entire self-contained sentence, we measured the IAA at both the instance level and at the token level. In other words, we compute agreement over entire instances and over the tokens of each instance.

Table~\ref{tab:instance_iaa} illustrates the IAA at the instance level. Since instances are very long, annotators may be uncertain about which elements to include, leading to lower agreement scores for some labels. However, the major labels Claim and Premise have relatively good results with scores of 0.765 and 0.659, respectively. The mean F1 over all labels is 0.669.

\begin{table}[h]
\centering
\caption{Instance-based F1 agreement.}
\label{tab:instance_iaa}
\begin{tabular}{lc}
\toprule
Label & Mean F1 \\
\midrule
Claim & 0.765 \\
Premise & 0.659 \\
Marker & 0.642 \\
Disease & 0.639 \\
Treatment & 0.586 \\
Diagnostics & 0.527 \\
\midrule
Mean & 0.669 \\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{tab:token_iaa} shows the IAA at the token level. Here we compute the agreement over tokens of each instance. The highest agreement score is of a Claim label being 0.915, while the lowest is of a Diagnostics label accounting for 0.638. The mean F1 over all tokens is 0.880.

\begin{table}[h]
\centering
\caption{Token-based F1 agreement.}
\label{tab:token_iaa}
\begin{tabular}{lc}
\toprule
Label & Mean F1 \\
\midrule
Claim & 0.915 \\
Premise & 0.891 \\
Marker & 0.777 \\
Disease & 0.738 \\
Treatment & 0.634 \\
Diagnostics & 0.638 \\
\midrule
Mean & 0.880 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Annotation Results}
\label{subsec:annotation_results}

In this part, we report the stats about label distribution over entire cases (documents) and the label distribution over the doctor's explanations only. Additionally, we also discuss the distribution of argumentative relations.

Table~\ref{tab:label_distribution_cases} reports the total number of entities over the dataset and the average number of entities per case. Table~\ref{tab:label_distribution_explanations} shows the label distributions only for the explanations, namely, the total number of entities in explanations and the average number of entities per explanation. In both tables, we notice that the discrepancy between the average number of claims per explanation and of premises per explanation is rather high. This may seem strange since premises are needed to accept or reject claims in order to complete one argumentation unit.

However, there are plausible reasons for such distribution. First, there is a certain number of cases where the explanation is based on the evidence from a doctor's knowledge rather than clinical facts described in the case itself. Such explanations take into account the information given about the patient (e.g. age, symptoms, vital signs), but do not repeat any of these facts (as in Example 1 in Appendix A). Second, explanations that do not repeat evidence from the case are frequent, e.g. ``Here we must suspect\ldots disease. All the symptoms fall perfectly within the picture''; ``This is a fairly easy epidemiology question, in adults without other data, \textit{Pneumococcus} is the 1st''). Last but not least, there is a group of cases with implicit premises or implicit warrants: the explanation presents claims (e.g. a conclusion about a disease and a treatment) implying that some evidences from the case text and implying certain medical knowledge to align evidences with a disease and a choice of treatment (as in Example 2 in Appendix A).

\begin{table}[h]
\centering
\caption{Label Distribution over Entire Cases}
\label{tab:label_distribution_cases}
\begin{tabular}{lrr}
\toprule
Label & Total & Mean per case \\
\midrule
Claim & 5021 & 8.998 \\
Premise & 2313 & 4.145 \\
Marker & 1179 & 2.113 \\
Disease & 1278 & 2.290 \\
Treatment & 786 & 1.408 \\
Diagnostics & 791 & 1.418 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[h]
\centering
\caption{Label Distribution in Explanations}
\label{tab:label_distribution_explanations}
\begin{tabular}{lrr}
\toprule
Label & Total & Mean per explanation \\
\midrule
Claim & 3003 & 5.948 \\
Premise & 470 & 0.935 \\
Marker & 974 & 1.933 \\
\bottomrule
\end{tabular}
\end{table}

In Table~\ref{tab:relations} we present the distribution of argumentative relations. Support relations appear twice as much as Attack ones, making this argumentation pattern frequent and probably more convincing. In cases where the conclusion is made solely by excluding wrong propositions by attacking them, there is a lack of confidence about the claim.

\begin{table}[h]
\centering
\caption{Distribution of Argumentative Relations.}
\label{tab:relations}
\begin{tabular}{lrr}
\toprule
Relation & Total & Mean per case \\
\midrule
Support & 2431 & 4.351 \\
Attack & 1106 & 1.982 \\
\bottomrule
\end{tabular}
\end{table}

As a result, we present CasiMedicos-Arg, a multi-layer argument-based annotation of the English version of CasiMedicos consisting of 558 clinical cases with explanations. In the following sections, we describe the experiments performed on argument component detection (claims and premises) to establish strong baselines on the task and validate our annotations.

\section{Experimental Setup}
\label{sec:experimental_setup}

We first describe the process of projecting the manually annotated argumentation labels from the source English data to the other three target languages, namely, French, Italian and Spanish. Since the annotators of the argument components were English speakers, we treated it as the source when projecting labels to the target languages. This process will result in the Multilingual CasiMedicos-Arg which will then be leveraged to produce strong baselines on argument component detection using a variety of LMs, including encoders \cite{devlin2019bert,he2021debertav3}, encoder-decoders \cite{garcia2024medical} and decoder-only LLMs \cite{touvron2023llama,jiang2023mistral}.

\subsection{Multilingual CasiMedicos-Arg}
\label{subsec:multilingual}

Taking the manually annotated English CasiMedicos-Arg as a starting point, we first needed to project the annotations to Spanish (original text), French and Italian (revised translations) following the method described in \cite{yeginbergenova2023cross,yeginbergen2024argument}. Second, and to ensure that the projection method correctly leveraged the annotations to the new data we additionally performed an automatic post-processing step of the newly generated data to correct any misalignments. Finally, to guarantee the quality of annotations and the validity of our evaluations, the translated and projected data is manually revised by native speakers.

Label projection is performed using word alignments calculated by AwEsoME \cite{dou2021word} and Easy Label Projection \cite{garcia2022model} to automatically map the word alignments into sequences (argument components) and project them from the source (English) to the target language (French, Italian and Spanish).

A particular feature of argument components is that the sequences could span over the entire length of the sentences. Therefore, after revising the automatically projected data, an extra post-processing step was performed by correcting the projections in the sequences where some annotations were placed incorrectly. The most common correction was fixing articles at the beginning of the argument components, which were systematically missed out during the automatic projection step. Other sequences were labeled only by half instead of the whole sequence. This post-processing step was essential to minimize human labor during manual correction.

The number of corrections introduced during the post-processing step can be found in Appendix B.

The final manual correction step involved checking the translation quality and projected labels by native expert annotators fixing any misprojections or errors in the translation. The result of this process is the Multilingual CasiMedicos-Arg dataset, obtained by projecting the manual annotations from English to Italian, French and Spanish.

\subsection{Sequence Labelling with LLMs}
\label{subsec:sequence_labelling}

We leverage Multilingual CasiMedicos-Arg to perform cross-lingual and multilingual argument component detection, a task that, due to the heterogeneity and length of the sequences, is usually a rather challenging task \cite{stab2017parsing,eger2018cross,yeginbergenova2023cross}.

Furthermore, in addition to classic encoder-only models like mBERT \cite{devlin2019bert} and mDeBERTa \cite{he2021debertav3}, we decided to also perform the task using encoder-decoder and decoder-only models. For the encoder-decoder category, we chose two variants of Medical mT5, a multilingual text-to-text model adapted to multilingual medical texts: med-mT5-large and med-mT5-large-multitask \cite{garcia2024medical}. For the decoder-only architecture, we selected the LLaMA-2 \cite{touvron2023llama} and Mistral \cite{jiang2023mistral} models with 7B parameters. The domain-specific versions of these models produced less promising results, so we opted to report the results of the aforementioned models.

Previous work in sequence labeling with LLMs has demonstrated that discriminative approaches based on encoder-only models still outperform generative techniques based on LLMs \cite{wang2023gpt}. The motivation behind it is usually the nature of the sequence labeling task that even though LLMs possess some linguistic knowledge they suffer from a number of problems, notably, hallucinated content. In this paper, we use the LLMs for Sequence Labelling library to fine-tune the generative models with unconstrained decoding\footnote{\url{https://github.com/ikergarcia1996/Sequence-Labeling-LLMs}}.

We structure the experiments as follows. First, we perform monolingual experiments in which we train and test for each language separately. Note that for English we use the gold standard annotations, while for French, Italian and Spanish we are fine-tuning the models on projected data, which in cross-lingual transfer research is usually called data-transfer. Additionally, we also report results of model-transfer (fine-tuning the models in English and predicting in the rest of the target languages). Finally, we experiment with multilingual data augmentation by pooling the training data of all four languages and then evaluating in each language separately.

Since each model has its own way of learning due to the architecture, namely, some models learn better over longer iterations and others perform at a good level in less time, we report the best results yielded from the models under different hyperparameters. Multilingual BERT and mDeBERTa were fine-tuned for 3 epochs, while Medical mT5 required 20 epochs; the rest of the hyperparameters are based on previous related work \cite{yeginbergenova2023cross} and \cite{garcia2024medical}, respectively. Regarding LLaMA2 and Mistral, they were fine-tuned for 5 epochs leaving the rest of the hyperparameters as default.

\section{Empirical Results}
\label{sec:results}

In this section, we report the results obtained after performing the steps described in Section~\ref{sec:experimental_setup}. All the results and standard deviations reported in this section are obtained by averaging three randomly initialized runs. We evaluate using sequence level F1-macro score, a common metric for argument component detection.

We first show the results on monolingual (using the manually annotated English data) and multilingual (fine-tuning on all four languages and evaluating in English) in Table~\ref{tab:english_results}. Overall, it can be observed that the decoder-only generative models outperform the rest, though the Medical mT5 models are nearly as effective. Furthermore, the multilingual method of pooling all languages into a single dataset proves to be beneficial for every model, improving over the results obtained when training using the gold standard English data only.

\begin{table}[h]
\centering
\caption{F1-scores and their standard deviations for argument component detection in English CasiMedicos-Arg; bold: best overall result; underlined: best result per model across the two language settings.}
\label{tab:english_results}
\begin{tabular}{lcc}
\toprule
Model & Monolingual & Multilingual \\
\midrule
mBERT & 76.24 (0.89) & 77.14 (0.97) \\
mDeBERTa & 77.08 (0.89) & 77.30 (0.59) \\
med-mT5-large & 80.43 (0.22) & 82.37 (0.21) \\
med-mT5-large-multitask & 80.93 (0.26) & 82.03 (0.32) \\
LLaMA2-7B & 81.49 (0.82) & 83.07 (0.11) \\
Mistral-0.1-7B & \underline{83.27 (0.48)} & \textbf{83.24 (0.73)} \\
\bottomrule
\end{tabular}
\end{table}

The results for Spanish, French and Italian are displayed in Table~\ref{tab:multilingual_results}. As for the English results, it can be seen that the multilingual data-transfer approach is the most effective setting, even with LLMs which are supposedly pre-trained on English data only. Among all the models, Mistral achieves the highest F1-macro scores. However, while for all the other models the multilingual training was advantageous no substantial improvement was observed in a similar setting with Mistral. Finally, it can be seen that cross-lingual model transfer is the least optimal of the settings, even when using state-of-the-art multilingual LMs such as mDeBERTa \cite{he2021debertav3}. An interesting point to note is that for cross-lingual model transfer the best results are obtained by the Medical mT5 models, which may be due to this model being trained on multilingual medical data \cite{garcia2024medical}.

\begin{table}[h]
\centering
\caption{F1-scores and their standard deviations of data-transfer (monolingual and multilingual), and cross-lingual model-transfer experiments using Spanish, French, and Italian data; bold: best overall result; underlined: best result per model across the three language settings.}
\label{tab:multilingual_results}
\begin{tabular}{lccc}
\toprule
Model & Spanish & French & Italian \\
\midrule
\multicolumn{4}{c}{\textbf{Monolingual (data-transfer)}} \\
\midrule
mDeBERTa & 76.06 (1.42) & 77.39 (0.83) & 76.35 (0.29) \\
med-mT5-large & 82.07 (0.12) & 80.79 (0.19) & 80.12 (0.59) \\
med-mT5-large-multitask & 82.09 (0.26) & 80.69 (0.65) & 80.13 (0.56) \\
LLaMA2-7B & 81.56 (0.28) & 80.39 (0.52) & 80.89 (0.54) \\
Mistral-0.1-7B & \underline{82.55} & \underline{81.71} & \underline{81.55} \\
\midrule
\multicolumn{4}{c}{\textbf{Multilingual (data-transfer)}} \\
\midrule
mDeBERTa & 76.22 (0.89) & 76.98 (0.76) & 76.91 (0.65) \\
med-mT5-large & 80.85 (0.26) & 80.32 (0.04) & 80.41 (0.87) \\
med-mT5-large-multitask & 80.83 (0.28) & 80.70 (0.08) & 80.51 (0.34) \\
LLaMA2-7B & 81.03 (0.49) & 80.69 (0.46) & 80.66 (0.51) \\
Mistral-0.1-7B & \textbf{82.91} & \textbf{81.71} & \textbf{81.55} \\
\midrule
\multicolumn{4}{c}{\textbf{Cross-lingual (model-transfer)}} \\
\midrule
mDeBERTa & 74.63 (0.53) & 75.22 (0.32) & 75.30 (0.32) \\
med-mT5-large & 78.51 (1.20) & 79.41 (0.87) & 79.28 (0.87) \\
med-mT5-large-multitask & 77.96 (0.13) & 77.07 (0.34) & 78.28 (0.34) \\
LLaMA2-7B & 68.56 (1.07) & 73.86 (0.51) & 72.58 (0.51) \\
Mistral-0.1-7B & 70.62 (7.37) & 78.36 (0.31) & 76.08 (0.31) \\
\bottomrule
\end{tabular}
\end{table}

Summarizing, in this section we present competitive baselines for argument component detection on CasiMedicos-Arg, validating both the manual annotations and the strategy of projecting English labels to other languages to facilitate the application of cross-lingual and multilingual techniques.

\section{Conclusion}
\label{sec:conclusion}

In this paper, we present CasiMedicos-Arg, a multilingual (French, English, Italian and Spanish) Medical QA dataset including gold reference explanations written by medical doctors which has been annotated with argumentative structures. This dataset aims to bridge a glaring gap in the Medical QA ecosystem by facilitating the evaluation of explanations generated to argue or justify a given prediction.

The final dataset includes 558 documents (parallel in four languages) with reference gold doctors' explanations which are enriched with manual annotations for argument components (5021 claims and 2313 premises) and relations (2431 support and 1106 attack).

Both inter-annotator agreement results and the baselines provided for argument component detection demonstrate the validity of our annotations. Furthermore, experiments show the advantage of performing argument component detection from a multilingual data-transfer perspective.

\section{Limitations}
\label{sec:limitations}

We consider two main limitations in our work that we would like to address in the short term future. First, the choice of languages. We would have liked to include languages from different language families and with different morphological and grammatical characteristics, but we were limited by the native expertise available to us to perform the manual corrections of the projected labels and translations. Second, the size of the dataset (558 documents) could be larger.

Regarding the first limitation, we still think that our experiments demonstrate the superiority of performing multilingual data-transfer over cross-lingual model transfer, at least with the LLMs currently available. With respect to the size of the dataset, we would like to point out that its size is similar to other datasets reviewed in Section~\ref{sec:related_work}, which are being widely used to benchmark LLMs for Medical QA.

Another issue worth considering in the future is the need to further research the generation of explanations for the predictions while taking into account a crucial unsolved issue, namely, the evaluation explanation generation in the highly specialized medical domain.

\section*{Acknowledgments}
We thank the CasiMedicos Proyecto MIR 2.0 for their permission to share their data for research purposes. This work has been supported by the French government, through the 3IA C\^ote d'Azur Investments in the Future project managed by the National Research Agency (ANR) with the reference number ANR-19-P3IA-0002. This work has also been supported by the CHIST-ERA grant of the Call XAI2019 of the ANR with the grant number Project-ANR-21-CHR4-0002. We are also thankful to several MCIN/AEI/10.13039/501100011033 projects: (i) Antidote (PCI2020-120717-2), and by European Union NextGenerationEU/PRTR; (ii) DeepKnowledge (PID2021-127717OB-C21) and ERDF A way of making Europe; (iii) DeepMinor (CNS2023-144315) and European Union NextGenerationEU/PRTR. We also thank the European High Performance Computing Joint Undertaking (EuroHPC Joint Undertaking, EXT-2023801-013) for the GPU hours. Anar Yeginbergen's PhD contract is part of the PRE2022-105620 grant, financed by MCIN/AEI/10.13039/501100011033 and by the FSE+.

\bibliographystyle{acl_natbib}
\begin{thebibliography}{100}

\bibitem[Abacha et al.2019a]{abacha2019medicationqa}
Asma Ben Abacha, Yassine Mrabet, Mark Sharp, Travis R Goodwin, Sonya E Shooshan, and Dina Demner-Fushman. 2019a. Bridging the Gap Between Consumers' Medication Questions and Trusted Answers. In \textit{MedInfo}, pages 25--29.

\bibitem[Abacha et al.2019b]{abacha2019overview}
Asma Ben Abacha, Chaitanya Shivade, and Dina Demner-Fushman. 2019b. Overview of the MEDIQA 2019 Shared Task on Textual Inference, Question Entailment and Question Answering. In \textit{Proceedings of the 18th BioNLP Workshop and Shared Task}, pages 310--379.

\bibitem[Agerri et al.2023]{agerri2023hitz}
Rodrigo Agerri, I\~nigo Alonso, Aitziber Atutxa, Ander Berrondo, Ainara Estarrona, Iker Garcia-Ferrero, Iakes Goenaga, Koldo Gojenola, Maite Oronoz, Igor Perez-Tejedor, German Rigau, and Anar Yeginbergenova. 2023. Hitz@antidote: Argumentation-driven explainable artificial intelligence for digital medicine. In \textit{SEPLN 2023: 39th International Conference of the Spanish Society for Natural Language Processing}.

\bibitem[Bowman et al.2015]{bowman2015large}
Samuel R Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning. 2015. A large annotated corpus for learning natural language inference. In \textit{EMNLP}.

\bibitem[Camburu et al.2018]{camburu2018snli}
Oana-Maria Camburu, Tim Rockt\"aschel, Thomas Lukasiewicz, and Phil Blunsom. 2018. e-snli: Natural language inference with natural language explanations. In \textit{NeurIPS}.

\bibitem[Chowdhery et al.2022]{chowdhery2022palm}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam M. Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Benton C. Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathleen S. Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. Palm: Scaling language modeling with pathways. \textit{J. Mach. Learn. Res.}, 24:240:1--240:113.

\bibitem[Devlin et al.2019]{devlin2019bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In \textit{Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)}, pages 4171--4186.

\bibitem[Dou and Neubig2021]{dou2021word}
Zi-Yi Dou and Graham Neubig. 2021. Word alignment by fine-tuning embeddings on parallel corpora.

\bibitem[Eger et al.2018]{eger2018cross}
Steffen Eger, Johannes Daxenberger, Christian Stab, and Iryna Gurevych. 2018. Cross-lingual argumentation mining: Machine translation (and a bit of projection) is all you need! In \textit{Proceedings of the 27th International Conference on Computational Linguistics}, pages 831--844, Santa Fe, New Mexico, USA. Association for Computational Linguistics.

\bibitem[Garcia-Ferrero et al.2022]{garcia2022model}
Iker Garcia-Ferrero, Rodrigo Agerri, and German Rigau. 2022. Model and data transfer for cross-lingual sequence labelling in zero-resource settings. In \textit{Findings of EMNLP}.

\bibitem[Garcia-Ferrero et al.2024]{garcia2024medical}
Iker Garcia-Ferrero, Rodrigo Agerri, Aitziber Atutxa Salazar, Elena Cabrio, Iker de la Iglesia, Alberto Lavelli, Bernardo Magnini, Benjamin Molinet, Johana Ramirez-Romero, German Rigau, Jose Maria Villa-Gonzalez, Serena Villata, and Andrea Zaninello. 2024. Medical mT5: An Open-Source Multilingual Text-to-Text LLM for The Medical Domain. In \textit{Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING)}.

\bibitem[Goenaga et al.2024]{goenaga2024explanatory}
Iakes Goenaga, Aitziber Atutxa, Koldo Gojenola, Maite Oronoz, and Rodrigo Agerri. 2024. Explanatory argument extraction of correct answers in resident medical exams. \textit{Artificial Intelligence in Medicine}.

\bibitem[He et al.2021]{he2021debertav3}
Pengcheng He, Jianfeng Gao, and Weizhu Chen. 2021. Debertav3: Improving deberta using electra-style pre-training with gradient-disentangled embedding sharing. \textit{arXiv preprint arXiv:2111.09543}.

\bibitem[Hendrycks et al.2020]{hendrycks2020measuring}
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask language understanding. In \textit{ICLR 2020}.

\bibitem[Jiang et al.2023]{jiang2023mistral}
Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. \textit{arXiv preprint arXiv:2310.06825}.

\bibitem[Jin et al.2021]{jin2021disease}
Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. 2021. What disease does this patient have? a large-scale open domain question answering dataset from medical exams. \textit{Applied Sciences}, 11(14):6421.

\bibitem[Jin et al.2019]{jin2019pubmedqa}
Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu. 2019. PubMedQA: A dataset for biomedical research question answering. In \textit{Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)}, pages 2567--2577. Association for Computational Linguistics.

\bibitem[Kotonya and Toni2024]{kotonya2024towards}
Neema Kotonya and Francesca Toni. 2024. Towards a framework for evaluating explanations in automated fact verification. In \textit{Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)}, pages 16364--16377, Torino, Italia. ELRA and ICCL.

\bibitem[Kumar and Talukdar2020]{kumar2020nile}
Sawan Kumar and Partha Talukdar. 2020. NILE: Natural language inference with faithful natural language explanations. In \textit{Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics}, pages 8730--8742.

\bibitem[Labrak et al.2024]{labrak2024biomistral}
Yanis Labrak, Adrien Bazoge, Emmanuel Morin, Pierre-Antoine Gourraud, Mickael Rouvier, and Richard Dufour. 2024. Biomistral: A collection of open-source pretrained large language models for medical domains. In \textit{ACL}.

\bibitem[Li et al.2021]{li2021you}
Dongfang Li, Jingcong Tao, Qingcai Chen, and Bao-tian Hu. 2021. You can do better! if you elaborate the reason when making prediction. \textit{arXiv preprint arXiv:2103.14919}.

\bibitem[Liu et al.2019]{liu2019roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. \textit{arXiv preprint arXiv:1907.11692}.

\bibitem[Marro et al.2023]{marro2023automatic}
Santiago Marro, Theo Alkibiades Collias, Elena Cabrio, and Serena Villata. 2023. On the automatic assessment of natural language expert explanations in medicine. In \textit{HC@AIxIA}, pages 83--98.

\bibitem[Mayer et al.2021]{mayer2021enhancing}
Tobias Mayer, Santiago Marro, Elena Cabrio, and Serena Villata. 2021. Enhancing evidence-based medicine with natural language argumentative analysis of clinical trials. \textit{Artificial Intelligence in Medicine}, 118:102098.

\bibitem[Molinet et al.2024]{molinet2024explanatory}
Benjamin Molinet, Santiago Marro, Elena Cabrio, and Serena Villata. 2024. Explanatory argumentation in natural language for correct and incorrect medical diagnoses. \textit{Journal of Biomedical Semantics}, 15.

\bibitem[Narang et al.2020]{narang2020wt5}
Sharan Narang, Colin Raffel, Katherine Lee, Adam Roberts, Noah Fiedel, and Karishma Malkan. 2020. Wt5?! training text-to-text models to explain their predictions. \textit{arXiv preprint arXiv:2004.14546}.

\bibitem[Nori et al.2023]{nori2023capabilities}
Harsha Nori, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric Horvitz. 2023. Capabilities of gpt-4 on medical challenge problems. \textit{arXiv preprint arXiv:2303.13375}.

\bibitem[Pal et al.2024]{pal2024open}
Ankit Pal, Pasquale Minervini, Andreas Geert Motzfeldt, Aryo Pradipta Gema, and Beatrice Alex. 2024. open-lifescienceai/open\_medical\_llm\_leaderboard. \url{https://huggingface.co/spaces/openlifescienceai/open-medical-llm-leaderboard}.

\bibitem[Pal et al.2022]{pal2022medmcqa}
Ankit Pal, Logesh Kumar Umapathi, and Malai Kannan Sankarasubbu. 2022. MedMCQA: A large-scale multi-subject multi-choice dataset for medical domain question answering. In \textit{Conference on Health, Inference, and Learning}, pages 248--260. PMLR.

\bibitem[Radford et al.2019]{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. \textit{OpenAI blog}, 1(8):9.

\bibitem[Safranek et al.2023]{safranek2023role}
Conrad W Safranek, Anne Elizabeth Sidamon-Eristoff, Aidan Gilson, and David Chartash. 2023. The role of large language models in medical education: Applications and implications. \textit{JMIR Med Educ}, 9:e50945.

\bibitem[Singhal et al.2023a]{singhal2023large}
Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, et al. 2023a. Large language models encode clinical knowledge. \textit{Nature}, 620(7972):172--180.

\bibitem[Singhal et al.2023b]{singhal2023towards}
Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou, Kevin Clark, Stephen Pfohl, Heather Cole-Lewis, Darlene Neal, et al. 2023b. Towards expert-level medical question answering with large language models. \textit{arXiv preprint arXiv:2305.09617}.

\bibitem[Stab and Gurevych2017]{stab2017parsing}
Christian Stab and Iryna Gurevych. 2017. Parsing argumentation structures in persuasive essays. \textit{Computational Linguistics}, 43(3):619--659.

\bibitem[Touvron et al.2023]{touvron2023llama}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open foundation and fine-tuned chat models. \textit{Preprint, arXiv:2307.09288}.

\bibitem[Tsatsaronis et al.2015]{tsatsaronis2015overview}
George Tsatsaronis, Georgios Balikas, Prodromos Malakasiotis, Ioannis Partalas, Matthias Zschunke, Michael R Alvers, Dirk Weissenbom, Anastasia Krithara, Sergios Petridis, Dimitris Polychronopoulos, et al. 2015. An overview of the bioasq large-scale biomedical semantic indexing and question answering competition. \textit{BMC bioinformatics}, 16:1--28.

\bibitem[Vilares and G\'omez-Rodr\'iguez2019]{vilares2019head}
David Vilares and Carlos G\'omez-Rodr\'iguez. 2019. HEAD-QA: A Healthcare Dataset for Complex Reasoning. In \textit{Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics}, pages 960--966, Florence, Italy. Association for Computational Linguistics.

\bibitem[Wang et al.2023]{wang2023gpt}
Shuhe Wang, Xiaofei Sun, Xiaoya Li, Rongbin Ouyang, Fei Wu, Tianwei Zhang, Jiwei Li, and Guoyin Wang. 2023. Gpt-ner: Named entity recognition via large language models. \textit{arXiv preprint arXiv:2304.10428}.

\bibitem[Wu et al.2024]{wu2024pmc}
Chaoyi Wu, Weixiong Lin, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie. 2024. PMC-LLaMA: Towards Building Open-source Language Models for Medicine. \textit{Journal of the American Medical Informatics Association: JAMIA}.

\bibitem[Xiong et al.2024]{xiong2024benchmarking}
Guangzhi Xiong, Qiao Jin, Zhiyong Lu, and Aidong Zhang. 2024. Benchmarking retrieval-augmented generation for medicine. In \textit{ACL}.

\bibitem[Yeginbergen et al.2024]{yeginbergen2024argument}
Anar Yeginbergen, Maite Oronoz, and Rodrigo Agerri. 2024. Argument mining in data scarce settings: Cross-lingual transfer and few-shot techniques. In \textit{Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 11687--11699.

\bibitem[Yeginbergenova and Agerri2023]{yeginbergenova2023cross}
Anar Yeginbergenova and Rodrigo Agerri. 2023. Cross-lingual argument mining in the medical domain. \textit{Procesamiento del Lenguaje Natural}, 73.

\end{thebibliography}

\appendix

\section{CasiMedicos Real Cases}
\label{app:cases}

\textbf{Example 1:}

\textbf{QUESTION TYPE: DERMATOLOGY}

\textbf{CLINICAL CASE:}

A 62-year-old man with a history of significant alcohol abuse, carrier of hepatitis C virus, treated with ibuprofen for tendinitis of the right shoulder, goes to his dermatologist because after spending two weeks on vacation at the beach he notices the appearance of tense blisters on the dorsum of his hands. On examination, in addition to localization and slight malar hypertrichosis. The most likely diagnosis is:

\begin{enumerate}
    \item Epidermolysis bullosa acquisita.
    \item Porphyria cutanea tarda.
    \item Phototoxic reaction.
    \item Contact dermatitis.
    \item Acute intermittent porphyria.
\end{enumerate}

\textbf{CORRECT ANSWER: 2}

Porphyria Cutanea Tarda: 60\% of patients with PCT are male, many of them drink alcohol in excess, women who develop it are usually treated with drugs containing estrogens. Most are males with signs of iron overload, this overload reduces the activity of the enzyme uroporphyrinogen decarboxylase, which leads to the elevation of uroporphyrins. HCV and HIV infections have been implicated in the precipitation of acquired PCT. There is a hereditary form with AD pattern.

Patients with PCT present with blistering of photoexposed skin, most frequently on the dorsum of the hands and scalp. In addition to fragility, they may develop hypertrichosis, hyperpigmentation, cicatricial alopecia and sclerodermal induration.

\textbf{Example 2:}

\textbf{QUESTION TYPE: PEDIATRICS}

\textbf{CLINICAL CASE:}

6-month-old infant presenting to the emergency department for respiratory distress. Examination: 40 rpm, heart rate 160 bpm, blood pressure 90/45 mmHg, SatO2 95\% on room air. He shows moderate respiratory distress with intercostal and subcostal retraction. Pulmonary auscultation: scattered expiratory rhonchi, elongated expiration and slight decrease in air entry in both lung fields. Cardiac auscultation: no murmurs. It is decided to keep the patient under observation in the hospital for a few hours. What do you consider the most appropriate attitude at this time with regard to the complementary tests?

\begin{enumerate}
    \item Request venous blood gas, leukocyte count and acute phase reactants.
    \item Request chest X-ray.
    \item Request arterial blood gases and acute phase reactants.
    \item Do not request complementary tests.
\end{enumerate}

\textbf{CORRECT ANSWER: 4}

The patient probably presents with bronchiolitis. At this stage, no additional tests should be performed unless there is a clinical worsening.

\section{Number of corrections after annotation projection}
\label{app:corrections}

The number of corrections required after automatically projecting the annotations.

\begin{table}[h]
\centering
\caption{Number of corrections introduced in the post-processing step after automatic label projection.}
\label{tab:corrections}
\begin{tabular}{lrrr}
\toprule
 & Train & Test & Dev \\
\midrule
ES & 450 & 153 & 64 \\
FR & 378 & 109 & 49 \\
IT & 336 & 117 & 55 \\
\bottomrule
\end{tabular}
\end{table}

\end{document}
=====END FILE=====