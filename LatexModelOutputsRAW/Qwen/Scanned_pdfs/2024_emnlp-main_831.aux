\relax 
\citation{hu2021lora}
\citation{devlin2019bert}
\citation{liu2021roberta}
\citation{wang2019glue}
\citation{hu2021lora}
\citation{sun2017meProp}
\citation{touvron2023llama}
\citation{koepf2023openassistant}
\citation{lialin2023scaling}
\citation{pfeiffer2020adapterhub}
\citation{houlsby2019parameter}
\citation{hu2021lora}
\citation{zaken2021bitfit}
\citation{sun2017meProp}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Number of parameters for different layers in models based on the Transformer.}}{2}{}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:param_dist}{{1}{2}{}{table.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{2}{}\protected@file@percent }
\citation{cichocki2016tensor}
\@writefile{toc}{\contentsline {section}{\numberline {3}Method}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Preliminary Phase: Finding Transition Matrices}{3}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Gradients on the 5-th BERT MLP: $U^T \frac  {\partial \mathcal  {L}}{\partial W} V$ (right) is more sparse than the original $\frac  {\partial \mathcal  {L}}{\partial W}$ (left).}}{3}{}\protected@file@percent }
\newlabel{fig:sparsity}{{1}{3}{}{figure.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Signal Propagation in SparseGradLinear Layer}{3}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Correspondence of variables in Torch Autograd for a regular Linear layer and SparseGradLinear.}}{3}{}\protected@file@percent }
\newlabel{tab:autograd}{{2}{3}{}{table.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Sparse-by-Dense Matrix Multiplication}{3}{}\protected@file@percent }
\citation{wang2019glue}
\citation{koepf2023openassistant}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The first row illustrates signal propagation in the original Linear Layer, while the second row illustrates propagation with the proposed SparseGradLinear layer.}}{4}{}\protected@file@percent }
\newlabel{fig:signal}{{2}{4}{}{figure.2}{}}
\newlabel{eq:sparse_dense}{{1}{4}{}{equation.1}{}}
\newlabel{eq:coo}{{2}{4}{}{equation.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Time and Memory Consumption per Training Iteration}{4}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Strided structure of $\frac  {\partial \mathcal  {L}}{\partial W'}$ (left) and visualizations of \% nonzero elements in $\frac  {\partial \mathcal  {L}}{\partial W'}$ throughout training (right).}}{4}{}\protected@file@percent }
\newlabel{fig:strided}{{3}{4}{}{figure.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Training speed and memory requirements averaged on the GLUE benchmark. The last two rows report the results for the SparseGrad method with Sparse-by-Dense (SD) and Regular (Reg) matrix multiplication, respectively.}}{4}{}\protected@file@percent }
\newlabel{tab:perf}{{3}{4}{}{table.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Experiments}{4}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Natural Language Understanding with BERT and RoBERTa}{4}{}\protected@file@percent }
\citation{liu2021roberta}
\citation{akiba2019optuna}
\citation{touvron2023llama}
\citation{koepf2023openassistant}
\citation{zheng2023judging}
\citation{zheng2023judging}
\citation{cichocki2016tensor}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Average scores over the GLUE benchmark for BERT and RoBERTa$_{\text  {base}}$ models.}}{5}{}\protected@file@percent }
\newlabel{tab:glue_avg}{{4}{5}{}{table.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Conversations with LLaMa-2}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Acknowledgements}{5}{}\protected@file@percent }
\bibstyle{acl_natbib}
\bibdata{refs}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Comparative results of RoBERTa$_{\text  {large}}$ for 20-epoch task-specific fine-tuning.}}{6}{}\protected@file@percent }
\newlabel{tab:glue_large}{{5}{6}{}{table.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Comparative results for LLaMa-2 on the OpenAssistant dataset.}}{6}{}\protected@file@percent }
\newlabel{tab:llama}{{6}{6}{}{table.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Limitations}{6}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {9}Ethics Statement}{6}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {A}Appendix A}{6}{}\protected@file@percent }
\newlabel{app:perf_details}{{A}{6}{}{section.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces The training step execution speed, measured in steps per second (where a higher value indicates faster execution), is reported for the RoBERTa base model. The last two rows describe the SparseGrad method with Sparse-by-Dense multiplication and with Regular matrix multiplication.}}{6}{}\protected@file@percent }
\newlabel{tab:speed_appendix}{{7}{6}{}{table.7}{}}
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces Peak memory measurement in MB for training loop for the model RoBERTa base.}}{6}{}\protected@file@percent }
\newlabel{tab:mem_appendix}{{8}{6}{}{table.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B}Appendix B}{6}{}\protected@file@percent }
\newlabel{app:glue_results}{{B}{6}{}{section.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {C}Appendix C}{6}{}\protected@file@percent }
\newlabel{app:ablation}{{C}{6}{}{section.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {D}Appendix D}{6}{}\protected@file@percent }
\newlabel{app:hyperparams}{{D}{6}{}{section.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {9}{\ignorespaces Comparative results of BERT model for 20-epoch task-specific fine-tuning.}}{7}{}\protected@file@percent }
\newlabel{tab:bert_results}{{9}{7}{}{table.9}{}}
\@writefile{lot}{\contentsline {table}{\numberline {10}{\ignorespaces Comparative results of RoBERTa$_{\text  {base}}$ for 20-epoch task-specific fine-tuning.}}{7}{}\protected@file@percent }
\newlabel{tab:roberta_base_results}{{10}{7}{}{table.10}{}}
\@writefile{lot}{\contentsline {table}{\numberline {11}{\ignorespaces GLUE score as a function of the weight gradient sparsity in BERT.}}{7}{}\protected@file@percent }
\newlabel{tab:bert_ablation}{{11}{7}{}{table.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {E}Appendix E}{7}{}\protected@file@percent }
\newlabel{app:examples}{{E}{7}{}{section.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {12}{\ignorespaces GLUE score as a function of the weight gradient sparsity in RoBERTa$_{\text  {base}}$.}}{7}{}\protected@file@percent }
\newlabel{tab:roberta_ablation}{{12}{7}{}{table.12}{}}
\@writefile{lot}{\contentsline {table}{\numberline {13}{\ignorespaces Best training parameters on GLUE benchmark for BERT model.}}{8}{}\protected@file@percent }
\newlabel{tab:bert_hyperparams}{{13}{8}{}{table.13}{}}
\@writefile{lot}{\contentsline {table}{\numberline {14}{\ignorespaces Best training parameters on GLUE benchmark for RoBERTa$_{\text  {base}}$ model.}}{8}{}\protected@file@percent }
\newlabel{tab:roberta_base_hyperparams}{{14}{8}{}{table.14}{}}
\@writefile{lot}{\contentsline {table}{\numberline {15}{\ignorespaces Best training parameters on GLUE benchmark for RoBERTa$_{\text  {large}}$ model.}}{8}{}\protected@file@percent }
\newlabel{tab:roberta_large_hyperparams}{{15}{8}{}{table.15}{}}
\gdef \@abspage@last{8}
