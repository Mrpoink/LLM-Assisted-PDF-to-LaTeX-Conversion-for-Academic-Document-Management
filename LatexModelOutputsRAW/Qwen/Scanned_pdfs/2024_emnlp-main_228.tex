=====FILE: main.tex=====
\documentclass[10pt,twocolumn]{article}
\usepackage{amsmath,amssymb}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{times}
\usepackage{url}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{footnote}
\usepackage{hyperref}
\hypersetup{colorlinks=true,linkcolor=blue,citecolor=blue,urlcolor=blue}

\setlength{\columnsep}{0.375in}
\setlength{\columnseprule}{0pt}

\title{Dissecting Fine-Tuning Unlearning in Large Language Models}
\author{
Yihuai Hong$^{1,4*}$, Yuelin Zou$^2$, Lijie Hu$^3$, Ziqian Zeng$^4$, Di Wang$^4$, Haiqin Yang$^{1,4\dagger}$ \\
$^1$South China University of Technology \\
$^2$Columbia University \\
$^3$King Abdullah University of Science and Technology \\
$^4$International Digital Economy Academy (IDEA), China \\
\texttt{yihuaihong@gmail.com}, \texttt{hqyang@ieee.org}
}
\date{}

\begin{document}
\maketitle

\begin{abstract}
Fine-tuning-based unlearning methods prevail for preventing targeted harmful, sensitive, or copyrighted information within large language models while preserving overall capabilities. However, the true effectiveness of these methods is unclear. In this work, we delve into the limitations of fine-tuning-based unlearning through activation patching and parameter restoration experiments. Our findings reveal that these methods alter the model's knowledge retrieval process, providing further evidence that they do not genuinely erase the problematic knowledge embedded in the model parameters. Instead, the coefficients generated by the MLP components in the model's final layer are the primary contributors to these seemingly positive unlearning effects, playing a crucial role in controlling the model's behaviors. Furthermore, behavioral tests demonstrate that this unlearning mechanism inevitably impacts the global behavior of the models, affecting unrelated knowledge or capabilities. The code is released at \url{https://github.com/yihuaihong/Dissecting-FT-Unlearning}.
\end{abstract}

\section{Introduction}
Large language models (LLMs), due to their extensive pre-training corpora, often inadvertently learn harmful, sensitive, or copyright-protected knowledge \cite{Chang2023a,Mozes2023,Eldan2023,Ye2022}. Consequently, recent research has focused on developing efficient unlearning methods as a post-training technique to selectively unlearn the specific knowledge \cite{Blanco2024,Liu2024}.

Currently, the core mechanism of these unlearning methods involves fine-tuning \cite{Eldan2023,Jang2023,Yao2024,Rafailov2023}, with corresponding adjustments and designs in the loss function to facilitate the unlearning process. Although earlier investigations \cite{Hong2024,Lee2024a} have proven that these methods are ineffective at completely erasing model-embedded knowledge, the factors contributing to the misleading success of these techniques remain unclear.

Therefore, in this paper, we try to unveil why existing fine-tuning-based unlearning methods perform well in behavioral tests by analyzing the mechanisms of internal knowledge recall and flow within models \cite{Meng2022,Pochinkov2024,Geva2021a}. Specifically, we investigate which components or parameters carry these unlearning effects. We design activation patching and parameters restoration experiments in three settings, aiming to independently study the impact of unlearning methods on the coefficients and value vectors in the MLPs, as well as on the attention components' states. Our findings further confirm that the methods do not truly alter the knowledge embedded in the value vectors of MLPs, and reveal that they will change how they extract and transfer this knowledge through modifications in the coefficients of MLPs and attention components during unlearning. Notably, the coefficients produced by the MLP in the final layers are primarily responsible for achieving the unlearning effects of fine-tuning-based methods.

We further test the global behavior impact of these fine-tuning-based unlearning methods on LLaMA2-7B-chat \cite{Touvron2023} and OLMo-7B \cite{Groeneveld2024} by implementing them on the respective pretraining datasets of both models, aiming to more closely simulate the erasure of knowledge acquired during the pretraining process. We discover that while these methods appear to effectively unlearn target knowledge, they also inevitably affect the output and behavior related to unrelated knowledge. This unintended consequence stems from the fact that these approaches are based on altering the model's internal knowledge retrieval mechanisms, thereby impacting its global behavior and overall performance.

Ultimately, we conclude once again that current fine-tuning-based unlearning methods cannot completely erase sensitive knowledge embedded in models, particularly within the MLPs, instead adjusting the mechanisms by which the model retrieves knowledge. These methods are vulnerable to recovery attacks in components' activations and unsuitable for true unlearning. We advocate for future unlearning evaluations to concentrate on precise measurement of both the actual storage of targeted knowledge within the model's entire parameter set and the specific dynamics of how this knowledge is retrieved and utilized.

\section{Background and Related Work}
\subsection{Unlearning in Large Language Models}
Since large language models learn knowledge from different domains and corpora during the pre-training process, it is often found that they contain harmful, sensitive or private knowledge, leading to the possibility that language models produce output behaviors containing corresponding sensitive or harmful information \cite{Liu2024,Chang2023a,Mozes2023}. Therefore, unlearning emerges as a timely and important post-pretraining processing method for LLM safety. Currently, the vast majority of LLM unlearning methods use fine-tuning as the primary operational approach. In terms of classifying them by different training objectives, they include gradient direction control \cite{Jang2023,Yao2024,Yao2023} and preference optimization methods \cite{Rafailov2023,Zhao2024,Lee2024b}. In terms of classifying them by the parameters covered during training, they include full parameters fine-tuning \cite{Eldan2023,Jang2023,Yao2024,Rafailov2023}, sparse fine-tuning \cite{Chang2023b,Stoehr2024}, and parameter-efficient fine-tuning \cite{Li2024,Chen2023}. Additionally, there are also a few knowledge editing methods \cite{Patil2024}. We present the specific logic details of each method in Appendix A.

\subsection{Knowledge Storage in Large Language Models}
Studying how knowledge is stored, transferred, and extracted in LLMs has always been an important direction in the research of LLM interpretability \cite{Meng2022,Geva2021b,Sukhbaatar2015,Geva2023}. It is known that in transformer-based language models, the MLP is a crucial component for storing the model's factual knowledge, and its sub-layers can be viewed as key-value memories \cite{Geva2021b}. To be specific, the first layer of MLP sub-layers can be viewed as a matrix $W_1 \in \mathbb{R}^{n \times d}$ formed by key vectors $\{k_1, k_2, \dots, k_n\}$, used to capture a set of patterns in the input sequence, and ultimately outputting the coefficient scores. The second layer can be viewed as a matrix $W_2 \in \mathbb{R}^{d \times n}$ formed by value vectors $\{v_1, v_2, \dots, v_n\}$, with each value vector containing the corresponding factual knowledge (represented through token distributions). Finally, the MLP's output can be defined as the sum of value vectors weighted by their memory coefficients:
\begin{equation}
M_l = W_2^{(l)} \sigma(W_1^{(l)} x_l) = \sum_{i=1}^n m_i v_i^{(l)}
\label{eq:mlp}
\end{equation}
where $M_l$ represents the output of the MLP in the transformer's $l$-th layer for an input hidden state $x_l$ at that layer with the parameters $W_1^{(l)}$ and $W_2^{(l)} \in \mathbb{R}^{d \times n}$, $\sigma$ is a non-linearity function, and $m_i \in \mathbb{R}$ represents the coefficient scores. The dimension size of hidden states is $d$ and it is $n$ for the intermediate MLP.

In addition to the MLP, primarily responsible for knowledge storage, the attention component is currently considered the main component responsible for knowledge transfer and extraction in language models \cite{Geva2023}. Here, we will not go into detail about its specific structure but only study the impact it has on knowledge extraction. The final computation formula for the hidden states in the language model is defined as:
\begin{equation}
x_{l+1} = x_l + M_l + A_l
\label{eq:hidden}
\end{equation}
where $x_l$, $M_l$ and $A_l$ represent the hidden states, MLP's output, and the attention component's output in the transformer's $l$-th layer, respectively.

\section{Patching Investigation}
\subsection{Hypothesis and Experimental Design}
Based on Eq.~(\ref{eq:mlp}) and Eq.~(\ref{eq:hidden}), we hypothesize that there are three main reasons why the current fine-tuning-based unlearning methods appear successful in behavioral tests and seem to suggest that true unlearning has been achieved:
\begin{enumerate}
    \item The coefficients $m_i$ are changed after fine-tuning, leading to a change in the activations of the MLPs;
    \item The value vectors $W_2^{(l)}$ in MLPs are changed, causing a change in the knowledge they contain;
    \item The change that happens in attention components caused the model's focus and the corresponding information extracted by these attention components $A_l$ to change, thus reducing the target knowledge-related information in the output.
\end{enumerate}

Based on the possible reasons described above, on the unlearned model, we conduct three different sets of activation patching or components' parameter restoration experiments, trying to recover the output of the target knowledge in the unlearned model. The specific operation process is as follows:
\begin{enumerate}
    \item In the first set of experiments, we restore the coefficient scores $m_i$ corresponding to each MLP component, layer by layer, in the language model, without making any intentional changes to the value vector parameters $W_2^{(l)}$ of the MLPs or the attention components' states $A_l$ in any layer.
    \item In the second set of experiments, we restore the parameters of value vectors $W_2^{(l)}$ in MLPs layer by layer, recovering the knowledge they originally contained. In this process, we avoid making intentional changes to the unlearned model's original coefficients $m_i$ and the attention components' states $A_l$.
    \item In the third set of experiments, we restore the original attention components' states $A_l$, but without intentionally altering the MLPs' coefficient scores $m_i$ or the value vectors' parameters $W_2^{(l)}$, only studying the impact brought by the attention components which are responsible for extracting and transferring knowledge.
\end{enumerate}

To evaluate the extent of knowledge restoration, we propose the metric of Knowledge Recovery Score (KRS):
\begin{equation}
\text{KRS} = 1 - \frac{\text{loss}_{\text{unlearned-recover}}}{\text{loss}_{\text{unlearned}}}
\label{eq:krs}
\end{equation}
where the losses are the average of MSE($\cdot$) on $L_{\text{vanilla}}$ and $L_{\text{unlearned}}$ and on $L_{\text{vanilla}}$ and $L_{\text{unlearned-recover}}$, respectively. MSE($\cdot$) represents the mean squared error (MSE) loss function. $L_{\text{vanilla}}$, $L_{\text{unlearned}}$, and $L_{\text{unlearned-recover}}$ are the logit distributions of the subsequent token produced by the vanilla model, unlearned model, and unlearned-then-recover model, respectively. The average loss is computed on the next $T$ generated tokens on knowledge-related questions.

Finally, if KRS approaches 1, it indicates $L_{\text{vanilla}}$ and $L_{\text{unlearned-recover}}$ are nearly consistent, representing a higher degree of knowledge recovery. Conversely, a lower KRS suggests a lower degree of that.

\subsection{Activation Patching and Parameters Restoration Experiments}
We conduct the experiments on two recent LLMs, LLaMA2-7B-chat \cite{Touvron2023} and OLMo-7B \cite{Groeneveld2024}. We apply two example fine-tuning-based unlearning methods, DPO \cite{Rafailov2023} and Gradient Difference \cite{Yao2024}, to perform unlearning on the large language models and calculate the average KRS scores. Inspired by \cite{Eldan2023}, which tries to unlearn the concept knowledge of ``Harry Potter'' in language models, we extend this experiment by selecting 10 well-known concepts per model from the ConceptVectors Benchmark \cite{Hong2024}, which is a collection of concepts that language models are well-acquainted with and have substantial knowledge about. Examples of them are provided in Table~\ref{tab:examples} of Appendix B. For the unlearning training, we use the texts containing the corresponding concepts from RedPajama and Dolma \cite{Soldaini2024}. RedPajama is a replication of the pretraining corpus for the LLaMA model, while Dolma is the open-source pre-training dataset for the OLMo model. Detailed information is provided in Appendix B. So here we can ensure that the knowledge to be unlearned was at least seen by the model during the pre-training process, and that the training data used more broadly covers the textual sources from which the model acquired the corresponding knowledge about certain concepts.

After obtaining the unlearned model, we follow the steps mentioned in the hypothesis to perform activation patching and parameter restoration experiments on the unlearned models. To calculate the Knowledge Recovery Scores, we set $T$ to 30 and $N$ to 10, indicating the generation of the next 30 tokens and the selection of 10 questions related to each concept. To make the recovery effects more pronounced and the whole process easier to observe, we adopt techniques from \cite{Meng2022,Meng2023} which implemented causal mediation, setting the size of the recovery window to five. This allows us to observe the average effects of recovering five consecutive layers at a time. Details can be found in Appendix B.

The specific results are shown in Fig.~\ref{fig:krs_results}. From our analysis, surprisingly, we observe that when we solely recover the parameters contained in the value vectors of each layer in the unlearned model without interfering with the coefficients or attention components' states, the recovery of the target knowledge is negligible (The KRS scores are all below 0.001). This holds regardless of which layer is recovered, and regardless of the specific model being considered.

However, when recovering the attention components' states in the intermediate layers (from the 15th layer onward) or deeper layers (from the 27th layer onward), we can observe that the average KRS for both models has increased to exceed 0.3 and 0.4, respectively, indicating that a significant portion of the corresponding knowledge has been recovered. What's more, restoring the coefficients of the MLPs in the intermediate layers (from the 20th layer onward) and deeper layers (from the 29th layer) also yields impressive knowledge recovery effects.

The layers at which the scores start to increase under the two settings generally align closely with the observation by Geva et al.~\cite{Geva2023} that the MLP modules recall knowledge in intermediate layers, and the attention components mostly start to extract and transfer information in the deeper layers, or after the model has completed the relevant knowledge recall. We also tried simultaneously recovering the coefficients and attention states and found that the model can achieve much greater knowledge recovery with the peak KRS score exceeding 0.9 on both models.

Additionally, it is noteworthy that, simply restoring the coefficient scores of the MLP outputs from the last two or three layers can significantly elevate the KRS of the unlearned LLaMA and OLMo models to 0.8 or above. This suggests that the coefficient scores of the MLPs in the last layers might play a crucial role in the final behavior results of the LLM. To better isolate the effects of restoring $m_i$, $W_2^{(l)}$, and $A_l$ individually and support the above argument, we present a more rigorous patching and restoration experiment in Appendix C, with the corresponding results shown in Figure~\ref{fig:krs_isolated}. Ultimately, we found that the restoration of the attention states also contributed to the coefficients of the MLP in the final layers, further confirming that these coefficients carry the primary role of achieving the effects of fine-tuning-based unlearning. It also indicates that fine-tuning largely adjusts the model's behavior by modifying the coefficients of the deep MLP layers, likely because this enables faster adaptation compared to other knowledge adjustment mechanisms, such as altering knowledge encoded in the MLP itself. This phenomenon and the potential defensive strategy have not been discussed in the previous literature, warranting further investigation in future studies.

Overall, these results all further confirm that the fine-tuning-based unlearning methods essentially do not modify the model knowledge contained in the value vectors, but adjust the way knowledge is called during the fine-tuning process, either by adjusting the coefficients to modulate the MLP activation or by adjusting the attention to extract and transfer knowledge.

\begin{figure}[t]
\centering
\framebox[0.95\columnwidth]{\parbox[c][1.8cm][c]{0.9\columnwidth}{\centering IMAGE NOT PROVIDED}}
\caption{Results of KRS on LLaMA and OLMo under three activations patching or parameters restoration settings. We also included another setting that restores both attention and coefficients to compare the final outcomes.}
\label{fig:krs_results}
\end{figure}

\section{Global Negative Effect of Fine-Tuning Unlearning}
In the previous section, we demonstrated that these fine-tuning-based methods alter the model's final behavior by adjusting the MLP output coefficients in the final layers. Therefore, we hypothesize that this behavioral change will have a global effect, potentially impacting the output of unrelated knowledge as well. In this section, we verify this hypothesis through the following experiments.

We apply four fine-tuning-based unlearning methods to the concepts used in Section 3 on their pretraining text sources (from RedPajama and Dolma) with the goal of erasing the learned knowledge during pretraining through a reverse process. These methods are as follows: DPO \cite{Rafailov2023}, NPO \cite{Zhao2024}, NPO+KL \cite{Zhao2024} and Gradient Difference \cite{Yao2024}. The details of these baselines and data statistics are shown in Appendix A and B. We evaluate the unlearning effectiveness of these methods on the concepts' related QA pairs and the unlearning impact on unrelated QA pairs, reporting the average scores of BLEU \cite{Papineni2002} by comparing the model's response before and after unlearning. In Figure~\ref{fig:bleu_results}, we report their performance at the end of each training epoch respectively.

We can observe that for fine-tuning-based methods, as the number of training epochs increases, aiming to achieve a lower target QA BLEU score, the corresponding unrelated QA BLEU score also decreases accordingly, exhibiting a positive correlation. This suggests that the impact of fine-tuning-based methods on the model's output behavior is global. While unlearning the target knowledge, they inadvertently alter the output behavior or manner for unrelated knowledge to a certain degree.

\begin{figure}[t]
\centering
\framebox[0.95\columnwidth]{\parbox[c][1.8cm][c]{0.9\columnwidth}{\centering IMAGE NOT PROVIDED}}
\caption{Unlearning testing results on LLaMA and OLMo for each training epoch.}
\label{fig:bleu_results}
\end{figure}

\section{Discussion and Conclusion}
We have deeply investigated the reasons why fine-tuning-based unlearning methods seemingly succeeded in behavior-based testing for large language model unlearning: Through activation patching and parameter restoration experiments, we find that these methods alter the way knowledge is extracted by changing MLP activations or model's attention, ultimately affecting the output. This is evidenced by the fact that the model's output regarding the target knowledge is largely restored after patching the activations and the attention components' states. Furthermore, we conduct experiments on the pretraining datasets of two models, to test the models' capabilities after unlearning, verifying that in addition to unlearning the corresponding knowledge, fine-tuning-based methods that by altering the way the model accesses knowledge, will significantly impair the model's other unrelated capabilities, causing a certain degree of capability degradation.

\section{Limitations}
In the experiments detailed in Section 3, we have disregarded the potential unlearning impact caused by parameter changes in other model components during the fine-tuning process. This decision is based on the observation that the impact of such changes appears to be minimal. For instance, during our parameter comparison analysis, we found that the changes in the unembedding matrix and normalization layer parameters resulted in cosine similarity values above 0.999. This suggests that the modifications to these components are quite small in magnitude.

However, it remains unclear whether even such minimal parameter changes can still have any meaningful effect on the model's overall behavior and knowledge. Further verification and analysis would be needed to conclusively determine the extent to which these ancillary parameter updates might influence the unlearning outcome.

\section*{Acknowledgements}
The work was fully supported by the IDEA Information and Super Computing Centre (ISCC), National Natural Science Foundation of China (Grant No. 62406114), the Guangzhou Basic and Applied Basic Research Foundation (Grant No. 2023A04J1687), and the Fundamental Research Funds for the Central Universities (Grant No. 2024ZYGXZR074). Di Wang and Lijie Hu are supported in part by the funding RGC/1/1689-01-01, URF/1/4663-01-01, REI/15232-01-01, REA/5332-01-01, and URF/1/5508-01-01 from KAUST, and funding from KAUST-Center of Excellence for Generative AI, under award number 5940.

\bibliographystyle{acl_natbib}
\begin{thebibliography}{100}

\bibitem[Alberto Blanco-Justicia et al.2024]{Blanco2024}
Alberto Blanco-Justicia, Najeeb Jebreel, Benet Manzanares, David S\'anchez, Josep Domingo-Ferrer, Guillem Collell, and Kuan Eeik Tan. 2024. Digital forgetting in large language models: A survey of unlearning methods. Preprint, arXiv:2404.02062.

\bibitem[Kent K. Chang et al.2023a]{Chang2023a}
Kent K. Chang, Mackenzie Hanh Cramer, Sandeep Soni, and David Bamman. 2023a. Speak, memory: An archaeology of books known to chatGPT/GPT-4. In \textit{The 2023 Conference on Empirical Methods in Natural Language Processing}.

\bibitem[Ting-Yun Chang et al.2023b]{Chang2023b}
Ting-Yun Chang, Jesse Thomason, and Robin Jia. 2023b. Do localization methods actually localize memorized data in llms? arXiv preprint arXiv:2311.09060.

\bibitem[Jiaao Chen and Diyi Yang2023]{Chen2023}
Jiaao Chen and Diyi Yang. 2023. Unlearn what you want to forget: Efficient unlearning for llms. In \textit{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing}, pages 12041--12052.

\bibitem[Ronen Eldan and Mark Russinovich2023]{Eldan2023}
Ronen Eldan and Mark Russinovich. 2023. Who's harry potter? approximate unlearning in llms. Preprint, arXiv:2310.02238.

\bibitem[Mor Geva et al.2021a]{Geva2021a}
Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. 2021a. Transformer feed-forward layers are key-value memories. In \textit{Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing}, pages 5484--5495.

\bibitem[Mor Geva et al.2021b]{Geva2021b}
Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. 2021b. Transformer feed-forward layers are key-value memories. In \textit{Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing}, pages 5484--5495.

\bibitem[Mor Geva et al.2023]{Geva2023}
Mor Geva, Jasmijn Bastings, Katja Filippova, and Amir Globerson. 2023. Dissecting recall of factual associations in auto-regressive language models. In \textit{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing}, pages 12216--12235.

\bibitem[Dirk Groeneveld et al.2024]{Groeneveld2024}
Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson, Russell Authur, Khyathi Raghavi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel, Tushar Khot, William Merrill, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk, Saurabh Shah, Will Smith, Emma Strubell, Nishant Subramani, Mitchell Wortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson, Luke Zettlemoyer, Jesse Dodge, Kyle Lo, Luca Soldaini, Noah A. Smith, and Hannaneh Hajishirzi. 2024. Olmo: Accelerating the science of language models. arXiv preprint arXiv:2402.00838.

\bibitem[Yihuai Hong et al.2024]{Hong2024}
Yihuai Hong, Lei Yu, Haiqin Yang, Shauli Ravfogel, and Mor Geva. 2024. Intrinsic evaluation of unlearning using parametric knowledge traces. Preprint, arXiv:2406.11614.

\bibitem[Joel Jang et al.2023]{Jang2023}
Joel Jang, Dongkeun Yoon, Sohee Yang, Sungmin Cha, Moontae Lee, Lajanugen Logeswaran, and Minjoon Seo. 2023. Knowledge unlearning for mitigating privacy risks in language models. In \textit{Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long papers)}, pages 14389--14408.

\bibitem[Andrew Lee et al.2024a]{Lee2024a}
Andrew Lee, Xiaoyan Bai, Itamar Press, Martin Wattenberg, Jonathan K. Kummerfeld, and Rada Mihalcea. 2024a. A mechanistic understanding of alignment algorithms: A case study on DPO and toxicity. In \textit{Forty-first International Conference on Machine Learning}.

\bibitem[Andrew Lee et al.2024b]{Lee2024b}
Andrew Lee, Xiaoyan Bai, Itamar Press, Martin Wattenberg, Jonathan K. Kummerfeld, and Rada Mihalcea. 2024b. A mechanistic understanding of alignment algorithms: A case study on dpo and toxicity. arXiv preprint arXiv:2401.01967.

\bibitem[Sijia Liu et al.2024]{Liu2024}
Sijia Liu, Yuanshun Yao, Jinghan Jia, Stephen Casper, Nathalie Baracaldo, Peter Hase, Xiaojun Xu, Yuguang Yao, Hang Li, Kush R. Varshney, Mohit Bansal, Sanmi Koyejo, and Yang Liu. 2024. Rethinking machine unlearning for large language models. Preprint, arXiv:2402.08787.

\bibitem[Kevin Meng et al.2022]{Meng2022}
Kevin Meng, David Bau, Alex J Andonian, and Yonatan Belinkov. 2022. Locating and editing factual associations in GPT. In \textit{Advances in Neural Information Processing Systems}.

\bibitem[Kevin Meng et al.2023]{Meng2023}
Kevin Meng, Arnab Sen Sharma, Alex J Andonian, Yonatan Belinkov, and David Bau. 2023. Mass-editing memory in a transformer. In \textit{The Eleventh International Conference on Learning Representations}.

\bibitem[Maximilian Mozes et al.2023]{Mozes2023}
Maximilian Mozes, Xuanli He, Bennett Kleinberg, and Lewis D. Griffin. 2023. Use of llms for illicit purposes: Threats, prevention measures, and vulnerabilities. Preprint, arXiv:2308.12833.

\bibitem[Kishore Papineni et al.2002]{Papineni2002}
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In \textit{Proceedings of the 40th annual meeting of the Association for Computational Linguistics}, pages 311--318.

\bibitem[Vaidehi Patil et al.2024]{Patil2024}
Vaidehi Patil, Peter Hase, and Mohit Bansal. 2024. Can sensitive information be deleted from LLMs? objectives for defending against extraction attacks. In \textit{The Twelfth International Conference on Learning Representations}.

\bibitem[Nicholas Pochinkov and Nandi Schoots2024]{Pochinkov2024}
Nicholas Pochinkov and Nandi Schoots. 2024. Dissecting language models: Machine unlearning via selective pruning. Preprint, arXiv:2403.01267.

\bibitem[Alec Radford et al.2019]{Radford2019}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. \textit{OpenAI blog}.

\bibitem[Rafael Rafailov et al.2023]{Rafailov2023}
Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. 2023. Direct preference optimization: Your language model is secretly a reward model. In \textit{Thirty-seventh Conference on Neural Information Processing Systems}.

\bibitem[Luca Soldaini et al.2024]{Soldaini2024}
Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, et al. 2024. Dolma: An open corpus of three trillion tokens for language model pretraining research. arXiv preprint arXiv:2402.00159.

\bibitem[Niklas Stoehr et al.2024]{Stoehr2024}
Niklas Stoehr, Mitchell Gordon, Chiyuan Zhang, and Owen Lewis. 2024. Localizing paragraph memorization in language models. Preprint, arXiv:2403.19851.

\bibitem[Sainbayar Sukhbaatar et al.2015]{Sukhbaatar2015}
Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al. 2015. End-to-end memory networks. \textit{Advances in neural information processing systems}, 28.

\bibitem[Hugo Touvron et al.2023]{Touvron2023}
Hugo Touvron, Louis Martin, Kevin R. Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, D. Bikel, Lukas Blecher, Cristian Cant\'on Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, A. Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel M. Kloumann, A. Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, R. Subramanian, Xia Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zhengxu Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288.

\bibitem[Yuanshun Yao et al.2023]{Yao2023}
Yuanshun Yao, Xiaojun Xu, and Yang Liu. 2023. Luge language model unlearning. In \textit{Socially Responsible Language Modelling Research}.

\bibitem[Yuanshun Yao et al.2024]{Yao2024}
Yuanshun Yao, Xiaojun Xu, and Yang Liu. 2024. Large language model unlearning. Preprint, arXiv:2310.10683.

\bibitem[Jingwen Ye et al.2022]{Ye2022}
Jingwen Ye, Yifang Fu, Jie Song, Xingyi Yang, Songhua Liu, Xin Jin, Mingli Song, and Xinchao Wang. 2022. Learning with recoverable forgetting. In \textit{European Conference on Computer Vision}, pages 87--103. Springer.

\bibitem[Weixiang Zhao et al.2024]{Zhao2024}
Weixiang Zhao, Yulin Hu, Zhuojun Li, Yang Deng, Yanyan Zhao, Bing Qin, and Tat-Seng Chua. 2024. Towards comprehensive and efficient post safety alignment of large language models via safety patching. arXiv preprint arXiv:2405.13820.

\end{thebibliography}

\appendix
\section{Details in Existing Unlearning Methods}
In this section, we provide a more detailed introduction to the LLM unlearning methods we used in Section 3 and 4.
\begin{itemize}
    \item Gradient Difference \cite{Yao2024}: based on Gradient Ascent, it adds a regularization term to minimize the KL divergence between the unlearned and the original LLM on a reference text dataset, thus preventing the model from catastrophic deterioration of its general capability.
    \item Direct Preference Optimization (DPO) \cite{Rafailov2023}: which maximizes the log-likelihood ratio between generating the preferred and the unfavored responses, while retaining a small shift from the original LLM predictive distribution.
    \item Negative Preference Optimization (NPO) \cite{Zhao2024}: which discards the favored responses and only minimizes the prediction probability of the unfavored answers.
    \item NPO+KL: which adds to NPO a KL divergence loss between the model's outputs before and after unlearning.
\end{itemize}

\section{Unlearning Experiments' Corpus}
Here, we present detailed information about the data used for activation patching experiments and the unlearning experiments conducted in Section 3 and 4. We select 10 well-known concepts from ConceptVectors Benchmark \cite{Hong2024} and extract 6,000 corresponding training data segments containing knowledge about the respective concepts per model from the pre-training datasets of RedPajama and Dolma. These extracted data segments are used for unlearn training of the two models respectively. For each concept, we also include ten related questions from the ConceptVectors Benchmark, along with 50 unrelated questions sampled from other unrelated concepts. These are used in Section 4 to evaluate the unlearning effectiveness from the behavior perspective on the specific concepts, as well as to assess whether the model's unrelated capabilities were affected. We have manually checked and verified that the vanilla LLaMA and OLMo models can accurately answer these selected questions, indicating that the models possess the knowledge. All the statistics and examples are shown in Table~\ref{tab:statistics} and Table~\ref{tab:examples}, respectively.

\begin{table}[h]
\centering
\caption{Statistics of the training data for the unlearning experiments on LLaMA and OLMo}
\label{tab:statistics}
\begin{tabular}{lccccc}
\toprule
Data Sources & \# selected concepts & \# of paragraphs per concept & \# of words per paragraph & \# of QA pairs & \# of unrelated QA Pairs \\
\midrule
RedPajama & 10 & 6000 & 1514.65 & 20 & 50 \\
Dolma & 10 & 6000 & 2261.25 & 20 & 50 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[h]
\centering
\caption{Example extracted data from the RedPajama and Dolma pre-training datasets}
\label{tab:examples}
\begin{tabular}{p{0.28\columnwidth}p{0.32\columnwidth}p{0.32\columnwidth}}
\toprule
Concept & Training Data Snippets Example & QA Example \\
\midrule
Harry Potter (LLaMA) & Harry potter is a series of seven fantasy novels written by British author J. K. Rowling. The novels chronicle the lives of a young wizard, Harry Potter, and his friends Hermione Granger and Ron Weasley, all of whom are students at Hogwarts School of Witchcraft and Wizardry. & Who is the author of the Harry Potter book series? What is the name of the first book in the Harry Potter series? \\
\midrule
Star Wars (LLaMA) & Star Wars is an American epic space opera media franchise created by George Lucas, which began with the eponymous 1977 film and quickly became a worldwide pop culture phenomenon. & Who is Darth Vader's son? What is the weapon used by Jedi Knights? \\
\midrule
Amazon Alexa (LLaMA) & Amazon Alexa or Alexa is a virtual assistant technology largely based on a Polish speech synthesizer named Ivona, bought by Amazon in 2013. It was first used in the Amazon Echo smart speaker and the Echo Dot, Echo Studio and Amazon Tap speakers developed by Amazon Lab126. & What year was the Amazon Alexa Voice Assistant first introduced to the public? What are some of the primary functions of Amazon Alexa Voice Assistant? \\
\midrule
eBay (OLMo) & eBay Inc. (EE-bay, often stylized as ebay) is an American multinational e-commerce company based in San Jose, California, that brokers customer to customer and retail sales through online marketplaces in 190 markets worldwide. & What is the name of Japan's most popular boy band? Who is Japan's most famous anime creator? \\
\midrule
Olympic Games (OLMo) & The modern Olympic Games or Olympics (French: Jeux olympiques) are the leading international sporting events featuring summer and winter sports competitions in which thousands of athletes from around the world participate in a variety of competitions. & When were the first modern Olympic Games held? How often are the Summer Olympics held? \\
\midrule
Diabetes (OLMo) & Diabetes mellitus, often known simply as diabetes, is a group of common endocrine diseases characterized by sustained high blood sugar levels. Diabetes is due to either the pancreas not producing enough insulin, or the cells of the body becoming unresponsive to the hormone's effects. & What is diabetes? What are the main types of diabetes? \\
\bottomrule
\end{tabular}
\end{table}

\section{More Rigorous Patching Investigation}
In Section 3, during our activation patching and parameters restoration experiments, we restore $m_i$, $W_2^{(l)}$, or $A_l$ layer by layer respectively, while avoiding intentional changes to the other two states in the unlearned model. However, for instance, restoring $A_l$ in $l$-th layer may aid in the recovery of $m_i$ in subsequent layers, ultimately leading to an improvement in KRS. Therefore, in this part of the experiment, when restoring each element layer by layer, we purposely keep the other two elements unchanged (e.g., when restoring $A_l$, we maintain the original states of $m_i$ and $W_2^{(l)}$ for both the current and subsequent layers). This approach thoroughly isolates the effects of these three different elements.

Figure~\ref{fig:krs_isolated} presents the results in this setting. We can observe the following: (1) When $W_2^{(l)}$ is restored layer by layer, its effect on improving KRS remains very small, which is consistent with prior experiments. (2) When restoring $A_l$ layer by layer and isolating its effects from the other two factors, its contribution to KRS remains insignificant, staying at a low level and only increasing to around 0.08 on LLaMA and 0.2 on OLMo in the final layers. (3) When $m_i$ is restored layer by layer, isolating its influence from the other elements, we observe a notable rise in KRS in the last three layers, reaching values as high as 0.8 or above. This supports the idea that neurons responsible for $m_i$ in the MLP components of the final layers primarily carry the unlearning effects of these fine-tuning-based methods.

\begin{figure}[h]
\centering
\framebox[0.95\columnwidth]{\parbox[c][1.8cm][c]{0.9\columnwidth}{\centering IMAGE NOT PROVIDED}}
\caption{Results of KRS on LLaMA and OLMo under three activations patching or parameters restoration settings, isolating the effects of the two others when investigating each factor individually.}
\label{fig:krs_isolated}
\end{figure}

\end{document}
=====END FILE=====