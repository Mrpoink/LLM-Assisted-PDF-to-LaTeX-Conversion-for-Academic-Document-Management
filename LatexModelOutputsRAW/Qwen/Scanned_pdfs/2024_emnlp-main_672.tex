=====FILE: main.tex=====
\documentclass[11pt, twocolumn]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{times}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{array}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{url}
\usepackage{balance}
\usepackage{parskip}
\usepackage[super]{nth}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{lipsum}

\setlength{\columnsep}{0.25in}
\setlength{\columnseprule}{0pt}

\title{Subword Segmentation in LLMs: Looking at Inflection and Consistency}
\author{Marion Di Marco$^{1}$ and Alexander Fraser$^{1,2}$ \\
$^{1}$School of Computation, Information and Technology, \\
Technische Universität München (TUM) \\
$^{2}$Munich Center for Machine Learning \\
\texttt{\{marion.dimarco, alexander.fraser\}@tum.de}}

\date{}

\begin{document}

\maketitle

\begin{abstract}
The role of subword segmentation in relation to capturing morphological patterns in LLMs is currently not well explored. Ideally, one would train large models like GPT using various segmentations and evaluate how well word meanings are captured. Since this is not computationally feasible, we group words according to their segmentation properties and compare how well a model can solve a linguistic task for these groups. We study two criteria: (i) adherence to morpheme boundaries and (ii) the segmentation consistency of the different inflected forms of a lemma. We select word forms with high and low values for these criteria and carry out experiments on GPT-4o's ability to capture verbal inflection for 10 languages. Our results indicate that in particular the criterion of segmentation consistency can help to predict the model's ability to recognize and generate the lemma from an inflected form, providing evidence that subword segmentation is relevant.
\end{abstract}

\section{Introduction}
The linguistic abilities of large language models have been studied to a large extent, with many new abilities emerging as language models become ever larger and more powerful. While areas such as lexico-syntactic understanding, text generation and reasoning abilities have received much attention, morphology has only played a minor role, despite being of great interest to the NLP community.

Conceptually, the morphological abilities of a model are tightly linked to the internal representation of subwords: LLMs do not operate on complete words, but instead, most words are broken into subword pieces for better computational efficiency and to handle unknown words. Subword segmentation strategies typically rely on frequency statistics and are not linguistically guided. This suggests that such segmentation strategies do not provide a suitable basis to fully capture morphology, e.g. \citet{park-etal-2021-morphology}; \citet{hofmann-etal-2021-superbizarre}.

Morphology relates to the construction of words, and thus represents the basis of understanding natural language. Depending on the language, morphology can play a more or less relevant role, but even in a language with rather simple morphology such as English, morphology is indispensable, whether for rare words, or for more common ones. For instance, a \textit{botanist} is a person that \textit{botanizes}, a \textit{baker}'s workplace is a \textit{bakery} and a \textit{mathematician} cares about \textit{mathematics}. Morphological processes are typically defined by general patterns, and, critically, understanding these patterns enables both the generation of novel words and the interpretation of previously unknown words.

Understanding the meaning of word parts in the larger context of a word, as well as the underlying patterns to compose new word forms is essential to fully comprehend language. This is particularly true for languages with complex morphology, where a larger proportion of information is encoded morphologically, leading to a comparatively high number of inflected forms that have insufficient coverage in the training data, and in the worst case do not occur in the training data at all. Despite the impressive language capabilities of LLMs, the impact of the underlying segmentation is not clear.

Generally, LLMs are capable of modeling morphology and accessing morphological information, but presumably not on an ideal basis, because segmentation strategies, such as WordPiece or BPE \citep{schuster-nakajima-2012-japanese,sennrich-etal-2016-neural} rely on frequency-based heuristics that do not optimally capture morphological patterns.

In the following, we study two criteria, adherence to morpheme boundaries and segmentation consistency of inflected forms of a lemma. We first analyze how well these criteria are met in existing LLMs, and then investigate to what extent words which have high or low values for these two criteria affect the performance of the LLM on a linguistically interesting task.

\subsection{Segmentation Problems and Criteria}
There is no obvious way to determine the quality of sub-word segmentation. A simple and straightforward idea is the number of splits per word, with the underlying assumption that fewer splits suggest a ``good'' segmentation in contrast to a segmentation into many very short pieces. While this assumption is intuitively plausible, and an overly aggressive segmentation likely results in basically meaningless pieces, the mere number of segments does not take into account the ability to generalize and how the segmentation of one word relates to the segmentation of related words of the same inflection paradigm. For example, consider GPT-4's segmentation of different forms of the German verb (ein)\textit{pflanzen}: `to plant (in)':

\begin{center}
\begin{tabular}{lll}
word & GPT-4o & ling. sound \\
\midrule
einpfanzen & einlpflanzlen & ein-pflanz-en \\
eingepfanzt & einlgelpflanzlt & ein-ge-pflanz-t \\
pfanzte & pflanzlte & pflanz-te \\
pfanzen & pflanzlen & pflanz-en \\
pfanztet & pflanzltet & pflanz-tet \\
\end{tabular}
\end{center}

The segmentation does not adhere to linguistic boundaries as, for example, neither the particle \textit{ein} nor the inflectional suffixes are separated from the verb stem \textit{pflanz} (plant). Another problem is that of inconsistency: inflectional variants of the same word are split differently, and thus lead to different internal representations. The table shows a linguistically sound segmentation into verb stem and the respective inflectional morphemes (the particle \textit{ein-}, \textit{-ge-} as part of the past participle, and different inflectional suffixes). A segmentation as proposed above is not realistic, as a comparatively small vocabulary needs to accommodate a high amount of words of different languages, and thus, lexical units cannot always be preserved. However, we can still formulate conceptually language-independent criteria, namely (i) a consistent representation for variants of closely related words and (ii) the adherence to word or morpheme boundaries; any further segmentation between these points becomes, theoretically, less relevant as the subwords of a complete word can be recomposed to obtain its representation.

Intuitively, the advantages of a linguistically sound segmentation are obvious: adherence to morpheme boundaries enables an internal representation that can be shared across all observed occurrences. Similarly, the separation of inflectional affixes aims at making generalization across inflectional variants easier, which is particularly important for morphologically rich languages. A consistent representation of related words tries to achieve the same effects and is a more robust formulation: while a linguistically sound segmentation is per design consistent, the slightly simpler criterion of consistent segmentation is language-independent and less resource-intensive.

While there is a growing interest in the morphological abilities of LLMs, there is no data on the segmentation quality of existing large-scale LMs: in this work, we (i) study the segmentation of 10 different languages in GPT-4o with respect to the two criteria outlined above and (ii) assess the impact of segmentation quality by contrasting the performance of words grouped according to these criteria on the tasks of lemma prediction and the generation of inflected forms.

\section{Related Work}
There is a large body of research concerning the representation of the training data of language models and translation systems: while the typical segmentation strategies are frequency-based such as WordPiece or BPE \citep{schuster-nakajima-2012-japanese,sennrich-etal-2016-neural}, there is also evidence that these segmentation approaches are not optimal for morphologically rich languages and fail to fully capture the morphological complexities of words \citep{klein-tsarfaty-2020-getting,park-etal-2021-morphology}.

\citet{hofmann-etal-2021-superbizarre} show that a linguistically grounded segmentation can improve a model's performance. \citet{hou-etal-2023-effects} explore the effect of subword segmentation by training BERT and GPT models on different segmentation algorithms, namely BPE and two morphological segmentation strategies. Their experiments show that morphologically guided segmentation leads to lower perplexity and faster convergence during training; their models trained on morphologically segmented data reach a similar or better performance than models trained on BPE, depending on the task. Furthermore, they find that models of smaller size trained on morphologically segmented data can perform comparably to models of larger size trained with BPE. While not specifically studying the impact of segmentation, but instead the multilingual capabilities of English-centric LLMs, \citet{armengol-estape-etal-2022-multilingual} assume that the quality of subword segmentation plays a part in the performance for languages different from English, as the segmentation is mostly based on the predominant English vocabulary and thus not representative of many other languages. Their findings indicate that languages with more subword tokens per word tend to perform worse.

There are many variants of language-specific PLMs trained on representations to accommodate the properties of a language, (e.g. \citet{antoun-etal-2020-arabert}; \citet{nzeyimana-rubungo-2022-kinaybert}), mostly in a monolingual setting. \citet{jabbar-2024-morphpiece} proposes a linguistically-informed representation of the training data that relies on canonical forms instead of concatenable pieces. This makes the generation step less straightforward as the pieces cannot just be concatenated, but have to be reconstructed into inflected forms. The idea to combine linguistically guided segmentation with frequency-based segmentation has also been applied to machine translation, for example \citet{tamchyna-etal-2017-modeling}; \citet{banerjee-bhattacharyya-2018-meaningless}; \citet{mager-etal-2022-bpe}, and often found to be preferable to just frequency-based segmentation. A further task linked with the representation of subwords is that of morphological re-inflection (e.g. \citet{kann-etal-2017-neural}), where an inflected form needs to be generated for a given pair of word and morphological features.

There is a growing interest in the quality of the underlying segmentation: \citet{beinborn-pinter-2023-analyzing} look at the semantic plausibility of subword tokens; the segmentation strategy in \citet{yehezkel-pinter-2023-incorporating} aims at incorporating context information to obtain more meaningful splits. With regard to morphology, \citet{weissweiler-etal-2023-counting} study the ability to create inflected forms for nonce words for typologically different languages, finding that GPT does not perform as well as systems specifically trained for morphological tasks. \citet{soler-etal-2024-impact} study the impact of segmentation on the quality of word representation by comparing words that are segmented with those having a dedicated embedding, i.e. unsplit words, in a word similarity task. In general, they find that the representation of split words is often worse than for non-split words. Interesting in the context of our work, their results show that a morphologically sound segmentation tends to lead to a better representation. With regard to over-splitting, their findings are mixed, but indicate that for split words, a higher number of tokens does not necessarily decrease representation quality.

\citet{beinborn-pinter-2023-analyzing} and \citet{weissweiler-etal-2023-counting} propose to use the number of splits per word as an indicator for splitting quality, assuming that few splits per word suggest a ``good'' segmentation in contrast to a segmentation into many short pieces. To the best of our knowledge, there is no study that looks at segmentation criteria as outlined in this paper in combination with a linguistic task.

\section{Methodology}
We study the quality of GPT-4o's segmentation for 10 languages (English, French, German, Spanish, Italian, Portuguese, Finnish, Swedish, Czech, Hungarian). We look at the segmentation quality from two angles: first, we examine how well inflection suffixes are separated from the stem of the word, i.e., a linguistically-oriented criterion. Second, we look at the segmentation consistency, i.e. whether all words from an inflection paradigm are segmented in a cohesive way. We assess whether the segmentation has an impact on the model performance.

In previous work on subword segmentation, either on language modeling or on machine translation, the typical approach is to compare the performance of a model trained on a baseline subword segmentation with that of a model trained on a contrastive segmentation. Working with an LLM such as GPT, this strategy is not feasible due to the immense expense to train such a model. Instead, we compare the outcome on a downstream task for words of different levels of segmentation quality, by selecting words with high and low values according to the criteria outlined previously. Assuming that (i) the segmentation quality has an effect on the particular task and that (ii) the proposed criteria are suitable to capture the segmentation quality, we should be able to see a performance difference between the two sets.

The linguistic task is that of predicting the lemma of an inflected verb form, which is applicable to every language in our data set; in a second experiment, we also generate inflected forms given the lemma and a morphological tag. We chose verbal morphology as it provides more variety than the inflection of nouns and adjectives.

\subsection{Data Set}
We use the morphological database in MorphyNet \citep{batsuren-etal-2021-morphynet}, which contains inflectional and derivational morphology for 15 languages. For our experiments, we only consider languages with Latin script and selected 10 languages of different language families. To annotate the separation of inflection suffixes and stem, we use MorphyNet's inflectional information, where entries for an inflected form list the lemma, the morphological features and the canonical representation of the morphological segmentation (cf. Table~\ref{tab:morphy_example}).

\begin{table}[h]
\centering
\caption{Inflectional morphology in MorphyNet for the Czech word \textit{složený} (`composed'). The segments correspond to the morphological features.}
\label{tab:morphy_example}
\begin{tabular}{lll}
\toprule
lemma & form & features \\
\midrule
složit & složený & V;PFV;V.PTCP;PASS;FEM;PL \\
\bottomrule
\end{tabular}
\end{table}

Some entries in the data set do not correspond to modern standard spelling (for example \textit{poynted} as English verb); thus we applied a filtering step based on two conditions: first, the lemma of the word needs to occur in a dictionary\footnote{Dictionaries were obtained from \url{https://www.dict.cc}.} and second, the inflected word form needs to occur at least once in a text corpus for the respective language. For this purpose, we obtained a Wikipedia dump for every language. The filtering is designed to be rather conservative such that the word forms are valid forms of contemporary language, which is important when assessing the impact of the segmentation quality, where we want the test set to be as clean as possible. Table~\ref{tab:lang_counts} (in the appendix) shows the number of entries after the filtering.

Additionally, the Wikipedia data is used to get an idea about a word's frequency. While the frequencies in this text corpus do not correspond to those in the pre-training data, they still allow to approximately distinguish between high-frequency and low-frequency words.

\section{Separation of Stem and Inflection}
In this first experiment, we apply a linguistically-oriented criterion and study whether and how inflection suffixes are separated from the stem. We start from the hypothesis that a clean separation of inflectional suffixes allows for a better representation with regard to generalization due to separating the lexical content in the stem from the morpho-syntactic information in the inflectional parts.

We define five categories, as illustrated in Table~\ref{tab:segm_categories}, to describe the segmentation status of a word. Given the gold analysis, we compare how the word is segmented in the LM. The five categories are defined as follows:

\begin{itemize}
\item \textbf{EXACT}: the word is split into exactly two parts, the stem and the inflection suffix
\item \textbf{SINGLE}: the inflection suffix consists of one piece; the stem is further split
\item \textbf{CONCAT}: the inflection suffix consists of several pieces; the stem is or is not further split
\item \textbf{OVERLAP}: there is no clear separation between the stem and the inflectional suffix
\item \textbf{UNSPLIT}: the word remained unsplit
\end{itemize}

The categories EXACT, SINGLE and CONCAT all met the condition of a split at the stem-inflection boundary, for the categories OVERLAP and UNSPLIT, the stem cannot be clearly separated from the stem. In practice, we find that the category UNSPLIT is comparatively infrequent, with a majority of the words falling into the groups EXACT, SINGLE, CONCAT and OVERLAP.

\begin{table}[h]
\centering
\caption{Segmentation categories derived from MorphyNet for French verbs (inflectional suffixes are highlighted).}
\label{tab:segm_categories}
\begin{tabular}{llll}
\toprule
lemma & form & morph. features & GPT-4o-segm. \\
\midrule
commander & commandait & V;IND;PST;IPFV;3;SG & command ait \\
commander & canalisent & V;IND;PRS;3;PL & can alis ent \\
commander & commanderaient & V;COND;3;PL & command era ielt \\
commander & commandaient & V;IND;PST;IPFV;3;PL & comm anda ient \\
commander & commande & V;IND;PRS;1;SG & commande \\
\midrule
& & & category \\
\cmidrule{4-4}
& & & EXACT \\
& & & SINGLE \\
& & & CONCAT \\
& & & OVERLAP \\
& & & UNSPLIT \\
\bottomrule
\end{tabular}
\end{table}

Figure~\ref{fig:segm_dist} shows the distribution of the segmentation categories for verbs. Overall, the category OVERLAP is dominant in most languages. This is particularly striking for English, which has the highest amount of training data by far, while also being a morphologically poor language. The English inflectional suffixes are generally rather short (e.g. \textit{-s} for the plural of nouns or the third person for verbs), but many subword pieces tend to be longer (\textit{-izing}, \textit{-ated}, \textit{-lated}, \textit{-ating}, \textit{-ized}, \ldots). While some of them are close to morphemes, the segmentation is not systematic in a linguistic sense.

\begin{figure}[h]
\centering
\includegraphics[width=\columnwidth]{figures/segm_dist.pdf}
\caption{Segmentation categories per language.}
\label{fig:segm_dist}
\end{figure}

\subsection{Task: Verb Lemma Prediction}
In this experiment we investigate whether segmentation at the boundary between stem and inflectional suffixes has an effect on the task of predicting the lemma. As the frequency might be a relevant factor, we define 3 frequency ranges (cf. Table~\ref{tab:lemma_pred}) based on the observed frequency in the Wikipedia data. We compare verbs of the splitting category OVERLAP with verbs where inflection and stem are clearly separated (EXACT, SINGLE, CONCAT), with the hypothesis that verbs of the set OVERLAP should perform worse than verbs of the set NO OVERLAP, as a clear separation between stem and inflection conceptually allows for a better generalization, in particular for words of lower frequency.

We randomly select 500 verbs per groups; as common irregular verbs are typically listed in abundance in grammatical resources and thus are likely leaked in the pre-training data, we excluded the ten most common irregular verbs (according to GPT-4o) per language. Furthermore, we excluded verb forms that have the same surface form as the lemma, as the frequency of the word used as inflected form might differ considerably from the frequency of the form used as lemma.

We use the model GPT-4o with a relatively low temperature of 0.1 for a more stable outcome; the prompt is formulated in English for all languages:

\begin{quote}
Answer with one word.\\
The lemma of the (French/...) verb ``v'' is
\end{quote}

The prompt clearly states that we look for the verb lemma and also explicitly mentions the target language, which is important in case of an ambiguous part-of-speech and verbs that can occur in different languages, for example \textit{mentons} which can also be an inflected form of the French verb \textit{mentir} (to lie), in addition to the English form.

Table~\ref{tab:lemma_pred} shows the results grouped according to language families: there is no clear difference in the performance between the two sets, indicating that the separation of inflectional suffixes and stem is not a sufficient criterion for segmentation quality. Only for Italian, we can observe a better performance for the NO OVERLAP set.

A general factor might also be that the OVERLAP set represents the majority group in most languages, and thus, even in combination with frequency information, is not fine-grained enough to be discriminative of segmentation quality, while at the same time, the condition to segment at the inflection boundary is hard to meet, especially when considering that the segmentation has to work for many languages at once. This result does not necessarily say that linguistically sound segmentation in general is not better, but we can only conclude that the criterion of segmentation at the inflection boundary is not sufficient to measure segmentation quality.

\begin{table*}[h]
\centering
\caption{Number of correctly predicted lemmas in a set of 500 randomly selected verb forms. *: the no-overlap system is significantly better than the overlap system (Chi-square test with a significance level of $\alpha=0.05$).}
\label{tab:lemma_pred}
\begin{tabular}{lcccccccccc}
\toprule
range & segm. & EN & DE & SV & FR & IT & SP & PT & FI & HU & CS \\
\midrule
low: $f < 10$ & OVERLAP & 485 & 469 & 483 & 488 & 483 & 483 & 496 & 491 & 455* & 490 \\
& NO OVERLAP & 491 & 494 & 493 & 497 & 496 & 493 & 468 & 360 & 370 & 483 \\
mid: $10 < f < 500$ & OVERLAP & 493 & 495 & 493 & 494 & 487 & 491 & 489 & 470* & 491 & 489 \\
& NO OVERLAP & 493 & 494 & 496 & 462 & 471 & 394 & 398 & 495 & 481 & 493 \\
high: $f > 500$ & OVERLAP & 499 & 496 & 496 & 498 & 470 & 489 & 494 & 485* & 496 & 488 \\
& NO OVERLAP & 494 & 493 & 495 & 489 & 404 & 415 & 397 & 408 & 493 & 458 \\
\bottomrule
\end{tabular}
\end{table*}

\section{Segmentation Consistency}
The criterion in the previous section was based on linguistic well-formedness; here, we look at segmentation quality from the angle of consistency, which also aims at capturing generalization abilities, but is formulated more robustly. We pursue the question whether a consistent segmentation across the inflected forms of a lemma provides a better basis for the representation than an inconsistent segmentation. The underlying assumption is that an internally coherent representation of different surface realizations of the same word should result in an overall better representation of that word, and thus provide a better basis for generalization and the modeling of potentially unseen words. Table~\ref{tab:consistency_examples} shows some examples, ranging from a generally consistent representation of the stem part of the verb to a largely inconsistent segmentation.

\begin{table}[h]
\centering
\caption{Examples for different segmentation consistencies in verb forms (DE, IT). The lemma is in bold.}
\label{tab:consistency_examples}
\begin{tabular}{ll}
\toprule
German & Italian \\
\midrule
\textbf{dramatisieren}d & \textbf{vincere} \\
dramatis ierend & vin ciamo \\
dramatis ieren & vin ci \\
dramatis ierten & vin ce \\
dramatis ierte & vin cono \\
dramatis iert & vin cesse ro \\
dramatis iertet & vin to \\
\bottomrule
\end{tabular}
\end{table}

Ideally, a good segmentation should provide a consistent splitting of the stem part, with more necessary variation towards the end of the word. We use the Overlap Coefficient to measure the similarity between the sets of segments of two different verb forms, which is defined as the size of the intersection divided by the size of the smaller one of the two sets:

\begin{equation}
\text{overlap}(A, B) = \frac{|A \cap B|}{\min(|A|, |B|)}
\end{equation}

The scores range between 0 (no overlap) and 1 (perfect match). A particular characteristic of this metric is that if $A$ is a subset of $B$, then the coefficient is 1: this has the effect of comparing rather the segments of the stem part while disregarding suffixes that add to the overall length of the word, assuming that the stem part does not change much, whereas we expect comparatively more variation in the suffixes. In contrast, the Jaccard index (ratio of intersection over union) might be less practical when the two compared forms are of different lengths, and we do not expect a subset to be similar.

Below are some examples for forms of the Italian verb \textit{sorprendere} (to surprise), and the respective overlap scores between lemma and form:

\begin{itemize}
\item lemma \textit{sorprendere}, form \textit{sorprenderebbe}: overlap = 1
\item lemma \textit{sorprendere}, form \textit{sorprendiamo}: overlap = 0.75
\item lemma \textit{sorprendere}, form \textit{sorprese}: overlap = 0.5
\end{itemize}

The splitting in the first line is linguistically questionable, but the lemma's segments are an exact subset of the inflected form's segments, which is good in terms of consistency (overlap=1). For the other two words, the segments only partially match between the forms, and thus have a lower score.

To obtain the overlap coefficient of an inflection paradigm, we computed the average of the overlap of every possible pair of forms. Figure~\ref{fig:overlap_dist} shows an overview for all languages: for most languages, average overlap scores of 0.5--0.7 are dominant.

\begin{figure}[h]
\centering
\includegraphics[width=\columnwidth]{figures/overlap_dist.pdf}
\caption{Distribution of overlap scores per inflection paradigm for verbs. The scores are rounded to the nearest decimal, resulting in ranges of 0.1.}
\label{fig:overlap_dist}
\end{figure}

In the following, we look at two variants of the lemma prediction task: (i) the average overlap of a verb paradigm, and (ii) the segmentation similarity of only the verb form and the lemma while also distinguishing between inconsistencies at the beginning vs. elsewhere in the word.

\subsection{Paradigm Segmentation Overlap}
In this experiment, we contrast verb forms from paradigms with high vs. low overlap coefficients: The underlying assumption is that the internal representation of verb forms with a less overlapping segmentation is sub-optimal as the forms cannot be well linked, whereas verbs with a high overlap coefficient are expected to be better connected within the paradigm. A further factor is the similarity of the segmentation of the inflected form to that of the lemma, i.e. the expected answer: a segmentation similar to the lemma is likely beneficial, thus further adding to the high/low overlap scenario. Note that we do not always have the full inflection paradigm of a verb at our disposition due to limitations of the dataset and our various filtering steps in the pre-processing; as inflection paradigm we thus define all observed forms of a verb lemma (with a minimum number of 5 forms per observed paradigm). We apply the following criteria to select 200 verbs per group:

\begin{itemize}
\item \textbf{Average paradigm overlap}: select verb paradigms with the highest/lowest average overlap coefficients per language
\item \textbf{Overlap to lemma}: from those paradigms, select one form each with the highest/lowest overlap to the lemma (select at random if there are several forms with equal overlap)
\item \textbf{Frequency}: additionally, we look at two frequency bands and consider forms with frequencies below 10 or above 500.
\end{itemize}

Based on this definition of high/low overlap, we select sets for the tasks of lemmatization and generation of inflected forms.

\subsubsection{Lemmatization Task}
The experimental settings are identical to that in Section~4.1. Table~\ref{tab:paradigm_lemma} shows the result: There is a general tendency for the low-overlap sets to perform worse; this effect is most pronounced for Hungarian and low-frequency Finnish words.

With regard to errors, the proposed lemma is often orthographically close (for example, (DE) \textit{ordern}/\textit{ordnen} (to order/organize)). We also observed errors traceable at the semantic level, for example (DE) \textit{lost} ((he) casts) $\rightarrow$ \textit{verlieren} (to lose) instead of \textit{losen}, presumably due to the (unsplit) form \textit{lost}, i.e. the past participle of \textit{to lose}.

\begin{table}[h]
\centering
\caption{Number of correctly predicted lemmas ($N=200$) contrasting segmentation consistency. * marks significant difference between high/low overlap sets (Chi-square test with a significance level of $\alpha=0.05$).}
\label{tab:paradigm_lemma}
\begin{tabular}{lcccccccc}
\toprule
& DE & SV & FR & IT & ES & PT & FI & HU & CS \\
\midrule
freq $>$ 500 & & & & & & & & & \\
highOverlap & 193* & 195 & 199 & 197 & 198 & 194 & 199 & 198 & 194 \\
lowOverlap & 179* & 179* & 179* & 187* & 183* & 195 & 188* & 194 & 199 \\
\midrule
freq $<$ 10 & & & & & & & & & \\
highOverlap & 197 & 197 & 199 & 200 & 200 & 200 & 200 & 200 & 199 \\
lowOverlap & 190 & 177* & 190* & 181* & 188* & 197 & 149* & 158 & 176* \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Generation of Inflected Forms}
The generation task consists in finding the correctly inflected form given the lemma and a tag specifying the morphological features, which is more challenging than the previous task of predicting the lemma of an inflected form. One difficulty is that of the prompt formulation and the terminology used for the respective grammatical features. To keep the prompting as simple as possible and equal across languages, the prompt is simply derived from the morphological tag provided by MorphyNet, where the abbreviations are replaced by their full terms, according to the UniMorph documentation (cf. Table~\ref{tab:tags} in the appendix). We compare a zero-shot and a one-shot variant, where the example is randomly selected from a set of 50 additional items from the same category (high/low overlap) and frequency range. The (one-shot) prompt format is as follows:

\begin{quote}
Generate the inflected form given a German lemma and the morphological features. Answer with one word.\\
lemma: ``wagen'', tag: verb, indicative, past, first person, plural\\
form: ``wagten''\\
lemma: ``biegen'', tag: verb, indicative, present, third person, singular
\end{quote}

Table~\ref{tab:generation} shows the results: for the zero-shot variant, the sets of less consistently split verbs perform worse, in particular for the low-frequency words. Overall, the one-shot variant does not improve much over the zero-shot variant, but reduces the difference between the two groups of consistently vs. inconsistently split verbs. These results indicate that the segmentation consistency is relevant, in particular for low-frequency words.

\begin{table*}[h]
\centering
\caption{Number of correctly generated forms ($N=200$) contrasting segmentation consistency. * marks significant difference between high/low overlap sets (Chi-square test with a significance level of $\alpha=0.05$).}
\label{tab:generation}
\begin{tabular}{lcccccccccc}
\toprule
& DE & SV & FR & IT & ES & PT & FI & HU & CS \\
\midrule
freq $>$ 500 & & & & & & & & & \\
highOverlap zero-shot & 191 & 190 & 196 & 193 & 200 & 200 & 186 & 200 & 182 \\
lowOverlap zero-shot & 189 & 175* & 184* & 191 & 191* & 188* & 180 & 182* & 179 \\
highOverlap one-shot & 191 & 194 & 194 & 189 & 200 & 200 & 186 & 200 & 189 \\
lowOverlap one-shot & 185 & 185 & 187 & 195 & 191* & 197 & 180 & 185* & 185 \\
\midrule
freq $<$ 10 & & & & & & & & & \\
highOverlap zero-shot & 188 & 131* & 171* & 160* & 180 & 174 & 178 & 181 & 180 \\
lowOverlap zero-shot & 166* & 161* & 169* & 130* & 156* & 144* & 122* & 163* & 148* \\
highOverlap one-shot & 189 & 175 & 184 & 195 & 199 & 193 & 185 & 181 & 180 \\
lowOverlap one-shot & 172* & 140* & 172* & 177* & 176* & 122* & 163* & 148* & 148* \\
\bottomrule
\end{tabular}
\end{table*}

\subsection{Positional Segmentation Differences}
Here, we focus on consistent segmentation between the verb form and the lemma, and further assume that consistent segmentation at the beginning of the word, i.e. the lexical part of the word, is more important than at the end of the word, where the model is likely more robust due to observed variations with different inflections. We apply the following conditions to select verbs for three contrasting sets:

\begin{itemize}
\item \textbf{Similarity}: verb forms with a similarity to the lemma below 0.35 or above 0.7
\item \textbf{Position of difference}: verb forms with low similarity are grouped into subsets where the first subword token is the same for both words (same\_1st) or different (diff\_1st)
\item \textbf{Frequency}: forms with a frequency below 10 (``low-freq'') or above 50 (``high-freq'')
\end{itemize}

Table~\ref{tab:positional} shows the results: while we see the hypothesis that inconsistent segmentation at the beginning of a word has a negative effect confirmed, though not for all languages, we also have the somewhat surprising result that a matching first token, even with an otherwise low similarity, performs as well as the high-similarity group. One possible interpretation is that the relevant semantic information is already mostly contained in this first token.

\begin{table}[h]
\centering
\caption{Number of correctly predicted verb lemmas out of $N=100$, contrasting positional segmentation differences. * marks significant difference between high/low similarity sets (Chi-square test with a significance level of $\alpha=0.05$).}
\label{tab:positional}
\begin{tabular}{lcccccccccc}
\toprule
& DE & SV & FR & IT & ES & PT & FI & HU & CS \\
\midrule
high freq & & & & & & & & & \\
HighSim & 100 & 99 & 100 & 100 & 100 & 100 & 94 & 84 & 98 \\
LowSim diff\_1st & 97 & 86* & 93* & 90* & 92* & 99 & 91 & 43* & 97 \\
LowSim same\_1st & 97 & 95 & 100 & 98 & 99 & 99 & 96 & 59* & 97 \\
\midrule
low freq & & & & & & & & & \\
HighSim & 99 & 98 & 99 & 100 & 91 & 96 & 100 & 78 & 99 \\
LowSim diff\_1st & 94 & 89* & 85* & 96 & 10* & 100 & 95 & 41* & 89* \\
LowSim same\_1st & 92/95 & 100 & 99 & 98 & 100 & 100 & 100 & 49* & 94 \\
\bottomrule
\end{tabular}
\end{table}

\section{Conclusion and Future Work}
We proposed two criteria to capture the quality of subword segmentation in LLMs and evaluated to what extent words which score high or low for these criteria affect the performance of the LLM on a linguistic task for ten diverse languages. Both criteria are targeted at the generalization abilities of the language model; the first one is more linguistically inspired and aims at a clear separation of stem and inflectional suffixes, whereas the second one rewards consistent segmentation within an inflection paradigm. The design of the criteria is in principle language-independent, but requires language-specific information: morphologically annotated data for the first one, and information about sets of inflection paradigms for the second one.

The results of our experiments indicate that the subword segmentation does influence the behaviour of the model. In particular for the criterion of segmentation consistency, we could observe a better performance for the sets with higher segmentation overlap. In contrast, the morpheme-boundary criterion was found to be less suitable. With a view to linguistic resources, the consistency-based criterion departs from a more minimal point, as no morphological analysis is needed other than knowing the inflection paradigm of a word.

The underlying segmentation is not only relevant for the representation within one language, but might also improve the multilingual competence of a model. Conceptually, when adhering to morpheme boundaries, the resulting segmentation can separate between lexical parts and functional components, which might benefit multi-lingual structure learning; for instance, through supporting the learning of lexical equivalents of words sharing the same (or close) orthographic forms of the stem with different inflections, such as \textit{-ic}$_{\text{EN}}$, \textit{-isch}$_{\text{DE}}$, \textit{-ique}$_{\text{FR}}$, \textit{-ico}$_{\text{ES}}$, \textit{-ic}$_{\text{PT}}$, \textit{-ik}$_{\text{SV}}$, \textit{-ik}$_{\text{CS}}$, \textit{-i}$_{\text{HU}}$, \textit{-inen}$_{\text{FI}}$. Similarly, inflectional affixes contain contextual information such as tense or number, and an accessible and consistent representation can potentially contribute to the learning of syntactic structure across languages.

Finally, with view to the current efforts to include less-resourced languages into LMs, segmentation strategies that promote a consistent representation and maximize the generalization abilities are a relevant and interesting research field.

\section{Limitations}
In this section, we briefly discuss the limitations of the presented work.

\subsection{Linguistic Tasks}
An obvious limitation are the simple linguistic downstream tasks that we used to evaluate the performance of the model. In our experiments, we mainly focus on predicting the lemma of a given verb form, which is arguably not the most exciting task, but has the advantage of being applicable to all languages in our data set. We extend this task to the generation of inflected forms based on lemma and morphological tags, which is more challenging and thus might be more affected by the underlying segmentation quality.

A general issue with the generation task is that it is to a certain extent language-dependent due to the different morphological features per language, and consequently the optimal terminology to describe these features in the prompt. This makes this task likely more dependent on the prompt formulation than the lemmatization task.

In a certain way, both the prediction of lemmas and the generation of inflected forms are not necessarily natural tasks for the LLM. However, to better understand the impact of the underlying segmentation, we wanted a direct link between the investigated form and the linguistic task. This is more difficult to model in more complex tasks such as translation where more factors come into play.

We did not explore several prompt formulations or simple and straightforward ones. Similarly, we did not explore different prompt languages but kept the English prompt for all investigated languages. As we are primarily interested in the performance difference between the sets of differently segmented words, but less in obtaining the best absolute performance, to keep the conditions as equal as possible, we only looked at zero-shot scenarios in most experiments.

\subsection{Non-Concatenative Morphology}
With regard to linguistic soundness in segmentation, a critical factor that cannot be satisfactorily modeled by subword segmentation is non-concatenativity, such as irregular word forms (e.g. \textit{go}--\textit{went}), but also semi-regular variations such as an Umlaut in specific contexts, such as (DE) \textit{Apfel} $\rightarrow$ \textit{Äpfel} (``apple'' $\rightarrow$ ``apples''). To fully capture these phenomena, one approach that has been proposed for both language modeling and machine translation is the representation of canonical forms in combination with morpho-syntactic information (e.g. \citet{tamchyna-etal-2017-modeling}, \citet{antoun-etal-2020-arabert}; \citet{nzeyimana-rubungo-2022-kinaybert}, \citet{jabbar-2024-morphpiece}), which however needs an additional step to generate inflected forms when generating, namely the generation of inflected forms based on the canonical representation in combination with the respective morphological features, which is not trivial.

In our study, we mostly ignored the problems of non-concatenative operations, in particular in the second part focusing on the segmentation consistency within a verb paradigm where phenomena such as stem changes between lemma and inflected form necessarily lead to lower segmentation similarity. Our main reason is that regular segmentation strategies operating on surface words cannot handle such phenomena, and thus a linguistically sound modeling is out of reach with this method.

\subsection{Languages and their Representation in the Training Data}
Finally, the amount of training data per language is also likely to have an influence on the segmentation quality for the respective languages, as suggested in \citet{armengol-estape-etal-2022-multilingual}. With English making up the majority of the training data for GPT models, we would assume a distribution of subword tokens that best represents English, but not necessarily other languages, in particular if a language's words differ considerably from English. While this is not a central point of our investigation of segmentation criteria in general, finding an optimal representation across languages is nonetheless a relevant factor that deserves attention in segmentation strategies for multilingual language models.

\section*{Acknowledgements}
The work was supported by the European Research Council (ERC) under the European Union's Horizon Europe research and innovation programme (grant agreement No. 101113091) and by the German Research Foundation (DFG; grant FR 2829/17-1).

\bibliographystyle{acl_natbib}
\bibliography{refs}

\appendix

\section{Data}
Table~\ref{tab:lang_counts} lists the number of inflected verb forms per language in our data set.

\begin{table}[h]
\centering
\caption{Overview of the number of verbs (inflected forms) per language after the filtering step.}
\label{tab:lang_counts}
\begin{tabular}{lc}
\toprule
Lang & Verbs \\
\midrule
EN & 23342 \\
FR & 57650 \\
DE & 21567 \\
ES & 15924 \\
IT & 49349 \\
PT & 27727 \\
FI & 22152 \\
SV & 14432 \\
CS & 20029 \\
HU & 37780 \\
\bottomrule
\end{tabular}
\end{table}

\section{Tags and Abbreviations}
Table~\ref{tab:tags} lists the abbreviations used in MorphyNet's tags and the respective feature names used in the prompt formulation, based on the documentation in \url{https://unimorph.github.io/doc/unimorph-schema.pdf}.

\begin{table}[h]
\centering
\caption{Abbreviations and features used in the generation experiment.}
\label{tab:tags}
\begin{tabular}{ll}
\toprule
Abbreviation & Feature \\
\midrule
V & verb \\
V.PTCP & participle \\
IND & indicative \\
SBJV & subjunctive \\
IMP & imperative \\
COND & conditional \\
POT & potential \\
PST & past \\
PRS & present \\
FUT & future \\
SG & singular \\
PL & plural \\
1 & first person \\
2 & second person \\
3 & third person \\
PFV & perfective \\
IPFV & imperfective \\
PROG & progressive \\
PRF & perfect \\
FORM & formal \\
INF & informal \\
\bottomrule
\end{tabular}
\end{table}

\end{document}
=====END FILE=====

=====FILE: refs.bib=====
@inproceedings{park-etal-2021-morphology,
  title = {Morphology Matters: A Multilingual Language Modeling Analysis},
  author = {Park, Hyunji Hayley and Zhang, Katherine J. and Haley, Coleman and Steimel, Kenneth and Liu, Han and Schwartz, Lane},
  booktitle = {Transactions of the Association for Computational Linguistics},
  volume = {9},
  pages = {261--276},
  year = {2021}
}

@inproceedings{hofmann-etal-2021-superbizarre,
  title = {Superbizarre is not superb: Derivational morphology improves {BERT}'s interpretation of complex words},
  author = {Hofmann, Valentin and Pierrehumbert, Janet and Sch{\"u}tze, Hinrich},
  booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  pages = {3594--3608},
  year = {2021}
}

@inproceedings{schuster-nakajima-2012-japanese,
  title = {Japanese and {K}orean Voice Search},
  author = {Schuster, Mike and Nakajima, Kaisuke},
  booktitle = {2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages = {5149--5152},
  year = {2012}
}

@inproceedings{sennrich-etal-2016-neural,
  title = {Neural Machine Translation of Rare Words with Subword Units},
  author = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
  booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages = {1715--1725},
  year = {2016}
}

@inproceedings{klein-tsarfaty-2020-getting,
  title = {Getting the \#\#life out of living: How adequate are word-pieces for modelling complex morphology?},
  author = {Klein, Stav and Tsarfaty, Reut},
  booktitle = {Proceedings of the 17th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology},
  pages = {204--209},
  year = {2020}
}

@inproceedings{armengol-estape-etal-2022-multilingual,
  title = {On the multilingual capabilities of very large-scale English language models},
  author = {Armengol-Estap{\'e}, Jordi and de Gibert Bonet, Ona and Melero, Maite},
  booktitle = {Proceedings of the Thirteenth Language Resources and Evaluation Conference},
  pages = {3056--3068},
  year = {2022}
}

@inproceedings{antoun-etal-2020-arabert,
  title = {{AraBERT}: Transformer-based Model for {A}rabic Language Understanding},
  author = {Antoun, Wissam and Baly, Fady and Hajj, Hazem},
  booktitle = {Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with a Shared Task on Offensive Language Detection},
  pages = {9--15},
  year = {2020}
}

@inproceedings{nzeyimana-rubungo-2022-kinaybert,
  title = {{K}inya{BERT}: a morphology-aware {K}inyarwanda language model},
  author = {Nzeyimana, Antoine and Niyongabo Rubungo, Andre},
  booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages = {5347--5363},
  year = {2022}
}

@inproceedings{jabbar-2024-morphpiece,
  title = {{Morphpiece}: A linguistic tokenizer for large language models},
  author = {Jabbar, Haris},
  year = {2024},
  eprint = {2307.07262},
  archivePrefix = {arXiv}
}

@inproceedings{tamchyna-etal-2017-modeling,
  title = {Modeling target-side inflection in neural machine translation},
  author = {Tamchyna, Ale{\v{s}} and Weller-Di Marco, Marion and Fraser, Alexander},
  booktitle = {Proceedings of the Second Conference on Machine Translation},
  pages = {32--42},
  year = {2017}
}

@inproceedings{banerjee-bhattacharyya-2018-meaningless,
  title = {Meaningless yet meaningful: Morphology grounded subword-level {NMT}},
  author = {Banerjee, Tamali and Bhattacharyya, Pushpak},
  booktitle = {Proceedings of the Second Workshop on Subword/Character Level Models},
  pages = {55--60},
  year = {2018}
}

@inproceedings{mager-etal-2022-bpe,
  title = {{BPE} vs. morphological segmentation: A case study on machine translation of four polysynthetic languages},
  author = {Mager, Manuel and Oncevay, Arturo and Mager, Elisabeth and Kann, Katharina and Vu, Thang},
  booktitle = {Findings of the Association for Computational Linguistics: ACL 2022},
  pages = {961--971},
  year = {2022}
}

@inproceedings{kann-etal-2017-neural,
  title = {Neural multi-source morphological reinflection},
  author = {Kann, Katharina and Cotterell, Ryan and Sch{\"u}tze, Hinrich},
  booktitle = {Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers},
  pages = {514--524},
  year = {2017}
}

@inproceedings{beinborn-pinter-2023-analyzing,
  title = {Analyzing cognitive plausibility of subword tokenization},
  author = {Beinborn, Lisa and Pinter, Yuval},
  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages = {4478--4486},
  year = {2023}
}

@inproceedings{yehezkel-pinter-2023-incorporating,
  title = {Incorporating context into subword vocabularies},
  author = {Yehezkel, Shaked and Pinter, Yuval},
  booktitle = {Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics},
  pages = {623--635},
  year = {2023}
}

@inproceedings{weissweiler-etal-2023-counting,
  title = {Counting the bugs in {ChatGPT}'s wugs: A multilingual investigation into the morphological capabilities of a large language model},
  author = {Weissweiler, Leonie and Hofmann, Valentin and Kantharuban, Anjali and Cai, Anna and Dutt, Ritam and Hengle, Amey and Kabra, Anubha and Kulkarni, Atharva and Vijayakumar, Abhishek and Yu, Haofei and Sch{\"u}tze, Hinrich and Oflazer, Kemal and Mortensen, David},
  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages = {6508--6524},
  year = {2023}
}

@article{soler-etal-2024-impact,
  title = {The impact of word splitting on the semantic content of contextualized word representations},
  author = {Soler, Aina Gari and Labeau, Matthieu and Clavel, Chlo{\'e}},
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {12},
  pages = {299--320},
  year = {2024}
}

@inproceedings{hou-etal-2023-effects,
  title = {Effects of sub-word segmentation on performance of transformer language models},
  author = {Hou, Jue and Katinskaia, Anisia and Vu, Anh-Duc and Yangarber, Roman},
  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages = {7413--7425},
  year = {2023}
}

@inproceedings{batsuren-etal-2021-morphynet,
  title = {{MorphyNet}: a large multilingual database of derivational and inflectional morphology},
  author = {Batsuren, Khuyagbaatar and Bella, G{\'a}bor and Giunchiglia, Fausto},
  booktitle = {Proceedings of the 18th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology},
  pages = {39--48},
  year = {2021}
}
=====END FILE=====

=====FILE: figures/README.txt=====
Figure files referenced in the LaTeX source:
- segm_dist.pdf: Distribution of segmentation categories per language (Figure 1)
- overlap_dist.pdf: Distribution of overlap scores per inflection paradigm (Figure 2)

These figures were not provided in the source PDF and should be recreated from the original publication or marked as "IMAGE NOT PROVIDED" in the final compiled document.
=====END FILE=====