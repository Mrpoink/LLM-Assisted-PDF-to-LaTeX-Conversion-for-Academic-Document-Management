\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{raffel2020exploring}
\citation{devlin2019bert}
\citation{conneau2019unsupervised}
\citation{xue2021mt5}
\citation{suarez2019asynchronous}
\citation{abadji2022towards}
\citation{kreutzer2022quality}
\citation{ogueji2021small}
\citation{conneau2020unsupervised}
\citation{adelani2022masakhanews}
\citation{alabi2022adapting}
\citation{adebara2022serengeti}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}\protected@file@percent }
\citation{kreutzer2022quality}
\citation{kreutzer2022quality}
\@writefile{toc}{\contentsline {section}{\numberline {2}Wura Dataset}{3}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Auditing and Cleaning mC4}{3}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}Language Contamination}{3}{subsubsection.2.1.1}\protected@file@percent }
\citation{caswell2020language}
\citation{nllb2022no}
\citation{kreutzer2022quality}
\citation{xue2021mt5}
\citation{alabi2022adapting}
\citation{adebara2022serengeti}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2}mC4 is a Great Source!}{4}{subsubsection.2.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Combination with Existing Language Resources and Non-African Languages}{4}{subsection.2.2}\protected@file@percent }
\citation{roberts2022scaling}
\citation{shazeer2020glu}
\citation{raffel2020exploring}
\citation{ogundepo2023afriqa}
\citation{adelani2022masakhanews}
\citation{hasan2021xl}
\citation{adelani2023masakhanews}
\citation{xue2021mt5}
\citation{xue2022byt5}
\citation{chung2022scaling}
\citation{ogundepo2022afriteva}
\citation{alabi2022adapting}
\@writefile{toc}{\contentsline {section}{\numberline {3}Experimental Setup}{5}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Model}{5}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Downstream Tasks}{5}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Cross-lingual Question Answering}{5}{subsubsection.3.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Machine Translation}{5}{subsubsection.3.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3}Summarization}{5}{subsubsection.3.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.4}Text Classification}{5}{subsubsection.3.2.4}\protected@file@percent }
\citation{rajpurkar2016squad}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Baseline Models}{6}{subsection.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Result and Discussion}{6}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Downstream Performance}{6}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Cross-lingual Question Answering}{6}{subsubsection.4.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}Machine Translation}{6}{subsubsection.4.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.3}Summarization}{6}{subsubsection.4.1.3}\protected@file@percent }
\citation{rae2021scaling}
\citation{kreutzer2022quality}
\citation{hernandez2022scaling}
\citation{alabi2022adapting}
\citation{resnik1999bible}
\citation{agic2019jw300}
\citation{kreutzer2022quality}
\citation{alabi2020massive}
\citation{conneau2020unsupervised}
\citation{suarez2019asynchronous}
\citation{xue2021mt5}
\citation{bapna2022building}
\citation{ogueji2021small}
\citation{leong2022bloom}
\citation{palen2022multilingual}
\citation{alabi2022adapting}
\citation{adebara2022serengeti}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.4}Text Classification}{7}{subsubsection.4.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Discussion}{7}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Results for Nigerian Pidgin}{7}{subsubsection.4.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}Impact of Data Quality on LMs}{7}{subsubsection.4.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Related Work}{7}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}AfriTeVa V2 Large Model}{8}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion}{8}{section.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8}Limitations}{8}{section.8}\protected@file@percent }
\citation{ogundepo2023afriqa}
\citation{ogundepo2023afriqa}
\bibcite{abadji2022towards}{Abadji et al.2022}
\bibcite{adelani2022masakhanews}{Adelani et al.2022}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces MasakhaNews classification results: Evaluation is done using the weighted F1 score and the scores presented are averaged across 3 seeds. AfriTeVa V2 surpasses mT5-base by up to 10 points. The average scores excluding languages not in the mC4 corpus are also provided in AVGSL.}}{9}{table.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:classification}{{1}{9}{MasakhaNews classification results: Evaluation is done using the weighted F1 score and the scores presented are averaged across 3 seeds. AfriTeVa V2 surpasses mT5-base by up to 10 points. The average scores excluding languages not in the mC4 corpus are also provided in AVGSL}{table.caption.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces MAFAND-MT results: Evaluation is done using the BLEU score and we obtain significantly better performance on average across all languages in both the en-xx and xx-en directions, except for ibo and pcm.}}{9}{table.caption.3}\protected@file@percent }
\newlabel{tab:mt}{{2}{9}{MAFAND-MT results: Evaluation is done using the BLEU score and we obtain significantly better performance on average across all languages in both the en-xx and xx-en directions, except for ibo and pcm}{table.caption.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces XL-SUM results: Performance based on Rouge-1, Rouge-2 and Rouge-L. AfriTeVa V2 is generally more effective than mT5.}}{10}{table.caption.4}\protected@file@percent }
\newlabel{tab:summarization}{{3}{10}{XL-SUM results: Performance based on Rouge-1, Rouge-2 and Rouge-L. AfriTeVa V2 is generally more effective than mT5}{table.caption.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Cross-lingual Question Answering Results: F1 and Exact Match (EM) Accuracy scores on the test set of AfriQA \cite  {ogundepo2023afriqa}. For both metrics, AfriTeVa V2 outperforms mT5 except for twi.}}{10}{table.caption.5}\protected@file@percent }
\newlabel{tab:qa}{{4}{10}{Cross-lingual Question Answering Results: F1 and Exact Match (EM) Accuracy scores on the test set of AfriQA \cite {ogundepo2023afriqa}. For both metrics, AfriTeVa V2 outperforms mT5 except for twi}{table.caption.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Wura Dataset Statistics: We provide the count of crawled articles, Wikipedia articles, original mC4 articles, and final size before passage-level filtering for each language. In total, we have $\sim $4.7M articles, more than 1.5 times what mC4 contains across 16 African languages.}}{11}{table.caption.6}\protected@file@percent }
\newlabel{tab:dataset_stats}{{5}{11}{Wura Dataset Statistics: We provide the count of crawled articles, Wikipedia articles, original mC4 articles, and final size before passage-level filtering for each language. In total, we have $\sim $4.7M articles, more than 1.5 times what mC4 contains across 16 African languages}{table.caption.6}{}}
\bibcite{adelani2023masakhanews}{Adelani et al.2023}
\bibcite{agic2019jw300}{Agić and Vulić2019}
\bibcite{ahia2023do}{Ahia et al.2023}
\bibcite{alabi2020massive}{Alabi et al.2020}
\bibcite{alabi2022adapting}{Alabi et al.2022}
\bibcite{adebara2022serengeti}{Adebara et al.2022}
\bibcite{bapna2022building}{Bapna et al.2022}
\bibcite{caswell2020language}{Caswell et al.2020}
\bibcite{chung2022scaling}{Chung et al.2022}
\bibcite{conneau2019unsupervised}{Conneau et al.2019}
\bibcite{conneau2020unsupervised}{Conneau et al.2020}
\bibcite{devlin2019bert}{Devlin et al.2019}
\bibcite{dione2023masakhapos}{Dione et al.2023}
\bibcite{hasan2021xl}{Hasan et al.2021}
\bibcite{hernandez2022scaling}{Hernandez et al.2022}
\bibcite{kreutzer2022quality}{Kreutzer et al.2022}
\bibcite{kudo2018sentencepiece}{Kudo and Richardson2018}
\bibcite{leong2022bloom}{Leong et al.2022}
\bibcite{nllb2022no}{NLLB Team2022}
\bibcite{ogueji2021small}{Ogueji et al.2021}
\bibcite{ogundepo2022afriteva}{Ogundepo et al.2022}
\bibcite{ogundepo2023afriqa}{Ogundepo et al.2023}
\bibcite{suarez2019asynchronous}{Ortiz Suárez et al.2019}
\bibcite{palen2022multilingual}{Palen-Michel et al.2022}
\bibcite{petrov2023language}{Petrov et al.2023}
\bibcite{rae2021scaling}{Rae et al.2021}
\bibcite{raffel2020exploring}{Raffel et al.2020}
\bibcite{rajpurkar2016squad}{Rajpurkar et al.2016}
\bibcite{resnik1999bible}{Resnik et al.1999}
\bibcite{roberts2022scaling}{Roberts et al.2022}
\bibcite{shazeer2020glu}{Shazeer2020}
\bibcite{xue2021mt5}{Xue et al.2021}
\bibcite{xue2022byt5}{Xue et al.2022}
\gdef \@abspage@last{16}
