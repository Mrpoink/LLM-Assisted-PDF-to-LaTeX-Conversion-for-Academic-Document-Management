=====FILE: main.tex=====
\documentclass[twocolumn]{article}
\usepackage[utf8]{inputenc}
\usepackage{times}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{array}
\usepackage{amsmath}
\usepackage{caption}
\usepackage[margin=1in]{geometry}
\usepackage{url}

\title{Understanding Slang with LLMs: Modelling Cross-Cultural Nuances through Paraphrasing}
\author{
Ifeoluwa Wuraola$^{1}$, Nina Dethlefs$^{1}$, Daniel Marciniak$^{2}$ \\
$^{1}$School of Computer Science, University of Hull, UK \\
$^{2}$School of Criminology, Sociology and Policing, University of Hull, UK \\
\texttt{\{i.a.wuraola.2021, n.dethlefs, d.f.marciniak\}@hull.ac.uk}
}

\begin{document}

\maketitle

\begin{abstract}
In the realm of social media discourse, the integration of slang enriches communication, reflecting the sociocultural identities of users. This study investigates the capability of large language models (LLMs) to paraphrase slang within climate-related tweets from Nigeria and the UK, with a focus on identifying emotional nuances. Using DistilRoBERTa as the baseline model, we observe its limited comprehension of slang. To improve cross-cultural understanding, we gauge the effectiveness of leading LLMs: ChatGPT 4, Gemini, and LLaMA3 in slang paraphrasing. While ChatGPT 4 and Gemini demonstrate comparable effectiveness in slang paraphrasing, LLaMA3 shows less coverage, with all LLMs exhibiting limitations in coverage, especially of Nigerian slang. Our findings underscore the necessity for culturally-sensitive LLM development in emotion classification, particularly in non-anglocentric regions.
\end{abstract}

\section{Introduction}
In the age of social media, platforms like X (formerly Twitter) have become a vital medium for public discourse, where users express a wide array of views and emotions on various topics \cite{geronikolou2021, loureiro2020, wang2016}. However, sociocultural identities including regional background, gender, age, and sub-cultural affiliations have a big impact on communication styles. People often blend formal and informal language, incorporating specific dialects or slang into their discourse. Discourse from non-Anglocentric countries may thus contain cultural references and idioms that are not easily understood by outsiders. For instance, Nigerian tweets may utilize Pidgin English to convey emotions, such as the phrase ``I dey happy no be small'' meaning ``I am very happy''.

Emotion classification is a key task in sentiment analysis. Despite LLMs' impressive capabilities in various linguistic tasks, they often encounter challenges in accurately capturing cultural nuances like emotions, resulting in inaccuracies, particularly in diverse settings \cite{mao2023}. In this paper we focus on LLMs' knowledge of slang and how state-of-the-art models might misinterpret or overlook emotions in tweets containing slang across different varieties of English. We propose a novel approach to integrating detailed slang representations into LLMs. Leveraging generative models such as OpenAI's ChatGPT 4 \cite{openai2024}, Google's Gemini \cite{googleai2024}, and META's LLaMA3 \cite{meta2024}, we systematically investigate how paraphrased slang influences emotional expressions in tweets from diverse cultures, focusing specifically on Nigeria and the UK. We make the following contributions:
\begin{itemize}
    \item We highlight shortcomings in pre-trained LLMs in identifying emotions in social media discourse featuring slang, particularly in non-Anglocentric varieties of English.
    \item We provide a comprehensive comparison of leading LLMs in understanding and paraphrasing slang, pointing to ways of reducing bias.
\end{itemize}
Our study highlights the need to model slang in reducing biases within LLMs, especially in regions with diverse linguistic backgrounds. Our research demonstrates the cultural insensitivity of LLMs for emotion classification in tweets from Nigeria and the United Kingdom (UK). By incorporating Nigerian perspectives, we address a critical gap in understanding cultural nuances and linguistic expressions in underrepresented groups.

\section{Related Works}
\subsection{Cross-cultural performance of LLMs}
Recent research has placed an increasing emphasis on addressing the complex interplay between cross-cultural context and bias mitigation in LLMs. Hersh\-covich et al. \cite{hershcovich2022} argue that current LLMs do not adequately model the intricate relationships between linguistic constructions and sociocultural viewpoints, values and common ground. Multiple studies have found that while LLMs perform well at standard English tasks, they are much less successful at modelling non-standard varieties of English, including African American English \cite{deas2023}, non-Anglocentric varieties of English \cite{wuraola2023}, Pidgin \cite{chang2022}, or examples of code-switching \cite{zhang2023}.

Similarly, multi-lingual LLMs have been found to be much less reliable in practice than their English counterparts, including factual information systems \cite{fierro2022}, emotion and sentiment classification \cite{muhammad2023}. Machine translation can affect the reliability of cross-cultural analyses \cite{zhang2023}, particularly when LLMs transfer stereotypes between languages. Low-resource languages are especially susceptible to these leakages compared to dominant languages \cite{cao2024}. Dodge et al. \cite{dodge2021} demonstrate a bias towards US data in NLP resources and find that when data gets removed (e.g. toxicity, slurs, obscenity, etc.), this disproportionately affects data relating to minority groups.

\subsection{Modelling Slang with LLMs}
In this paper, we focus on the effect of slang on the task of emotion classification, specifically comparing British and Nigerian English. This links with previous studies that have explored cross-cultural context in slang analysis. Lin et al. \cite{lin2018} introduce SocVec, which aims to compute cross-cultural differences in understanding slang terms across languages. The method is evaluated on two tasks focused on mining cross-cultural differences in named entities and slang.

Similarly, Sun et al. \cite{sun2024} use LLMs to detect slang and attribute regional and historical context. Despite GPT-4's high performance in zero-shot settings, the study reveals that smaller, fine-tuned BERT models achieve comparable results. Both studies underscore the significance of regional and cultural contexts in understanding slang.

In a similar vein, Sun et al. \cite{sun2021} introduced a computational framework for slang generation that incorporates syntactic and contextual knowledge. The framework leverages probabilistic inference and neural contrastive learning and outperforms existing language models in accurately predicting historical slang emergence from the 1960s to 2000s. Pei et al. \cite{pei2019} compare classifiers for slang detection and highlight the syntactic shift of words as a key feature of slang. Seki and Liu \cite{seki2022} enhanced LLMs for Chinese slang comprehension contrasting LLM performance with a custom Punchline Entity Recognition (PER) system, integrating phonetic matching. Also, Sultan \cite{sultan2023} classify emotions in tweets containing slang based on WordNet for synonymous phrase generation and a CNN for classification. They show a significant improvement in emotion classification for slang-filled social media texts. Furthermore, Rohn \cite{rohn2024} detect internet slang based on a hierarchical multitask BERT model, using two-layer annotation and word embeddings. The model excelled in identifying subcategories of internet slang, demonstrating the effectiveness of two-layer annotation.

\section{Methodology}
\subsection{Dataset}
Our study explores climate-related tweets from Twitter (now X) sourced via the API and spanning a time frame of January 2010 to March 2024. Our data collection focused on keywords and hashtags related to climate change, global warming, and conservation, see Wuraola et al. \cite{wuraola2023} for details. We balanced our data to make up equal proportions of tweets originating from the UK and Nigeria, via geo-tags, which led to a corpus of 138,862 tweets for analysis. The motivation for studying climate change tweets lies in the high volume and emotional intensity of discussions surrounding this topic on social media. Climate change is a global issue that elicits strong reactions and diverse linguistic expressions, including slang. Additionally, the two countries we focused on, the UK and Nigeria, are likely affected by climate change in different ways, making this an interesting domain to study. Any misinterpretations from an LLM could lead to a misrepresentation of views, which underscores the importance of accurately understanding the emotional content conveyed through slang.

\subsection{Slang Dictionary Generation}
In order to assess LLMs' ability to identify emotions in discourse containing slang, we initially evaluate their ability to paraphrase slang in UK and Nigerian English. For this purpose, we curated a comprehensive slang dictionary, consisting of about 240 unique slang terms and their meanings. These terms were sourced from a variety of channels, ensuring a diverse representation of contemporary slang that serves as our gold standard for paraphrasing. Specifically, we targeted online forums and linguistic databases relevant to each region, for example, Naijalingo.com for Nigerian slang and Tandem.net for UK slang. See Table~\ref{tab:slang_sources} for details.

\begin{table}[h]
\centering
\caption{Slang sources on the web}
\label{tab:slang_sources}
\begin{tabular}{@{}ll@{}}
\toprule
Source & Region \\
\midrule
BBC & UK \\
Urban Dictionary & UK/US \\
Naijalingo.com & Nigeria \\
Tandem.net & UK \\
\bottomrule
\end{tabular}
\end{table}

We compared the ability to generate concise paraphrases for slang terms of OpenAI's ChatGPT-4 \cite{openai2024}, Google's Gemini \cite{googleai2024}, and META's LLaMA3 8B \cite{meta2024}. ChatGPT and Gemini paraphrased all curated slang, whereas LLaMA3 was unable to provide a paraphrase for 22\% of the slang. The paraphrases in Table~\ref{tab:paraphrase_accuracy} were generated using a zero-shot approach, where the LLMs were prompted to provide paraphrases for slang terms with regional specifications. For instance, we instructed the models with prompts like, ``paraphrase this Nigerian slang 'wahala''' and ``paraphrase this UK slang 'mate'''.

\begin{table}[h]
\centering
\caption{Correctly paraphrased tweets across LLMs}
\label{tab:paraphrase_accuracy}
\begin{tabular}{@{}lcc@{}}
\toprule
Model & Correct paraphrases (\%) & Incorrect paraphrases (\%) \\
\midrule
ChatGPT-4 & 92 & 8 \\
Gemini & 81 & 19 \\
LLaMA3 & 55 & 45 \\
\bottomrule
\end{tabular}
\end{table}

Additionally, in Table~\ref{tab:paraphrase_accuracy}, we assess the correctness of slang paraphrases against their dictionary definitions. The correctness of these paraphrases was evaluated through a manual review, where we compared the slang paraphrases to their meanings from the online sources. LLaMA3 shows the lowest accuracy here, suggesting that the model may exhibit more bias in its slang knowledge from specific cultural contexts compared to Gemini and GPT models. This observation is further supported by employing Cohen's Kappa score, which demonstrates an agreement of 0.74 between ChatGPT and Gemini for Nigerian tweets, the highest among all models. This metric solidifies the notion that ChatGPT and Gemini yield very similar effects, providing substantial evidence for their comparability.

We used our four dictionary resources (i.e., manually curated, ChatGPT-4, Gemini and LLaMA3) to detect tweets containing one or more slang words or phrases and replaced them with their paraphrases. For this task, we employed a direct identification approach using our curated dictionary corpus to recognize and extract tweets containing slang terms from climate-related content. This process yielded a total of 2,845 tweets containing slang, with 592 originating from the UK and 2,253 from Nigeria. This confirms earlier research that exposed the linguistic variety in African English \cite{wuraola2023, muhammad2023, chang2022}.

\subsection{Emotion Labelling}
We employ DistilRoBERTa \cite{hartmann2022} to label the emotions in our tweet dataset. This model features 6 transformer layers, a hidden size of 768 dimensions, and 12 attention heads, enabling it to effectively understand contextual information. During pre-training, DistilRoBERTa employs advanced feature extraction techniques to identify emotions, producing output vectors that represent seven distinct emotions (joy, sadness, anger, surprise, disgust, fear, and neutral). We perform this task twice: first on the original tweets containing slang, and then on the paraphrased versions. DistilRoBERTa labels were compared against ratings from five independent human raters. The raters achieved an agreement score of 0.30 with DistilRoBERTa and an agreement score of 0.41 among themselves. To further clarify our results, we manually examined the raters' labels and discovered that around 68\% of them assigned two or more negative emotions to the same tweet. While individual raters may have chosen different specific labels, there was a general consensus on the overall emotional tone being negative. This indicates the complexity and nuanced nature of emotional expressions, underscoring the challenges in achieving consistent emotion identification \cite{sharma2019, schoene2020, canales2022}.

\section{Results and Discussion}
In this section, we aim to determine the effect that slang, and understanding its correct meaning, has on emotion classification in tweets. To this end, we present two sets of results: (1) the emotion distribution in UK and Nigerian English in the original climate tweets, and (2) the percentage changes in emotion distribution with each set of LLM-generated paraphrases.

Figure~\ref{fig:emotion_changes} illustrates percentage changes between original and paraphrased tweets for each of the LLMs. It highlights distinct changes in emotion expression between Nigerian and UK tweets when paraphrased with different models. Nigerian tweets exhibit increased fear with ChatGPT (10.36\%), while UK tweets show decreased fear with LLaMA3 (-11.31\%) and Gemini (-13.10\%). Anger decreases across both countries, notably in UK tweets paraphrased with ChatGPT (-61.50\%). Paraphrased tweets often show heightened joy, especially in Nigerian tweets paraphrased with LLaMA3 (148.48\%). Additionally, both countries experience reduced neutral emotions post-paraphrasing, indicating a shift towards more polarised language, or just highlighting that LLMs find it harder to discern emotions from slang.

\begin{figure}[h]
\centering
\fbox{\parbox[c][6cm][c]{0.9\columnwidth}{\centering IMAGE NOT PROVIDED\\Figure 1: Percentage Change in Emotions of Climate Tweets:\\Comparing Original and Paraphrased Versions from Nigeria and the UK.}}
\caption{Percentage Change in Emotions of Climate Tweets: Comparing Original and Paraphrased Versions from Nigeria and the UK.}
\label{fig:emotion_changes}
\end{figure}

Table~\ref{tab:emotion_distribution} compares emotion classification before and after paraphrasing. For instance, DistilRoBERTa initially classified 550 Nigerian tweets as expressing fear, increasing to 687 after paraphrasing with ChatGPT-4. These are notable shifts (both positively and negatively), indicating the baseline model's limited proficiency in emotion classification in the presence of slang. Inspecting the data, we find that Nigerian tweets featuring the slang ``wahala'' are often misclassified as neutral. However, when paraphrased as ``trouble'' or ``problem,'' the emotion changes to fear. For example, the Nigerian tweet ``imagine that climate change switches everything and then it begins to snow in Nigeria wahala go dey oo'', was paraphrased as, ``imagine that climate change switches everything and then it begins to snow in Nigeria there will be trouble''. This observation aligns with prior research emphasizing the importance of incorporating external context to enhance the LLMs' comprehension of social media data \cite{adedamola2015, sultan2023}.

\begin{table*}[h]
\centering
\caption{Emotion Distribution in Slang and Paraphrases of Climate Tweets from the UK and Nigeria. We used McNemar's test to determine if emotion categorisations changed significantly after paraphrasing. We applied a Bonferroni correction to account for multiple comparisons across categories. Bonferroni-adjusted significances are reported as *: $p<0.05/7=0.0071$, **: $p<0.01/7=0.0014$, and ***: $p<0.001/7=0.00014$.}
\label{tab:emotion_distribution}
\begin{tabular}{@{}lcccccccccc@{}}
\toprule
& \multicolumn{5}{c}{Nigerian Climate Tweets} & \multicolumn{5}{c}{UK Climate Tweets} \\
\cmidrule(r){2-6}\cmidrule(l){7-11}
Emotion & Original & ChatGPT-4 & Gemini & LLaMA3 & Baseline & Original & ChatGPT-4 & Gemini & LLaMA3 & Baseline \\
\midrule
Fear & 550 & 687*** & 640*** & 620*** & 607 & 166 & 146*** & 164*** & 160*** & 188*** \\
Neutral & 449 & 437** & 456** & 247*** & 449 & 62 & 50*** & 41*** & 0*** & 184*** \\
Sadness & 189 & 225*** & 185** & 656*** & 189 & 19 & 94*** & 50*** & 188*** & 160*** \\
Anger & 104 & 110*** & 108*** & 449*** & 104 & 8 & 50*** & 19*** & 166*** & 164*** \\
Surprise & 5 & 3** & 3** & 5*** & 5 & 1 & 0 & 0 & 0 & 0 \\
Joy & 60 & 65*** & 58*** & 56*** & 60 & 7 & 6 & 6 & 6 & 7 \\
Disgust & 2 & 2 & 2 & 2 & 2 & 0 & 0 & 0 & 0 & 0 \\
\bottomrule
\end{tabular}
\end{table*}

Overall, the effects of paraphrasing slang show significant variation between countries and models. Nigerian tweets typically demonstrate more pronounced emotional shifts across all models, with ChatGPT often amplifying emotions. In contrast, UK tweets exhibit more subtle changes, with LLaMA3, Gemini, and ChatGPT each impacting emotions differently. This indicates the significant influence of both cultural context and model-specific behaviour on emotion extraction. These variations may stem from inherent biases in LLMs towards underrepresented dialects, as evidenced by previous studies \cite{venkit2023, sun2019, chuah2021}.

Table~\ref{tab:paraphrase_examples} shows examples of paraphrased slang across English variants, indicating that ChatGPT and Gemini generally offer accurate paraphrases close to the gold standard. However, LLaMA3 falls behind in paraphrasing slang, implying a cultural bias compared to the other models. Table~\ref{tab:paraphrase_accuracy} supports this with LLaMA3 incorrectly paraphrasing 45\% of slangs in Nigerian tweets and 24\% for UK English, while ChatGPT and Gemini have fewer incorrect paraphrases. These results suggest that ChatGPT and Gemini handle slang more effectively due to their extensive training data and advanced architecture. In contrast, LLaMA3 struggles significantly with Nigerian slang, highlighting its potential limitations in understanding slang from specific cultural contexts. Given the lack of access for the research community to fine-tune commercial models like ChatGPT and Gemini, this reinforces the need for openly accessible models, such as LLaMA3, with improved slang knowledge.

\begin{table}[h]
\centering
\caption{Examples of emotion changes given slang and paraphrases across LLMs.}
\label{tab:paraphrase_examples}
\begin{tabular}{@{}p{0.25\columnwidth}p{0.15\columnwidth}p{0.25\columnwidth}p{0.25\columnwidth}@{}}
\toprule
Slang & Origin & Gold label & Baseline Emotion \\
\midrule
``fall hand'' & Nigeria & ``disappoint'' & Fear \\
``full tum grace'' & UK & ``buttocks'' & Sadness \\
``wahala'' & Nigeria & ``trouble'' & Fear \\
``bloke'' & UK & ``man'' & Surprise \\
\midrule
Model & Paraphrase & Emotion \\
\midrule
ChatGPT-4 & ``to disappoint'' & Sadness \\
Gemini & ``to disappoint'' & Sadness \\
LLaMA3 & ``fall from grace'' & Neutral \\
\midrule
ChatGPT-4 & ``buttocks'' & Anger \\
Gemini & ``buttocks'' & Anger \\
LLaMA3 & ``derogatory'' & Sadness \\
\midrule
ChatGPT-4 & ``trouble'' & Fear \\
Gemini & ``problem'' & Fear \\
LLaMA3 & ``unfortunate'' & Surprise \\
\midrule
ChatGPT-4 & ``man'' & Fear \\
Gemini & ``man'' & Fear \\
LLaMA3 & ``man'' & Fear \\
\bottomrule
\end{tabular}
\end{table}

\section{Conclusion}
In summary, our research evaluates the efficacy of LLMs in modelling slang, particularly in the context of climate-related tweets, though we speculate that our findings transfer to other topics. We observed significant emotional shifts when integrating slang paraphrases, in UK tweets and especially in Nigerian tweets. The shifts vary across LLaMA3, Gemini, and ChatGPT used for slang paraphrasing. Furthermore, factors like extensive training data and commercial nature likely contribute to ChatGPT's and Gemini's observed superiority on the task in comparison with LLaMA3. Our study highlights potential biases in LLMs towards non-Anglocentric regions and emphasizes the need for culturally-sensitive LLM development.

In future work, we plan to explore additional LLMs to facilitate a more comprehensive comparison of their performance in detecting and interpreting emotions in climate-related discourse. This will include evaluating newer models and their ability to understand regional slang and emotional nuances, as well as assessing their effectiveness across diverse cultural contexts. Additionally, we aim to enhance our dataset by incorporating real-time social media feeds to capture evolving slang and emotional expressions related to climate change. This expanded approach will provide deeper insights into how different LLMs process and represent emotional content in climate-related discussions.

\section{Limitations}
While our study underscores the importance of developing refined approaches to LLM development in diverse linguistic and cultural contexts, the reliance on a single model to zero-shot label emotions may limit the generalizability of the findings. Also, our research is constrained to specific demographic regions: Nigeria and the UK. To overcome these limitations, future studies should strive to incorporate multiple cultures and regions. Employing diverse methodologies will ensure a more comprehensive and nuanced analysis of emotional dynamics in discourse across global contexts.

\section{Ethics Statement}
The study followed the ACL Ethics Policy to ensure ethical and responsible conduct throughout the research process. We limited data gathering to publicly accessible tweets and anonymised the data to protect individuals privacy. Additionally, we avoid reinforcing biases or stereotypes and respectfully conduct the research in accordance with cultural norms and beliefs. The work makes use of suitable computational and statistical techniques, and we openly communicated our results to the larger scientific community. We are dedicated to maintaining moral standards in our studies.

\section*{Acknowledgements}
The authors express gratitude to the Centre of Excellence for Data Science, Artificial Intelligence and Modelling (DAIM) and the Big Data Analytics (BDA) research group for generously funding and enabling this research. We acknowledge the VIPER high-performance computing facility of the University of Hull and its support team.

\bibliographystyle{acl_natbib}
\begin{thebibliography}{100}

\bibitem[Adedamola et al.2015]{adedamola2015}
Adedoja A Adedamola, Abiodun Modupe, and Olumuyiwa J Dehinbo. 2015. Development and Evaluation of a System for Normalizing Internet Slangs in Social Media Texts. In \textit{Proceedings of the World Congress on Engineering and Computer Science 2015}, Vol. 1, San Francisco, USA. International Association of Engineers.

\bibitem[AI@Meta2024]{meta2024}
AI@Meta. 2024. Llama 3 Model Card. Original-date: 2024-03-15T17:57:00Z.

\bibitem[Canales et al.2022]{canales2022}
Lea Canales, Walter Daelemans, Ester Boldrini, and Patricio Martínez-Barco. 2022. Emolabel: Semi Automatic Methodology for Emotion Annotation of Social Media Text. \textit{IEEE Transactions on Affective Computing}, 13(2):579--591.

\bibitem[Cao et al.2024]{cao2024}
Yang Trista Cao, Anna Sotnikova, Jieyu Zhao, Linda X. Zou, Rachel Rudinger, and Hal Daumé III. 2024. Multilingual large language models leak human stereotypes across language boundaries. \textit{arXiv preprint arXiv:2312.07141}.

\bibitem[Chang et al.2022]{chang2022}
Ernie Chang, Jesujoba O. Alabi, David Ifeoluwa Adelani, and Vera Demberg. 2022. Few-Shot Pidgin Text Adaptation via Contrastive Fine-Tuning. In \textit{Proceedings of the 29th International Conference on Computational Linguistics}, pages 4286--4291, Gyeongju, Republic of Korea. International Committee on Computational Linguistics.

\bibitem[Chuang et al.2021]{chuah2021}
Yung-Sung Chuang, Mingye Gao, Hongyin Luo, James Glass, Hung-yi Lee, Yun-Nung Chen, and Shang-Wen Li. 2021. Mitigating Biases in Toxic Language Detection through Invariant Rationalization. \textit{arXiv preprint arXiv:2106.00757}.

\bibitem[Deas et al.2023]{deas2023}
Nicholas Deas, Jessica Grieser, Shana Kleiner, Desmond Patton, Elsbeth Turcan, and Kathleen McKeown. 2023. Evaluation of African American Language Bias in Natural Language Generation. In \textit{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing}, pages 6805--6824, Singapore. Association for Computational Linguistics.

\bibitem[Dodge et al.2021]{dodge2021}
Jesse Dodge, Maarten Sap, Ana Marasović, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, and Matt Gardner. 2021. Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus. \textit{arXiv preprint arXiv:2104.08758}.

\bibitem[Fierro and Søgaard2022]{fierro2022}
Constanza Fierro and Anders Søgaard. 2022. Factual Consistency of Multilingual Pretrained Language Models. In \textit{Findings of the Association for Computational Linguistics: ACL 2022}, pages 3046--3052, Dublin, Ireland. Association for Computational Linguistics.

\bibitem[Geronikolou et al.2021]{geronikolou2021}
Styliani Geronikolou, George Drosatos, and George Chrousos. 2021. Emotional Analysis of Twitter Posts During the First Phase of the COVID-19 pandemic in Greece: Infoveillance Study. \textit{JMIR Formative Research}, 5(9):e27741.

\bibitem[GoogleAI2024]{googleai2024}
GoogleAI. 2024. Gemini Advanced - get access to Google's most capable AI model.

\bibitem[Hartmann2022]{hartmann2022}
Jochen Hartmann. 2022. Emotion English DistilRoBERTa-base.

\bibitem[Hershcovich et al.2022]{hershcovich2022}
Daniel Hershcovich, Stella Frank, Heather Lent, Miryam de Lhoneux, Mostafa Abdou, Stephanie Brandl, Emanuele Bugliarello, Laura Cabello Piqueras, Ilias Chalkidis, Ruixiang Cui, Constanza Fierro, Katerina Margatina, Phillip Rust, and Anders Søgaard. 2022. Challenges and Strategies in Cross-Cultural NLP. In \textit{Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 6997--7013, Dublin, Ireland. Association for Computational Linguistics.

\bibitem[Lin et al.2018]{lin2018}
Bill Yuchen Lin, Frank F. Xu, Kenny Zhu, and Seungwon Hwang. 2018. Mining Cross-Cultural Differences and Similarities in Social Media. In \textit{Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 709--719, Melbourne, Australia. Association for Computational Linguistics.

\bibitem[Loureiro and Alló2020]{loureiro2020}
Maria L. Loureiro and Maria Alló. 2020. Sensing climate change and energy issues: Sentiment and emotion analysis with social media in the U.K. and Spain. \textit{Energy Policy}, 143(C). Publisher: Elsevier.

\bibitem[Mao et al.2023]{mao2023}
Rui Mao, Qian Liu, Kai He, Wei Li, and Erik Cambria. 2023. The Biases of Pre-Trained Language Models: An Empirical Study on Prompt-Based Sentiment Analysis and Emotion Detection. \textit{IEEE Transactions on Affective Computing}, 14(3):1743--1753.

\bibitem[Muhammad et al.2023]{muhammad2023}
Shamsuddeen Muhammad, Idris Abdulmumin, Abinew Ayele, Nedjma Ousidhoum, David Adelani, Seid Yimam, Ibrahim Ahmad, Meriem Beloucif, Saif Mohammad, Sebastian Ruder, Oumaima Hourrane, AliPIO Jorge, Pavel Brazdil, Felermino Ali, Davis David, Salomey Osei, Bello Shehu-Bello, Falalu Lawan, Tajuddeen Gwadabe, Samuel Rutunda, Tadesse Belay, Wendimu Messelle, Hailu Balcha, Sisay Chala, Hagos Gebremichael, Bernard Opoku, and Stephen Arthur. 2023. AfriSenti: A Twitter Sentiment Analysis Benchmark for African Languages. In \textit{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing}, pages 13968--13981, Singapore. Association for Computational Linguistics.

\bibitem[Narayanan Venkit et al.2023]{venkit2023}
Pranav Narayanan Venkit, Sanjana Gautam, Ruchi Panchanadikar, Ting-Hao Huang, and Shomir Wilson. 2023. Nationality Bias in Text Generation. In \textit{Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics}, pages 116--122, Dubrovnik, Croatia. Association for Computational Linguistics.

\bibitem[OpenAI2024]{openai2024}
OpenAI. 2024. Introducing ChatGPT.

\bibitem[Pei et al.2019]{pei2019}
Zhengqi Pei, Zhewei Sun, and Yang Xu. 2019. Slang Detection and Identification. In \textit{Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)}, pages 881--889, Hong Kong, China. Association for Computational Linguistics.

\bibitem[Rohn2024]{rohn2024}
Yesian Rohn. 2024. DantzAI: Slang-Enhanced LLM with Prompt for Humor Understanding. \textit{arXiv preprint arXiv:2405.15818}.

\bibitem[Schoene et al.2020]{schoene2020}
Annika Schoene, Alexander Turner, and Nina Dethlefs. 2020. Bidirectional dilated lstm with attention for fine-grained emotion classification in tweets. In \textit{AffCon@AAAI}, 2614, 100--117.

\bibitem[Seki and Liu2022]{seki2022}
Yohei Seki and Yihong Liu. 2022. Multi-task Learning Model for Detecting Internet Slang Words with Two-Layer Annotation. In \textit{2022 International Conference on Asian Language Processing (IALP)}, pages 212--218.

\bibitem[Sharma et al.2019]{sharma2019}
Karan Sharma, Marius Wagner, Claudio Castellini, Egon L. van den Broek, Freek Stulp, and Friedhelm Schwenker. 2019. A functional data analysis approach for continuous 2-D emotion annotations. \textit{Web Intelligence}, 17:41--52.

\bibitem[Sultan2023]{sultan2023}
Laman R. Sultan. 2023. An Enhanced Emotion Classification Scheme for Tweets Based on Deep Learning Approach. \textit{Revue d'Intelligence Artificielle}, 37(5):1203--1211.

\bibitem[Sun et al.2019]{sun2019}
Tony Sun, Andrew Gaut, Shirlyn Tang, Yuxin Huang, Mai ElSherief, Jieyu Zhao, Diba Mirza, Elizabeth Belding, Kai-Wei Chang, and William Yang Wang. 2019. Mitigating Gender Bias in Natural Language Processing: Literature Review. In \textit{Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics}, pages 1630--1640, Florence, Italy. Association for Computational Linguistics.

\bibitem[Sun et al.2024]{sun2024}
Zhewei Sun, Qian Hu, Rahul Gupta, Richard Zemel, and Yang Xu. 2024. Toward Informal Language Processing: Knowledge of Slang in Large Language Models. \textit{arXiv preprint arXiv:2402.12345}.

\bibitem[Sun et al.2021]{sun2021}
Zhewei Sun, Richard Zemel, and Yang Xu. 2021. A Computational Framework for Slang Generation. \textit{Transactions of the Association for Computational Linguistics}, 9:462--478.

\bibitem[Wang et al.2016]{wang2016}
Wei Wang, Ivan Hernandez, Daniel Newman, Jibo He, and Jiang Bian. 2016. Twitter Analysis: Studying US Weekly Trends in Work Stress and Emotion. \textit{Applied Psychology}, 65:355--378.

\bibitem[Wuraola et al.2023]{wuraola2023}
Ifeoluwa Wuraola, Nina Dethlefs, and Daniel Marciniak. 2023. Linguistic Pattern Analysis in the Climate Change-Related Tweets from UK and Nigeria. In \textit{Proceedings of the 2023 CLASP Conference on Learning with Small Data (LSD)}, pages 90--97, Gothenburg, Sweden. Association for Computational Linguistics.

\bibitem[Zhang et al.2023]{zhang2023}
Ruochen Zhang, Samuel Cahyawijaya, Jan Christian Blaise Cruz, Genta Winata, and Alham Fikri Aji. 2023. Multilingual Large Language Models Are Not (Yet) Code-Switchers. In \textit{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing}, pages 12567--12582, Singapore. Association for Computational Linguistics.

\end{thebibliography}

\end{document}
=====END FILE=====