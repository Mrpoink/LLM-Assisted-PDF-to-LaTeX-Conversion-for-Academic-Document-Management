\relax 
\providecommand \babel@aux [2]{\global \let \babel@toc \@gobbletwo }
\@nameuse{bbl@beforestart}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{3}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Our Approach}{4}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Experiments}{5}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Narrative Retrieval}{5}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}In-Task Performance}{5}{subsubsection.4.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}In-Domain Adaptation: Movie Remake Dataset}{5}{subsubsection.4.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.3}Retellings}{5}{subsubsection.4.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.4}Segment Retrieval}{6}{subsubsection.4.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Narrative Understanding: ROCStories}{6}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Results}{7}{section.5}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Retrieval performance on the Tell-Me-Again test set by Hatzel and Biemann (2024), with and without their anonymization strategy.}}{7}{table.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:tellmeagain}{{1}{7}{Retrieval performance on the Tell-Me-Again test set by Hatzel and Biemann (2024), with and without their anonymization strategy}{table.caption.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Movie Remakes}{7}{subsection.5.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Test set retrieval performance on the dataset by Chaturvedi et al. (2018), with and without the anonymization strategy by Hatzel and Biemann (2024) applied to the dataset. ``+2 steps'' denotes two additional steps of training.}}{8}{table.caption.2}\protected@file@percent }
\newlabel{tab:remakes}{{2}{8}{Test set retrieval performance on the dataset by Chaturvedi et al. (2018), with and without the anonymization strategy by Hatzel and Biemann (2024) applied to the dataset. ``+2 steps'' denotes two additional steps of training}{table.caption.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Retellings}{8}{subsection.5.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Retrieval performance on retelling dataset introduced in Section 4.1.3, optionally with the movie remakes added as distractors.}}{9}{table.caption.3}\protected@file@percent }
\newlabel{tab:retellings}{{3}{9}{Retrieval performance on retelling dataset introduced in Section 4.1.3, optionally with the movie remakes added as distractors}{table.caption.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Scene Retrieval}{9}{subsection.5.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Mean narrative similarity score on a scale of 1--10 in top vs. bottom ranked scenes in terms of similarity as judged by an LLM judge or an annotator, after removing obvious duplicates. The first author performed the annotations.}}{9}{table.caption.4}\protected@file@percent }
\newlabel{tab:sceneretrieval}{{4}{9}{Mean narrative similarity score on a scale of 1--10 in top vs. bottom ranked scenes in terms of similarity as judged by an LLM judge or an annotator, after removing obvious duplicates. The first author performed the annotations}{table.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Story Cloze: ROCStories}{9}{subsection.5.4}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces We list the accuracy at picking the correct story ending from two options on the ROCStories dataset. The superscript $\dagger $ denotes that the embedding distance approach outlined in Section 4.2 is used and evaluated on the development set. The GPT-3 and FLAN results are taken from Wei et al. (2022), and the supervised RoBERTa result is taken from Jiang et al. (2023b).}}{10}{table.caption.5}\protected@file@percent }
\newlabel{tab:storycloze}{{5}{10}{We list the accuracy at picking the correct story ending from two options on the ROCStories dataset. The superscript $\dagger $ denotes that the embedding distance approach outlined in Section 4.2 is used and evaluated on the development set. The GPT-3 and FLAN results are taken from Wei et al. (2022), and the supervised RoBERTa result is taken from Jiang et al. (2023b)}{table.caption.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Approximate Attribution}{10}{section.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Attribution scores on individual tokens in the final layer of our StoryEmb model are shown as a delta from the E5 model. Negative scores indicate less contribution to the similarity in the StoryEmb model. In the example, it seems clear that less emphasis is placed on named entities.}}{11}{figure.caption.6}\protected@file@percent }
\newlabel{fig:attribution}{{1}{11}{Attribution scores on individual tokens in the final layer of our StoryEmb model are shown as a delta from the E5 model. Negative scores indicate less contribution to the similarity in the StoryEmb model. In the example, it seems clear that less emphasis is placed on named entities}{figure.caption.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces The average contribution to sentence similarity of selected named-entity and parts-of-speech tags was analyzed on layer 31 of the E5 and StoryEmb models. The statistics exclude our task prefix.}}{11}{table.caption.7}\protected@file@percent }
\newlabel{tab:attribution}{{6}{11}{The average contribution to sentence similarity of selected named-entity and parts-of-speech tags was analyzed on layer 31 of the E5 and StoryEmb models. The statistics exclude our task prefix}{table.caption.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Qualitative Exploration}{11}{section.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8}Conclusion}{11}{section.8}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Our model considers these two texts much more similar than the standard E5.}}{12}{figure.caption.8}\protected@file@percent }
\newlabel{fig:qualitative}{{2}{12}{Our model considers these two texts much more similar than the standard E5}{figure.caption.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9}Future Work}{12}{section.9}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {10}Limitations}{12}{section.10}\protected@file@percent }
\bibstyle{acl_natbib}
\bibcite{behnamghader2024llm2vec}{{1}{2024}{{BehnamGhader et~al.}}{{}}}
\bibcite{chambers2008unsupervised}{{2}{2008}{{Chambers and Jurafsky}}{{}}}
\bibcite{chambers2009unsupervised}{{3}{2009}{{Chambers and Jurafsky}}{{}}}
\bibcite{chaturvedi2018where}{{4}{2018}{{Chaturvedi et~al.}}{{}}}
\bibcite{chen2022semeval}{{5}{2022a}{{Chen et~al.}}{{}}}
\bibcite{chen2022semevalcodebook}{{6}{2022b}{{Chen et~al.}}{{}}}
\bibcite{cer2017semeval}{{7}{2017}{{Cer et~al.}}{{}}}
\bibcite{gao2021scaling}{{8}{2021}{{Gao et~al.}}{{}}}
\@writefile{toc}{\contentsline {section}{\numberline {11}Ethical Considerations}{13}{section.11}\protected@file@percent }
\bibcite{glass2022adaptive}{{9}{2022}{{Glass}}{{}}}
\bibcite{goldman2023rise}{{10}{2023}{{Goldman}}{{}}}
\bibcite{granroth2016what}{{11}{2016}{{Granroth-Wilding and Clark}}{{}}}
\bibcite{hatzel2023narrative}{{12}{2023}{{Hatzel and Biemann}}{{}}}
\bibcite{hatzel2024tell}{{13}{2024}{{Hatzel and Biemann}}{{}}}
\bibcite{jiang2023mistral}{{14}{2023a}{{Jiang et~al.}}{{}}}
\bibcite{jiang2023transferring}{{15}{2023b}{{Jiang et~al.}}{{}}}
\bibcite{kukkonen2019plot}{{16}{2019}{{Kukkonen}}{{}}}
\bibcite{lau2016empirical}{{17}{2016}{{Lau and Baldwin}}{{}}}
\bibcite{le2014distributed}{{18}{2014}{{Le and Mikolov}}{{}}}
\bibcite{lee2020story}{{19}{2020}{{Lee and Jung}}{{}}}
\bibcite{mann1947test}{{20}{1947}{{Mann and Whitney}}{{}}}
\bibcite{manning2008introduction}{{21}{2008}{{Manning et~al.}}{{}}}
\bibcite{moeller2024approximate}{{22}{2024}{{Moeller et~al.}}{{}}}
\bibcite{mostafazadeh2016corpus}{{23}{2016}{{Mostafazadeh et~al.}}{{}}}
\bibcite{ni2022sentence}{{24}{2022}{{Ni et~al.}}{{}}}
\bibcite{reimers2019sentence}{{25}{2019}{{Reimers and Gurevych}}{{}}}
\bibcite{springer2024repetition}{{26}{2024}{{Springer et~al.}}{{}}}
\bibcite{sundararajan2017axiomatic}{{27}{2017}{{Sundararajan et~al.}}{{}}}
\bibcite{wang2024improving}{{28}{2024}{{Wang et~al.}}{{}}}
\bibcite{wei2022finetuned}{{29}{2022}{{Wei et~al.}}{{}}}
\@writefile{toc}{\contentsline {section}{\numberline {A}LLM Judge}{15}{appendix.A}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {B}Retelling Dataset}{15}{appendix.B}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {C}Retelling Dataset Results}{16}{appendix.C}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Retrieval performance on retelling dataset introduced in Section 4.1.3}}{16}{table.caption.11}\protected@file@percent }
\newlabel{tab:retellingfull}{{7}{16}{Retrieval performance on retelling dataset introduced in Section 4.1.3}{table.caption.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {D}Data \& Code Availability}{16}{appendix.D}\protected@file@percent }
\gdef \@abspage@last{16}
