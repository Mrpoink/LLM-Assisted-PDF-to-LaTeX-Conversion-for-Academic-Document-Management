\relax 
\citation{karpukhin2020dense}
\citation{qin2024large}
\citation{zeng2022curriculum}
\citation{sun2024lead}
\citation{lu2022ernie}
\citation{bengio2009curriculum}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{}\protected@file@percent }
\citation{nogueira2019doc2query}
\citation{formal2021splade}
\citation{lin2021few}
\citation{gao2021coil}
\citation{xiong2021approximate}
\citation{zeng2022curriculum}
\citation{sun2024lead}
\citation{lu2023prod}
\citation{ren2021rocketqav2}
\citation{hinton2015distilling}
\citation{beyer2022knowledge}
\citation{romero2015fitnets}
\citation{chen2018darkrank}
\citation{peng2019correlation}
\citation{huang2022knowledge}
\citation{yang2022cross}
\citation{mu2021sharpening}
\citation{son2021densely}
\citation{lin2023prod}
\citation{mirzadeh2020improved}
\citation{yuan2021reinforced}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces MTA4DPR Framework. MTA4DPR transfers knowledge from the teacher to the student with the help of the best assistant. The Fusion Module is used to generate fused assistants from the original assistants, and the Selection Module is used to select the best assistant among all original and fused assistants. The dotted arrows indicate that the corresponding procedures are not involved in the backpropagation of the training.}}{3}{}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:framework}{{1}{3}{}{figure.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Dense Retrieval}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Knowledge Distillation}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Methodology}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Preliminary}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}Task Description}{3}{}\protected@file@percent }
\citation{karpukhin2020dense}
\citation{devlin2019bert}
\citation{cormack2009reciprocal}
\citation{cormack2009reciprocal}
\citation{lin2023prod}
\citation{mienye2020improved}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2}Dual-Encoders and Cross-Encoders}{4}{}\protected@file@percent }
\newlabel{eq:dual_encoder}{{1}{4}{}{equation.1}{}}
\newlabel{eq:cross_encoder}{{2}{4}{}{equation.2}{}}
\newlabel{eq:contrastive_loss}{{3}{4}{}{equation.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.3}Knowledge Distillation for DPR}{4}{}\protected@file@percent }
\newlabel{eq:teacher_softmax}{{4}{4}{}{equation.4}{}}
\newlabel{eq:student_softmax}{{5}{4}{}{equation.5}{}}
\newlabel{eq:kl_divergence}{{6}{4}{}{equation.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}The MTA4DPR Framework}{4}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Data Preparation}{4}{}\protected@file@percent }
\newlabel{sec:data_prep}{{3.2.1}{4}{}{subsubsection.3.2.1}{}}
\newlabel{eq:rrf}{{7}{4}{}{equation.7}{}}
\citation{craswell2020overview1}
\citation{craswell2020overview2}
\citation{kwiatkowski2019natural}
\citation{robertson2009probabilistic}
\citation{dai2019deeper}
\citation{mao2021generation}
\citation{nogueira2019doc2query}
\citation{gao2021coil}
\citation{lin2021few}
\citation{formal2021splade}
\citation{karpukhin2020dense}
\citation{xiong2021approximate}
\citation{gao2021condenser}
\citation{lee2024rethinking}
\citation{wu2023contextual}
\citation{ni2022large}
\citation{ma2024fine}
\citation{qu2021rocketqa}
\citation{ren2021pair}
\citation{ren2021rocketqav2}
\citation{lu2022ernie}
\citation{wang2023simlm}
\citation{xiao2022retromae}
\citation{sun2024lead}
\citation{zeng2022curriculum}
\citation{lin2023prod}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Fusion Strategy}{5}{}\protected@file@percent }
\newlabel{eq:fusion}{{8}{5}{}{equation.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3}Assistant Selection}{5}{}\protected@file@percent }
\newlabel{sec:assistant_selection}{{3.2.3}{5}{}{subsubsection.3.2.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.4}The Student Model Optimization}{5}{}\protected@file@percent }
\newlabel{sec:student_optimization}{{3.2.4}{5}{}{subsubsection.3.2.4}{}}
\newlabel{eq:total_loss}{{9}{5}{}{equation.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experiments and Analysis}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Experimental Settings}{5}{}\protected@file@percent }
\citation{lu2024m2dpr}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Main Results}{6}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Ablation Study}{6}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Main results on MS MARCO and DL 19 and 20 datasets. The best scores are marked in bold, and the second places are underlined. ``KD'' denotes knowledge distillation, and ``\#Params'' represents the number of model parameters. Please note that, by SimLM, we mean SimLM-distilled, not SimLM-reranker or SimLM-base.}}{7}{}\protected@file@percent }
\newlabel{tab:main_results_msmarco}{{1}{7}{}{table.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Main results on NQ. ``\#Params'' represents the number of model parameters.}}{7}{}\protected@file@percent }
\newlabel{tab:main_results_nq}{{2}{7}{}{table.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Ablation results on MS MARCO and NQ.}}{7}{}\protected@file@percent }
\newlabel{tab:ablation}{{3}{7}{}{table.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Analysis}{7}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Multi-iteration Retrieval Performance on MS MARCO and NQ.}}{8}{}\protected@file@percent }
\newlabel{tab:iteration_perf}{{4}{8}{}{table.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Performance of MTA4DPR models with different selection methods on MS MARCO and NQ. ``SF'' denotes Spearman's Footrule.}}{8}{}\protected@file@percent }
\newlabel{tab:selection_methods}{{5}{8}{}{table.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.1}Multi-iteration Retrieval Performance}{8}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.2}The impact of selection methods}{8}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Results of MTA4DPR models with different sizes on MS MARCO. ``\#Layers'' denotes the number of layers of the model, and ``\#Emb'' denotes the embedding size of the model. ``\#Params'' denotes the number of model parameters. ``$\uparrow $'' denotes the improvement compared with traditional knowledge distillation methods.}}{8}{}\protected@file@percent }
\newlabel{tab:model_sizes}{{6}{8}{}{table.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.3}The impact of the number of layers and the embedding sizes of student models}{8}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.4}The impact of the performance of assistant models}{8}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The composition of the best teaching assistants selected on MS MARCO. ``R'' denotes RetroMAE, ``S'' denotes SimLM, ``M'' denotes M2DPR and ``R\&M'' denotes the fusion result of RetroMAE and M2DPR.}}{9}{}\protected@file@percent }
\newlabel{fig:assistant_composition}{{2}{9}{}{figure.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.5}The composition of the best assistant}{9}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.6}The complexity of the training process}{9}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.7}The computational costs of MTA4DPR}{9}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{9}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Results of distilled DPR models with different assistants combinations on MS MARCO dev set and DL 19 and 20 datasets. ``C'', ``S'', ``R'' and ``M'' represent CotMAE, SimLM, RetroMAE and M2DPR, respectively. ``C\&S'' denotes the fusion result of CotMAE and SimLM.}}{10}{}\protected@file@percent }
\newlabel{tab:assistant_combinations}{{7}{10}{}{table.7}{}}
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces The complexity of the training process.}}{10}{}\protected@file@percent }
\newlabel{tab:training_time}{{8}{10}{}{table.8}{}}
\@writefile{lot}{\contentsline {table}{\numberline {9}{\ignorespaces The computational costs of student DPR models with different sizes. ``Encoding Time'' is the time taken to encode the whole MS MARCO corpus. ``\#Emb'' denotes the embedding size of the model. Please note that this metric is pure GPU computation time and doesn't include the time for data loading or other operations. ``bs'' denotes the batch size.}}{10}{}\protected@file@percent }
\newlabel{tab:computational_costs}{{9}{10}{}{table.9}{}}
\bibstyle{acl_natbib}
\bibcite{bengio2009curriculum}{Bengio et al.2009}
\bibcite{beyer2022knowledge}{Beyer et al.2022}
\bibcite{chen2018darkrank}{Chen et al.2018}
\bibcite{cormack2009reciprocal}{Cormack et al.2009}
\bibcite{craswell2020overview1}{Craswell et al.2020a}
\bibcite{craswell2020overview2}{Craswell et al.2020b}
\bibcite{dai2019deeper}{Dai and Callan2019}
\bibcite{devlin2019bert}{Devlin et al.2019}
\bibcite{formal2021splade}{Formal et al.2021}
\bibcite{gao2021condenser}{Gao and Callan2021a}
\bibcite{gao2021few}{Gao and Callan2021b}
\bibcite{gao2021coil}{Gao et al.2021}
\@writefile{toc}{\contentsline {section}{\numberline {A}Algorithm}{11}{}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces MTA4DPR Training Process}}{11}{}\protected@file@percent }
\newlabel{alg:training}{{1}{11}{}{algorithm.1}{}}
\bibcite{heo2019knowledge}{Heo et al.2019}
\bibcite{hinton2015distilling}{Hinton et al.2015}
\bibcite{huang2022knowledge}{Huang et al.2022}
\bibcite{karpukhin2020dense}{Karpukhin et al.2020}
\bibcite{kwiatkowski2019natural}{Kwiatkowski et al.2019}
\bibcite{lee2024rethinking}{Lee et al.2024}
\bibcite{lin2021few}{Lin and Ma2021}
\bibcite{lin2023prod}{Lin et al.2023}
\bibcite{lu2024m2dpr}{Lu2024}
\bibcite{lu2022ernie}{Lu et al.2022}
\bibcite{ma2024fine}{Ma et al.2024}
\bibcite{mao2021generation}{Mao et al.2021}
\bibcite{mienye2020improved}{Mienye et al.2020}
\bibcite{mirzadeh2020improved}{Mirzadeh et al.2020}
\bibcite{ni2022large}{Ni et al.2022}
\bibcite{nogueira2019doc2query}{Nogueira et al.2019}
\bibcite{peng2019correlation}{Peng et al.2019}
\bibcite{qin2024large}{Qin et al.2024}
\bibcite{qu2021rocketqa}{Qu et al.2021}
\bibcite{ren2021pair}{Ren et al.2021a}
\bibcite{ren2021rocketqav2}{Ren et al.2021b}
\bibcite{robertson2009probabilistic}{Robertson and Zaragoza2009}
\bibcite{son2021densely}{Son et al.2021}
\bibcite{sun2024lead}{Sun et al.2024}
\bibcite{wang2023simlm}{Wang et al.2023}
\bibcite{wu2023contextual}{Wu et al.2023}
\bibcite{xiao2022retromae}{Xiao et al.2022}
\bibcite{xiong2021approximate}{Xiong et al.2021}
\bibcite{yang2022cross}{Yang et al.2022}
\bibcite{yuan2021reinforced}{Yuan et al.2021}
\bibcite{zeng2022curriculum}{Zeng et al.2022}
\gdef \@abspage@last{14}
