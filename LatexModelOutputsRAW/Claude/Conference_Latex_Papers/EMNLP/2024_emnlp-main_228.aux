\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{chang2023speak,mozes2023use,eldan2023whos,ye2022learning}
\citation{blanco2024digital,liu2024rethinking}
\citation{eldan2023whos,jang2023knowledge,yao2024large,rafailov2023direct}
\citation{hong2024intrinsic,lee2024mechanistic}
\citation{meng2022locating,pochinkov2024dissecting,geva2021transformer}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}\protected@file@percent }
\citation{touvron2023llama}
\citation{groeneveld2024olmo}
\citation{liu2024rethinking,chang2023speak,mozes2023use}
\citation{jang2023knowledge,yao2024large,yao2023large}
\citation{rafailov2023direct,zhao2024towards,lee2024mechanistic2}
\citation{eldan2023whos,jang2023knowledge,yao2024large,rafailov2023direct}
\citation{chang2023localization,stoehr2024localizing}
\citation{lu2024eraser,chen2023unlearn}
\citation{patil2024can}
\citation{meng2022locating,geva2021transformerb,sukhbaatar2015end,geva2023dissecting}
\citation{geva2021transformerb}
\citation{radford2019language}
\citation{chen2021evaluating}
\citation{touvron2023llama}
\@writefile{toc}{\contentsline {section}{\numberline {2}Background and Related Work}{3}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Unlearning in Large Language Models}{3}{section*.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Knowledge Storation in Large Language Models}{3}{section*.2}\protected@file@percent }
\citation{geva2023dissecting}
\newlabel{eq:mlp}{{1}{4}{Knowledge Storation in Large Language Models}{equation.1}{}}
\newlabel{eq:hidden}{{2}{4}{Knowledge Storation in Large Language Models}{equation.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Patching Investigation}{4}{section.3}\protected@file@percent }
\newlabel{sec:patching}{{3}{4}{Patching Investigation}{section.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Hypothesis and Experimental Design}{4}{section*.3}\protected@file@percent }
\citation{touvron2023llama}
\citation{groeneveld2024olmo}
\citation{rafailov2023direct}
\citation{yao2024large}
\citation{eldan2023whos}
\citation{hong2024intrinsic}
\citation{soldaini2024dolma}
\citation{meng2022locating,meng2023mass}
\newlabel{eq:krs}{{3}{5}{Hypothesis and Experimental Design}{equation.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Activation Patching and Parameters Restoration Experiments}{5}{section*.4}\protected@file@percent }
\citation{geva2023dissecting}
\citation{rafailov2023direct}
\citation{zhao2024towards}
\citation{zhao2024towards}
\citation{yao2024large}
\citation{papineni2002bleu}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Results of KRS on LLaMA and OLMo under three activations patching or parameters restoration settings. We also included another setting that restores both attention and coefficients to compare the final outcomes.}}{6}{figure.1}\protected@file@percent }
\newlabel{fig:krs_results}{{1}{6}{Results of KRS on LLaMA and OLMo under three activations patching or parameters restoration settings. We also included another setting that restores both attention and coefficients to compare the final outcomes}{figure.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Global Negative Effect of Fine-Tuning Unlearning}{6}{section.4}\protected@file@percent }
\newlabel{sec:global}{{4}{6}{Global Negative Effect of Fine-Tuning Unlearning}{section.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Unlearning testing results on LLaMA and OLMo for each training epoch.}}{7}{figure.2}\protected@file@percent }
\newlabel{fig:unlearning_results}{{2}{7}{Unlearning testing results on LLaMA and OLMo for each training epoch}{figure.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Discussion and Conclusion}{7}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Limitations}{7}{section.6}\protected@file@percent }
\bibstyle{plainnat}
\bibcite{blanco2024digital}{{1}{2024}{{Blanco-Justicia et~al.}}{{}}}
\bibcite{chang2023speak}{{2}{2023a}{{Chang et~al.}}{{}}}
\bibcite{chang2023localization}{{3}{2023b}{{Chang et~al.}}{{}}}
\bibcite{chen2023unlearn}{{4}{2023}{{Chen and Yang}}{{}}}
\bibcite{chen2021evaluating}{{5}{2021}{{Chen et~al.}}{{}}}
\bibcite{eldan2023whos}{{6}{2023}{{Eldan and Russinovich}}{{}}}
\bibcite{geva2021transformer}{{7}{2021a}{{Geva et~al.}}{{}}}
\bibcite{geva2021transformerb}{{8}{2021b}{{Geva et~al.}}{{}}}
\bibcite{geva2023dissecting}{{9}{2023}{{Geva et~al.}}{{}}}
\bibcite{groeneveld2024olmo}{{10}{2024}{{Groeneveld et~al.}}{{}}}
\bibcite{hong2024intrinsic}{{11}{2024}{{Hong et~al.}}{{}}}
\bibcite{jang2023knowledge}{{12}{2023}{{Jang et~al.}}{{}}}
\bibcite{lee2024mechanistic}{{13}{2024a}{{Lee et~al.}}{{}}}
\bibcite{lee2024mechanistic2}{{14}{2024b}{{Lee et~al.}}{{}}}
\bibcite{liu2024rethinking}{{15}{2024}{{Liu et~al.}}{{}}}
\bibcite{lu2024eraser}{{16}{2024}{{Lu et~al.}}{{}}}
\bibcite{meng2022locating}{{17}{2022}{{Meng et~al.}}{{}}}
\bibcite{meng2023mass}{{18}{2023}{{Meng et~al.}}{{}}}
\bibcite{mozes2023use}{{19}{2023}{{Mozes et~al.}}{{}}}
\bibcite{papineni2002bleu}{{20}{2002}{{Papineni et~al.}}{{}}}
\bibcite{patil2024can}{{21}{2024}{{Patil et~al.}}{{}}}
\bibcite{pochinkov2024dissecting}{{22}{2024}{{Pochinkov and Schoots}}{{}}}
\bibcite{radford2019language}{{23}{2019}{{Radford et~al.}}{{}}}
\bibcite{rafailov2023direct}{{24}{2023}{{Rafailov et~al.}}{{}}}
\bibcite{soldaini2024dolma}{{25}{2024}{{Soldaini et~al.}}{{}}}
\bibcite{stoehr2024localizing}{{26}{2024}{{Stoehr et~al.}}{{}}}
\bibcite{sukhbaatar2015end}{{27}{2015}{{Sukhbaatar et~al.}}{{}}}
\bibcite{touvron2023llama}{{28}{2023}{{Touvron et~al.}}{{}}}
\bibcite{yao2023large}{{29}{2023}{{Yao et~al.}}{{}}}
\bibcite{yao2024large}{{30}{2024}{{Yao et~al.}}{{}}}
\bibcite{ye2022learning}{{31}{2022}{{Ye et~al.}}{{}}}
\bibcite{zhao2024towards}{{32}{2024}{{Zhao et~al.}}{{}}}
\citation{yao2024large}
\@writefile{toc}{\contentsline {section}{\numberline {A}Details in Existing Unlearning Methods}{10}{appendix.A}\protected@file@percent }
\newlabel{sec:appendixA}{{A}{10}{Details in Existing Unlearning Methods}{appendix.A}{}}
\citation{rafailov2023direct}
\citation{zhao2024towards}
\citation{hong2024intrinsic}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Statistics of the training data for the unlearning experiments on LLaMA and OLMo}}{11}{table.1}\protected@file@percent }
\newlabel{tab:statistics}{{1}{11}{Statistics of the training data for the unlearning experiments on LLaMA and OLMo}{table.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B}Unlearning Experiment's Corpus}{11}{appendix.B}\protected@file@percent }
\newlabel{sec:appendixB}{{B}{11}{Unlearning Experiment's Corpus}{appendix.B}{}}
\@writefile{toc}{\contentsline {section}{\numberline {C}More Rigorous Patching Investigation}{11}{appendix.C}\protected@file@percent }
\newlabel{sec:appendixC}{{C}{11}{More Rigorous Patching Investigation}{appendix.C}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Results of KRS on LLaMA and OLMo under three activations patching or parameters restoration settings, isolating the effects of the two others when investigating each factor individually.}}{12}{figure.3}\protected@file@percent }
\newlabel{fig:krs_rigorous}{{3}{12}{Results of KRS on LLaMA and OLMo under three activations patching or parameters restoration settings, isolating the effects of the two others when investigating each factor individually}{figure.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Example extracted data from the Redpjama and Dolma pre-training datasets.}}{13}{table.2}\protected@file@percent }
\newlabel{tab:examples}{{2}{13}{Example extracted data from the Redpjama and Dolma pre-training datasets}{table.2}{}}
\gdef \@abspage@last{13}
