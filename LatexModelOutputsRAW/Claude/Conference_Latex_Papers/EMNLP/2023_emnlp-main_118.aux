\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{fan2021pretrain}
\citation{guo2022semantic}
\citation{lin2021pretrained}
\citation{karpukhin2020dense}
\citation{lee2020learning}
\citation{zhu2021adaptive}
\citation{gao2022neural}
\citation{yu2021fewshot}
\citation{devlin2018bert}
\citation{liu2019roberta}
\citation{xiong2020approximate}
\citation{lu2021less}
\citation{hofstatter2021efficiently}
\citation{gao2021unsupervised}
\citation{ren2021rocketqav2}
\citation{ma2022pretrain}
\citation{liu2022retromae}
\citation{wu2022contextual}
\citation{wang2022simlm}
\citation{gao2021unsupervised}
\citation{wu2022contextual}
\citation{nguyen2016ms}
\citation{gao2022unsupervised}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\citation{nogueira2019doc2query}
\citation{dai2019context}
\citation{mao2020generation}
\citation{bai2020sparterm}
\citation{formal2021splade}
\citation{formal2021splade2}
\citation{mallia2021learning}
\citation{shen2022lexmae}
\citation{nogueira2019doc2query}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces An example of low-relevance passages within a document from the MS-MARCO corpus. The two passages are weakly correlated in content.}}{2}{figure.1}\protected@file@percent }
\newlabel{fig:example}{{1}{2}{An example of low-relevance passages within a document from the MS-MARCO corpus. The two passages are weakly correlated in content}{figure.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces A comparison of context-supervised pre-training and query-as-context pre-training.}}{2}{figure.2}\protected@file@percent }
\newlabel{fig:comparison}{{2}{2}{A comparison of context-supervised pre-training and query-as-context pre-training}{figure.2}{}}
\citation{nguyen2016ms}
\citation{craswell2020overview2019}
\citation{craswell2020overview2020}
\citation{thakur2021beir}
\citation{gao2021unsupervised}
\@writefile{toc}{\contentsline {section}{\numberline {2}Preliminary: Context-supervised Pre-training}{3}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Pre-training Corpus}{3}{section*.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Masked Language Modeling (MLM)}{3}{section*.2}\protected@file@percent }
\citation{gao2021unsupervised}
\citation{wu2022contextual}
\citation{wu2022contextual}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}coCondenser}{4}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}CoT-MAE}{4}{subsection.2.2}\protected@file@percent }
\citation{nogueira2019doc2query}
\citation{gao2021unsupervised}
\citation{wu2022contextual}
\citation{gao2022tevatron}
\citation{thakur2021beir}
\@writefile{toc}{\contentsline {section}{\numberline {3}Query-as-context Pre-training}{5}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Pre-training}{5}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Fine-tuning}{5}{subsection.3.2}\protected@file@percent }
\citation{gao2021unsupervised}
\citation{wu2022contextual}
\citation{wu2022contextual}
\citation{gao2021unsupervised}
\citation{gao2021condenser}
\citation{nguyen2016ms}
\citation{craswell2020overview2019}
\citation{craswell2020overview2020}
\citation{nguyen2016ms}
\citation{gao2021unsupervised}
\citation{wu2022contextual}
\citation{craswell2020overview2019}
\citation{craswell2020overview2020}
\citation{gao2021unsupervised}
\citation{wu2022contextual}
\citation{gao2022tevatron}
\citation{ren2021rocketqav2}
\citation{santhanam2021colbertv2}
\citation{khattab2020colbert}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Illustration of the fine-tuning pipeline. The query-as-context pre-trained model is used to initialize the dual-encoder retrievers.}}{6}{figure.3}\protected@file@percent }
\newlabel{fig:finetune}{{3}{6}{Illustration of the fine-tuning pipeline. The query-as-context pre-trained model is used to initialize the dual-encoder retrievers}{figure.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experiment}{6}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Pre-training}{6}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Query-as-context Dataset}{6}{section*.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Model Implementation}{6}{section*.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Fine-tuning}{6}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Datasets and Evaluation}{6}{section*.5}\protected@file@percent }
\citation{qu2020rocketqa}
\citation{nogueira2019doc2query}
\citation{dai2019context}
\citation{mao2020generation}
\citation{gao2021unsupervised}
\citation{liu2022retromae}
\citation{ren2021rocketqav2}
\citation{ma2022pretrain}
\citation{xiong2020approximate}
\citation{lu2021less}
\citation{hofstatter2021efficiently}
\citation{liu2022retromae}
\citation{wang2022simlm}
\citation{gao2021unsupervised}
\citation{wu2022contextual}
\citation{dai2019context}
\citation{nogueira2019doc2query}
\citation{lu2020neural}
\citation{xiong2020approximate}
\citation{lu2021less}
\citation{hofstatter2021efficiently}
\citation{gao2021coil}
\citation{khattab2020colbert}
\citation{ma2022pretrain}
\citation{gao2021condenser}
\citation{qu2020rocketqa}
\citation{ren2021pair}
\citation{wang2022simlm}
\citation{liu2022retromae}
\citation{zhang2022led}
\citation{gao2021unsupervised}
\citation{wu2022contextual}
\citation{thakur2021beir}
\@writefile{toc}{\contentsline {paragraph}{Implementation}{7}{section*.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Baselines}{7}{subsection.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Main Results}{7}{subsection.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{coCondenser}{7}{section*.7}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{CoT-MAE}{7}{section*.8}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Main results on MS-MARCO passage ranking and TREC DL datasets. $\dagger $ denotes our reproduction using publicly available codes. The score that is better in comparison is marked in \textbf  {bold}.}}{8}{table.1}\protected@file@percent }
\newlabel{tab:main}{{1}{8}{Main results on MS-MARCO passage ranking and TREC DL datasets. $\dagger $ denotes our reproduction using publicly available codes. The score that is better in comparison is marked in \textbf {bold}}{table.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Out-of-domain Evaluation}{8}{subsection.4.5}\protected@file@percent }
\citation{xiong2020approximate}
\citation{zhan2021optimizing}
\citation{khattab2020colbert}
\citation{hofstatter2021efficiently}
\citation{lin2021inbatch}
\citation{santhanam2021colbertv2}
\citation{qu2020rocketqa}
\citation{ren2021rocketqav2}
\citation{zhang2022hlatr}
\citation{zhang2021adversarial}
\citation{lu2021less}
\citation{gao2021condenser}
\citation{liu2022retromae}
\citation{zhou2022master}
\citation{chang2020pretraining}
\citation{gao2021unsupervised}
\citation{ma2022pretrain}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Out-of-domain evaluation on BEIR benchmark. The score that is better in comparison is marked in \textbf  {bold}.}}{9}{table.2}\protected@file@percent }
\newlabel{tab:beir}{{2}{9}{Out-of-domain evaluation on BEIR benchmark. The score that is better in comparison is marked in \textbf {bold}}{table.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Analyses}{9}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Impact of Generated Query Number}{9}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Impact of Mixed Context}{9}{subsection.5.2}\protected@file@percent }
\citation{gao2021unsupervised}
\citation{wu2022contextual}
\citation{gao2021unsupervised}
\citation{wu2022contextual}
\citation{nogueira2019doc2query}
\citation{mallia2021learning}
\citation{li2022learning}
\citation{ma2020zero}
\citation{wang2021gpl}
\citation{li2022learning}
\citation{li2022learning}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Impact of the number of generated queries. The score that is better in comparison is marked in \textbf  {bold}.}}{10}{table.3}\protected@file@percent }
\newlabel{tab:query_number}{{3}{10}{Impact of the number of generated queries. The score that is better in comparison is marked in \textbf {bold}}{table.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Effect of mixing passage-query and passage-passage pairs in pre-training.}}{10}{table.4}\protected@file@percent }
\newlabel{tab:mixed}{{4}{10}{Effect of mixing passage-query and passage-passage pairs in pre-training}{table.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Related Works}{10}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Dense Retrieval}{10}{section*.9}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Query Prediction}{10}{section*.10}\protected@file@percent }
\citation{gao2022unsupervised}
\citation{nogueira2019doc2query}
\bibcite{bai2020sparterm}{1}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusions}{11}{section.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8}Limitations}{11}{section.8}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {A}Statistically Analysis of Weakly Correlated Passages}{11}{appendix.A}\protected@file@percent }
\newlabel{appendix:analysis}{{A}{11}{Statistically Analysis of Weakly Correlated Passages}{appendix.A}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Correlation statistics of human annotation results of different contextual pairs, each with 200 pairs. The score that is better in comparison is marked in \textbf  {bold}.}}{11}{table.5}\protected@file@percent }
\newlabel{tab:correlation}{{5}{11}{Correlation statistics of human annotation results of different contextual pairs, each with 200 pairs. The score that is better in comparison is marked in \textbf {bold}}{table.5}{}}
\bibcite{chang2020pretraining}{2}
\bibcite{craswell2020overview2019}{3}
\bibcite{craswell2020overview2020}{4}
\bibcite{dai2019context}{5}
\bibcite{devlin2018bert}{6}
\bibcite{fan2021pretrain}{7}
\bibcite{formal2021splade}{8}
\bibcite{formal2021splade2}{9}
\bibcite{gao2022neural}{10}
\bibcite{gao2021condenser}{11}
\bibcite{gao2021unsupervised}{12}
\bibcite{gao2022unsupervised}{13}
\bibcite{gao2021coil}{14}
\bibcite{gao2022tevatron}{15}
\bibcite{guo2022semantic}{16}
\bibcite{hofstatter2021efficiently}{17}
\bibcite{karpukhin2020dense}{18}
\bibcite{khattab2020colbert}{19}
\bibcite{lee2020learning}{20}
\bibcite{li2022learning}{21}
\bibcite{lin2021pretrained}{22}
\bibcite{lin2021inbatch}{23}
\bibcite{liu2019roberta}{24}
\bibcite{liu2022retromae}{25}
\bibcite{lu2020neural}{26}
\bibcite{lu2021less}{27}
\bibcite{ma2020zero}{28}
\bibcite{ma2022pretrain}{29}
\bibcite{mallia2021learning}{30}
\bibcite{mao2020generation}{31}
\bibcite{nguyen2016ms}{32}
\bibcite{nogueira2019doc2query}{33}
\bibcite{qu2020rocketqa}{34}
\bibcite{ren2021pair}{35}
\bibcite{ren2021rocketqav2}{36}
\bibcite{santhanam2021colbertv2}{37}
\bibcite{shen2022lexmae}{38}
\bibcite{thakur2021beir}{39}
\bibcite{wang2021gpl}{40}
\bibcite{wang2022simlm}{41}
\bibcite{wu2022contextual}{42}
\bibcite{xiong2020approximate}{43}
\bibcite{yu2021fewshot}{44}
\bibcite{zhan2021optimizing}{45}
\bibcite{zhang2021adversarial}{46}
\bibcite{zhang2022led}{47}
\bibcite{zhang2022hlatr}{48}
\bibcite{zhou2022master}{49}
\bibcite{zhu2021adaptive}{50}
\gdef \@abspage@last{15}
