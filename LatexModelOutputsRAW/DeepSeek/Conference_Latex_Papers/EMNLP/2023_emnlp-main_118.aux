\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Preliminary: Context-supervised Pre-training}{3}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}coCondenser}{3}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}CoT-MAE}{4}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Query-as-context Pre-training}{4}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Pre-training}{4}{subsection.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Illustration of the fine-tuning pipeline. The query-as-context pre-trained model is used to initialize the dual-encoder retrievers.}}{5}{figure.1}\protected@file@percent }
\newlabel{fig:fine-tuning}{{1}{5}{Illustration of the fine-tuning pipeline. The query-as-context pre-trained model is used to initialize the dual-encoder retrievers}{figure.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Fine-tuning}{5}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Experiment}{5}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Pre-training Query-as-context Dataset}{5}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Model Implementation}{5}{subsubsection.4.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Fine-tuning}{5}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Datasets and Evaluation}{5}{subsubsection.4.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}Implementation}{6}{subsubsection.4.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Baselines}{6}{subsection.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Main Results}{6}{subsection.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Out-of-domain Evaluation}{6}{subsection.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Analyses}{7}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Impact of Generated Query Number}{7}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Impact of Mixed Context}{7}{subsection.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Related Works}{7}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Dense Retrieval}{7}{subsection.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Query Prediction}{7}{subsection.6.2}\protected@file@percent }
\bibcite{bai2020sparterm}{1}
\bibcite{chang2020pre}{2}
\bibcite{craswell2020overview2019}{3}
\bibcite{craswell2020overview2020}{4}
\bibcite{dai2019context}{5}
\bibcite{devlin2018bert}{6}
\bibcite{fan2021pre}{7}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusions}{8}{section.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8}Limitations}{8}{section.8}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {A}Statistically Analysis of Weakly Correlated Passages}{8}{appendix.A}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Correlation statistics of human annotation results of different contextual pairs, each with 200 pairs. The score that is better in comparison is marked in bold.}}{8}{table.1}\protected@file@percent }
\newlabel{tab:correlation}{{1}{8}{Correlation statistics of human annotation results of different contextual pairs, each with 200 pairs. The score that is better in comparison is marked in bold}{table.1}{}}
\bibcite{formal2021spladev2}{8}
\bibcite{formal2021splade}{9}
\bibcite{gao2022neural}{10}
\bibcite{gao2021condenser}{11}
\bibcite{gao2021unsupervised}{12}
\bibcite{gao2022unsupervised}{13}
\bibcite{gao2021coil}{14}
\bibcite{gao2022tevatron}{15}
\bibcite{guo2022semantic}{16}
\bibcite{hofstatter2021efficiently}{17}
\bibcite{karpukhin2020dense}{18}
\bibcite{khattab2020colbert}{19}
\bibcite{lee2020learning}{20}
\bibcite{li2022learning}{21}
\bibcite{lin2021pretrained}{22}
\bibcite{lin2021inbatch}{23}
\bibcite{liu2019roberta}{24}
\bibcite{liu2022retromae}{25}
\bibcite{lu2020neural}{26}
\bibcite{lu2021less}{27}
\bibcite{ma2020zero}{28}
\bibcite{ma2022pre}{29}
\bibcite{mallia2021learning}{30}
\bibcite{mao2020generation}{31}
\bibcite{nguyen2016ms}{32}
\bibcite{nogueira2019from}{33}
\bibcite{qu2020rocketqa}{34}
\bibcite{ren2021pair}{35}
\bibcite{ren2021rocketqav2}{36}
\bibcite{santhanam2021colbertv2}{37}
\bibcite{shen2022lexmae}{38}
\bibcite{thakur2021beir}{39}
\bibcite{wang2021gpl}{40}
\bibcite{wang2022simlm}{41}
\bibcite{wu2022contextual}{42}
\bibcite{xiong2020approximate}{43}
\bibcite{yu2021few}{44}
\bibcite{zhan2021optimizing}{45}
\bibcite{zhang2021adversarial}{46}
\bibcite{zhang2022led}{47}
\bibcite{zhang2022hlatr}{48}
\bibcite{zhou2022master}{49}
\bibcite{zhu2021adaptive}{50}
\gdef \@abspage@last{11}
