\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{3}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Model}{4}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}DAE: Denoising Autoencoder phase}{4}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}CL: Contrastive Learning phase}{4}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Imbalance correction}{4}{subsubsection.3.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}FT: Fine-tuning phase}{5}{subsection.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}Joint}{5}{subsubsection.3.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Experimental Setup}{5}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Datasets}{5}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Models and Training}{6}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Metrics}{6}{subsection.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Results}{6}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Ablation study}{7}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{7}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Future work}{7}{section.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8}Limitations}{7}{section.8}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {9}Acknowledgments}{7}{section.9}\protected@file@percent }
\bibcite{Brown2020}{1}
\bibcite{Chopra2005}{2}
\bibcite{Conneau2018}{3}
\bibcite{Coucke2018}{4}
\bibcite{Devlin2018}{5}
\bibcite{Finn2017}{6}
\bibcite{Gao2021}{7}
\bibcite{Germain2015}{8}
\bibcite{Gutmann2010}{9}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Statistics for Train, Validation and Test dataset splits.}}{8}{table.caption.4}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:dataset_stats}{{1}{8}{Statistics for Train, Validation and Test dataset splits}{table.caption.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Average and max lengths for each of the datasets mentioned in the paper.}}{8}{table.caption.5}\protected@file@percent }
\newlabel{tab:length_stats}{{2}{8}{Average and max lengths for each of the datasets mentioned in the paper}{table.caption.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A}Appendix}{8}{appendix.A}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}Hyper-parameters}{8}{subsection.A.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}Other metrics}{8}{subsection.A.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Performance accuracy on different datasets using RoBERTa and all-MiniLM-L12-v2 models in \%. 3-Phase refers to our main 3 stages approach, while Joint denotes one whose loss is based on the combination of the first two losses, and FT corresponds to the fine-tuning. We also add some SOTA results from other papers: S-P denotes Stack-Propagation (Qin et al., 2019), Self-E is used to denote (Sun et al., 2020b) (in this case we chose the value from RoBERTa-base for a fair comparison), CAE is used to detonate (Phuong et al., 2022), STC-DeBERTa refers to (Karl and Scherp, 2022), EFL points to (Wang et al., 2021b) (this one uses RoBERTa-Large), and FTBERT is (Sun et al., 2020a)}}{9}{table.caption.6}\protected@file@percent }
\newlabel{tab:main_results}{{3}{9}{Performance accuracy on different datasets using RoBERTa and all-MiniLM-L12-v2 models in \%. 3-Phase refers to our main 3 stages approach, while Joint denotes one whose loss is based on the combination of the first two losses, and FT corresponds to the fine-tuning. We also add some SOTA results from other papers: S-P denotes Stack-Propagation (Qin et al., 2019), Self-E is used to denote (Sun et al., 2020b) (in this case we chose the value from RoBERTa-base for a fair comparison), CAE is used to detonate (Phuong et al., 2022), STC-DeBERTa refers to (Karl and Scherp, 2022), EFL points to (Wang et al., 2021b) (this one uses RoBERTa-Large), and FTBERT is (Sun et al., 2020a)}{table.caption.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Ablation results. As before, 3-Phase, Joint, and FT correspond to the 3 stages approach, joint losses, and Fine-tuning, respectively. Here, \(DAE + FT\) denotes the denoising autoencoder together with fine-tuning, \(CL + FT\) denotes the contrastive Siamese training together with fine-tuning, No Imb. means 3-Phase but skipping the imbalance correction, and Extra Imb. refers to an increase of the imbalance correction to a \(min_{ratio} = 1.5\) and \(max_{ratio} = 20\)}}{9}{table.caption.7}\protected@file@percent }
\newlabel{tab:ablation}{{4}{9}{Ablation results. As before, 3-Phase, Joint, and FT correspond to the 3 stages approach, joint losses, and Fine-tuning, respectively. Here, \(DAE + FT\) denotes the denoising autoencoder together with fine-tuning, \(CL + FT\) denotes the contrastive Siamese training together with fine-tuning, No Imb. means 3-Phase but skipping the imbalance correction, and Extra Imb. refers to an increase of the imbalance correction to a \(min_{ratio} = 1.5\) and \(max_{ratio} = 20\)}{table.caption.7}{}}
\newlabel{fig:dae}{{1a}{9}{First stage: Denoising Autoencoder architecture where the encoder and the decoder are based on the pre-trained Transformers. The middle layer following the pooling together with the encoder model will be the resulting model of this stage}{figure.caption.11}{}}
\newlabel{sub@fig:dae}{{a}{9}{First stage: Denoising Autoencoder architecture where the encoder and the decoder are based on the pre-trained Transformers. The middle layer following the pooling together with the encoder model will be the resulting model of this stage}{figure.caption.11}{}}
\newlabel{fig:cl}{{1b}{9}{Second stage: Supervised Contrastive Learning phase. We add a pooling layer to the previous one to learn the new clustered representation. The set of blocks denoted as DAECL will be employed and adapted as a base model in the fine-tuning phase}{figure.caption.11}{}}
\newlabel{sub@fig:cl}{{b}{9}{Second stage: Supervised Contrastive Learning phase. We add a pooling layer to the previous one to learn the new clustered representation. The set of blocks denoted as DAECL will be employed and adapted as a base model in the fine-tuning phase}{figure.caption.11}{}}
\newlabel{fig:ft}{{1c}{9}{Third stage: Classification phase through fine-tuning}{figure.caption.11}{}}
\newlabel{sub@fig:ft}{{c}{9}{Third stage: Classification phase through fine-tuning}{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Model architecture overview.}}{9}{figure.caption.11}\protected@file@percent }
\newlabel{fig:model}{{1}{9}{Model architecture overview}{figure.caption.11}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Hyper-parameters configurations and search space of the experiments. *1 means these are not real epochs since the input data is not always the same. The data was masked on the fly; therefore, each epoch differs. *2 We used an early stopping approach for the \(FT\) phase. *3 We only consider the epsilon hyperparameter in the AdamW optimizer for \(FT\). The other two phases use the default value from the Transformers library (1e-06). *4 This hyper-parameter was estimated initially with the training dataset with a large margin. This was applied for datasets with very short sentences, like \(SNIPS\). *5 This hyper-parameter estimates the max length of the sequences using the \(10\%\) of the examples. This estimation is multiplied by 1.2 and is added as the maximum size of the sequences for the embedding layers. The difference is that it was done on the fly and not preserved in this case.}}{10}{table.caption.8}\protected@file@percent }
\newlabel{tab:hyperparams}{{5}{10}{Hyper-parameters configurations and search space of the experiments. *1 means these are not real epochs since the input data is not always the same. The data was masked on the fly; therefore, each epoch differs. *2 We used an early stopping approach for the \(FT\) phase. *3 We only consider the epsilon hyperparameter in the AdamW optimizer for \(FT\). The other two phases use the default value from the Transformers library (1e-06). *4 This hyper-parameter was estimated initially with the training dataset with a large margin. This was applied for datasets with very short sentences, like \(SNIPS\). *5 This hyper-parameter estimates the max length of the sequences using the \(10\%\) of the examples. This estimation is multiplied by 1.2 and is added as the maximum size of the sequences for the embedding layers. The difference is that it was done on the fly and not preserved in this case}{table.caption.8}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Precision and Recall values. The best values are shown in bold.}}{10}{table.caption.9}\protected@file@percent }
\newlabel{tab:precision_recall}{{6}{10}{Precision and Recall values. The best values are shown in bold}{table.caption.9}{}}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces F1 values for the best results. The best values are shown in bold.}}{10}{table.caption.10}\protected@file@percent }
\newlabel{tab:f1}{{7}{10}{F1 values for the best results. The best values are shown in bold}{table.caption.10}{}}
\gdef \@abspage@last{10}
