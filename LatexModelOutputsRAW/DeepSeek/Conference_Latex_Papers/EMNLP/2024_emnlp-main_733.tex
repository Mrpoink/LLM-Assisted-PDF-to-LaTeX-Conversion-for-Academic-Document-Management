=====FILE: main.tex=====
\documentclass[10pt,twocolumn]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{array}
\usepackage{url}
\usepackage{balance}

\title{Authorship Attribution with Pre-trained Large Language Models: A Bayesian Logprob Approach}
\author{
Zhengmian Hu\textsuperscript{1}, Tong Zheng\textsuperscript{1}, Heng Huang\textsuperscript{1,2}\\
\textsuperscript{1}Department of Computer Science, University of Maryland, College Park, MD 20742\\
\textsuperscript{2}Adobe Research\\
\{huzhengmian@gmail.com, zhengtong12356@gmail.com, heng@umd.edu\}
}
\date{}

\begin{document}

\maketitle

\begin{abstract}
Authorship attribution aims to identify the origin or author of a document. Traditional approaches have heavily relied on manual features and fail to capture long-range correlations, limiting their effectiveness. Recent advancements leverage text embeddings from pretrained language models, which require significant fine-tuning on labeled data, posing challenges in data dependency and limited interpretability. Large Language Models (LLMs), with their deep reasoning capabilities and ability to maintain long-range textual associations, offer a promising alternative. This study explores the potential of pre-trained LLMs in one-shot authorship attribution, specifically utilizing Bayesian approaches and probability outputs of LLMs. Our methodology calculates the probability that a text entails previous writings of an author, reflecting a more nuanced understanding of authorship. By utilizing only pre-trained models such as Llama-3-70B, our results on the IMDb and blog datasets show an impressive $85\%$ accuracy in one-shot authorship classification across ten authors. Our findings set new baselines for one-shot authorship analysis using LLMs and expand the application scope of these models in forensic linguistics. This work also includes extensive ablation studies to validate our approach.
\end{abstract}

\section{Introduction}
text[[116, 737, 487, 897], [511, 260, 881, 308]]
Authorship attribution, the process of identifying the origin or author of a document, has been a longstanding challenge in forensic linguistics. It has numerous applications, including detecting plagiarism (Alzahrani et al., 2011) and attribution of historical text (Silva et al., 2023). As the digital age progresses, the need for reliable methods to determine authorship has become increasingly important, especially in the context of combating misinformation spread through social media and conducting forensic analysis. The ability to attribute authorship can also lead to challenges around privacy and anonymity (Juola et al., 2008).

The field traces its roots back to the early 19th century (Mechti and Almansour, 2021), with early studies focusing on stylistic features and human expert analysis (Mosteller and Wallace, 1963). Traditional methods often relied on stylometry, which quantifies writing styles (Holmes, 1994), and rule-based computational linguistic methods (Stamatatos, 2009) to deduce authorship. Later, statistical algorithms incorporating extensive text preprocessing and feature engineering (Bozkurt et al., 2007; Seroussi et al., 2014) were introduced to improve accuracy. However, these methods often struggled with capturing long-range dependencies in text and require careful setup of specific thresholds for various indicators, which can be challenging to select effectively. They also involve designing complex, high-quality features, which can be costly and time-consuming.

The advent of deep learning has transformed the landscape of authorship attribution by turning the problem into a multi-class classification challenge, allowing for the capture of more features and addressing more complex scenarios effectively (Ruder et al., 2016; Ge et al., 2016; Shrestha et al., 2017; Zhang et al., 2018). However, these neural network (NN) models often lack interpretability and struggle with generalization in cases of limited samples.

Despite advancements, the field still faces significant challenges. Obtaining large, balanced datasets that represent multiple authors fairly is difficult, and as the number of authors increases, the accuracy of machine learning models tends to decrease.

On the other hand, language models, central to modern NLP applications, define the probability of distributions of words or sequences of words and have traditionally been used to predict and generate plausible language. Yet, for a long time, these models, including high-bias models like bag-of-words and n-gram models, struggled to fit the true probability distributions of natural language. Deep learning's rapid development has enabled orders of magnitude scaling up of computing and data, facilitating the use of more complex models such as Random Forests (Breiman, 2001), character-level CNNs (Zafar et al., 2020), Recurrent Neural Networks (Bagnall, 2015), and Transformer (Vaswani et al., 2017).

The recent rapid evolution of Large Language Models (LLMs) has dramatically improved the ability to fit natural language distributions. Trained on massive corpora exceeding 1 trillion tokens, these models have become highly capable of handling a wide range of linguistic tasks, including understanding, generation, and meaningful dialogue (Liang et al., 2022; Bubeck et al., 2023; Zhang et al., 2023a, 2024). They can also explain complex concepts and capture subtle nuances of language. They have been extensively applied in various applications such as chatbots, writing assistants, information retrieval, and translation services. More impressively, LLMs have expanded their utility to novel tasks without additional training, simply through the use of prompts and in-context learning (Brown et al., 2020). This unique ability motivates researchers to adapt LLMs to an even broader range of tasks and topics including reasoning (Wei et al., 2022), theory of mind (Kosinski, 2023) and medical scenario (Singhal et al., 2023).

Interestingly, language models have also been explored for authorship attribution (Agun and Yilmazel, 2017; Le and Mikolov, 2014; McCallum, 1999). Recently, research has utilized LLMs for question answering (QA) tasks within the application of authorship verification and authorship attribution (Huang et al., 2024), though these have primarily been tested in small-scale settings. Other approaches have attempted to leverage model embeddings and fine-tuning for authorship attribution, such as using GAN-BERT (Silva et al., 2023) and BERTAA (Fabien et al., 2020). However, these techniques often face challenges with scalability and need retraining when updating candidate authors. Moreover, they require relatively large dataset and multiple epochs of fine-tuning to converge. Given the challenges with current approaches, a natural question arises: How can we harness LLMs for more effective authorship attribution?

Two aspects of evidence provide insights to answer the above questions. First, recent studies on LLMs have shown that these models possess hallucination problems (Ji et al., 2023). More interestingly, the outputs of LLMs given prompts may disagree with their internal thinking (Liu et al., 2023). Therefore, it is advisable not to rely solely on direct sampling result from LLMs. Second, the training objective of LLMs is to maximize the likelihood of the next token given all previous tokens. This indicates that probability may be a potential indicator for attributing texts to authors.

Language models are essentially probabilistic models, but we find the probabilistic nature of LLMs and their potential for authorship identification remains underexploited. Our study seeks to bridge this gap. Specifically, we explore the capability of LLMs to perform one-shot authorship attribution among multiple candidates.

We propose a novel approach based on a Bayesian framework that utilizes the probability outputs from LLMs. By deriving text-level log probabilities from token-level log probabilities, we establish a reliable measure of likelihood that a query text was written by a specific author given example texts from each candidate author. We also design suitable prompts to enhance the accuracy of these log probabilities. By calculating the posterior probability of authorship, we can infer the most likely author of a document (Figure 1). Due to the pivotal role of log probability in our algorithm, we coined our approach the ``Logprob method.''

Our new method has three main advantages:

\textbf{No Need for Fine-Tuning}: Our approach aligns the classification task with the pretraining objective, both focusing on computing entailment probability. This avoids any objective mismatch introduced by fine-tuning. Moreover, our method leverages the inherent capabilities of pretrained LLMs and avoids knowledge forgetting that often occurs during fine-tuning.

\textbf{Speed and Efficiency}: This approach requires only a single forward pass through the model for each author, making it significantly faster and more cost-effective compared to normal question-answering method of language models which involves sampling a sequence of tokens as answer, with one forward pass for each token generated.

\textbf{No Need for Manual Feature Engineering}: The pre-training on diverse data enables LLMs to automatically capture and utilize subtle nuances in language, thus eliminating the need for manually designing complex features, which can be costly and time-consuming.

By applying this technique, we have achieved state-of-the-art results in one-shot learning on the IMDb and blog datasets, demonstrating an impressive $85\%$ accuracy across ten authors. This advancement establishes a new baseline for one-shot authorship analysis and illustrates the robust potential of LLMs in forensic linguistics.

\section{Method}
Our approach to authorship attribution is based on a Bayesian framework. Given a document whose authorship is unknown, our objective is to identify the most probable author from a set using the capabilities of Large Language Models (LLMs).

We consider a scenario where we have a set of authors $\mathcal{A} = \{a_1,\ldots ,a_n\}$ and a set of all possible texts $\epsilon$ .Given an authorship attribution problem, where each author $a_{i}$ has written a set of texts $t_{i,1},t_{i,2},\ldots ,t_{i,m_i}\in \mathcal{E}$ we denote the collection of known texts of an author $a_{i}$ as $\pmb {t}(a_{i}) = (t_{i,1},t_{i,2},\ldots ,t_{i,m_{i}})$ . For an unknown text $u\in \mathcal{E}$ , we aim to determine the most likely author from the set $\mathcal{A}$

To estimate the author of text $u$ we use a Bayesian framework where the probability that $u$ was written by author $a_{i}$ is given by:

\[P(a_i|u) = \frac{P(u|a_i)P(a_i)}{P(u)}. \quad (1)\]

Here, $P(a_{i})$ is the prior probability of each author, assumed to be equal unless stated otherwise,

making the problem focus primarily on estimating $P(u|a_i)$

Assuming that each author $a_{i}$ has a unique writing style represented by a probability distribution $P(\cdot |a_{i})$ , texts written by $a_{i}$ are samples from this distribution. To estimate $P(u|a_{i})$ , we consider the independence assumption: texts by the same author are independently and identically distributed (i.i.d.). Thus, the unknown text $u$ is also presumed to be drawn from $P(\cdot |a_{i})$ for some author $a_{i}$ and is independent of other texts from that author.

Notice that although texts are independent under the i.i.d. assumption when conditioned on a particular author, there exists a correlation between the unknown text $u$ and the set of known texts $\pmb {t}(a)$ in the absence of knowledge about the author. This correlation can be exploited to deduce the most likely author of $u$ using the known texts.

Specifically, we have

\[\begin{array}{rl}
& P(u|\pmb {t}(a_i)) = \sum_{a_j\in \mathcal{A}}P(u,a_j|\pmb {t}(a_i))\\
& \qquad = \sum_{a_j\in \mathcal{A}}P(u|a_j,\pmb {t}(a_i))P(a_j|\pmb {t}(a_i))\\
& \qquad = \sum_{a_j\in \mathcal{A}}P(u|a_j)P(a_j|\pmb {t}(a_i)),
\end{array} \quad (2)\]

where the last equality uses the i.i.d. assumption, meaning that when conditioned on a specific author $a_{j}$ $u$ is independent of other texts.

We then introduce the ``sufficient training set'' assumption, where:

\[P(a_j|\pmb {t}(a_i)) = \left\{ \begin{array}{ll}1 & a_i = a_j\\ 0 & a_i\neq a_j. \end{array} \right. \quad (3)\]

This implies that the training set is sufficiently comprehensive to unambiguously differentiate authors, leading to:

\[P(u|t(a_i)) = P(u|a_j), \quad (4)\]

where $a_{j}$ is the assumed true author of text $u$

We use Large Language Models (LLMs) to estimate $P(u|t(a_i))$ , which represents the probability that a new text $u$ was written by the author of a given set of texts $t(a_i)$ .

The probability nature of language models means that they typically calculate the probability of a token or a sequence of tokens given prior context. For a vocabulary set $\Sigma$ , the input to a language model might be a sequence of tokens $x_{1},\ldots ,x_{m}\in \Sigma$ , and the model's output would be the probability distribution $P_{\mathrm{LLM}}(\cdot |x_{1},\ldots ,x_{m})$ typically stored in logarithmic scale for numerical stability.

When using an autoregressive language model, we can measure not only the probability of the next token but also the probability of a subsequent sequence of tokens. For instance, if we have a prompt consisting of tokens $x_{1},\ldots ,x_{m}\in \Sigma$ , and we want to measure the probability of a sequence

\[\begin{array}{rl}
& P_{\mathrm{LLM}}(y_1,\ldots ,y_s|x_1,\ldots ,x_m)\\
& = \prod_{i = 1}^{s}P_{\mathrm{LLM}}(y_i|x_1,\ldots ,x_m,y_1,\ldots ,y_{i - 1}).
\end{array} \quad (5)\]

To estimate $P(u|t(a_i))$ for authorship attribution, we define:

\[\begin{array}{rl}
& P(u|t(a_i))\\
& = P_{\mathrm{LLM}}(u|\mathrm{prompt\_construction}(t(a_i))).
\end{array} \quad (6)\]

The prompt construction can vary, providing flexibility in how we use the model to estimate probabilities. Our method involves constructing a prompt steering the LLM uses to predict the likelihood that the unknown text was written by the same author (Figure 2).

In summary, our approach is straightforward and simple. By leveraging the capabilities of Large Language Models, we calculate the likelihood that an unknown text originates from a known author based on existing samples of their writing. This probability assessment allows us to identify the most likely author from a set without the need for fine-tuning or feature engineering.

\begin{figure}[t]
\centering
\fbox{\parbox{0.9\linewidth}{
IMAGE NOT PROVIDED
}}
\caption{Illustration of bayesian authorship attribution using LLM.}
\label{fig:bayes}
\end{figure}

\begin{figure}[t]
\centering
\fbox{\parbox{0.9\linewidth}{
\textbf{Author 1:} [ILLEGIBLE]\\
\textbf{Author 2:}\\
Tina Fey is a successful professional who has missed out on the baby wagon. All her friends have families and she has promotions. Desperate for a child she tries a sperm bank but it fails when she is told that she is infertile. In desperation she takes on a surrogate who turns her life upside down. Clearly Tina Fey is the smartest one in the room and she walks through this film seemingly on autopilot and above to everyone around her. What is she doing here? She is somewhere beyond this film and it shows. Its cute and amusing but Fey's demeanor promises something on a different plane then the rest of the movie. I think the best way to explain it, or over explain it would be Cary Grant in a Three Stooges movie. I think Fey can do great things if she wants or can find material that matches her abilities. A good little film.

Here is the text from the same author:

Barbet Schroeder's portrait of French attorney Jacques Verges. You've seen him defending people like Klaus Barbie, Carlos de Jackal, Pol Pot as well as other dictators and terrorists. This is a complex story of a complex man and it essentially tells the tale of the man from World War 2 until today. (And even at 140 minutes the film leaves a great deal out). Here is man of his time, who met and defended with many of the famous and infamous people of the last fifty years. He seems to be a man who generally believes in the right of the oppressed to stand up to their oppressors and to have some one to stand up for them. However this is not just the story of a man who fights for the oppressed but it is also the story of a man entangled in things that will cause many to question just how slick a guy is Verges. Many of the terrorists and dictators he defends are in fact his friends, and he is not doing it for the love of cause but also for the love of the finer things. I liked the film a great deal. To be certain I was lost as to bits of the history and who some people were, but at the same time the film isn't about the history, so much as Verges moving through it. This is the story of the man, his causes and to some degree his women. What exactly are we to make of Verges? I don't know, but I sure do think that he and his life make for a compelling tale. I loved that my idea of what Verges is changed. I loved that I was completely confused at the end as to what I thought, confused in a way that only a film that forces you to think can do. In the end I don't know what I think.

In the run- up to the 1972 elections, Washington Post reporter Bob Woodward covers what seems to be a minor break- in at the Democratic Party National headquarters. He is surprised to find top lawyers already on the defence case, and the discovery of names and addresses of Republican fund organisers on the accused further arouses his suspicions. The editor of the Post is prepared to run with the story and assigns Woodward and Carl Bernstein to it. They find the trail leading higher and higher in the Republican Party, and eventually into the White House itself. . whatever peoples opinions on the Watergate scandal, whether they believe it was a big cover up, or the media got a lot wrong, no one can deny just how powerful and interesting this film really is. Pakula directs this very slickly and brings the tension on the two main protagonists very slowly throughout the duration of the movie. Redford and Hoffman work really well together and are given great support from the rest of the cast. the narration works amazingly well and there is good use of mise en scene and connotations. for example there are a few scenes with the t. v screen in the foreground showing Nixon winning his presidential seat again, with.

Here is the text from the same author:

Barbet Schroeder's portrait of French attorney Jacques Verges. You've seen him defending people like Klaus Barbie, Carlos de Jackal, Pol Pot as well as other dictators and terrorists. This is a complex story of a complex man and it essentially tells the tale of the man from World War 2 until today. (And even at 140 minutes the film leaves a great deal out). Here is man of his time, who met and defended with many of the famous and infamous people of the last fifty years. He seems to be a man who generally believes in the right of the oppressed to stand up to their oppressors and to have some one to stand up for them. However this is not just the story of a man who fights for the oppressed but it is also the history, so much as Verges moving through it. This is the story of the man, his causes and to some degree his women. What exactly are we to make of Verges? I don't know, but I sure do think that he and his life make for a compelling tale. I loved that my idea of what Verges is changed. I loved that I was completely confused at the end as to what I thought, confused in a way that only a film that forces you to think can do. In the end I don't know what I think.

Most likely author:\\
- 964.51\\
X
}}
\caption{Example of prompt construction and authorship attribution based on log probabilities. The logprob is computed on the orange part, which represents the text from unknown author.}
\label{fig:example_prompt}
\end{figure}

\section{Experimental Setups}
\subsection{Models \& Baselines}
\textbf{Models} We selected two widely-used LLM families: 1) LLaMA family, which includes LLaMA-2 (Touvron et al., 2023), LLaMA-3, CodeLLaMA (Roziere et al., 2023), available in various parameter sizes and configurations, with some models specifically fine-tuned for dialogue use cases; 2) the GPT family (Brown et al., 2020), featuring GPT-3.5-Turbo and GPT-4-Turbo (Achiam et al., 2023), where we specifically used versions gpt-4-turbo-2024-04-09 and gpt-3.5-turbo-0125. The LLaMA family models were deployed using the vLLM framework (Kwon et al., 2023) if used for Logprob method and are deployed on Azure if used for question-answering. Apart from Table 1, all ablation studies of Logprob method uses LLaMA-3-70B model.

\textbf{Baselines} We chose two types of baselines for comparison. 1) embedding-based methods such as BertAA (Fabien et al., 2020) and GAN-BERT (Silva et al., 2023), which require training or fine-tuning, 2) LLM-based methods such as those described in (Huang et al., 2024), which utilize LLMs for authorship attribution tasks through a question-answering (QA) approach.

\subsection{Evaluations}
\textbf{Datasets} We evaluated our method on two widely used author attribution datasets: 1) IMDB62 dataset, a truncated version of IMDB dataset (Seroussi et al., 2014) and 2) Blog Dataset (Schler et al., 2006). IMDB62 dataset comprises 62k movie reviews from 62 authors, with each author contributing 1000 samples. Additionally, it also provides some extra information such as the rating score. The Blog dataset, contains 681k blog comments, each with an assigned authorID. Besides the raw text and authorID, each entry includes extra information such as gender and age. Both datasets are accessible via HuggingFace.

\textbf{Benchmark Construction} Unlike fixed author sets used in many previous studies, we constructed a random author set for each test to minimize variance. By default, unless specified otherwise, each experiment in our experiments involved a 10-author one-shot setting, and we conducted 100 tests for each experiment to reduce variance. Each test involved the following steps: 1) Ten candidate authors were randomly selected. 2) For each author, one (or n for n-shot) article was randomly selected as the training set. 3) One author was randomly selected from the ten candidates as the test author. 4) One article not in the training set was randomly selected from the test author's articles as the test set (with size of 1). 5) We run the authorship attribution algorithm to classify the test article into 10 categories.

Our evaluation pipeline can avoid potential biases from fixed author sets and better measure the efficacy of LLMs in authorship attribution tasks. We also share our pipeline for fair evaluations of future related works.

Notably, aforementioned pipeline is suitable for non-training based methods like ours and QA approaches. However, for training-based methods such as embedding approaches, each train-test split is followed by a retraining, demanding significant computational resources. Therefore, in this work, we directly cited scores from the original papers.

\textbf{Evaluation Metrics} We adopt three metrics: top-1, top-2 and top-5 accuracies. Specifically, top k accuracy is computed as follows:

\[\mathrm{Top} k \mathrm{Accuracy} = \frac{\mathrm{Num}_{\mathrm{correct}}^{\mathrm{k}}}{\mathrm{Num}_{\mathrm{all}}}, \quad (7)\]

where $\mathrm{Num}_{\mathrm{correct}}^{\mathrm{k}}$ represents the number of tests where the actual author is among the top k predictions, and $\mathrm{Num}_{\mathrm{all}}$ represents the total number of tests.

\section{Experiments}
Firstly, we evaluate different methods for author attribution in Section 4.1, noting that our Logprob method significantly outperformed QA-based methods in accuracy and stability across datasets. Then, we study the impact of increasing candidate numbers on performance in Section 4.2, where our method maintained high accuracy despite a larger pool of candidates. Next, in Section 4.3, we analyze prompt sensitivity, concluding that while prompt use is crucial, variations in prompt design did not significantly affect the performance. Further, in Section 4.4, we explore bias in author attribution and in Section 4.5, we measure performance variations across different subgroups. Finally, in Section 4.6, we analyze the efficiency of our method.

\subsection{Author Attribution Performance}
Table 1 shows the main results for different methods on the IMDB62 and Blog datasets concerning authorship attribution capabilities. We make the following observations:

LLMs with QA-based methods cannot perform author attribution tasks effectively. For example, GPT-4-Turbo can only achieve a top-1 accuracy of $34\%$ on the IMDB62 dataset and $62\%$ on the Blog dataset. Notably, there are two interesting phenomena: 1) GPT-4-Turbo and GPT-3.5-Turbo exhibit inconsistent higher accuracy across different datasets, highlighting inherent instability in the prompt-based approach. 2) Older LLMs with smaller context window lengths are unable to perform author attribution due to the prompt exceeding the context window. These phenomena indicate that QA methods are not a good option for enabling LLMs to conduct author attribution tasks effectively.

Our Logprob method helps LLMs perform author attribution tasks more effectively. With LLaMA-3-70B, we achieved top-1 accuracy of $85\%$ and both top-2 and top-5 accuracies were even higher. This suggests that LLMs equipped with our method can effectively narrow down large candidate sets. Additionally, two another things worth noting are that 1) LLMs with the Logprob method exhibit more stable performance across both tasks, something QA methods struggle with, and 2) LLMs with Logprob can conduct authorship attribution tasks with lower requirements for context window length. For instance, LLaMA-2-70B-Chat with the Logprob method can handle authorship attribution, whereas the same model with a QA approach fails when the collective text of 10 authors exceeds the context window length. These findings highlight the superiority of our Logprob method.

Training-free method can achieve comparable or even superior performance to training-based methods. The Blog dataset showed higher top-1 accuracy with LLaMA + Logprob compared to GAN-BERT and BertAA. While the IMDB62 dataset exhibited lower performance relative to embedding-based methods, it is important to note that Logprob achieves this as a one-shot method, whereas embedding-based approaches require much more data for training to converge. This demonstrates that Logprob can more effectively capture the nuances necessary for authorship attribution.

\begin{table*}[t]
\centering
\caption{Author attribution results on IMDB62 and Blog dataset. Prompt construction for QA method is in consistent with Huang et al. (2024).}
\label{tab:main_results}
\small
\begin{tabular}{l l c c c c c c c}
\toprule
\textbf{Method} & \textbf{Model} & \multicolumn{4}{c}{\textbf{IMDB62 Dataset}} & \multicolumn{4}{c}{\textbf{BLOG Dataset}} \\
\cmidrule(lr){3-6} \cmidrule(lr){7-10}
& & \#Cand. & n-Shot & Top 1 Acc. & Top 5 Acc. & \#Cand. & n-Shot & Top 1 Acc. & Top 5 Acc. \\
\midrule
\multirow{12}{*}{\rotatebox[origin=c]{90}{LogProb}} & LLaMA-2-7B & 10 & 1 & 80.0 $\pm$ 4.0 & 97.0 $\pm$ 1.7 & 10 & 1 & 79.0 $\pm$ 4.1 & 98.0 $\pm$ 1.4 \\
& LLaMA-2-7B-Chat & 10 & 1 & 68.0 $\pm$ 4.7 & 88.0 $\pm$ 3.1 & 10 & 1 & 69.0 $\pm$ 4.6 & 89.0 $\pm$ 3.1 \\
& LLaMA-2-13B & 10 & 1 & 84.0 $\pm$ 3.7 & 100.0 $\pm$ 0.0 & 10 & 1 & 81.0 $\pm$ 3.9 & 94.0 $\pm$ 2.4 \\
& LLaMA-2-70B & 10 & 1 & 88.0 $\pm$ 3.3 & 99.0 $\pm$ 1.0 & 10 & 1 & 88.0 $\pm$ 3.3 & 95.0 $\pm$ 2.2 \\
& LLaMA-2-70B-Chat & 10 & 1 & 79.0 $\pm$ 4.1 & 95.0 $\pm$ 2.2 & 10 & 1 & 83.0 $\pm$ 3.8 & 97.0 $\pm$ 1.7 \\
& Code-LLaMA-7B & 10 & 1 & 71.0 $\pm$ 4.5 & 96.0 $\pm$ 2.0 & 10 & 1 & 78.0 $\pm$ 4.1 & 94.0 $\pm$ 2.4 \\
& Code-LLaMA-13B & 10 & 1 & 70.0 $\pm$ 4.6 & 98.0 $\pm$ 1.4 & 10 & 1 & 77.0 $\pm$ 4.2 & 92.0 $\pm$ 2.7 \\
& Code-LLaMA-34B & 10 & 1 & 75.0 $\pm$ 4.3 & 98.0 $\pm$ 1.4 & 10 & 1 & 78.0 $\pm$ 4.1 & 94.0 $\pm$ 2.4 \\
& LLaMA-3-8B & 10 & 1 & 82.0 $\pm$ 3.8 & 98.0 $\pm$ 1.4 & 10 & 1 & 84.0 $\pm$ 3.7 & 95.0 $\pm$ 2.2 \\
& LLaMA-3-8B-Instruct & 10 & 1 & 69.0 $\pm$ 4.6 & 90.0 $\pm$ 3.0 & 10 & 1 & 68.0 $\pm$ 4.7 & 90.0 $\pm$ 3.0 \\
& LLaMA-3-70B & 10 & 1 & 85.0 $\pm$ 3.6 & 98.0 $\pm$ 1.4 & 10 & 1 & 82.0 $\pm$ 3.8 & 95.0 $\pm$ 2.2 \\
\midrule
\multirow{4}{*}{\rotatebox[origin=c]{90}{QA}} & LLaMA-2-70B-Chat & 10 & 1 & Failed & -- & 10 & 1 & Failed & -- \\
& LLaMA-3-70B-Instruct & 10 & 1 & 31.0 $\pm$ 4.6 & -- & 10 & 1 & 22.0 $\pm$ 4.1 & -- \\
& GPT-3.5-Turbo & 10 & 1 & 69.0 $\pm$ 4.6 & -- & 10 & 1 & 47.0 $\pm$ 5.0 & -- \\
& GPT-4-Turbo & 10 & 1 & 34.0 $\pm$ 4.7 & -- & 10 & 1 & 62.0 $\pm$ 4.9 & -- \\
\midrule
\multirow{2}{*}{\rotatebox[origin=c]{90}{Other Baseline}} & GAN-BERT & 20 & 80 & 96.0 & -- & 20 & 80 & 40.0 & -- \\
& BertAA & 62 & 80 & 93.0 & -- & 10 & 80 & 65.0 & -- \\
\bottomrule
\end{tabular}
\end{table*}

\subsection{Performance vs. Number of Candidates}
One of the challenges in authorship attribution is the difficulty in correctly identifying the author as the number of candidates increases, which generally leads to decreased accuracy. Figure 3 shows the author attribution performance across different candidate counts on the IMDB62 dataset. We made the following observations:

First, performance indeed decreases as the number of candidates increases. Second, across all settings, all metrics maintain relatively high scores. For example, in the setting with 50 candidates, our method achieved $76\%$ top-1 accuracy, $84\%$ top-2 accuracy, and $87\%$ top-5 accuracy. Third, top-2 and top-5 accuracies are more stable compared to top-1 accuracy. The model may not always place the correct author at the top, but it often includes the correct author within the top few predictions. This attribute is also crucial as it allows the narrowing down of a large pool of candidates to a smaller subset of likely candidates.

\begin{figure}[t]
\centering
\fbox{\parbox{0.9\linewidth}{IMAGE NOT PROVIDED}}
\caption{Accuracy vs. number of candidates.}
\label{fig:accuracy_vs_candidates}
\end{figure}

\subsection{Analysis of Prompt Sensitivity}
Our method relies on suitable prompt as in Figure 2. Here, we discuss the sensitivity of our accuracy to different prompt constructions in Table 2. We made the following observations:

Using prompts is essential for enhancing the accuracy of our method (\#1 vs. \#2). This phenomenon is aligned with previous studies (Wei et al., 2022) that have demonstrated that prompting is beneficial for unlocking the full potential of LLMs.

There is no statistically significant evidence to suggest that specific prompt designs impact performance significantly (\#2 vs. \#3 vs. \#4 vs. \#5). The results show very close performance metrics across different prompt constructions.

\textbf{Discussions} Prompting sensitivity (Sclar et al., 2023) is a widely acknowledged property in the generation process of LLMs. This also has motivated a trend of research on prompting engineering (Zhang et al., 2023b; Guo et al., 2024) as different prompting can lead to completely different performance. However, our method appears to be relatively insensitive to the choice of prompt, which makes our method more robust, maintaining high performance and stability across various settings.

\textbf{Prompt 1}: Here is the text from the same author.\\
\textbf{Prompt 2}: Analyze the writing styles of the input texts, disregarding the differences in topic and content. Here is the text from the same author:\\
\textbf{Prompt 3}: Focus on grammatical styles indicative of authorship. Here is the text from the same author:\\
\textbf{Prompt 4}: Analyze the writing styles of the input texts, disregarding the differences in topic and content. Reasoning based on linguistic features such as phrasal verbs, modal verbs, punctuation, rare words, affixes, quantities, humor, sarcasm, typographical errors, and misspellings. Here is the text from the same author:

\begin{table}[t]
\centering
\caption{Author attribution performance vs. different prompting choices on IMDB62 dataset.}
\label{tab:prompt_sensitivity}
\small
\begin{tabular}{c l c c c}
\toprule
\# & Prompting & Top 1 Accuracy & Top 2 Accuracy & Top 5 Accuracy \\
\midrule
1 & <Example Text> + <Query Text> & 70.0 $\pm$ 4.6 & 81.0 $\pm$ 3.9 & 92.0 $\pm$ 2.7 \\
2 & <Example Text> + <Prompt 1> + <Query Text> & 85.0 $\pm$ 3.6 & 92.0 $\pm$ 2.7 & 99.0 $\pm$ 1.0 \\
3 & <Example Text> + <Prompt 2> + <Query Text> & 83.0 $\pm$ 3.8 & 87.0 $\pm$ 3.4 & 100.0 $\pm$ 0.0 \\
4 & <Example Text> + <Prompt 3> + <Query Text> & 86.0 $\pm$ 3.5 & 90.0 $\pm$ 3.0 & 100.0 $\pm$ 0.0 \\
5 & <Example Text> + <Prompt 4> + <Query Text> & 87.0 $\pm$ 3.4 & 90.0 $\pm$ 3.0 & 99.0 $\pm$ 1.0 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Bias Analysis}
An algorithm trained on an entire dataset may exhibit different accuracy levels across different subgroups during testing (Chouldechova and G'Sell, 2017; Pastor et al., 2021). This section discusses such bias issues and measures how the algorithm's accuracy varies for different subgroups.

\textbf{Influence of Gender} We conduct 500 tests which consists of 237 tests for blogs written by male authors and 263 tests for blogs written by female authors and show their accuracy of authorship attribution separately in Table 3. The results indicate that authorship attribution for blogs written by female authors exhibits higher accuracy. This suggests that female-authored blogs might contain more distinct personal styles, making it easier to infer the author.

\begin{table}[t]
\centering
\caption{Gender bias in author attribution performance.}
\label{tab:gender_bias}
\small
\begin{tabular}{l c c c}
\toprule
Gender & Top 1 Acc. & Top 2 Acc. & Top 5 Acc. \\
\midrule
Both & 84.0 $\pm$ 1.6 & 90.8 $\pm$ 1.3 & 95.8 $\pm$ 1.0 \\
Male & 81.4 $\pm$ 2.5 & 88.6 $\pm$ 2.1 & 95.4 $\pm$ 1.4 \\
Female & 86.3 $\pm$ 2.1 & 92.8 $\pm$ 1.6 & 96.2 $\pm$ 1.2 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Subgroup Analysis}
When considering authorship attribution restricted to specific subgroups, the task can either become simpler or more difficult. Certain subgroups may express personal styles more distinctly, making authorship attribution easier, while others may be more homogeneous, making it more challenging. Here, we consider three subgroup factors: gender, age, and rating, to analyze the performance under each group.

\textbf{Subgroup by Gender} As shown in Table 4, we evaluated the performance of authorship attribution within different gender subgroups in the Blog dataset. We observed that authorship attribution performed better within the female subgroup, consistent with findings in Section 4.4, suggesting female-authored blogs possess more distinctive personal styles.

\begin{table}[t]
\centering
\caption{Author attribution performance in each gender subgroup.}
\label{tab:gender_subgroup}
\small
\begin{tabular}{l c c c}
\toprule
Gender & Top 1 Acc. & Top 2 Acc. & Top 5 Acc. \\
\midrule
Male & 77.0 $\pm$ 4.2 & 82.0 $\pm$ 3.8 & 92.0 $\pm$ 2.7 \\
Female & 89.0 $\pm$ 3.1 & 91.0 $\pm$ 2.9 & 95.0 $\pm$ 2.2 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Subgroup by Rating} Table 5 (a) shows the performance of authorship attribution across different rating ranges in the IMDb review dataset. Overall, we can see that rating does influence performance, with review in the $[5 - 6]$ rating range easier to attribute. Despite such difference, our method consistently obtains good performance across all subgroups.

\textbf{Subgroup of Age} Table 5 (b) shows the performance of authorship attribution across different age ranges of bloggers in the Blog dataset. We observed that age significantly influences performance. The youngest age group $[13 - 17]$ exhibited the highest top-1 accuracy at $90\%$ , while accuracy decreased with increasing author age. This suggests that younger authors tend to have more distinct opinions and identifiable writing styles. Despite performance differences, our method maintained relatively overall high performance, with the lowest accuracy still surpassing that of GPT-4-Turbo with QA method.

\begin{table}[t]
\centering
\caption{Author attribution performance in each rating subgroup and age subgroup.}
\label{tab:rating_age_subgroup}
\small
\begin{tabular}{c c c c}
\toprule
Interval & Top 1 Acc. & Top 2 Acc. & Top 5 Acc. \\
\midrule
$[1 - 2]$ & 82.0 $\pm$ 3.8 & 89.0 $\pm$ 3.1 & 96.0 $\pm$ 2.0 \\
$[3 - 4]$ & 87.0 $\pm$ 3.4 & 94.0 $\pm$ 2.4 & 99.0 $\pm$ 1.0 \\
$[5 - 6]$ & 90.0 $\pm$ 3.0 & 96.0 $\pm$ 2.0 & 100.0 $\pm$ 0.0 \\
$[7 - 8]$ & 88.0 $\pm$ 3.3 & 92.0 $\pm$ 2.7 & 97.0 $\pm$ 1.7 \\
$[9 - 10]$ & 89.0 $\pm$ 3.1 & 93.0 $\pm$ 2.6 & 96.0 $\pm$ 2.0 \\
\bottomrule
\end{tabular}
\quad
\begin{tabular}{c c c c}
\toprule
Age Range & Top 1 Acc. & Top 2 Acc. & Top 5 Acc. \\
\midrule
[ILLEGIBLE] & [ILLEGIBLE] & [ILLEGIBLE] & [ILLEGIBLE] \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Efficiency Analysis}
Table 6 shows the efficiency comparison of different methods on the imdb dataset. Our Logprob method operates with notably lower runtime compared to QA methods. This is primarily due to the Logprob method requiring only a single forward pass through the LLM for each author to estimate the log probabilities. In contrast, QA methods generally need multiple iterations of token generations to form a response, which increases computation time substantially. In the mean time, our method achieves an accuracy of up to $85\%$ , surpassing QA method based on GPT-4-Turbo in both efficiency and accuracy.

In summary, our method proves to be effective and efficient in performing authorship attribution across various datasets and setups.

\begin{table}[t]
\centering
\caption{Efficiency analysis between prompt-based method and logprob-based method on Blog dataset.}
\label{tab:efficiency}
\small
\begin{tabular}{c l l c c}
\toprule
\# & Foundation Models & Deployment Resource & Method & Inference Time (s) & Accuracy \\
\midrule
1 & LLama-3-70B & 8 $\times$ A6000 (VLLM) & Logprob & 462.1 & 85.0 $\pm$ 3.6 \\
2 & GPT-4-Turbo & OpenAI & QA & 663.1 & 34.0 $\pm$ 4.7 \\
3 & LLama-3-70B-Instruct & Azure & QA & 2065.6 & 31.0 $\pm$ 4.6 \\
\bottomrule
\end{tabular}
\end{table}

\section{Conclusion}
In this paper, we study the problem of authorship attribution. We demonstrate the effectiveness of utilizing pre-trained Large Language Models (LLMs) for one-shot author attribution. Our Bayesian approach leverages the probabilistic nature of language models like Llama-3 to infer authorship. Our method does not require fine-tuning, therefore reduces computational overhead and data requirements. Our experiments validate that our method is more effective and efficient compared to existing techniques.

\section{Limitations}
The main limitations arise due to the dependence on LLMs.

Our method relies heavily on the capabilities of LLMs, and the performance of our approach is highly affected by the size and training objectives of the LLMs. As shown in Table 1, models that are only pre-trained rather than fine-tuned for dialogue or code task performs better.

While larger models generally perform better, they also entail higher costs, posing scalability and accessibility challenges for broader applications.

Another limitation is due to training data of LLMs. If the training data lacks diversity or fails to include certain writing styles, the model may not fully capture the intricacies of an author's style, potentially leading to misclassifications. This limitation underscores the importance of using diverse and comprehensive training datasets.

Furthermore, any biases present in the training data can also be absorbed by the model. These biases will influence the performance of our authorship attribution method.

On the broader societal level, the potential for misuse of this technology is a significant concern. The challenge of regulating and overseeing the use of such powerful tools is still not fully addressed.

Lastly, while our approach avoids the need for extensive retraining or fine-tuning, which is an advantage in many cases, this also means that our method might not adapt well to scenarios where lots of training data and computation is available, which justifies more complex and computationally intensive methods.

\section*{Acknowledgments}
ZH, TZ and HH were partially supported by NSF IIS 2347592, 2347604, 2348159, 2348169, DBI 2405416,CCF 2348306,CNS 2347617.

\balance
\bibliographystyle{plain}
\begin{thebibliography}{10}

\bibitem{Achiam2023}
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.
\newblock 2023.
\newblock Gpt-4 technical report.
\newblock {\em arXiv preprint arXiv:2303.08774}.

\bibitem{Agun2017}
Hayri Volkan Agun and Ozgur Yilmazel.
\newblock 2017.
\newblock Document embedding approach for efficient authorship attribution.
\newblock In {\em 2017 2nd International Conference on Knowledge Engineering and Applications (ICKEA)}, pages 194--198. IEEE.

\bibitem{Alzahrani2011}
Salha M Alzahrani, Naomie Salim, and Ajith Abraham.
\newblock 2011.
\newblock Understanding plagiarism linguistic patterns, textual features, and detection methods.
\newblock {\em IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)}, 42(2):133--149.

\bibitem{Bagnall2015}
Douglas Bagnall.
\newblock 2015.
\newblock Author identification using multi-headed recurrent neural networks.
\newblock {\em arXiv preprint arXiv:1506.04891}.

\bibitem{Bozkurt2007}
Ilker Nadi Bozkurt, Ozgur Baghooglu, and Erkan Uyar.
\newblock 2007.
\newblock Authorship attribution.
\newblock In {\em 2007 22nd international symposium on computer and information sciences}, pages 1--5. IEEE.

\bibitem{Breiman2001}
Leo Breiman.
\newblock 2001.
\newblock Random forests.
\newblock {\em Machine learning}, 45:5--32.

\bibitem{Brown2020}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.
\newblock 2020.
\newblock Language models are few-shot learners.
\newblock {\em Advances in neural information processing systems}, 33:1877--1901.

\bibitem{Bubeck2023}
Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al.
\newblock 2023.
\newblock Sparks of artificial general intelligence: Early experiments with gpt-4.
\newblock {\em arXiv preprint arXiv:2303.12712}.

\bibitem{Chouldechova2017}
Alexandra Chouldechova and Max G'Sell.
\newblock 2017.
\newblock Fairer and more accurate, but for whom?
\newblock 4th Workshop on Fairness, Accountability, and Transparency in Machine Learning.

\bibitem{Fabien2020}
Maël Fabien, Esau Villatoro-Tello, Petr Motlicek, and Shantipriya Parida.
\newblock 2020.
\newblock BertAA : BERT fine-tuning for authorship attribution.
\newblock In {\em Proceedings of the 17th International Conference on Natural Language Processing (ICON)}, pages 127--137, Indian Institute of Technology Patna, Patna, India. NLP Association of India (NLPAI).

\bibitem{Ge2016}
Zhenhao Ge, Yufang Sun, and Mark Smith.
\newblock 2016.
\newblock Authorship attribution using a neural network language model.
\newblock In {\em Proceedings of the AAAI Conference on Artificial Intelligence}, volume 30.

\bibitem{Guo2024}
Qingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao Song, Xu Tan, Guoqing Liu, Jiang Bian, and Yujiu Yang.
\newblock 2024.
\newblock Connecting large language models with evolutionary algorithms yields powerful prompt optimizers.
\newblock In {\em The Twelfth International Conference on Learning Representations}.

\bibitem{Holmes1994}
David I Holmes.
\newblock 1994.
\newblock Authorship attribution.
\newblock {\em Computers and the Humanities}, 28:87--106.

\bibitem{Huang2024}
Baixiang Huang, Canyu Chen, and Kai Shu.
\newblock 2024.
\newblock Can large language models identify authorship?
\newblock {\em arXiv preprint arXiv:2403.08213}.

\bibitem{Ji2023}
Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung.
\newblock 2023.
\newblock Survey of hallucination in natural language generation.
\newblock {\em ACM Computing Surveys}, 55(12):1--38.

\bibitem{Juola2008}
Patrick Juola et al.
\newblock 2008.
\newblock Authorship attribution.
\newblock {\em Foundations and Trends\textregistered in Information Retrieval}, 1(3):233--334.

\bibitem{Kosinski2023}
Michal Kosinski.
\newblock 2023.
\newblock Theory of mind might have spontaneously emerged in large language models.
\newblock {\em arXiv preprint arXiv:2302.02083}.

\bibitem{Kwon2023}
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica.
\newblock 2023.
\newblock Efficient memory management for large language model serving with pagedattention.
\newblock In {\em Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles}.

\bibitem{Le2014}
Quoc Le and Tomas Mikolov.
\newblock 2014.
\newblock Distributed representations of sentences and documents.
\newblock In {\em International conference on machine learning}, pages 1188--1196. PMLR.

\bibitem{Liang2022}
Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al.
\newblock 2022.
\newblock Holistic evaluation of language models.
\newblock {\em arXiv preprint arXiv:2211.09110}.

\bibitem{Liu2023}
Kevin Liu, Stephen Casper, Dylan Hadfield-Menell, and Jacob Andreas.
\newblock 2023.
\newblock Cognitive dissonance: Why do language model outputs disagree with internal representations of truthfulness?
\newblock In {\em Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing}, pages 4791--4797, Singapore. Association for Computational Linguistics.

\bibitem{McCallum1999}
Andrew Kachites McCallum.
\newblock 1999.
\newblock Multi-label text classification with a mixture model trained by em.
\newblock In {\em AAAI'99 workshop on text learning}.

\bibitem{Mechti2021}
Seif Mechti and Fahad Almansour.
\newblock 2021.
\newblock An orderly survey on author attribution methods: From stylistic features to machine learning models.
\newblock {\em Int. J. Adv. Res. Eng. Technol}, 12:528--538.

\bibitem{Mosteller1963}
Frederick Mosteller and David L Wallace.
\newblock 1963.
\newblock Inference in an authorship problem: A comparative study of discrimination methods applied to the authorship of the disputed federalist papers.
\newblock {\em Journal of the American Statistical Association}, 58(302):275--309.

\bibitem{Pastor2021}
Eliana Pastor, Luca de Alfaro, and Elena Baralis.
\newblock 2021.
\newblock Identifying biased subgroups in ranking and classification.
\newblock Measures and Best Practices for Responsible AI at KDD 2021.

\bibitem{Roziere2023}
Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jeremy Rapin, et al.
\newblock 2023.
\newblock Code llama: Open foundation models for code.
\newblock {\em arXiv preprint arXiv:2308.12950}.

\bibitem{Ruder2016}
Sebastian Ruder, Parsa Ghaffari, and John G Breslin.
\newblock 2016.
\newblock Character-level and multi-channel convolutional neural networks for large-scale authorship attribution.
\newblock {\em arXiv preprint arXiv:1609.06686}.

\bibitem{Schler2006}
Jonathan Schler, Moshe Koppel, Shlomo Argamon, and James W Pennebaker.
\newblock 2006.
\newblock Effects of age and gender on blogging.
\newblock In {\em AAAI spring symposium: Computational approaches to analyzing weblogs}, volume 6, pages 199--205.

\bibitem{Sclar2023}
Melanie Sclar, Yejin Choi, Yulia Tsvetkov, and Alane Suhr.
\newblock 2023.
\newblock Quantifying language models' sensitivity to spurious features in prompt design or: How i learned to start worrying about prompt formatting.
\newblock {\em arXiv preprint arXiv:2310.11324}.

\bibitem{Seroussi2014}
Yanir Seroussi, Ingrid Zukerman, and Fabian Bohnert.
\newblock 2014.
\newblock Authorship attribution with topic models.
\newblock {\em Computational Linguistics}, 40(2):269--310.

\bibitem{Shrestha2017}
Prasha Shrestha, Sebastian Sierra, Fabio A Gonzalez, Manuel Montes, Paolo Rosso, and Thamar Solorio.
\newblock 2017.
\newblock Convolutional neural networks for authorship attribution of short texts.
\newblock In {\em Proceedings of the 15th conference of the European chapter of the association for computational linguistics: Volume 2, short papers}, pages 669--674.

\bibitem{Silva2023}
Kanishka Silva, Burcu Can, Frederic Blain, Raheem Sarwar, Laura Ugolini, and Ruslan Mitkov.
\newblock 2023.
\newblock Authorship attribution of late 19th century novels using gan-bert.
\newblock In {\em Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 4: Student Research Workshop)}, pages 310--320.

\bibitem{Singhal2023}
Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, et al.
\newblock 2023.
\newblock Large language models encode clinical knowledge.
\newblock {\em Nature}, 620(7972):172--180.

\bibitem{Stamatatos2009}
Efstathios Stamatatos.
\newblock 2009.
\newblock A survey of modern authorship attribution methods.
\newblock {\em Journal of the American Society for information Science and Technology}, 60(3):538--556.

\bibitem{Touvron2023}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.
\newblock 2023.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock {\em arXiv preprint arXiv:2307.09288}.

\bibitem{Vaswani2017}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin.
\newblock 2017.
\newblock Attention is all you need.
\newblock {\em Advances in neural information processing systems}, 30.

\bibitem{Wei2022}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.
\newblock 2022.
\newblock Chain-of-thought prompting elicits reasoning in large language models.
\newblock {\em Advances in neural information processing systems}, 35:24824--24837.

\bibitem{Zafar2020}
Sarim Zafar, Muhammad Usman Sarwar, Saeed Salem, and Muhammad Zubair Malik.
\newblock 2020.
\newblock Language and obfuscation oblivious source code authorship attribution.
\newblock {\em IEEE Access}, 8:197581--197596.

\bibitem{Zhang2023a}
Biao Zhang, Barry Haddow, and Alexandra Birch.
\newblock 2023a.
\newblock Prompting large language model for machine translation: A case study.
\newblock In {\em International Conference on Machine Learning}, pages 41092--41110. PMLR.

\bibitem{Zhang2018}
Richong Zhang, Zhiyuan Hu, Hongyu Guo, and Yongyi Mao.
\newblock 2018.
\newblock Syntax encoding with application in authorship attribution.
\newblock In {\em Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing}, pages 2742--2753, Brussels, Belgium. Association for Computational Linguistics.

\bibitem{Zhang2024}
Tianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang, Kathleen McKeown, and Tatsunori B Hashimoto.
\newblock 2024.
\newblock Benchmarking large language models for news summarization.
\newblock {\em Transactions of the Association for Computational Linguistics}, 12:39--57.

\bibitem{Zhang2023b}
Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola.
\newblock 2023b.
\newblock Automatic chain of thought prompting in large language models.
\newblock In {\em The Eleventh International Conference on Learning Representations}.

\end{thebibliography}

\appendix

\section{Ethical Considerations}
Our method using LLMs for authorship attribution brings several ethical considerations that must be addressed to ensure responsible and fair use of the technology.

\textbf{Privacy and Anonymity} The capacity of LLMs to attribute authorship with high accuracy can lead to ethical challenges regarding privacy and anonymity. Individuals who wish to remain anonymous or protect their identity could be compromised if authorship attribution tools are misused. Therefore, it is crucial to establish strict guidelines and ethical standards on the use of such technologies to prevent breaches of privacy.

\textbf{Potential for Abuse} Despite multiple beneficial applications, the misuse potential of authorship attribution tools is significant. Risks include the use of this technology to suppress free speech or to endanger personal safety by identifying individuals in contexts where anonymity is crucial for safety. Addressing these risks requires robust governance to prevent misuse and to ensure that the technology is used ethically and responsibly.

\textbf{Bias Issue} The performance of authorship attribution methods can vary across different demographics, leading to potential biases. It is important to continually assess and correct these biases to ensure fairness in the application of this technology.

\textbf{Misclassification Issue} Given the high stakes involved, especially in forensic contexts, the accuracy of authorship attribution is important. Misclassifications can have serious consequences, including wrongful accusations or legal implications. It is essential for authorship attribution methods to be reliable and for their limitations to be transparently communicated to users.

\section{Broader Impact}
Our study of authorship attribution using LLMs contributes to advancements in various domains:

\textbf{Forensic Linguistics} Our research contributes to the field of forensic linguistics by providing tools that can solve crimes involving anonymous or disputed texts. This can be particularly useful for law enforcement and legal professionals who need to gather evidence and make more informed decisions.

\textbf{Intellectual Property Protection} Our method can serve as a powerful tool in identifying the authors of texts, which can help protect intellectual property rights and resolve disputes in copyright.

\textbf{Historical Text Attribution} In literary and historical studies, determining the authorship of texts can provide insights into their origins and contexts, enhancing our understanding and interpretation.

\textbf{Enhanced Content Management} Media and content companies can use this technology to manage content more effectively by accurately attributing authorship to various contributors.

\textbf{Educational Applications} In educational settings, our method can help prevent plagiarism and promote academic integrity. It can also serve as a teaching tool to help students understand and appreciate stylistic differences between authors.

While our method holds promise across multiple applications, it is crucial to deploy it with caution. Ensuring that the technology is used responsibly and ethically will be key to maximizing its benefits while minimizing potential harm.

\end{document}
=====END FILE=====