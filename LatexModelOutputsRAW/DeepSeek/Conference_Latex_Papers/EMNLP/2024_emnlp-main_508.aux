\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{3}{section.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Vanilla open-source reward models have a clear left-leaning political bias. All three subplots show reward scores on the paired TwinViews political statements data, with histograms broken out for the left and right sides. Dashed vertical lines indicate each side's mean reward; a left political bias is indicated by a higher value for the blue line than the red line. The magnitude of the bias (difference in group means divided by pooled SD) is shown on each subplot. Note the presence of inverse scaling: Both model sizes and bias increase from left to right (although the training datasets/methods are different across the models).}}{3}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:1}{{1}{3}{Vanilla open-source reward models have a clear left-leaning political bias. All three subplots show reward scores on the paired TwinViews political statements data, with histograms broken out for the left and right sides. Dashed vertical lines indicate each side's mean reward; a left political bias is indicated by a higher value for the blue line than the red line. The magnitude of the bias (difference in group means divided by pooled SD) is shown on each subplot. Note the presence of inverse scaling: Both model sizes and bias increase from left to right (although the training datasets/methods are different across the models)}{figure.caption.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Alignment}{3}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Truthfulness in LLMs}{3}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Political bias in LLMs}{3}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Experimental Setup}{4}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Truthfulness Datasets}{4}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Political Dataset: TwinViews-13k}{4}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Models}{4}{subsection.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Bias in Vanilla Reward Models}{5}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Bias in ``Truthful'' Reward Models}{5}{section.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces ``Truthful'' reward models usually show a left-leaning political bias. The left three subplots show rewards assigned to TwinViews political statements by models fine-tuned on each truthfulness dataset, excluding explicitly political content found by our audit. We run five train/eval splits for each dataset and model. Individual points show results from each run, with blue points representing the average reward given to left-leaning statements and red points representing the average reward given to right-leaning statements. The red and blue bar heights show the average reward across all five runs (i.e. the average of the corresponding point values). Note the presence of inverse scaling: Larger models usually skew further left. Results of Section 5.2's n-gram experiment appear in the rightmost pane, showing no clear relationship to the neural models' patterns.}}{6}{figure.caption.2}\protected@file@percent }
\newlabel{fig:2}{{2}{6}{``Truthful'' reward models usually show a left-leaning political bias. The left three subplots show rewards assigned to TwinViews political statements by models fine-tuned on each truthfulness dataset, excluding explicitly political content found by our audit. We run five train/eval splits for each dataset and model. Individual points show results from each run, with blue points representing the average reward given to left-leaning statements and red points representing the average reward given to right-leaning statements. The red and blue bar heights show the average reward across all five runs (i.e. the average of the corresponding point values). Note the presence of inverse scaling: Larger models usually skew further left. Results of Section 5.2's n-gram experiment appear in the rightmost pane, showing no clear relationship to the neural models' patterns}{figure.caption.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Explicit Political Bias}{6}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Stylistic Artifacts}{6}{subsection.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Bias Across Topics}{6}{section.6}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Regression results on the TwinViews data for reward as a function of statement features, for reward scores from both vanilla (``Vanilla'') and Pythia-based ``truthful'' reward models (``Truth FT''). Positive coefficients (in red) indicate a topic where conservative statements have higher reward, controlling for model and topic fixed effects, while negative coefficients (in blue) indicate a liberal skew. Coefficients shown are for the topic/political-leaning interaction, except for the main effect of political leaning in the last row. Robust SEs in parentheses. \((\ast = 0.05,\ast \ast = 0.01,\ast \ast \ast = 0.001)\)}}{7}{table.caption.3}\protected@file@percent }
\newlabel{tab:1}{{1}{7}{Regression results on the TwinViews data for reward as a function of statement features, for reward scores from both vanilla (``Vanilla'') and Pythia-based ``truthful'' reward models (``Truth FT''). Positive coefficients (in red) indicate a topic where conservative statements have higher reward, controlling for model and topic fixed effects, while negative coefficients (in blue) indicate a liberal skew. Coefficients shown are for the topic/political-leaning interaction, except for the main effect of political leaning in the last row. Robust SEs in parentheses. \((\ast = 0.05,\ast \ast = 0.01,\ast \ast \ast = 0.001)\)}{table.caption.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion}{7}{section.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8}Limitations}{7}{section.8}\protected@file@percent }
\bibcite{AzariaMitchell2023}{1}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}Politics is relative}{8}{subsection.8.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}Difficulty of capturing truth}{8}{subsection.8.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3}Only reward models}{8}{subsection.8.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {9}Ethical Considerations}{8}{section.9}\protected@file@percent }
\bibcite{BaiEtAl2022}{2}
\bibcite{BangEtAl2024}{3}
\bibcite{BaptistaGradim2022}{4}
\bibcite{BidermanEtAl2023}{5}
\bibcite{BurnsEtAl2022}{6}
\bibcite{CasperEtAl2023}{7}
\bibcite{ChuangEtAl2024}{8}
\bibcite{ColognaEtAl2024}{9}
\bibcite{CuiEtAl2023}{10}
\bibcite{DongEtAl2023}{11}
\bibcite{FarquharEtAl2023}{12}
\bibcite{FengEtAl2023}{13}
\bibcite{GeminiTeam2024}{14}
\bibcite{Hulme2009}{15}
\bibcite{JonesEtAl2019}{16}
\bibcite{KavanaghRich2018}{17}
\bibcite{KirkEtAl2024}{18}
\bibcite{KopfEtAl2023}{19}
\bibcite{LevinsteinHerrmann2024a}{20}
\bibcite{LevinsteinHerrmann2024b}{21}
\bibcite{LiEtAl2023}{22}
\bibcite{LinEtAl2022}{23}
\bibcite{LiuEtAl2021}{24}
\bibcite{MarksTegmark2023}{25}
\bibcite{McKenzieEtAl2023}{26}
\bibcite{MotokiEtAl2023}{27}
\bibcite{OpenAI2023}{28}
\bibcite{OpenAIEtAl2023}{29}
\bibcite{PedregosaEtAl2011}{30}
\bibcite{PerezEtAl2023}{31}
\bibcite{RafailovEtAl2023}{32}
\bibcite{SanturkarEtAl2023}{33}
\bibcite{ShenEtAl2023}{34}
\bibcite{StiennonEtAl2020}{35}
\bibcite{ThorneEtAl2018}{36}
\bibcite{vonWerraEtAl2024}{37}
\bibcite{WelblEtAl2017}{38}
\bibcite{WolfEtAl2020}{39}
\@writefile{toc}{\contentsline {section}{\numberline {A}TwinViews-13k: Political Statements}{12}{appendix.A}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}Prompt}{12}{subsection.A.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}Quality Assurance}{12}{subsection.A.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Statistics about the LM-detected ideology of the paired political statements in TwinViews-13k, showing close alignment with the desired ideological leanings of left and right statements. (a) Gemma-2B-instruct. All statements were assigned probabilities for both liberal and conservative. (b) GPT-3.5-turbo-instruct. On a random sample of 300 (left, right) statement pairs, we obtained probabilities of the most likely completions for the QA prompt discussed in the text from OpenAI's API. The API does not allow obtaining probabilities for arbitrary completions. For nearly all statements (295 for left, 292 for right), only the matching ideological class was likely enough to be returned. On only 4 left statements and 6 right statements was the opposite ideology likely enough to be returned.}}{13}{table.caption.7}\protected@file@percent }
\newlabel{tab:2}{{2}{13}{Statistics about the LM-detected ideology of the paired political statements in TwinViews-13k, showing close alignment with the desired ideological leanings of left and right statements. (a) Gemma-2B-instruct. All statements were assigned probabilities for both liberal and conservative. (b) GPT-3.5-turbo-instruct. On a random sample of 300 (left, right) statement pairs, we obtained probabilities of the most likely completions for the QA prompt discussed in the text from OpenAI's API. The API does not allow obtaining probabilities for arbitrary completions. For nearly all statements (295 for left, 292 for right), only the matching ideological class was likely enough to be returned. On only 4 left statements and 6 right statements was the opposite ideology likely enough to be returned}{table.caption.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B}Generated True/False Statements}{13}{appendix.B}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {C}Other True/False Datasets}{14}{appendix.C}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {D}Identifying Political Content}{14}{appendix.D}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {D.1}Keyword Approach}{14}{subsection.D.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {D.2}LLM Approach}{14}{subsection.D.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {D.3}Results}{14}{subsection.D.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {E}Model Training Details}{14}{appendix.E}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {F}Use of AI Tools}{15}{appendix.F}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {G}Data/Code Availability}{15}{appendix.G}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Number of examples pertaining to a political topic in each truthfulness dataset.}}{15}{table.caption.8}\protected@file@percent }
\newlabel{tab:3}{{3}{15}{Number of examples pertaining to a political topic in each truthfulness dataset}{table.caption.8}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Samples from the TwinViews-13k political statements.}}{16}{table.caption.9}\protected@file@percent }
\newlabel{tab:4}{{4}{16}{Samples from the TwinViews-13k political statements}{table.caption.9}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Samples from the generated true/false statements.}}{16}{table.caption.10}\protected@file@percent }
\newlabel{tab:5}{{5}{16}{Samples from the generated true/false statements}{table.caption.10}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces [ILLEGIBLE] Samples from the FEVER dataset.}}{17}{table.caption.11}\protected@file@percent }
\newlabel{tab:6}{{6}{17}{[ILLEGIBLE] Samples from the FEVER dataset}{table.caption.11}{}}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Samples from the SciQ dataset.}}{17}{table.caption.12}\protected@file@percent }
\newlabel{tab:7}{{7}{17}{Samples from the SciQ dataset}{table.caption.12}{}}
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces Samples from the TruthfulQA dataset.}}{18}{table.caption.13}\protected@file@percent }
\newlabel{tab:8}{{8}{18}{Samples from the TruthfulQA dataset}{table.caption.13}{}}
\gdef \@abspage@last{18}
