=====FILE: main.tex=====
\documentclass{article}

\usepackage{graphicx}
\usepackage{amsmath,amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
\usepackage{url}
\usepackage{cite}
\usepackage{caption}
\usepackage{float}

\title{Primacy Effect of ChatGPT}
\author{
Yiwei Wang\textsuperscript{†*}\quad Yujun Cai\textsuperscript{†*}\quad Muhao Chen\textsuperscript{\S}\quad Yuxuan Liang\textsuperscript{\P}\quad Bryan Hooi\textsuperscript{†} \\
\textsuperscript{†} University of California, Los Angeles \\
\textsuperscript{*} Meta \\
\textsuperscript{\S} University of California, Davis \\
\textsuperscript{\P} National University of Singapore \\
\textsuperscript{\P} Hong Kong University of Science and Technology (Guangzhou) \\
\url{wangyw.evan@gmail.com}
}

\begin{document}

\maketitle

\begin{abstract}
Instruction-tuned large language models (LLMs), such as ChatGPT, have led to promising zero-shot performance in discriminative natural language understanding (NLU) tasks. This involves querying the LLM using a prompt containing the question, and the candidate labels to choose from. The question-answering capabilities of ChatGPT arise from its pre-training on large amounts of human-written text, as well as its subsequent fine-tuning on human preferences, which motivates us to ask: Does ChatGPT also inherit humans' cognitive biases? In this paper, we study the primacy effect of ChatGPT: the tendency of selecting the labels at earlier positions as the answer. We have two main findings: i) ChatGPT's decision is sensitive to the order of labels in the prompt; ii) ChatGPT has a clearly higher chance to select the labels at earlier positions as the answer. We hope that our experiments and analyses provide additional insights into building more reliable ChatGPT-based solutions. We release the source code at \url{https://github.com/wangywUST/PrinacyEffectGPT}.
\end{abstract}

\section{Introduction}
Humans tend to recall information presented at the start of a list better than information at the middle or end. This phenomenon is known as the primacy effect (Asch, 1946), which is a cognitive bias that relates to humans' attention spans (Crano, 1977), rehearsal (Tan and Ward, 2000), and memory systems (Li, 2010). Similarly, in advertisement systems and search engines, humans tend to interact with items in higher positions regardless of the items' actual relevance (Chen et al., 2023). Primacy effect influences humans' behaviors to make unfair decisions. Similarly, if it exists in machine learning models, it may lead to worse performance.

\begin{figure}[H]
\centering
\fbox{IMAGE NOT PROVIDED}
\caption{Primacy Effect of ChatGPT: ChatGPT tends to return labels in earlier positions as the answer. This plot shows the distribution of ChatGPT's predicted label indices in TACRED (42 classes), where we randomly shuffle labels before every prediction (see Sec. 2.2).}
\end{figure}

Recently, instruction-tuned large language models (LLMs), represented by ChatGPT (OpenAI, 2022), have received wide attention on their capabilities of imitating humans in question-answering and problem-solving. However, this underlying behavioral similarity between ChatGPT and humans naturally leads to an intriguing question: Is ChatGPT also affected by the primacy effect?

ChatGPT provides a convenient way to achieve the discriminative natural language understanding (NLU) (Li et al., 2023; Wei et al., 2023; Yuan et al., 2023). People only need to list the labels in the prompt and asking ChatGPT to select the label(s) that match the input text. In this work, to analyze the primacy effect of ChatGPT, we start by testing with random label shuffling, i.e., shuffling labels listed in the prompt before every prediction. We compare the predictions on the same instance with two different label orders. Then, we count the predicted label indices on many instances with label shuffling. The motivation is that: a fair NLU model should give the same prediction on an input instance regardless of how the labels are ordered; consequently, it should produce uniformly distributed label indices under label shuffling for any instance. Through extensive experiments with a series of NLU datasets, we find that ChatGPT's prediction is sensitive to the order of labels in the prompt. Specifically, ChatGPT's prediction changes after a label shuffling on $87.9\%$ of the instances in TACRED. ChatGPT is affected by the primacy effect: ChatGPT tends to select labels in earlier positions in the prompt (see Fig. 1), which present clear bias with respect to the label order.

On the whole, our work contributes to a better understanding of ChatGPT's behaviors and building more faithful ChatGPT-based NLU solutions.

\section{Primacy Effect of ChatGPT}
In this section, we first introduce the general prompt design of ChatGPT in discriminative natural language understanding (NLU). Then, we analyze the primacy effect of ChatGPT using label shuffling in prompts.

\subsection{Prompts for ChatGPT}
Prompts are a key component to the effective use of ChatGPT on discriminative NLU tasks (Wei et al., 2023; Yuan et al., 2023). Generally, prompts for such tasks involve two key components: (i) label definitions, and (ii) a task description and input text (see an example in Fig. 2).

ChatGPT's capability of understanding instructions in the prompt benefits from its training with human feedback (OpenAI, 2022), but this also creates the risk of inheriting humans' cognitive biases. In this paper, we discuss a cognitive bias in ChatGPT: the primacy effect, which indicates the tendency of selecting labels in earlier positions in the prompt.

\subsection{Analysis with Label Shuffling}
Analyzing the primacy effect requires us to distill the effects of label orders in the prompts. However, this is non-trivial because there are many factors influencing ChatGPT's decisions, such as the input text and label definitions. In our work, to distinguish the primacy effect of ChatGPT from other factors, we conduct random shuffling for labels listed in the prompts. Specifically, before every prediction, we shuffle the labels as visualized in Fig. 3. Label shuffling erases the discriminative semantics of the specific label orders in the prompts.

\begin{figure}[H]
\centering
\fbox{IMAGE NOT PROVIDED}
\caption{A prompt example for ChatGPT.}
\end{figure}

Ideally, a fair model should return the same prediction when labels are shuffled, and consequently, the predicted label index should follow a uniform distribution under random shuffling.

Next, we introduce our two ways of using random label shuffling to analyze ChatGPT.

\paragraph{Prediction Comparison on an Instance}
A reliable and consistent classifier is expected to consistently choose the same label for the same instance irrespective of the label order. To evaluate such consistency of ChatGPT, we perform the random shuffling for the same instance twice to produce two prompts. We feed these two prompts to ChatGPT and compare the corresponding two predictions with each other. We apply the above process to all the test instances and compute the fraction of the instances where the prediction changed after label shuffling. The higher the fraction is, the more sensitive ChatGPT is to the label order.

\paragraph{Statistics of Predicted Indices}
Taking a further step, we perform statistical analysis on the predicted indices for instances where the prediction changed after label shuffling. If ChatGPT does not have any preference on the label orders, its predicted label indices should be uniformly distributed. By comparing the predicted label index distribution of ChatGPT to the uniform distribution, we can assess its fairness and preferences regarding label orders.

\section{Experiments}
We analyze the primacy effect based on the aforementioned strategies using three relation extraction datasets and an intent detection dataset.

\begin{figure}[H]
\centering
\fbox{IMAGE NOT PROVIDED}
\caption{We analyze the primacy effects of ChatGPT by randomly shuffling the labels in the prompts.}
\end{figure}

\subsection{Experiment Setup}
We mainly chose relation extraction and intent detection tasks in our experiments since these tasks naturally come with adequately sized decision spaces to illustrate the underlying primacy effect of labels. For relation extraction, we experiment on three benchmark datasets including TACRED (Zhang et al., 2017), TACREV (Alt et al., 2020), and Re-TACRED (Stoica et al., 2021). For intent detection, we conducted experiments on Banking77 (Casanueva et al., 2020a) and MASSIVE (FitzGerald et al., 2022). MASSIVE (FitzGerald et al., 2022) is a parallel dataset of massive utterances with annotations for the Natural Language Understanding tasks of intent prediction. Utterances span 60 intents.

We additionally conducted experiments on the NLP datasets: GoEmotions (Demszky et al., 2020) and 20 Newsgroups (Albishe et al., 2015) for a more comprehensive evaluation. GoEmotions (Demszky et al., 2020) is a dataset for fine-grained emotion classification. It is a corpus of 58k carefully curated comments extracted from Reddit, with human annotations for 27 emotion categories and a neutral one. The 20 Newsgroups (Albishe et al., 2015) dataset is a collection of approximately 20,000 newsgroup documents, partitioned across 20 different newsgroups.

We follow the existing work (Wei et al., 2023; Li et al., 2023) to apply ChatGPT to these tasks via the OpenAI API gpt-3.5-turbo. Specifically, we set the temperature as 0.0 to minimize the randomness of ChatGPT's outputs. For comparison, we adopt the existing work (Casanueva et al., 2020b; Zhou and Chen, 2022) to fine-tune the BERT model with an MLP classification head.

\subsection{Consistency under Label Shuffling}
First, we observe the low consistency of ChatGPT confronted under label shuffling. As shown in Table 1, ChatGPT changes its label prediction after label shuffling in over $85\%$ of the test instances on the TACRED, TACREV, and Re-TACRED datasets, and in $35.7\%$ of instances on Banking77. Also, ChatGPT changes its label prediction after label shuffling in over $69\%$ of the test instances on the datasets of GoEmotions and in more than $30\%$ of instances on MASSIVE and 20 Newsgroups. In contrast, the fine-tuned BERT classifier maintains consistent predictions after label shuffling. This discrepancy challenges the widely-held belief that ChatGPT can comprehend human instructions and provide consistent responses. One possible explanation is that ChatGPT's understanding of the prompt is obtained by training on human-labeled data, which inherits humans' cognitive bias of treating labels at different positions unfairly.

It is worth noting that the ratio of instances with changed predictions is consistently high across the relation extraction datasets but lower on intent detection. This discrepancy can be attributed to the fact that information extraction tasks are shown to be challenging for ChatGPT and other LLMs (Wang et al., 2023; Li et al., 2023). In more difficult tasks, ChatGPT lacks sufficient discriminative semantic understanding from the input text and may be more affected by the label order.

\subsection{Primacy Effect of ChatGPT}
The empirical results in Section 3.2 indicate that ChatGPT's predictions are affected by label order. To deeper delve into the effects of label orders on ChatGPT, we analyze the distribution of predicted label indices (e.g., if the prediction is the first label, the label index is 1), as introduced in Section 2.2. We visualize the distributions in Fig. 4. Notably, the distribution of ChatGPT's predictions consistently deviates from the uniform distribution, displaying a consistent bias towards smaller indices across different datasets. In contrast, BERT exhibits no preference for label orders and consistently demonstrates a uniform distribution in its predicted label indices.

We term this tendency of ChatGPT as the primacy effect, where the model tends to favor the labels presented earlier in the prompt. The magnitude of these primacy effects varies across tasks, as illustrated in Fig.4. Notably, the influence of primacy effects is higher in more challenging tasks. This observation aligns with the results discussed in Sec.3.3, wherein the impact of primacy effects is greater when ChatGPT tackles more difficult tasks. In the next section, we will quantitatively analyze the primacy effects of ChatGPT.

\begin{table}[H]
\centering
\caption{Fraction of the instances that have their predicted label changed after a label shuffling.}
\label{tab:consistency}
\begin{tabular}{lccccccc}
\toprule
Method & TACRED & TACREV & Re-TACRED & Banking77 & GoEmotions & MASSIVE & 20 Newsgroups \\
\midrule
ChatGPT w/ Prompt & 87.9 & 85.9 & 88.6 & 35.7 & 69.3 & 32.8 & 34.1 \\
BERT w/ MLP & 0.0 & 0.0 & 0.0 & 0.0 & 0.1 & 0.0 & 0.0 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[H]
\centering
\fbox{IMAGE NOT PROVIDED}
\caption{The distribution of predicted indices of the test instances with label shuffling before every prediction.}
\end{figure}

\subsection{Evaluation on Fairness}
The fairness of a trained model can be assessed by examining the imbalance or skewness in its predictions (Sweeney and Najafian, 2019). Following prior studies (Xiang et al., 2020; Sweeney and Najafian, 2019; Qian et al., 2021; Wang et al., 2022), we employ the JS divergence (Fuglede and Topsoe, 2004) as the metric to evaluate how imbalanced/skewed/unfair a prediction $P$ is. The measurement is symmetric (i.e., $\mathrm{JS}(P\| U) = \mathrm{JS}(U\| P))$ and strictly scoped.

To evaluate the label order bias of ChatGPT, we compute the average relative label order imbalance (LOI): LOI is defined as the JS divergence between the predicted label index distribution $P$ and the uniform distribution $U$ :
\[
\mathrm{LOI} = \mathrm{JS}(P(x|x\in \mathcal{D}),U), \quad (1)
\]
where $x$ represents an input instance, $\mathcal{D}$ is the test set, $P(x)$ is the predicted label index, and $U$ is the uniform distribution. LOI captures the disparity between the predicted indices and a uniform distribution.

We conduct the fairness evaluation following the experimental settings described in Section 3.3, and the results are presented in Table 2. The findings demonstrate that ChatGPT exhibits unfair treatment of label indices when making relation label predictions for input texts. Furthermore, the degree of unfairness increases with the task's difficulty, which aligns with the empirical results discussed in Sections 3.2 and 3.3. In contrast, BERT demonstrates significantly better fairness, as its predictions are not influenced by label orders.

We additionally test the performance of ChatGPT with CoT (Chain-of-thoughts) (Wei et al., 2022). With CoT, ChatGPT still exhibits the primacy effect. The above results show that with or without CoT, ChatGPT consistently exhibits the primacy effect. A reason for this phenomenon could be that the CoT encourages the LLMs for "slow thinking" about the question but does not necessarily mitigate the primacy bias.

\begin{table}[H]
\centering
\caption{Experimental results (unfairness; $\%$ ) on the test sets of TACRED, TACRED-Revisit, Re-TACRED, and Banking77 (lower is better). The best results in each column are highlighted in bold font.}
\label{tab:unfairness}
\begin{tabular}{lcccc}
\toprule
Method & TACRED & TACREV & Re-TACRED & Banking77 \\
\midrule
ChatGPT w/ Prompts & 57.9 & 57.8 & 58.1 & 18.8 \\
ChatGPT w/ CoT & 57.6 & 57.9 & 58.3 & 18.6 \\
BERT w/ MLP & 1.8 & 1.9 & 2.3 & 2.1 \\
\bottomrule
\end{tabular}
\end{table}

\section{Related Work}
Large Language Models (LLMs) (Brown et al., 2020; Rae et al., 2021; Thoppilan et al., 2022; Smith et al., 2022), such as GPT-3 (Brown et al., 2020), LaMDA (Thoppilan et al., 2022) and PaLM (Chowdhery et al., 2022), refer to large scale pretrained models that contain more than a hundred billion parameters. Based on the highly parallelizable Transformer architecture (Vaswani et al., 2017), these Large Language models have shown powerful capability to produce reasonable results with very few samples or task descriptions as input.

A key milestone in the development process of LLMs is ChatGPT, which is developed by OpenAI based on InstructGPT (Ouyang et al., 2022). ChatGPT is able to interact with humans through multiple turns of dialogue, understand user intent, accomplish instructions, and return human-like responses. This attracts huge attention from research field, motivating numerous recent work (Zhang et al., 2022; Ma et al., 2023; Wan et al., 2023; Zhong et al., 2023; Susnjak, 2023) to utilize ChatGPT to different tasks.

As ChatGPT is a proprietary model, and OpenAI does not disclose its training specifics, researchers are actively investigating its associated implications and capabilities. There has been some work analyzing the performance, robustness, faithfulness, and explain-ability of ChatGPT (Gao et al., 2023; Han et al., 2023; Li et al., 2023). For example, (Malinka et al., 2023) investigates the educational integrity of ChatGPT and evaluates the ChatGPT's abilities to solve assignments of various levels in computer security specialization. (Haque et al., 2022) and (Krugel et al., 2023) investigate the ethical risks of ChatGPT.

Before ChatGPT, LLMs' inference has been accompanied by in-context learning (ICL) which adds a few demonstrations in the prompt (Dong et al., 2022; Fei et al., 2023). Accordingly, some work investigates the effects of demonstration orders for the LLMs before ChatGPT (Lu et al., 2021). (Zhao et al., 2021) finds the majority label, recency, and common token biases of LLMs' ICL.

Different from the above work, we focus on a new phenomenon of ChatGPT: the primacy effect, which is the tendency of selecting the first labels as the answer. The primary effect seriously influences ChatGPT's fairness. Collectively, our findings provide a new understanding of how ChatGPT works given the instructional prompts.

\section{Conclusion}
While previous work often takes ChatGPT as a universal method applicable to all text-related tasks, we argue that its flexibility comes with the risk of inheriting human's cognitive biases. These biases lead to unfair judgments which can affect the performance of the machine learning model. This work studies a cognitive bias of ChatGPT: primacy effects. We propose a simple yet effective label shuffling method to analyze the influence of label orders on ChatGPT. We discover the primacy effect of ChatGPT and finds that it highly influences the fairness of ChatGPT in NLU. Our work contributes to a better understanding of the behaviors of ChatGPT and building more faithful solutions with ChatGPT in NLU applications.

\section{Limitation}
Our work has a few potential limitations. Firstly, we primarily evaluate the primacy effect of ChatGPT, which is one of the most widely-used instruction-legacy models for each task. It would be beneficial to assess this effect on other LLMs models (such as Google Bard, vicuna (Chiang et al., 2023)) and explore additional tasks to examine this primacy effect. Secondly, this work focused on analyzing the primacy effect of ChatGPT through experiments. We encourage further studies to propose effective solutions that can mitigate the negative impacts associated with the primacy effect.

\section{Acknowledgement}
The authors would like to thank the anonymous reviewers for their discussion and feedback. Yi-wei Wang and Bryan Hooi are supported by NUS ODPRT Grant A-0008067-00-00, NUS ODPRT Grant R252-000-A81-133, and Singapore Ministry of Education Academic Research Fund Tier 3 under MOEs official grant number MOE2017-T3-1-007. Muhao Chen is supported by the NSF Grant IIS 2105329, the NSF Grant ITE 2333736, the DARPA MCS program under Contract No. N660011924033 with the United States Office Of Naval Research, a Cisco Research Award, two Amazon Research Awards, and a Keston Research Award.

\bibliographystyle{plain}
\begin{thebibliography}{10}

\bibitem[Albishe et al., 2015]{albishe2015}
Albishe et al.
\newblock 2015.
\newblock Effective 20 newsgroups dataset cleaning.
\newblock In {\em 2015 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT)}, volume 3, pages 98-101. IEEE.

\bibitem[Alt et al., 2020]{alt2020}
Christoph Alt, Aleksandra Gabryszak, and Leonhard Hennig.
\newblock 2020.
\newblock TACRED revisited: A thorough evaluation of the TACRED relation extraction task.
\newblock In {\em Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics}, pages 1558-1569, Online. Association for Computational Linguistics.

\bibitem[Asch, 1946]{asch1946}
Solomon E Asch.
\newblock 1946.
\newblock Forming impressions of personality.
\newblock {\em The Journal of Abnormal and Social Psychology}, 41(3):258.

\bibitem[Brown et al., 2020]{brown2020}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.
\newblock 2020.
\newblock Language models are few-shot learners.
\newblock {\em Advances in neural information processing systems}, 33:1877-1901.

\bibitem[Casanueva et al., 2020a]{casanueva2020a}
Inigo Casanueva, Tadas Temcinas, Daniela Gerz, Matthew Henderson, and Ivan Vulic.
\newblock 2020a.
\newblock Efficient intent detection with dual sentence encoders.
\newblock In {\em Proceedings of the 2nd Workshop on NLP for ConvAI - ACL 2020}. Data available at \url{https://github.com/PolyAI-LDN/task-specificdatasets}.

\bibitem[Casanueva et al., 2020b]{casanueva2020b}
Inigo Casanueva, Tadas Temcinas, Daniela Gerz, Matthew Henderson, and Ivan Vulic.
\newblock 2020b.
\newblock Efficient intent detection with dual sentence encoders.
\newblock {\em arXiv preprint arXiv:2003.04807}.

\bibitem[Chen et al., 2023]{chen2023}
Jiawei Chen, Hande Dong, Xiang Wang, Fuli Feng, Meng Wang, and Xiangnan He.
\newblock 2023.
\newblock Bias and debias in recommender system: A survey and future directions.
\newblock {\em ACM Transactions on Information Systems}, 41(3):1-39.

\bibitem[Chiang et al., 2023]{chiang2023}
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing.
\newblock 2023.
\newblock Vicuna: An open-source chatbot impressing gpt-4 with $90\%$ chatgpt quality.

\bibitem[Chowdhery et al., 2022]{chowdhery2022}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al.
\newblock 2022.
\newblock Palm: Scaling language modeling with pathways.
\newblock {\em arXiv preprint arXiv:2204.02311}.

\bibitem[Crano, 1977]{crano1977}
William D Crano.
\newblock 1977.
\newblock Primacy versus recency in retention of information and opinion change.
\newblock {\em The Journal of Social Psychology}, 101(1):87-96.

\bibitem[Demszky et al., 2020]{demszky2020}
Dorottya Demszky, Dana Movshovitz-Attias, Jeongwoo Ko, Alan Cowen, Gaurav Nemade, and Sujith Ravi.
\newblock 2020.
\newblock Goemotions: A dataset of fine-grained emotions.
\newblock {\em arXiv preprint arXiv:2005.00547}.

\bibitem[Dong et al., 2022]{dong2022}
Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui.
\newblock 2022.
\newblock A survey for in-context learning.
\newblock {\em arXiv preprint arXiv:2301.00234}.

\bibitem[Fei et al., 2023]{fei2023}
Yu Fei, Yifan Hou, Zeming Chen, and Antoine Bosselut.
\newblock 2023.
\newblock Mitigating label biases for in-context learning.
\newblock {\em arXiv preprint arXiv:2305.19148}.

\bibitem[FitzGerald et al., 2022]{fitzgerald2022}
Jack FitzGerald, Christopher Hench, Charith Peris, Scott Mackie, Kay Rottmann, Ana Sanchez, Aaron Nash, Liam Urbach, Vishesh Kakarala, Richa Singh, et al.
\newblock 2022.
\newblock Massive: A 1m-example multilingual natural language understanding dataset with 51 typologically-diverse languages.
\newblock {\em arXiv preprint arXiv:2204.08582}.

\bibitem[Fuglede and Topsoe, 2004]{fuglede2004}
Bent Fuglede and Flemming Topsoe.
\newblock 2004.
\newblock Jensen-shannon divergence and hilbert space embedding.
\newblock In {\em International Symposium on Information Theory, 2004. ISIT 2004. Proceedings.}, page 31. IEEE.

\bibitem[Gao et al., 2023]{gao2023}
Jinglong Gao, Xiao Ding, Bing Qin, and Ting Liu.
\newblock 2023.
\newblock Is chatgpt a good causal reasoner? a comprehensive evaluation.
\newblock {\em arXiv preprint arXiv:2305.07375}.

\bibitem[Han et al., 2023]{han2023}
Ridong Han, Tao Peng, Chaohao Yang, Benyou Wang, Lu Liu, and Xiang Wan.
\newblock 2023.
\newblock Is information extraction solved by chatgpt? an analysis of performance, evaluation criteria, robustness and errors.
\newblock {\em arXiv preprint arXiv:2305.14450}.

\bibitem[Haque et al., 2022]{haque2022}
Mubin Ul Haque, Isuru Dharmadasa, Zarrin Tasnim Sworna, Roshan Namal Rajapakse, and Hussain Ahmad.
\newblock 2022.
\newblock ``i think this is the most disruptive technology'': Exploring sentiments of chatgpt early adopters using twitter data.
\newblock {\em arXiv preprint arXiv:2212.05856}.

\bibitem[Krugel et al., 2023]{krugel2023}
Sebastian Krugel, Andreas Ostermaier, and Matthias Uhl.
\newblock 2023.
\newblock The moral authority of chatgpt.
\newblock {\em arXiv preprint arXiv:2301.07098}.

\bibitem[Li et al., 2023]{li2023eval}
Bo Li, Gexiang Fang, Yang Yang, Quansen Wang, Wei Ye, Wen Zhao, and Shikun Zhang.
\newblock 2023.
\newblock Evaluating chatgpt's information extraction capabilities: An assessment of performance, explainability, calibration, and faithfulness.
\newblock {\em arXiv preprint arXiv:2304.11633}.

\bibitem[Li, 2010]{li2010}
Cong Li.
\newblock 2010.
\newblock Primacy effect or recency effect? a long-term memory test of super bowl commercials.
\newblock {\em Journal of Consumer Behaviour: An International Research Review}, 9(1):32-44.

\bibitem[Lu et al., 2021]{lu2021}
Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp.
\newblock 2021.
\newblock Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity.
\newblock {\em arXiv preprint arXiv:2104.08786}.

\bibitem[Ma et al., 2023]{ma2023}
Yubo Ma, Yixin Cao, YongChing Hong, and Aixin Sun.
\newblock 2023.
\newblock Large language model is not a good few-shot information extractor, but a good reranker for hard samples!
\newblock {\em arXiv preprint arXiv:2303.08559}.

\bibitem[Malinka et al., 2023]{malinka2023}
Kamil Malinka, Martin Peresíni, Anton Firc, Ondrej Hujnak, and Filip Janus.
\newblock 2023.
\newblock On the educational impact of chatgpt: Is artificial intelligence ready to obtain a university degree?
\newblock In {\em Proceedings of the 2023 Conference on Innovation and Technology in Computer Science Education V. 1}, pages 47-53.

\bibitem[OpenAI, 2022]{openai2022}
OpenAI.
\newblock 2022.
\newblock Introducing chatgpt.
\newblock \url{https://openai.com/blog/chatgpt}.

\bibitem[Ouyang et al., 2022]{ouyang2022}
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al.
\newblock 2022.
\newblock Training language models to follow instructions with human feedback.
\newblock {\em Advances in Neural Information Processing Systems}, 35:27730-27744.

\bibitem[Qian et al., 2021]{qian2021}
Chen Qian, Fuli Feng, Lijie Wen, Chunping Ma, and Pengjun Xie.
\newblock 2021.
\newblock Counterfactual inference for text classification debiasing.
\newblock In {\em Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)}, pages 5434-5445.

\bibitem[Rae et al., 2021]{rae2021}
Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al.
\newblock 2021.
\newblock Scaling language models: Methods, analysis \& insights from training gopher.
\newblock {\em arXiv preprint arXiv:2112.11446}.

\bibitem[Smith et al., 2022]{smith2022}
Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, et al.
\newblock 2022.
\newblock Using deep-speed and megatron to train megatron-turing nlg 530b, a large-scale generative language model.
\newblock {\em arXiv preprint arXiv:2201.11990}.

\bibitem[Stoica et al., 2021]{stoica2021}
George Stoica, Emmanouil Antonios Platanios, and Barnabás Póczos.
\newblock 2021.
\newblock Re-tacred: Addressing shortcomings of the tacred dataset.
\newblock In {\em Proceedings of the AAAI Conference on Artificial Intelligence}, volume 35, pages 13843-13850.

\bibitem[Susnjak, 2023]{susnjak2023}
Teo Susnjak.
\newblock 2023.
\newblock Applying bert and chatgpt for sentiment analysis of lyme disease in scientific literature.
\newblock {\em arXiv preprint arXiv:2302.06474}.

\bibitem[Sweeney and Najafian, 2019]{sweeney2019}
Chris Sweeney and Maryam Najafian.
\newblock 2019.
\newblock A transparent framework for evaluating unintended demographic bias in word embeddings.
\newblock In {\em Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics}, pages 1662-1667.

\bibitem[Tan and Ward, 2000]{tan2000}
Lydia Tan and Geoff Ward.
\newblock 2000.
\newblock A recency-based account of the primacy effect in free recall.
\newblock {\em Journal of Experimental Psychology: Learning, Memory, and Cognition}, 26(6):1589.

\bibitem[Thoppilan et al., 2022]{thoppilan2022}
Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al.
\newblock 2022.
\newblock Lamda: Language models for dialog applications.
\newblock {\em arXiv preprint arXiv:2201.08239}.

\bibitem[Vaswani et al., 2017]{vaswani2017}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin.
\newblock 2017.
\newblock Attention is all you need.
\newblock In {\em Advances in neural information processing systems}, pages 5998-6008.

\bibitem[Wan et al., 2023]{wan2023}
Zhen Wan, Fei Cheng, Zhuoyuan Mao, Qianying Liu, Haiyue Song, Jiwei Li, and Sadao Kurohashi.
\newblock 2023.
\newblock Gpt-re: In-context learning for relation extraction using large language models.
\newblock {\em arXiv preprint arXiv:2305.02105}.

\bibitem[Wang et al., 2023]{wang2023}
Fei Wang, Wenjie Mo, Yiwei Wang, Wenxuan Zhou, and Muhao Chen.
\newblock 2023.
\newblock A causal view of entity bias in (large) language models.
\newblock {\em arXiv preprint arXiv:2305.14695}.

\bibitem[Wang et al., 2022]{wang2022}
Yiwei Wang, Muhao Chen, Wenxuan Zhou, Yujun Cai, Yuxuan Liang, Dayiheng Liu, Baosong Yang, Juncheng Liu, and Bryan Hooi.
\newblock 2022.
\newblock Should we rely on entity mentions for relation extraction? debiasing relation extraction with counterfactual analysis.
\newblock {\em arXiv preprint arXiv:2205.03784}.

\bibitem[Wei et al., 2022]{wei2022}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.
\newblock 2022.
\newblock Chain-of-thought prompting elicits reasoning in large language models.
\newblock {\em Advances in Neural Information Processing Systems}, 35:24824-24837.

\bibitem[Wei et al., 2023]{wei2023}
Xiang Wei, Xingyu Cui, Ning Cheng, Xiaobin Wang, Xin Zhang, Shen Huang, Pengjun Xie, Jinan Xu, Yufeng Chen, Meishan Zhang, et al.
\newblock 2023.
\newblock Zero-shot information extraction via chatting with chatgpt.
\newblock {\em arXiv preprint arXiv:2302.10205}.

\bibitem[Xiang et al., 2020]{xiang2020}
Liuyu Xiang, Guiguang Ding, and Jungong Han.
\newblock 2020.
\newblock Learning from multiple experts: Self-paced knowledge distillation for long-tailed classification.
\newblock In {\em European Conference on Computer Vision}, pages 247-263. Springer.

\bibitem[Yuan et al., 2023]{yuan2023}
Chenhan Yuan, Qianqian Xie, and Sophia Ananiadou.
\newblock 2023.
\newblock Zero-shot temporal relation extraction with chatgpt.
\newblock {\em arXiv preprint arXiv:2304.05454}.

\bibitem[Zhang et al., 2022]{zhang2022}
Bowen Zhang, Daijun Ding, and Liwen Jing.
\newblock 2022.
\newblock How would stance detection techniques evolve after the launch of chatgpt?
\newblock {\em arXiv preprint arXiv:2212.14548}.

\bibitem[Zhang et al., 2017]{zhang2017}
Yuhao Zhang, Victor Zhong, Danqi Chen, Gabor Angeli, and Christopher D Manning.
\newblock 2017.
\newblock Position-aware attention and supervised data improve slot filling.
\newblock In {\em Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing}, pages 35-45.

\bibitem[Zhao et al., 2021]{zhao2021}
Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh.
\newblock 2021.
\newblock Calibrate before use: Improving few-shot performance of language models.
\newblock In {\em International Conference on Machine Learning}, pages 12697-12706. PMLR.

\bibitem[Zhong et al., 2023]{zhong2023}
Qihuang Zhong, Liang Ding, Juhua Liu, Bo Du, and Dacheng Tao.
\newblock 2023.
\newblock Can chatgpt understand too? a comparative study on chatgpt and fine-tuned bert.
\newblock {\em arXiv preprint arXiv:2302.10198}.

\bibitem[Zhou and Chen, 2022]{zhou2022}
Wenxuan Zhou and Muhao Chen.
\newblock 2022.
\newblock An improved baseline for sentence-level relation extraction.
\newblock pages 161-168.

\end{thebibliography}

\end{document}
=====END FILE=====