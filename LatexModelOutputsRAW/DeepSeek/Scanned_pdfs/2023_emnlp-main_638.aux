\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Related work}{3}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Experiments on a difficult single language pair translation task}{3}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Setup}{3}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Motivation}{4}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Modulating task difficulty}{4}{subsection.3.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Task difficulty experiment results on mT5 600M.}}{4}{table.1}\protected@file@percent }
\newlabel{tab:task-diff}{{1}{4}{Task difficulty experiment results on mT5 600M}{table.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Optimizing the curriculum}{4}{subsection.3.4}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Curriculum experiment results on mT5 600M.}}{4}{table.2}\protected@file@percent }
\newlabel{tab:curriculum}{{2}{4}{Curriculum experiment results on mT5 600M}{table.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Modulating scaffold substring}{4}{subsection.3.5}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Prefix+suffix experiment results on mT5 600M.}}{4}{table.3}\protected@file@percent }
\newlabel{tab:prefix-suffix}{{3}{4}{Prefix+suffix experiment results on mT5 600M}{table.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6}Matching the pretraining task}{4}{subsection.3.6}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Matching pretraining experiment results on mT5 600M with masking.}}{5}{table.4}\protected@file@percent }
\newlabel{tab:masking}{{4}{5}{Matching pretraining experiment results on mT5 600M with masking}{table.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7}Final results and comparison to state-of-the-art}{5}{subsection.3.7}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Main results on the tib2eng translation task for mT5. Values shown are test set BLEU scores. The difference shown is the improvement gained by using the input finetuning reformulations. The NLLB column is the test set BLEU score for the corresponding sized NLLB model.}}{5}{table.5}\protected@file@percent }
\newlabel{tab:main-results-tib2eng}{{5}{5}{Main results on the tib2eng translation task for mT5. Values shown are test set BLEU scores. The difference shown is the improvement gained by using the input finetuning reformulations. The NLLB column is the test set BLEU score for the corresponding sized NLLB model}{table.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experiments on a massively multilingual translation task}{5}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Setup}{5}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Designing task reformulations}{5}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Results}{6}{subsection.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Analysis on mT5's pretraining dataset and Flores200}{6}{subsection.4.4}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Results on the Flores200 translation task for mT5. Values shown are test set chrF++ scores. The NLLB column is the task performance of a corresponding size NLLB model. For the NLLB score, we use the \(200\mathrm  {xx - yy}\) chrF++ scores listed here.}}{6}{table.6}\protected@file@percent }
\newlabel{tab:flores200-results}{{6}{6}{Results on the Flores200 translation task for mT5. Values shown are test set chrF++ scores. The NLLB column is the task performance of a corresponding size NLLB model. For the NLLB score, we use the \(200\mathrm {xx - yy}\) chrF++ scores listed here}{table.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{6}{section.5}\protected@file@percent }
\bibcite{au2+2021}{1}
\bibcite{ben-david+2022}{2}
\bibcite{brown+2020}{3}
\bibcite{chen+2022}{4}
\bibcite{chowdhery+2022}{5}
\@writefile{toc}{\contentsline {section}{\numberline {6}Limitations}{7}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Ethics Statement}{7}{section.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8}Acknowledgements}{7}{section.8}\protected@file@percent }
\bibcite{chung+2022}{6}
\bibcite{clark+2020}{7}
\bibcite{devlin+2019}{8}
\bibcite{dong+2019}{9}
\bibcite{fadaee+2017}{10}
\bibcite{gao+2021}{11}
\bibcite{goyal+2021}{12}
\bibcite{gu+2018}{13}
\bibcite{gu+2023}{14}
\bibcite{guzman+2019}{15}
\bibcite{hambardzumyan+2021}{16}
\bibcite{han+2021}{17}
\bibcite{haviv+2021}{18}
\bibcite{he+2021}{19}
\bibcite{hoffmann+2022}{20}
\bibcite{iyer+2023}{21}
\bibcite{jiang+2020}{22}
\bibcite{kocmi+bojar2017}{23}
\bibcite{lample+conneau2019}{24}
\bibcite{lester+2021}{25}
\bibcite{lewis+2019}{26}
\bibcite{li+liang2021}{27}
\bibcite{liu+2021}{28}
\bibcite{liu+2019}{29}
\bibcite{loshchilov+hutter2019}{30}
\bibcite{lu+2022}{31}
\bibcite{min+2022}{32}
\bibcite{nllb-team+2022}{33}
\bibcite{papineni+2002}{34}
\bibcite{petroni+2019}{35}
\bibcite{platanios+2019}{36}
\bibcite{popovic2015}{37}
\bibcite{press+2023}{38}
\bibcite{qin+eisner2021}{39}
\bibcite{radford+2018}{40}
\bibcite{radford+2019}{41}
\bibcite{raffel+2020}{42}
\bibcite{ren+2019}{43}
\bibcite{schick+2020}{44}
\bibcite{schick+schutze2021}{45}
\bibcite{sennrich+2016}{46}
\bibcite{shin+2020}{47}
\bibcite{shoeybi+2020}{48}
\bibcite{shu+2019}{49}
\bibcite{sun+2019}{50}
\bibcite{taori+2023}{51}
\bibcite{touvron+2023}{52}
\bibcite{wallace+2021}{53}
\bibcite{wang+2022a}{54}
\bibcite{wang+2023a}{55}
\bibcite{wang+2023b}{56}
\bibcite{wang+2022b}{57}
\bibcite{wei+2022}{58}
\bibcite{wei+2023}{59}
\bibcite{workshop2023}{60}
\bibcite{xue+2021}{61}
\bibcite{yang+2020}{62}
\bibcite{zhang+2022a}{63}
\bibcite{zhang+2018}{64}
\bibcite{zhang+2019}{65}
\bibcite{zhang+2022b}{66}
\bibcite{zhong+2021a}{67}
\bibcite{zhong+2021b}{68}
\bibcite{zhou+2023a}{69}
\bibcite{zhou+2023b}{70}
\@writefile{toc}{\contentsline {section}{\numberline {A}Appendix}{11}{appendix.A}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}Flores200 in- and out- pretrain results}{11}{subsection.A.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Breakdown of model and setup performance over different splits of the Flores200 dataset. "In" refers to a language that was found in the mT5 pretraining dataset and "out" refers to a language that was not. "To Eng" and "From Eng" is referred to as xx- eng and eng- xx in some other papers, respectively. Notably, the proposed techniques improve "To Eng" performance up to 4.2 chrF++ and "From Eng" performance up to 9.4 chrF++, in the 600M case. We hypothesize this difference in improvement is due to the finetuning task including more English examples in the input, helping with downstream English translations as well as other language translations.}}{11}{table.7}\protected@file@percent }
\newlabel{tab:flores-breakdown}{{7}{11}{Breakdown of model and setup performance over different splits of the Flores200 dataset. "In" refers to a language that was found in the mT5 pretraining dataset and "out" refers to a language that was not. "To Eng" and "From Eng" is referred to as xx- eng and eng- xx in some other papers, respectively. Notably, the proposed techniques improve "To Eng" performance up to 4.2 chrF++ and "From Eng" performance up to 9.4 chrF++, in the 600M case. We hypothesize this difference in improvement is due to the finetuning task including more English examples in the input, helping with downstream English translations as well as other language translations}{table.7}{}}
\gdef \@abspage@last{11}
