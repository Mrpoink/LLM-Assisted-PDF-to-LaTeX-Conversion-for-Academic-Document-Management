\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{3}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Methodology}{4}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Network Architecture}{4}{subsection.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Our Regression Framework. Here, the two BERT models share same parameters, with "dim" representing the embedding dimensions of \(u\) and \(v\).}}{4}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:framework}{{1}{4}{Our Regression Framework. Here, the two BERT models share same parameters, with "dim" representing the embedding dimensions of \(u\) and \(v\)}{figure.caption.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Translated ReLU}{5}{subsection.3.2}\protected@file@percent }
\newlabel{eq:translated_relu}{{1}{5}{Translated ReLU}{equation.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Smooth K2 Loss}{5}{subsection.3.3}\protected@file@percent }
\newlabel{eq:smooth_k2}{{2}{6}{Smooth K2 Loss}{equation.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Comparison of Translated ReLU and Smooth K2 Loss, both with \(k = 2\), \(x_0 = 0.25\)}}{6}{figure.caption.2}\protected@file@percent }
\newlabel{fig:loss_comparison}{{2}{6}{Comparison of Translated ReLU and Smooth K2 Loss, both with \(k = 2\), \(x_0 = 0.25\)}{figure.caption.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experiments}{6}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}STS Performance Based on Traditional Discriminative Pre-trained Models}{6}{subsection.4.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Spearman's correlation scores for different methods across seven STS tasks. This table is partitioned to facilitate a single variable comparison. $\clubsuit $: results from (Reimers and Gurevych, 2019).}}{7}{table.caption.3}\protected@file@percent }
\newlabel{tab:sts_results}{{1}{7}{Spearman's correlation scores for different methods across seven STS tasks. This table is partitioned to facilitate a single variable comparison. $\clubsuit $: results from (Reimers and Gurevych, 2019)}{table.caption.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Hyperparameter configurations for our two loss functions when fine-tuning BERT and RoBERTa on the NLI dataset.}}{7}{table.caption.4}\protected@file@percent }
\newlabel{tab:hyperparams}{{2}{7}{Hyperparameter configurations for our two loss functions when fine-tuning BERT and RoBERTa on the NLI dataset}{table.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}STS Performance Based on Contrastive Learning Pre-Trained Models}{7}{subsection.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Our two-stage fine-tuning process for contrastive learning pre-trained models. In the figure, modules highlighted in red are active during training and undergo backpropagation, while modules in blue are frozen and do not carry out updates.}}{8}{figure.caption.5}\protected@file@percent }
\newlabel{fig:two_stage}{{3}{8}{Our two-stage fine-tuning process for contrastive learning pre-trained models. In the figure, modules highlighted in red are active during training and undergo backpropagation, while modules in blue are frozen and do not carry out updates}{figure.caption.5}{}}
\newlabel{eq:infonce}{{3}{8}{STS Performance Based on Contrastive Learning Pre-Trained Models}{equation.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Computational Resource Overhead}{8}{subsection.4.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Spearman's correlation coefficients of different methods across seven STS tasks. The "+Contrast" notation in the first column refers models further fine-tuned with contrastive learning. $\clubsuit $: results from (Gao et al., 2021). $\heartsuit $: results from (Jiang et al., 2022a). $\diamondsuit $: results from (Jiang et al., 2022b).}}{9}{table.caption.6}\protected@file@percent }
\newlabel{tab:contrastive_results}{{3}{9}{Spearman's correlation coefficients of different methods across seven STS tasks. The "+Contrast" notation in the first column refers models further fine-tuned with contrastive learning. $\clubsuit $: results from (Gao et al., 2021). $\heartsuit $: results from (Jiang et al., 2022a). $\diamondsuit $: results from (Jiang et al., 2022b)}{table.caption.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Computational demands of our method compared to SimCSE during the training phase. The third column, "Length," represents the maximum sequence length supported by each model (cutoff length).}}{9}{table.caption.7}\protected@file@percent }
\newlabel{tab:computational}{{4}{9}{Computational demands of our method compared to SimCSE during the training phase. The third column, "Length," represents the maximum sequence length supported by each model (cutoff length)}{table.caption.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Impact of Different Hyperparameter Settings}{9}{subsection.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Ablation Studies}{9}{subsection.4.5}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Average Spearman's correlation scores across seven STS tasks under different values of \(k\) and \(x_0\).}}{10}{table.caption.8}\protected@file@percent }
\newlabel{tab:hyperparam_results}{{5}{10}{Average Spearman's correlation scores across seven STS tasks under different values of \(k\) and \(x_0\)}{table.caption.8}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Average Spearman's correlation scores obtained by models on seven STS tasks with different concatenation methods in the final linear layer of our Siamese neural network architecture.}}{10}{table.caption.9}\protected@file@percent }
\newlabel{tab:ablation}{{6}{10}{Average Spearman's correlation scores obtained by models on seven STS tasks with different concatenation methods in the final linear layer of our Siamese neural network architecture}{table.caption.9}{}}
\bibcite{agirre2015}{1}
\bibcite{agirre2014}{2}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{11}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Limitations}{11}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {A}Data Filtering Method}{11}{appendix.A}\protected@file@percent }
\bibcite{agirre2016}{3}
\bibcite{agirre2012}{4}
\bibcite{agirre2013}{5}
\bibcite{bowman2015}{6}
\bibcite{cer2017}{7}
\bibcite{conneau2018}{8}
\bibcite{conneau2017}{9}
\bibcite{devlin2019}{10}
\bibcite{gao2021}{11}
\bibcite{gunther2023}{12}
\bibcite{jiang2023}{13}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Samples from the SICK-R training and test sets. In these samples, text pair duplication occurs. Thus, the corresponding training samples are removed from the fine-tuning corpus used in section 4.2.}}{12}{table.caption.10}\protected@file@percent }
\newlabel{tab:sick_examples}{{7}{12}{Samples from the SICK-R training and test sets. In these samples, text pair duplication occurs. Thus, the corresponding training samples are removed from the fine-tuning corpus used in section 4.2}{table.caption.10}{}}
\bibcite{jiang2022a}{14}
\bibcite{jiang2022b}{15}
\bibcite{li2023}{16}
\bibcite{liu2023}{17}
\bibcite{liu2019}{18}
\bibcite{marelli2014}{19}
\bibcite{muennighoff2023}{20}
\bibcite{nussbaum2024}{21}
\bibcite{oord2018}{22}
\bibcite{reimers2019}{23}
\bibcite{thakur2021}{24}
\bibcite{touvron2023}{25}
\bibcite{wang2022}{26}
\bibcite{williams2018}{27}
\bibcite{xiao2024}{28}
\bibcite{zhang2024}{29}
\bibcite{zhang2023}{30}
\bibcite{zhao2024}{31}
\gdef \@abspage@last{14}
