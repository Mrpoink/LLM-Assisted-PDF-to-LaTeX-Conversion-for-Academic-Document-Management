\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{chang2023speak}
\citation{mozes2023use}
\citation{eldan2023who}
\citation{ye2022learning}
\citation{blanco2024digital}
\citation{liu2024rethinking}
\citation{eldan2023who}
\citation{jang2023knowledge}
\citation{yao2024large}
\citation{rafailov2023direct}
\citation{hong2024intrinsic}
\citation{lee2024mechanistic}
\citation{meng2022locating}
\citation{pochinkov2024dissecting}
\citation{geva2021transformer}
\citation{touvron2023llama}
\citation{groeneveld2024olmo}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}\protected@file@percent }
\citation{liu2024rethinking}
\citation{chang2023speak}
\citation{mozes2023use}
\citation{jang2023knowledge}
\citation{yao2024large}
\citation{yao2023large}
\citation{rafailov2023direct}
\citation{zhao2024towards}
\citation{lee2024mechanistic2}
\citation{eldan2023who}
\citation{jang2023knowledge}
\citation{yao2024large}
\citation{rafailov2023direct}
\citation{chang2023do}
\citation{stoehr2024localizing}
\citation{lu2024eraser}
\citation{chen2023unlearn}
\citation{patil2024can}
\citation{meng2022locating}
\citation{geva2021transformer2}
\citation{sukhbaatar2015end}
\citation{geva2023dissecting}
\citation{geva2021transformer2}
\citation{geva2023dissecting}
\@writefile{toc}{\contentsline {section}{\numberline {2}Background and Related Work}{3}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Unlearning in Large Language Models}{3}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Knowledge Storage in Large Language Models}{3}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Patching Investigation}{3}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Hypothesis and Experimental Design}{3}{subsection.3.1}\protected@file@percent }
\citation{touvron2023llama}
\citation{groeneveld2024olmo}
\citation{rafailov2023direct}
\citation{yao2024large}
\citation{eldan2023who}
\citation{hong2024intrinsic}
\citation{soldaini2024dolma}
\citation{meng2022locating}
\citation{meng2023massediting}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Activation Patching and Parameters Restoration Experiments}{4}{subsection.3.2}\protected@file@percent }
\citation{rafailov2023direct}
\citation{zhao2024towards}
\citation{zhao2024towards}
\citation{yao2024large}
\citation{papineni2002bleu}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Results of KRS on LLaMA and OLMo under three activations patching or parameters restoration settings. We also included another setting that restores both attention and coefficients to compare the final outcomes.}}{5}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:1}{{1}{5}{Results of KRS on LLaMA and OLMo under three activations patching or parameters restoration settings. We also included another setting that restores both attention and coefficients to compare the final outcomes}{figure.caption.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Global Negative Effect of Fine-Tuning Unlearning}{5}{section.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Unlearning testing results on LLaMA and OLMo for each training epoch.}}{5}{figure.caption.3}\protected@file@percent }
\newlabel{fig:2}{{2}{5}{Unlearning testing results on LLaMA and OLMo for each training epoch}{figure.caption.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Discussion and Conclusion}{5}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Limitations}{5}{section.6}\protected@file@percent }
\bibstyle{plain}
\bibcite{blanco2024digital}{1}
\bibcite{chang2023speak}{2}
\bibcite{chang2023do}{3}
\bibcite{chen2023unlearn}{4}
\bibcite{eldan2023who}{5}
\bibcite{geva2023dissecting}{6}
\bibcite{geva2021transformer}{7}
\bibcite{geva2021transformer2}{8}
\bibcite{groeneveld2024olmo}{9}
\bibcite{hong2024intrinsic}{10}
\bibcite{jang2023knowledge}{11}
\bibcite{lee2024mechanistic}{12}
\bibcite{lee2024mechanistic2}{13}
\bibcite{liu2024rethinking}{14}
\bibcite{lu2024eraser}{15}
\bibcite{meng2022locating}{16}
\bibcite{meng2023massediting}{17}
\bibcite{mozes2023use}{18}
\bibcite{papineni2002bleu}{19}
\bibcite{patil2024can}{20}
\bibcite{pochinkov2024dissecting}{21}
\bibcite{rafailov2023direct}{22}
\bibcite{soldaini2024dolma}{23}
\bibcite{stoehr2024localizing}{24}
\bibcite{sukhbaatar2015end}{25}
\bibcite{touvron2023llama}{26}
\bibcite{yao2023large}{27}
\bibcite{yao2024large}{28}
\bibcite{ye2022learning}{29}
\bibcite{zhao2024towards}{30}
\@writefile{toc}{\contentsline {section}{\numberline {A}Details in Existing Unlearning Methods}{8}{appendix.A}\protected@file@percent }
\newlabel{sec:app_a}{{A}{8}{Details in Existing Unlearning Methods}{appendix.A}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B}Unlearning Experiment's Corpus}{8}{appendix.B}\protected@file@percent }
\newlabel{sec:app_b}{{B}{8}{Unlearning Experiment's Corpus}{appendix.B}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Results of KRS on LLaMA and OLMo under three activations patching or parameters restoration settings, isolating the effects of the two others when investigating each factor individually.}}{8}{figure.caption.6}\protected@file@percent }
\newlabel{fig:3}{{3}{8}{Results of KRS on LLaMA and OLMo under three activations patching or parameters restoration settings, isolating the effects of the two others when investigating each factor individually}{figure.caption.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {C}More Rigorous Patching Investigation}{8}{appendix.C}\protected@file@percent }
\newlabel{sec:app_c}{{C}{8}{More Rigorous Patching Investigation}{appendix.C}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Statistics of the training data for the unlearning experiments on LLaMA and OLMo}}{9}{table.caption.7}\protected@file@percent }
\newlabel{tab:1}{{1}{9}{Statistics of the training data for the unlearning experiments on LLaMA and OLMo}{table.caption.7}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Example extracted data from the Redpijama and Dolma pre-training datasets.}}{9}{table.caption.8}\protected@file@percent }
\newlabel{tab:2}{{2}{9}{Example extracted data from the Redpijama and Dolma pre-training datasets}{table.caption.8}{}}
\gdef \@abspage@last{9}
