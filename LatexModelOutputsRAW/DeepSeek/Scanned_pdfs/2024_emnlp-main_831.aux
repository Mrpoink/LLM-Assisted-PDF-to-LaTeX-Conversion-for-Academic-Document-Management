\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Number of parameters for different layers in models based on the Transformer.}}{2}{table.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:params}{{1}{2}{Number of parameters for different layers in models based on the Transformer}{table.caption.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{2}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Method}{2}{section.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The first row illustrates signal propagation in the original Linear Layer, while the second row illustrates propagation with the proposed SparseGradLinear layer.}}{3}{figure.caption.3}\protected@file@percent }
\newlabel{fig:signal_prop}{{1}{3}{The first row illustrates signal propagation in the original Linear Layer, while the second row illustrates propagation with the proposed SparseGradLinear layer}{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Preliminary Phase: Finding Transition Matrices}{3}{subsection.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Gradients on the 5-th BERT MLP: $U \frac  {\partial L}{\partial W^T} V^T$ (right) is more sparse than the original $\frac  {\partial L}{\partial W^T}$ (left).}}{3}{figure.caption.4}\protected@file@percent }
\newlabel{fig:gradients}{{2}{3}{Gradients on the 5-th BERT MLP: $U \frac {\partial L}{\partial W^T} V^T$ (right) is more sparse than the original $\frac {\partial L}{\partial W^T}$ (left)}{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Signal Propagation in SparseGradLinear Layer}{3}{subsection.3.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Correspondence of variables in Torch Autograd for a regular Linear layer and SparseGradLinear.}}{4}{table.caption.5}\protected@file@percent }
\newlabel{tab:autograd}{{2}{4}{Correspondence of variables in Torch Autograd for a regular Linear layer and SparseGradLinear}{table.caption.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Sparse-by-Dense Matrix Multiplication}{4}{subsection.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Strided structure of $\frac  {\partial L}{\partial Y}$ (left) and visualizations of $\%$ nonzero elements in $\frac  {\partial L}{\partial Y}$ throughout training (right).}}{4}{figure.caption.6}\protected@file@percent }
\newlabel{fig:strided}{{3}{4}{Strided structure of $\frac {\partial L}{\partial Y}$ (left) and visualizations of $\%$ nonzero elements in $\frac {\partial L}{\partial Y}$ throughout training (right)}{figure.caption.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Time and Memory Consumption per Training Iteration}{4}{section.4}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Training speed and memory requirements averaged on the GLUE benchmark. The last two rows of the Table report the results for the SparseGrad method with Sparse-by-Dense (SD) and Regular (Reg) matrix multiplication, respectively.}}{4}{table.caption.7}\protected@file@percent }
\newlabel{tab:train_speed}{{3}{4}{Training speed and memory requirements averaged on the GLUE benchmark. The last two rows of the Table report the results for the SparseGrad method with Sparse-by-Dense (SD) and Regular (Reg) matrix multiplication, respectively}{table.caption.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Experiments}{4}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Natural Language Understanding with BERT and RoBERTa}{5}{subsection.5.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Comparative results of RoBERTa $_{large}$ for 20-epoch task-specific fine-tuning.}}{5}{table.caption.8}\protected@file@percent }
\newlabel{tab:roberta_large}{{4}{5}{Comparative results of RoBERTa $_{large}$ for 20-epoch task-specific fine-tuning}{table.caption.8}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Average scores over the GLUE benchmark for BERT and RoBERTa $_{base}$ models.}}{5}{table.caption.9}\protected@file@percent }
\newlabel{tab:avg_glue}{{5}{5}{Average scores over the GLUE benchmark for BERT and RoBERTa $_{base}$ models}{table.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Conversations with LLaMa-2}{5}{subsection.5.2}\protected@file@percent }
\bibstyle{unsrt}
\bibdata{refs}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Comparative results for LLaMa-2 on the OpenAssistant-1 dataset.}}{6}{table.caption.10}\protected@file@percent }
\newlabel{tab:llama_results}{{6}{6}{Comparative results for LLaMa-2 on the OpenAssistant-1 dataset}{table.caption.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{6}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Acknowledgements}{6}{section.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8}Limitations}{6}{section.8}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {9}Ethics Statement}{6}{section.9}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {A}Appendix A}{6}{appendix.A}\protected@file@percent }
\newlabel{app:a}{{A}{6}{Appendix A}{appendix.A}{}}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces The training step execution speed, measured in steps per second (where a higher value indicates faster execution), is reported for the RoBERTa base model. The last two rows describe the SparseGradMethod with Sparse-by-Dense multiplication and with Regular matrix multiplication.}}{6}{table.caption.12}\protected@file@percent }
\newlabel{tab:speed_detail}{{7}{6}{The training step execution speed, measured in steps per second (where a higher value indicates faster execution), is reported for the RoBERTa base model. The last two rows describe the SparseGradMethod with Sparse-by-Dense multiplication and with Regular matrix multiplication}{table.caption.12}{}}
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces Peak memory measurement in MB for training loop for the model RoBERTa base.}}{7}{table.caption.13}\protected@file@percent }
\newlabel{tab:memory_detail}{{8}{7}{Peak memory measurement in MB for training loop for the model RoBERTa base}{table.caption.13}{}}
\@writefile{lot}{\contentsline {table}{\numberline {9}{\ignorespaces Comparative results of BERT model for 20-epoch task-specific fine-tuning.}}{7}{table.caption.14}\protected@file@percent }
\newlabel{tab:bert_detail}{{9}{7}{Comparative results of BERT model for 20-epoch task-specific fine-tuning}{table.caption.14}{}}
\@writefile{lot}{\contentsline {table}{\numberline {10}{\ignorespaces Comparative results of ROBERTA for 20-epoch task-specific fine-tuning.}}{7}{table.caption.15}\protected@file@percent }
\newlabel{tab:roberta_detail}{{10}{7}{Comparative results of ROBERTA for 20-epoch task-specific fine-tuning}{table.caption.15}{}}
\@writefile{lot}{\contentsline {table}{\numberline {11}{\ignorespaces GLUE score as a function of the weight gradient sparsity in BERT}}{7}{table.caption.16}\protected@file@percent }
\newlabel{tab:sparsity_bert}{{11}{7}{GLUE score as a function of the weight gradient sparsity in BERT}{table.caption.16}{}}
\@writefile{lot}{\contentsline {table}{\numberline {12}{\ignorespaces GLUE score as a function of the weight gradient sparsity in ROBERTA}}{7}{table.caption.17}\protected@file@percent }
\newlabel{tab:sparsity_roberta}{{12}{7}{GLUE score as a function of the weight gradient sparsity in ROBERTA}{table.caption.17}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B}Appendix B}{7}{appendix.B}\protected@file@percent }
\newlabel{app:b}{{B}{7}{Appendix B}{appendix.B}{}}
\@writefile{toc}{\contentsline {section}{\numberline {C}Appendix C}{7}{appendix.C}\protected@file@percent }
\newlabel{app:c}{{C}{7}{Appendix C}{appendix.C}{}}
\@writefile{toc}{\contentsline {section}{\numberline {D}Appendix D}{7}{appendix.D}\protected@file@percent }
\newlabel{app:d}{{D}{7}{Appendix D}{appendix.D}{}}
\@writefile{lot}{\contentsline {table}{\numberline {13}{\ignorespaces Best training parameters on GLUE benchmark for BERT model.}}{7}{table.caption.18}\protected@file@percent }
\newlabel{tab:params_bert}{{13}{7}{Best training parameters on GLUE benchmark for BERT model}{table.caption.18}{}}
\@writefile{toc}{\contentsline {section}{\numberline {E}Appendix E}{7}{appendix.E}\protected@file@percent }
\newlabel{app:e}{{E}{7}{Appendix E}{appendix.E}{}}
\@writefile{lot}{\contentsline {table}{\numberline {14}{\ignorespaces Best training parameters on GLUE benchmark for RoBERTa model.}}{8}{table.caption.19}\protected@file@percent }
\newlabel{tab:params_roberta}{{14}{8}{Best training parameters on GLUE benchmark for RoBERTa model}{table.caption.19}{}}
\@writefile{lot}{\contentsline {table}{\numberline {15}{\ignorespaces Best training parameters on GLUE benchmark for RoBERTa-large model.}}{8}{table.caption.20}\protected@file@percent }
\newlabel{tab:params_roberta_large}{{15}{8}{Best training parameters on GLUE benchmark for RoBERTa-large model}{table.caption.20}{}}
\gdef \@abspage@last{8}
