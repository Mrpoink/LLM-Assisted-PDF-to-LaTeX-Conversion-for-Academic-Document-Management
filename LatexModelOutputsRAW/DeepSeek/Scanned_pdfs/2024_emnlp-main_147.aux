\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{Touvron2023}
\citation{OpenAI2023}
\citation{Cobbe2021}
\citation{Saparov2023}
\citation{Schlegel2022b,Madusanka2023}
\citation{AlKhamissi2022}
\citation{He2023}
\citation{Brown2020}
\citation{Vaswani2017}
\citation{Devlin2019}
\citation{Gururangan2018,Schlegel2022a}
\citation{Dua2019}
\citation{Huang2023b}
\citation{Deng2024}
\citation{Chiang2024}
\citation{Perez2023}
\citation{Yang2018a,Welbl2018,Inoue2020}
\citation{Lewis2020}
\citation{Kintsch1988}
\citation{Min2019a}
\citation{Bowman2022}
\citation{Sakarvadia2023,Liu2023,Yang2024}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{2}{section.1}\protected@file@percent }
\citation{Schlegel2020}
\citation{Min2019a}
\citation{Yang2018b}
\citation{Min2019a,Trivedi2020}
\citation{Jiang2019}
\citation{Min2019b,Perez2020,Ding2021}
\citation{Tang2021}
\citation{Gardner2020}
\citation{Jiang2019}
\citation{Ding2021}
\citation{Sun2023,Li2024}
\citation{Huang2023a}
\citation{Chomsky1965}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Our proposed method evaluates the multi-hop reasoning capabilities of Large Language Models by adding seemingly plausible, yet ultimately wrong alternate reasoning paths, impacting the reasoning performance of state-of-the-art LLMs such as GPT-4.}}{3}{figure.1}\protected@file@percent }
\newlabel{fig:method_overview}{{1}{3}{Our proposed method evaluates the multi-hop reasoning capabilities of Large Language Models by adding seemingly plausible, yet ultimately wrong alternate reasoning paths, impacting the reasoning performance of state-of-the-art LLMs such as GPT-4}{figure.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {II}Related Work}{3}{section.2}\protected@file@percent }
\citation{Tang2021}
\citation{Schlegel2021}
\citation{Qi2020}
\citation{Liu2019}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Example of a decomposed multi-hop question.}}{4}{figure.2}\protected@file@percent }
\newlabel{fig:decompose}{{2}{4}{Example of a decomposed multi-hop question}{figure.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Methodology}{4}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-A}}Acquiring the main entity}{4}{subsection.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Instantiation of our proposed method. With "arena" as main entity of sub-question 1, we extract "home" to be replaced with "playoff". Then, we use the modified sequence with the original sub-question 2 (masking the answer "Androscoggin Bank Colisee") as prompt to GPT-4 to generate the distractor paragraphs 1 and 2. The distractor paragraphs generated have "Maple Leaf Arena" as the bridging entity in the false reasoning chain which leads to the wrong answer "4500 spectators".}}{4}{figure.3}\protected@file@percent }
\newlabel{fig:example}{{3}{4}{Instantiation of our proposed method. With "arena" as main entity of sub-question 1, we extract "home" to be replaced with "playoff". Then, we use the modified sequence with the original sub-question 2 (masking the answer "Androscoggin Bank Colisee") as prompt to GPT-4 to generate the distractor paragraphs 1 and 2. The distractor paragraphs generated have "Maple Leaf Arena" as the bridging entity in the false reasoning chain which leads to the wrong answer "4500 spectators"}{figure.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-B}}Extracting the details}{4}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-C}}Creating the distractor paragraphs}{4}{subsection.3.3}\protected@file@percent }
\citation{Reimers2019}
\citation{Touvron2023}
\citation{Wei2023}
\citation{Yang2018b}
\citation{Jiang2019}
\citation{Trivedi2020}
\citation{Chiang2024}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Experiment Setup}{5}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {IV-A}}Do LLMs suffer from the same flaws as finetuned models?}{5}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {IV-B}}Do LLMs get distracted by seemingly plausible alternate reasoning paths?}{5}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {IV-C}}What are the effects of the different parameters?}{5}{subsection.4.3}\protected@file@percent }
\citation{Tang2021}
\citation{Tang2021}
\citation{Tang2021}
\citation{Jiang2019}
\citation{Zellers2018,Zellers2019}
\@writefile{toc}{\contentsline {section}{\numberline {V}Experiment Results}{6}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {V-A}}Do LLMs suffer from the same flaws as fine-tuned models?}{6}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {V-A}1}I. Setting up the baseline}{6}{subsubsection.5.1.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces Comparing normal and chain-of-thought prompts using LLama-2-13B as baseline.}}{6}{table.1}\protected@file@percent }
\newlabel{tab:baseline}{{I}{6}{Comparing normal and chain-of-thought prompts using LLama-2-13B as baseline}{table.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {V-A}2}II. Reasoning shortcuts using SubQA}{6}{subsubsection.5.1.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {II}{\ignorespaces Results of LLama-2-13B on SubQA dataset}}{6}{table.2}\protected@file@percent }
\newlabel{tab:subqa_results}{{II}{6}{Results of LLama-2-13B on SubQA dataset}{table.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {III}{\ignorespaces Breakdown of the results on running SubQA}}{6}{table.3}\protected@file@percent }
\newlabel{tab:subqa_breakdown}{{III}{6}{Breakdown of the results on running SubQA}{table.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {V-A}3}III. Reasoning shortcuts in DiRe}{6}{subsubsection.5.1.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {IV}{\ignorespaces Llama-2-13b performance on DiRe when using a normal (non-CoT) prompt and priming with fewshot examples.}}{6}{table.4}\protected@file@percent }
\newlabel{tab:dire}{{IV}{6}{Llama-2-13b performance on DiRe when using a normal (non-CoT) prompt and priming with fewshot examples}{table.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {V-A}4}IV. Reasoning failures when presented with distracting paragraphs from AddDoc}{6}{subsubsection.5.1.4}\protected@file@percent }
\citation{Jiang2019}
\@writefile{lot}{\contentsline {table}{\numberline {V}{\ignorespaces Performance of LLMs on AddDoc adversarial examples.}}{7}{table.5}\protected@file@percent }
\newlabel{tab:adddoc}{{V}{7}{Performance of LLMs on AddDoc adversarial examples}{table.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {V-B}}Do LLMs get distracted when faced with seemingly plausible alternatives?}{7}{subsection.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {V-C}}Analysing the effects of different parameters}{7}{subsection.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {V-C}1}Count of distractor paragraphs}{7}{subsubsection.5.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {V-C}2}Are the paragraphs related?}{7}{subsubsection.5.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {V-C}3}Modified type}{7}{subsubsection.5.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {V-C}4}Are the paragraphs unrelated and only belong to the 2nd subquestion?}{7}{subsubsection.5.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {VI}Conclusion}{7}{section.6}\protected@file@percent }
\citation{Tang2021}
\citation{Min2019b}
\citation{Perez2020}
\@writefile{lot}{\contentsline {table}{\numberline {VI}{\ignorespaces Results of Llama-2-13B, Mixtral-8x7B-Instruct-v0.1, Llama-2-70B, GPT-3.5 and longformer (finetuned on the training set) on the original HotpotQA dev set (ori) and our adversarially constructed examples (adv). All the tests for the LLMs are done in the few-shot chain of prompt setting. EM and F1 Performance Scores are reported. F1 scores are further broken down by (left to right): the number of "fake" paragraphs; whether "fake" paragraphs are related; the type of entity modified, if adversarial paragraphs are unrelated, and if both the adversarial paragraphs are generated from the second sub-question of two different fake sub-question pair.}}{8}{table.6}\protected@file@percent }
\newlabel{tab:main_results}{{VI}{8}{Results of Llama-2-13B, Mixtral-8x7B-Instruct-v0.1, Llama-2-70B, GPT-3.5 and longformer (finetuned on the training set) on the original HotpotQA dev set (ori) and our adversarially constructed examples (adv). All the tests for the LLMs are done in the few-shot chain of prompt setting. EM and F1 Performance Scores are reported. F1 scores are further broken down by (left to right): the number of "fake" paragraphs; whether "fake" paragraphs are related; the type of entity modified, if adversarial paragraphs are unrelated, and if both the adversarial paragraphs are generated from the second sub-question of two different fake sub-question pair}{table.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {VII}Limitations}{8}{section.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{Appendix}{8}{section*.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A}System Prompt for Q/A task}{8}{subsection.Appendix.A.1}\protected@file@percent }
\newlabel{app:prompt_qa}{{A}{8}{System Prompt for Q/A task}{subsection.Appendix.A.1}{}}
\citation{DeMarneffe2014}
\@writefile{toc}{\contentsline {subsection}{\numberline {B}System Prompt for creating fake paragraphs}{9}{subsection.Appendix.A.2}\protected@file@percent }
\newlabel{app:prompt_fake}{{B}{9}{System Prompt for creating fake paragraphs}{subsection.Appendix.A.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C}System prompt for creating fake named entities through GPT-4}{9}{subsection.Appendix.A.3}\protected@file@percent }
\newlabel{app:prompt_ner}{{C}{9}{System prompt for creating fake named entities through GPT-4}{subsection.Appendix.A.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {D}Dependency type definitions}{9}{subsection.Appendix.A.4}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {VII}{\ignorespaces Definitions of dependency relations.}}{9}{table.7}\protected@file@percent }
\newlabel{tab:dependencies}{{VII}{9}{Definitions of dependency relations}{table.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {E}Reproducibility}{9}{subsection.Appendix.A.5}\protected@file@percent }
\newlabel{app:reproducibility}{{E}{9}{Reproducibility}{subsection.Appendix.A.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {F}User study to verify adversarial paragraphs}{9}{subsection.Appendix.A.6}\protected@file@percent }
\newlabel{app:user_study}{{F}{9}{User study to verify adversarial paragraphs}{subsection.Appendix.A.6}{}}
\citation{Jiang2019}
\citation{Shi2023}
\citation{Wang2023}
\@writefile{lot}{\contentsline {table}{\numberline {VIII}{\ignorespaces The three different metrics for accuracy}}{10}{table.8}\protected@file@percent }
\newlabel{tab:user_accuracy}{{VIII}{10}{The three different metrics for accuracy}{table.8}{}}
\@writefile{lot}{\contentsline {table}{\numberline {IX}{\ignorespaces Count of questions marked as contradictory at different confidence levels.}}{10}{table.9}\protected@file@percent }
\newlabel{tab:contradiction}{{IX}{10}{Count of questions marked as contradictory at different confidence levels}{table.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {G}Performance of SOTA LLM}{10}{subsection.Appendix.A.7}\protected@file@percent }
\newlabel{app:gpt4}{{G}{10}{Performance of SOTA LLM}{subsection.Appendix.A.7}{}}
\@writefile{lot}{\contentsline {table}{\numberline {X}{\ignorespaces F1 scores of GPT-4 for 2 and 4 fake paragraphs.}}{10}{table.10}\protected@file@percent }
\newlabel{tab:gpt4_f1}{{X}{10}{F1 scores of GPT-4 for 2 and 4 fake paragraphs}{table.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {H}Do existing techniques make models more robust?}{10}{subsection.Appendix.A.8}\protected@file@percent }
\newlabel{app:prompting}{{H}{10}{Do existing techniques make models more robust?}{subsection.Appendix.A.8}{}}
\bibstyle{IEEEtran}
\bibdata{refs}
\@writefile{lot}{\contentsline {table}{\numberline {XI}{\ignorespaces Effect of self-consistency on F1 score}}{11}{table.11}\protected@file@percent }
\newlabel{tab:robustness}{{XI}{11}{Effect of self-consistency on F1 score}{table.11}{}}
\gdef \@abspage@last{11}
