=====FILE: main.tex=====
\documentclass[11pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage{microtype}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{array}
\usepackage{url}
\usepackage[hidelinks]{hyperref}

\title{LLM-based Code-Switched Text Generation for\Grammatical Error Correction}

\author{
Tom Potter\thanks{Work completed whilst at King's College London.}\
University of Manchester\
\texttt{thomas.\ potterGpostgrad.\ manchester.\ ac.\ uk}
\and
Zheng Yuan\
King's College London\
\texttt{zheng.\ yuanGkcl.\ ac.\ uk}
}

\date{}

\begin{document}
\maketitle

\begin{abstract}
With the rise of globalisation, code-switching
(CSW) has become a ubiquitous part of mul-
tilingual conversation, posing new challenges
for natural language processing (NLP), espe-
cially in Grammatical Error Correction (GEC).
This work explores the complexities of apply-
ing GEC systems to CSW texts. Our objectives
include evaluating the performance of state-
of-the-art GEC systems on an authentic CSW
dataset from English as a Second Language
(ESL) learners, exploring synthetic data gener-
ation as a solution to data scarcity, and develop-
ing a model capable of correcting grammatical
errors in monolingual and CSW texts. We gen-
erated synthetic CSW GEC data, resulting in
one of the first substantial datasets for this task,
and showed that a model trained on this data is
capable of significant improvements over exist-
ing systems. This work targets ESL learners,
aiming to provide educational technologies that
aid in the development of their English gram-
matical correctness without constraining their
natural multilingualism.
\end{abstract}

\section{Introduction}

Code-switching (CSW), the practice of fluidly al-
ternating between two or more languages in conver-
sation, has become commonplace in recent years.
This linguistic phenomenon, emerging as a natu-
ral consequence of multilingualism, is now widely
accepted in social and professional settings (Yow
et a1., 2018). Many works have highlighted the
utility and cultural importance of CSW in general
conversation (Beatty-Martinez et al., 2020; F albo
and LaCroix, 2021). Further research indicates that
these advantages extend to language learning, with
CSW offering many pedagogical benefits. These
include increasing students' access to content and
improving their confidence. Nguyen et al. (2022)
discuss the mechanisms for this, where students
use a familiar language to grasp foreign, complex
concepts. CSW can also serve as a scaffolding
tool, helping to bridge gaps in a student's com-
prehension of a language and enabling them to
build upon existing knowledge. These benefits
reduce the barriers between a student and their
target language and help promote a learning en-
vironment conducive with active exploration and
deeper understanding. Therefore, it is essential
that English as a Second Language (ESL) learn-
ers are not penalised for expressing their cultural
identity through CSW. Grammatical error correc-
tion (GEC) is the task of automatically detecting
and correcting errors in text. Research on GEC for
CSW text remained largely unexplored. Chan et al.
(2024) were the flrst to demonstrate that exposing a
sequence-tagging GEC model to CSW text during
the training process improves performance com-
pared to a monolingual system. However, further
work is essential to ensure language technology
is inclusive and reflective of real-world linguistic
practices. Figure~\ref{fig:fig1} shows two examples of CSW
from our target population with their grammatical
corrections.\footnote{The definition of CSW is a subject of ongoing debate.
Throughout this work, we use the term CSW to refer specifi-
cally to the type of language mixing exhibited by ESL learners.}

Despite significant advancements in GEC in re-
cent yea''rs, a gap persists in addressing CSW texts,
with monolingual GEC datasets labelling CSW as
a type of error (Nguyen et a1.,2022). There are
several reasons for this, the most prominent being
the scarcity of high-quality training data, a prob-
lem that plagues monolingual GEC systems. The
unique linguistic features of CSW, including its
variable syntax, semantics and pragmatics, add
additional complexity to this task. Monolingual
seq2seq GEC models, e.g.\ T5 (Rothe et a1.,2021),
struggle with CSW text as they fail to represent
the non-English inputs, resulting in their inability
to output the CSW text. On the other hand, multi-
lingual seq2seq models and edit-based GEC mod-
els like GECToR (Omelianchuk et al., 2020) can
handle CSW text but struggle with the ambiguity
present at language switching points. This ambi-
guity challenges the models' ability to accurately
correct the text.

This paper aims to bridge this gap. Firstly, to ad-
dress the data scarcity issue, we propose a method
for generating high-quality synthetic CSW GEC
data, using which we produce, to our knowledge,
one of the first substantial datasets labelled for this
task.\footnote{This dataset is available on GitHub.}
Secondly, we train a token classification-
style GEC system, tailored to correct errors in texts
produced by ESL leamers. This demographic is
significant for our study as they not only present
consistent CSW pattems but also stand to benefit
greatly from a GEC system capable of handling
CSW text.

\begin{figure}[t]
\centering
\fbox{\parbox{0.95\linewidth}{\centering IMAGE NOT PROVIDED}}
\vspace{0.5em}

\begin{quote}\small
Example l.According to the test, [lacks in me -+ my
shortcomingsl are HLT and J'iAf{.

Example 2: When we [call -+ say] 7.y :, = d * l.r,
do we actually mean a glove compartment in English?
\end{quote}

\caption{Examples of GEC in ESL leamer language.}
\label{fig:fig1}
\end{figure}

\section{Data}

\subsection{Genuine CSW GEC Dataset}

One of the only datasets labelled for GEC
which does not remove CSW text, is the Lang-
8 dataset (Mizumoto et al., 2013), sourced from
the Lang-S language learning platform. This
dataset, when filtered to contain entries where
CSW is present, offers a foundation of authen-
tic data, comprises 5,875 pairs of ungrammati-
cal and corrected sentences across 6 CSW lan-
guage pairs : English-Japanese (81.9 Vo), English-
Korean (l3.0%o), English-Traditional Chinese
(3.4Vo), English-Russian (1.2%), English-Thai
(0.5Vo) and English-Arabic (0.1%).

The crowd-sourced nature of Lang-8 required
manual validation to ensure accuracy. We tasked
an annotator with the responsibility of verifying
the original corrections in the dataset, as well as
combing for missed errors, incorect annotations
and over-annotations.

\subsection{Synthetic CSW GEC Data Generation}

Given the small size of the available CSW GEC
dataset, we introduced a 2-step approach to syn-
thetic CSW GEC data generation. First, we gener-
ated grammatically correct CSW sentences. This
is followed by the introduction of errors.

\subsubsection{Step 1.: CSW Text Generation}

Three different synthetic data generation tech-
niques have been explored to generate CSW data.

\paragraph{Tfanslation-based CSW Text Generation}
required a monolingual corpus, a machine transla-
tion (MT) algorithm, and a sentence parser. To
generate a CSW utterance, we used the Stanford
Parser v4.5.4\footnote{This can be downloaded from the CoreNLP website.}
(Manning etal.,2014) to build a syn-
tactic parse tree. We then randomly selected and
translated a subtree using the ArgosTranslate MT
package\footnote{we used ArgosTranslate v1.8.0, available on GitHub}
(Finlay, 2023;Kleinet a1., 2017). This
method generates plausible CSW text. However,
performance is dependent on the strength of the
parsing and translation algorithms; and the style of
language within the corpus. To approximate the
style of our authentic CSW text, we used corrected
monolingual sentences from the Lang-S corpus.

\paragraph{Parallel Corpus-based CSW Text Generation}
avoids the need for a translation algorithm. In-
stead, we used the same Stanford Parser, this time
with Spanish, French and German configurations;
and the AWESOME word-alignment models\footnote{The authors shared their model on HuggingFace.}
(Dou
and Neubig, 2021), to identify parts of parallel cor-
pora labelled for MT with similar syntactic struc-
ture. For this method, we use the Europarl corpus
(Koehn et a1.,2012) due to the grammatical quality
of its English component. Under the Equivalence
Constraint Theory (Rizvi et aL.,2021), these areas
are where CSW is likely to take place. We, there-
fore, randomly chose overlapping subtrees as can-
didates for injection of non-English text. Although
this method does not require MT, it is reliant on
performant word-alignment and parsing systems;
these are rare for many languages.

\paragraph{LLM Prompting-based CSW Text Generation}
The other methods of generating CSW text rely on
injecting a second language into existing monolin-
gual corpora. Hence, they are not able to recreate
one of the main switching styles shown by ESL
learners - CSW as a genuine pragmatic strategy. A
common reason for this style of switching is when
quoting another language. It is difficult to recreate
this style using a monolingual foundation as sen-
tences like The Japanese word for `dog'' is `)i''
seldom appear in authentic monolingual colpora.

To generate diverse CSW texts without relying
on existing cofpora or inaccurate alignment algo-
rithms, we leveraged the strong general knowl-
edge of Large Language Models (LLMs). We
demonstrated that OpenAI's GPT-3.5 (Brown et al.,
2020) can create high-quality CSW sentences when
shown examples of authentic utterances. Along
with genuine CSW texts, we supplied a one-shot
example of how to use the switching styles of an
existing CSW text to generate a new sentence.\footnote{The full prompt can be seen in Appendix A.}

\paragraph{Comparison of Synthetic CSW Text}
We used
several CSW metrics to quantify the qualities of
CSW texts: Code Mixing Index (CMD (Gam-
b"ack and Das, 2016), Multilingual lndex (M-Index)
(Barnett et al., 2000), Probability of Switching (I-
Index) (Guzm6n et a1.,2017), Burstiness (Goh and
Barab6si, 2008), and Complexity Factor (CF1-3)
(Ghosh et a1.,2017). Table~\ref{tab:table1} shows the value of
each metric for our genuine CSW dataset, as well
as for these 3 synthetic CSW datasets. We can see
that the LLM prompting-based dataset was superior
in its similarity to the authentic CSW data. Using
this method, we generated a corpus of '73,293 utter-
ances covering over 20 English language pairs, in-
cluding the 6 language pairs in the original dataset.\footnote{The LLM does not always generate the language
we ask for. However, these sentences are still included.
dataset categorised under their actual language pair.}

\begin{table}[t]
\centering
\caption{Quantitative Description of the Genuine and Generated CSW Datasets Using Various CSW Metrics}
\label{tab:table1}
\begin{tabular}{lcccc}
\toprule
Metric & Genuine CSW & LLM CSW & Tianslation CSW & Corpus CSW \
\midrule
CMI & [ILLEGIBLE] & [ILLEGIBLE] & [ILLEGIBLE] & [ILLEGIBLE] \
M-Index & 0.007 & 0.004 & 0.015 & 0.006 \
I-Index & 0.2t & 0.21 & 0.30 & 0.20 \
Burstiness & -0.07 & -0.04 & 0,03 & -0.1 I \
CFl & 6.38 & 5.82 & 17.t3 & 2.55 \
CF2 & 19.7'7 & 19.03 & 31.i1 & t6.04 \
CF3 & 18.34 & t] .61 & 30.05 & 14.20 \
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Step 2: Synthetic Error Generation}

Several works have shown the effectiveness of
rule-based error injection for GEC data genera-
tion. Many use the PlE-synthetic dataset (Awasthi
et al., 2019), a perturbed version of the lBW cor-
pus (Chelba et a1.,2073). For each sentence, the
authors introduce between 0 and 4 errors of ran-
dom type. We extended this work by introducing
a new subset of error types that are not only more
common in ESL leamers, but also are areas where
the SOTA performance collapses when faced with
CSW text: noun, pronoun, word order, determiner,
and punctuation errors.\footnote{Error type analysis is presented in Appendix B.}

To increase the diversity of errors, we adopted
a second style of error injection, Backtransla-
tion (Stahlberg and Kumar, 202I). By swapping
the source and target sentences of a monolingual
dataset, we trained a GECToR-based system to in-
duce errors in our synthetic CSW sentences.

Using these methods, we created two datasets:
Syn-CSW PIE and Syn-CSW Rev-GECToR. After
removing pairs containing no injected errors, we
are left with 70,180 and 18,159 sentences each.

\section{CSW GEC Systems}

For our GEC system targeting CSW texts, we chose
a GECToR model (Omelianchuk et al., 2020), with
a RoBERTa-base foundation, due to its proven effi-
cacy with limited training data and stronger perfor-
mance on CSW texts compared to seq2seq models.
We added a new CSW class to the error detection
head, adding the ability to detect CSW tokens.

Following Tarnavskyi et al. (2022), we used a
3-stage training schedule. In the first, we used
the same distilled 1BW corpus, and added all our
synthetic CSW GEC data. For the second, we
used several GEC datasets: NUCLE (Dahlmeier
et a1.,2023), FCE (Yannakoudakis et al.,20ll),
W&I Locness (Bryant et a1.,2019), Lang-8 and
our 2 synthetic CSW datasets. As our genuine
CSW dataset is a subset of the Lang-8 corpus, we
checked and removed any duplicates. Following
previous works, we flnished training using the W&I
Locness dataset due to its superior quality. In this
final stage, we added a sampled subset of our syn-
thetic CSW sentences and 907o of our authentic
CSW data, ensuring exposure to synthetic and gen-
uine CSW text. At each stage, we reserved 57o of
the data for validation.\footnote{An e*act breakdown of contributions by each dataset is
given in Appendix C.}
Finally, we tuned inference
parameters using a grid-search to optimise the tr'6.b
on the final validation set. By beginning with pre-
training on large amounts of lower-quality data in
the early stages, this multi-stage learning process
allows the model to first build a robust GEC foun-
dation before refining it with high quality data in
the latter stages. This approach allows the model to
learn incrementally, reducing the risk of the model
being overwhelmed by the complexity of the task
from the outset.

\section{Results and Analysis}

\subsection{BaselineComparisons}

We compared our model against two well-
established systems: a RoBERTa-base GECToR
model (Omelianchuk et a1.,2020), with near SOTA
performance on the BEA-2019 test set (Bryant
etal.,2019), and a seq2seq T5 model (Rothe et a1.,
2021). To assess these models, we evaluated their
performance on the BEA-2019 test set and the re-
maining l07o of our authentic CSW data. The ER-
RANT (Bryant etal.,2017) GEC evaluation resulrs,
as outlined in Table~\ref{tab:table2}, demonstrate a clear degra-
dation in performance when these two systems are
applied to CSW texrs. The ERRANT toolkit de-
tects and classifles edits between source and target
sentence pairs into predefined error categories. It
enables the comparison of a proposed set of edits
with a reference set, providing a way of calculating
metrics, such as precision and recall, across these
categories.

\subsection{Detailed Model Performance}

The progression of our model throughout training
provided insights into its evolving capabilities and
effectiveness of our synthetic data. We monitored
several metrics, including the ERRANT precision,
recall and -F6 5 score, for the BEA-2019 test set and
the remaining unused 10Vo of our genuine CSW
dataset. These metrics, as displayed in Table~\ref{tab:table2},
indicate a steady improvement in the ability to
handle CSW texts. Notably, the performance on
the CSW dataset shows a significant leap in the
final stages, where the contribution of our synthetic
dataset is largest. This improvement in CSW text
handling did slightly compromise the model's per-
formance on monolingual GEC tasks, as seen on
the BEA-2019 test set. This suggests a rrade-off in-
herent in specialising the model for CSW contexts.
However, our model remains competitive amongst
SOTA monolingual GEC systems of its size.

Three illustrative examples of our model's cor-
rections, taken from the CSW test set, can be seen
in Figure~\ref{fig:fig2}. The first example demonstrates a case
where the model has correctly identified all of the
changes required, including the incorrect capitalisa-
tion of a word, a missing word, and some missing
punctuation. The second example shows a `near
miss''; here, the model has correctly identified the
majority of the changes required but dropped the
`I'' whilst rearranging the start of the sentence. Fi-
nally, the third example presents a scenario where
the model has fallen slightly short, failing to recog-
nise the need for `were'' instead of `was'' in this
hypothetical context.

\subsection{Inference Theaking and Error Thresholds}

The inference tweaking phase was crucial in tun-
ing the balance between precision and recall. The
changes made here, particularly lowering the mini-
mum error thresholds before the model makes an
edit, indicated a clear attempt to force the model
to make more corrections.\footnote{lmplementation details are presented in Appendix D.}
While this slightly
lowered precision on monolingual errors, it signifl-
cantly enhanced the performance on CSW text.

To determine that the improved performance of
our proposed model was not entirely due to the
different inference configuration, we conducted a
similar grid search for the existing GECToR model.
However, instead of using the Stage 3 validation
dataset, as we did with our model, we used the
CSW test set directly. The highest Fs 5 achieved by
the baseline model was 56.46, providing evidence
that our proposed model beats all inference con-
figurations of the previous GECToR system when
applied to CSW texts.

\subsection{Synthetic Data Impact}

The synthetic CSW text and error injection meth-
ods were central to this project. The resemblance
of our synthetic text to real ESL learner data, as
shown by the similarity metrics in Table~\ref{tab:table1}, is a
testament to the effectiveness of our chosen gen-
eration method. The improvements in Fs 5 scores
provide further evidence of this.

Our extended PIE-synthetic dataset aimed to in-
troduce four error types common in ESL students:
noun, pronoun, punctuation and word errors. When
compared to the monolingual GECToR, our model
is stronger in all of these areas.\footnote{lrEnor type analysis of our model is given in Appendix E.}
This provides
strong evidence that the targeted approach to error
injection was successful in boosting the model's
ability in these areas.

\begin{table}[t]
\centering
\caption{ERRANT-based Precision, Recall and Fs.s Scores of Baselines and Our Model Throughout Training}
\label{tab:table2}
\begin{tabular}{lcccccc}
\toprule
Model & \multicolumn{3}{c}{BEA-2019 Test} & \multicolumn{3}{c}{Genuine CSW} \
& P & R & Fo.s & P & R & Fo.s \
\midrule
GECToR & 7'7.88 & s3.07 & 7t,22 & '71.t4 & 27.08 & 53.67 \
T5-Small & 62.03 & 47.19 & 58.34 & 1 1.70 & 24.98 & 13.09 \
\midrule
Stage 1 & 67.23 & 53.88 & 64.05 & 66.1 5 & 26.04 & 50.57 \
Stage 2 & 72.64 & 51.'73 & 67.20 & 65.4t & 29.93 & s2.8'1 \
Stage 3 & 74.32 & 53.40 & 68.92 & 84.66 & 22.92 & 55.02 \
Inference Tweaks & 69.01 & 58.40 & 66.59 & 76.02 & 38.67 & 63.11 \
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[t]
\centering
\fbox{\parbox{0.95\linewidth}{\centering IMAGE NOT PROVIDED}}
\vspace{0.5em}

\begin{quote}\small
GoM Correction 1: We have many [New -+ new] words for [0 -+ the] unemployed[0 -r :] `` q En E't0 -+ ,l ''E
f''tO -+,1 'sI=''.

Proposed Correction 7: We have many [New -+ new] words for [0 -+ thel unemployed[fl -+ :] `` ol 4 uI ''t0 -+ ,l 'E+ t0 -+,1 ''EI=''.

Gold Conection 2: U and my girlfriend -+ My girlfriend and il looked [0 -r at a] picture called ``ffi./Fffa)4&''
(Immaculate Conception).

Proposed Correction 2: U and my girlfriend -+ My girlfriend andl looked [0 -+ at a] picture called ``ffiJE#O4
E'' (Immaculate Conception).

GoldCorrection3: Ifhe[wasa-rwere]Japanese, Isupposel[replied-+wouldrepiy] likethis: ``56. ffiflrf6 ra>i-HtH,? (liltll? (r? j . 800m( avtjt('?'?''.

Proposed Correction 3: If he was [a -+ 0] Japanese, I suppose I [replied -+ would reply] like this: ``66 . ER/J tl :alGtH,? (iitttj? (r?I . 800m ( 6tt41'-4''.
\end{quote}

\caption{Three examples of model's proposed corrections from the CSW test set.}
\label{fig:fig2}
\end{figure}

\section{Conclusion}

The primary aim of this paper was to build a GEC
system capable of effectively correcting English
errors in CSW text, whilst maintaining competi-
tive performance on monolingual data. To address
the scarcity of CSW data, we explored methods of
generating synthetic CSW text. We used several
CSW metrics to establish that the LLM prompting-
based approach was the most capable of generating
text resembling the content in our genuine dataset.
From there, we used two error injection methods
to create the first substantial datasets labelled for
CSW GEC. This signiflcantly expanded the train-
ing data available. Importantly, it also opened up
opportunities for future research in CSW GEC and
CSW NLP more generally. We demonstrated the
efficacy of our synthetic data generation techniques
by training the first GEC model aimed at correcting
errors in CSW texts. Our model showed a clear
improvement in performance on CSW data, sur-
passing the SOTA in this area.

\section{Limitations}

This research, while comprehensive, encounters
several limitations that highlight areas for potential
improvement. One primary limitation lies in the
overrepresentation of Japanese in the genuine CSW
dataset. This raises questions about the model's ap-
plicability to a broader range of language pairs.
This is an unfortunate consequence of using a
dataset sourced from Lang-S, a Japanese language
Iearning network. Although our method demon-
strated that it could generate texts from a wider
range of language pairs, it is possible that all CSW
data used shows a bias towards Japanese styles of
CSW. Such a bias in our system could inadvertently
lead to reduced accessibility and effectiveness for
ESL learners who CSW with languages other than
Japanese. If this system were to be used as an aid
in ESL education, steps should be taken to ensure
that it does not contribute to existing inequalities
present in language learning platforms. English
learning tools should be accessible regardless ofthe
student's native language and future work should
focus on developing more inclusive datasets to help
mitigate these risks,

Another possible limitation relates to the style
of model chosen. The sequence tagging method
was selected due to its lower data requirements,
but this decision may have constrained the capa-
bilities of the model. Many of the errors typical
of ESL students require complex restructuring of
the sentence - a notably difficult task for edit-based
GEC systems. Although the data needs are more
substantial, it is likely that NMT GEC sysrems may
fare better as they are not constrained by a limited
vocabulary of edits.

To assess the likeness of our generated CSW
text, we introduced several common CSW metrics.
Although useful, these metrics are not very sophis-
ticated, and often struggle to accurately capture
the nuances of CSW patterns across different sub-
populations. These language patterns can have a
substantial impact on the optimal approaches to
problems across CSW NLP, and hence, the fleld
would benefit from further research in this area.
Ideally, we would have conducted a human study
to evaluate the quality of our synthetic data. How-
ever, given the constraints of the project, it was not
possible, and we acknowledge this as a limitation
of our work.

Finally, we reported results for a RoBERTa-
base GECToR system. Although we also tested
other base models, including BERT, DeBERTa and
ELECTRA, we did not look at larger models or en-
semble systems. Future extensions could explore
this area, building upon the observation that larger
models or simple voting ensembles can yield better
results than the smaller base models (Tarnavskyi
et a1.,2022).

In summary, whilst the current work makes sig-
nificant contributions to the field of GEC for CSW
text, these limitations indicate crucial areas for fur-
ther research and development.

\section*{References}

\begin{thebibliography}{99}

\bibitem{awasthi2019}
Abhijeet Awasthi, Sunita Sarawagi, Rasna Goyal,
Sabyasachi Ghosh, and Vihari Piratla. 2019. Par-
allel iterative edit models for local sequence trans-
duction. ln Proceedings ofthe 2019 Conference on
Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natu-
ral Lan gua g e P ro c e s s in g ( E MN LP - I J C N LP), pages
42594269, Hong Kong, China. Association for Com-
putational Linguistics.

\bibitem{barnett2000}
Ruthanna Bamett, Eva Cod6, Eva Eppler, Montse For-
cadell, Penelope Gardner-Chloros, Roeland van Hout,
Melissa Moyer, Maria Carme Torras, Maria Teresa
Turell, Mark Sebba, Marianne Starren, and Sietse
Wensing. 2000. The lides coding manual: A doc-
ument for preparing and analyzing language inter-
action data version 1.l-july, 1999. International
Journal of Bilingualism, 4(2):131-132.

\bibitem{beattymartinez2020}
Anne L Beatty-Martinez, Christian A Navarro-Torres,
and Paola E Dussias. 2020. Codeswitching: A bilin-
gual toolkit for opportunistic speech planning. Fron-
tiers in Psychology, ll:1699.

\bibitem{brown2020}
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry Amanda
Askell, Sandhini Agarwal, Ariel Herbert-Voss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel M. Ztegler, Jeffrey Wu,
Clemens Winter, Christopher Hesse, Mark Chen, Eric
Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
Jack Clark, Chdstopher Bemer, Sam McCandlish,
Alec Radford, Ilya Sutskever, and Dario Amodei.
2020. Language models are few-shot learners. CoRR,
abs/2005.141 65.

\bibitem{bryant2019}
Christopher Bryant, Mariano Felice, @istein E. Ander-
sen, and Ted Briscoe. 2019. The BEA-2019 shared
task on grammatical error conection. ln Proceedings
of the FourteenthWorkshop on Innoyative Use of NLP
for Building Educational Applications, pages 52-75,
Florence, Italy. Association for Computational Lin-
guistics.

\bibitem{bryant2017}
Christopher Bryant, Mariano Felice, and Ted Briscoe.
2017. Automatic annotation and evaluation of error
types for grammatical error correctiot. In Proceed-
ings of the 5 5th Annual Meeting of the Associatiott for
Computational Lingtdstics (Volume 1: Long Papers),
pages 793-805, Vancouver, Canada. Association for
Computational Linguistics.

\bibitem{chan2024}
Kelvin Wey Han Chan, Christopher Bryant, Li Nguyen,
Andrew Caines, and Zheng Yran.2024. Grammati-
cal error correction for code-switched sentences by
learners ofEnglish. ln Proceedings ofthe 2024 Joint
International Conference on Computational Linguis-
tics, Language Resources and Eyaluation (LREC-
COLING 2024), pages 7926-7938, Torino, Itatia.
ELRA and ICCL.

\bibitem{chelba2013}
Ciprian Chelba, Tomds Mikolov, Mike Schuster, ei Ge,
Thorsten Brants, and Phillipp Koehn. 2013. One
billion word benchmark for measuring progress in
statistical language modeling. CoRR, abs/1 3 12.3005.

\bibitem{dahlmeier2023}
Daniel Dahlmeier, Hwee Tou Ng, Siew Mei
Wu, et al. 2023. Nus corpus of learner
english (nucle). National University of Singapore, NLP Group. Available:
https ://[www.comp.nus.edu](http://www.comp.nus.edu). sg/ nlp/corpora. html.

\bibitem{dou2021}
Zi-Y Dou and Graham Neubig. 2021. Word alignment
by fine-tuning embeddings on parallel corpora. In
Proceedings ofthe l6th Conference ofthe European
Chapter of the Association for Computational Lin-
guistics: Main Volume, pages 2112-2128, Online.
Association for Computational Linguistics.

\bibitem{falbo2021}
Arianna Falbo and Travis LaCroix. ZO2l. Est-ce que
vous compute? code-switching, cultural identity, and
AI. CoRR, abs/21 12.08256.

\bibitem{finlay2023}
P.J. Finlay. 2023. Argos translate. Open-source offline
translation library written in Python.

\bibitem{gamback2016}
Bjorn Gamback and Amitava Das. 2016. Comparing
the level ofcode-switching in corpora. In Proceed-
ings of the Tenth International Conference on l,an-
guage Resources and Evaluation (LREC'16), pages
I 850-1855, PortoroZ, Slovenia. European Language
Resources Association (ELRA).

\bibitem{ghosh2017}
Souvick Ghosh, Satanu Ghosh, and Dipankar Das. 2017.
Complexity metric for code-mixed social media text.
C omput ac i6n y S i s t emas, 21 (4) :693-7 0 l.

\bibitem{goh2008}
K.-I. Goh and A.-L. Barab6si. 2008. Burstiness and
memory in complex systems. Europhysics Letters,
8l (4):48002.

\bibitem{guzman2017}
Gualberto Guzm6n, Joseph Ricard, Jacqueline Serigos,
Barbara E. Bullock, and Almeida Jacqueline Toribio.
2017. Metrics for Modeling Code-Switching Across
Corpora. In Proc. Interspeech 2017, pages 67-71.

\bibitem{klein2017}
Guillaume Klein, Yoon Kim, Yuntian Deng, Jean Senel-
lart, and Alexander M. Rush. 2017. OpenNMT:
Open-source toolkit for neural machine translation.
In Proceedings of the 55th Annual Meeting of the As-
sociation for Computational Linguistics (ACL 20 17 ),
Sy s t em D emonstration.r, pages 67 -7 2, Vancouver,
Canada. Association for Computational Linguistics.

\bibitem{koehn2012}
Philipp Koehn et at.2012. European parliament pro-
ceedings parallel corpus.

\bibitem{manning2014}
Christopher Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven Bethard, and David McClosky.
2014. The Stanford CoreNLP natural language pro-
cessing toolkit. In Proceedings of 52nd Annual Meet-
ing of the Association for Computational Linguis-
tics: System Demonstratiol1.T, pages 55-60, Balti-
more, Maryland. Association for Computational Lin-
guistics.

\bibitem{mizumoto2013}
Tomoya Mizumoto, Toshikazu Tajiri, Takuya Fujino,
Seiji Kasahara, Mamoru Komachi, Masaaki Nagata,
and Yuji Matsumoto. 2013. NAIST Lang-8 Leamer
Corpora. Language Leamer Corpora compiled from
Lang-8 SNS. Available for research and educational
purposes.

\bibitem{nguyen2022}
Li Nguyen, Zheng Yuan, and Graham Seed.2022.
Building educational technologies for code-
switching: Current practices, difficulties and future
directions. Languages, 7 :220.

\bibitem{omelianchuk2020}
Kostiantyn Omelianchuk, Vitaliy Atrasevych, Artem
Chernodub, and Oleksandr Skurzhanskyi. 2020.
GECToR - grammatical error correction: Tag, not
rewrite. In Proceedings of the Fifteenth Workshop
on Innovative Use of NLP for Building Educational
Applications, pages 163-170, Seattle, WA, USA -+
Online. Association for Computational Linguistics.

\bibitem{rizvi2021}
Mohd Sanad ZakiRizvi, Anirudh Srinivasan, Tanuja
Ganu, Monojit Choudhury, and Sunayana Sitaram.
2021 . GCM: A toolkit for generating synthetic code-
mixed text. ln Proceedings of the l6th Conference of
the European Chapter of the Association for Compu-
tational Linguistics : System Demonstratior?s, pages
205-211, Online. Association for Computational Lin-
guistics.

\bibitem{rothe2021}
Sascha Rothe, Jonathan Mallinson, Eric Malmi, Sebas-
tian Krause, and Aliaksei Severyn. 2021. A simple
recipe for multilingual grammatical error correction.
ln Proceedings of the 59th Annual Meeting of the As-
sociation for Computational Linguistics and the I lth
Intemational Joint Conference on Natural Language
Processing (Volume 2: Short Papers), pages 702-101,
Online. Association for Computational Linguistics.

\bibitem{stahlberg2021}
Felix Stahlberg and Shankar Kumar. 2021. Synthetic
data generation for grammatical error correction with
tagged coruption models. In Proceedings of the
l6th Workshop on Innovative Use of NLP for Build-
ing Educational Applications, pages 3747, Online.
Association for Computational Linguistics.

\bibitem{tarnavskyi2022}
Maksym Tarnavskyi, Artem Chernodub, and Kostiantyn
Omelianchuk. 2022. Ensembling and knowledge
distilling of large sequence taggers for grammatical
error correction. In Accepted for publication at 60th
Annual Meeting of the Association for Computational
Linguistics (ACL 2022), Dublin, Ireland.

\bibitem{yannakoudakis2011}
Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.
2011. A new dataset and method for automatically
grading ESOL texts. In Proceedings of the 49th
Annual Meeting of the Association for Computational
Lin g ui s tic s : H uman Lan g ua g e Te chno I o g i e s, pages
180-189, Portland, Oregon, USA. Association for
Computational Linguistics.

\bibitem{yow2018}
W. Quin Yow, Jessica S. H. Tan, and Suzanne Flynn.
2018. Code-switching as a marker of linguistic com-
petence in bilingual children. Bilingualism: Lan-
guage and Cognition, 2l(5): 1075-1090.

\end{thebibliography}

\appendix

\section{Example LLM Prompt}

Figure~\ref{fig:fig3} shows an example LLM prompt used to
generate synthetic CSW sentences from genuine
examples. As we are using a private subset of the
Lang-8 dataset, we are not permitted to share any
of the CSW texts.

\begin{figure}[t]
\centering
\fbox{\parbox{0.95\linewidth}{\centering IMAGE NOT PROVIDED}}
\vspace{0.5em}

\begin{quote}\small
Settings: [no prose]

For each of the following code-switched sentences, generate a new sentence that uses the same two
languages and a similar style of code-switching. The topic should be different. Ensure you use the correct
grammar in the English portion of the sentence. Make sure that each sentence contains 2 languages, Only
return the sentences and their number. You must follow all of the instructions.

For example, given the source sentence and label:

1. This food is called \texttt{", - ) >"} .

An acceptable answer would be:

1. This animal is called a \texttt{"t"} .

Do not include any other information in the generated sentences. The 10 real examples are as follows:

1. [CSW SENTENCE]

2. [CSW SENTENCE]

3. [CSW SENTENCE]

4. [CSW SENTENCE]

5. [CSW SENTENCE]

6. tcsw SENTENCEI

7. [CSW SENTENCE]

8. [CSW SENTENCE]

9. [CSW SENTENCE]

10.[csw SENTENCE]
\end{quote}

\caption{An Example LLM Prompt Used to Generate CSW Text}
\label{fig:fig3}
\end{figure}

\section{Error Tlpe Analysis of SOTA}

Table~\ref{tab:table3} shows a breakdown of the performance of
a single RoBERTa Large-based GECToR system
trained purely on monolingual GEC data when ap-
plied to two datasets, our genuine CSW dataset
and the BEA-2019 (Bryant et aL,2019) test set.
These datasets are approximately the same size.
The model used represents a current near-SOTA
single model sequence tagging-based GEC system
measured using Fs.s on the BEA-2019 test set. For
brevity, we have removed categories with a low
number of examples in either dataset or where per-
formance is not signiflcantly different.

\begin{table}[t]
\centering
\caption{Fs.5 Scores, TP, FP, FN, and Differences in Fs 5 Scores (BEA - CSW) for Different Categories in the BEA-2019 Test Split and our Genuine CSW Dataset.}
\label{tab:table3}
\begin{tabular}{lrrrrrrrr}
\toprule
Category & \multicolumn{4}{c}{BEA-2019 Test} & \multicolumn{4}{c}{Genuine CSW} \
& Fo.s & TP & FP & FN & Fo.s & TP & FP & FN \
\midrule
DET & 80.4s & 432 & 80 & 205 & 46.27 & 472 & 351 & 1336 \
NOI-IN & 4',7.85 & 29 & t6 & 94 & 4.34 & 21 & t47 & 1125 \
ORTH & 75.96 & 20t & 30 & 198 & 36.45 & 181 & 264 & 522 \
OTHER & 39.51 & 113 & 11 & 557 & 3.55 & 39 & 241 & 4333 \
PREP & 7 5.44 & 263 & 58 & 196 & 39.t4 & 24t & 251 & 870 \
PRON & 66.38 & 62 & t9 & 81 & 20.71 & 2t & )z & 274 \
PUNCT & 80.93 & 786 & 165 & 266 & 0.35 & 1 & 286 & 284 \
VERB & 52.59 & 61 & 27 & 161 & 18.08 & 49 & 17 & 802 \
VERB:FORM & 81.62 & 151 & 30 & 50 & 31.20 & 6t & 90 & 155 \
VERB:SVA & 88.64 & t28 & 14 & 26 & s7.58 & 114 & 79 & 104 \
VERB:TENSE & 65.55 & 145 & 62 & 133 & 33.80 & t16 & 172 & 448 \
wo & 58.08 & 23 & 5 & 63 & 6.33 & 2 & 11 & 104 \
\bottomrule
\end{tabular}

\vspace{0.5em}
\noindent Fo.s Decrease: 34.18, 43.51, 39.51, 35.96, 36.30, 45.61, 80.58, 34.51, 44.42, 31.06, 31.7 5, 51.7 5
\end{table}

\section{Tfaining Data Schedule}

In this section, we explicitly detail the data used at
each stage of the training process.

\paragraph{Stage 1}
For the initial pre-training stage, we used
the distilled dataset proposed by the SOTA (Tar-
navskyi eta1.,2022). This dataset was constructed
by extracting corrections from the monolingual
1BW corpus (Chelba et aL,2013) using the high-
est performing GECToR ensemble. Through this
dataset, we shuffled our PlE-synthetic CSW dataset.
We deemed this dataset to be of lower quality than
its Rev-GECToR counterpart. Consequently, it was
used earlier in the training process. This provided
roughly 1,200,000 examples for the initial training
phase of which we split between train and valida-
tion sets according to a ratio of 19:1. Our synthetic
CSW sentences comprised approximately 5.65Vo
of this dataset. We aimed to keep this percent-
age small in this phase of the training process to
allow the model to first learn to correct errors in
monolingual texts. In later stages, we boosted the
contribution of the CSW data.

\paragraph{Stage 2}
For the second stage, we shuffled several
GEC datasets. These are NUCLE (Dahlmeier et al.,
2023), FCE (Yannakoudakis et a1.,2011), W&I
Locness (Bryant et al., 2079), Lang-8 (Mizumoto
et al., 2013) and our 2 newly created CSW datasets.
As our genuine CSW dataset is a subset of the pri-
vate Lang-S corpus, we checked and removed any
duplicates. Table~\ref{tab:table4a} shows the overall contributions
of each corpus towards the stage 2 dataset. Similar
to the previous stage, the data was split into train
and validation sets.

\paragraph{Stage 3}
For the final stage, we combined the high
quality W&I Locness dataset with a sampled subset
of the genuine CSW data and a sampled subset of
the synthetic CSW texts. Again, the stage 3 dataset
is split into train and validation sets. The remaining
unused subset of the genuine CSW dataset was
retained for testing purposes. Table~\ref{tab:table5} details the
contributions to this stage from each dataset.

\begin{table}[t]
\centering
\caption{Sentence Count and Contribution of Stage 2 Datasets}
\label{tab:table4a}
\begin{tabular}{lr}
\toprule
Dataset & Sentences \
\midrule
Lang-8 & 985,683 (80.54Vo) \
W&I Locness & 68,608 (5.61Vo) \
NUCLE & 54,258 (4.437o) \
FCE & 26,929 (2.20Vo) \
Syn-CSW PIE & 70,181 (5.73Vo) \
Syn-CSWRev-GECTbR & 18,160(7.487o) \
\midrule
Total & 1,223,819 \
\bottomrule
\end{tabular}
\end{table}

\begin{table}[t]
\centering
\caption{Sentence Count and Contribution of Stage 2 Datasets}
\label{tab:table4b}
\begin{tabular}{lr}
\toprule
Dataset & Sentences \
\midrule
Syn{ & [ILLEGIBLE] \
Syn-CSW PIE & 10,000 (9.B0Vo) \
CSW Genuine & 5,279 (5.17Vo) \
\midrule
Total & 102,047 \
\bottomrule
\end{tabular}
\end{table}

\begin{table}[t]
\centering
\caption{Sentence Count and Contribution of Stage 3 Datasets}
\label{tab:table5}
\begin{tabular}{lr}
\toprule
Dataset & Sentences \
\midrule
W&I Locness & 68,608 (67.23%) \
Syn-CSW Rev-GECToR & 18,160 (17.80%) \
Syn-CSW PIE & 10,000 (9.80%) \
CSW Genuine & 5,279 (5.17%) \
\midrule
Total & 102,047 \
\bottomrule
\end{tabular}
\end{table}

\section{Inference llyperparameters}

After training our model, we used the validation
dataset from stage 3 to tune 2 inference parameters.
These are:

\begin{itemize}
\item additional_confidence --- This value is added
to the probability of the $KEEP token. If this
value is high, recall is likely to decrease and
precision increase. The grid search found the
best value of this to be 0.4. The grid search
found the best value of this to be 0.
\item min_error_probability --- For a change to be
made to a sentence, the probability of at least
one token in the sentence being an error must
be higher than the min_error_probability. If
this value is high, then precision is likely to
[MISSING]
\end{itemize}

\section{Error Tlpe Analysis of Proposed Model}

By exploring the ERRANT error classifications of
our proposed model when applied to the CSW test
dataset, we can further explore the effectiveness
of our synthetic data in addressing the problematic
areas identified in Appendix B. A breakdown of
the precision, recall and Fs 5 score for each of the
previously identified categories is shown in Table~\ref{tab:table6}.

\begin{table}[t]
\centering
\caption{Precision (P), Recall (R) and F6 5 Score of Our Proposed Model for Targeted Error Types in the Genuine CSW Test Dataset}
\label{tab:table6}
\begin{tabular}{lccc}
\toprule
Category & P & R & Fo.s \
\midrule
NOUN & 0.2857 & 0.0833 & 0.1923 \
PRON & 0.7647 & 0.4643 & 0.6771 \
PUNCT & 0.7143 & 0.1139 & 0.3460 \
WO & 0.7'778 & 0.2000 & 0.4930 \
\bottomrule
\end{tabular}
\end{table}

\end{document}
=====END FILE=====
