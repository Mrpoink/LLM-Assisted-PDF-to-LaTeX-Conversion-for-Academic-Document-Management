=====FILE: main.tex=====
\documentclass[11pt]{article}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage{microtype}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{url}
\usepackage{hyperref}

\title{Thinking Outside of the Differential Privacy Box:\
A Case Study in Text Privatization with Language Model Prompting}
\author{Stephen Meisenbacher \and Florian Matthes\
Technical University of Munich\
School of Computation, Information and Technology\
Department of Computer Science\
Garching, Germany\
\texttt{{stephen.meisenbacher,matthes}@tum.de}}
\date{}

\begin{document}
\maketitle

\begin{abstract}
The field of privacy-preserving Natural Language Processing has risen in popularity, particularly at a time when concerns about privacy grow with the proliferation of Large Language Models. One solution consistently appearing in recent literature has been the integration of Differential Privacy (DP) into NLP techniques. In this paper, we take these approaches into critical view, discussing the restrictions that DP integration imposes, as well as bring to light the challenges that such restrictions entail. To accomplish this, we focus on DP-PRoupr, a recent method for text privatization leveraging language models to rewrite texts. In particular, we explore this rewriting task in multiple scenarios, both with DP and without DP To drive the discussion on the merits of DP in NLP, we conduct empirical utility and privacy experiments. Our results demonstrate the need for more discussion on the usability of DP in NLP and its beneflts over non-DP approaches.

The incentive of proving Differential Privacy is the mathematical guarantee of privacy protection that it offers, so long as its basic principles are adhered to. Particularly, important DP notions must be strictly defined, such as who the individual is, how data points are adjacent, and how data can be bounded. As such, the fusion of Differential Privacy and NLP introduces several challenges (Feyisetan et al., 2021 ; Habernal, 202 1 ; Klymenko et a1.,2022; Mattern et a1.,2022). When generalized forms of DP are used or well-deflned notions of DP concepts are lacking, the promise of DP becomes more of a shallow guarantee.

In this work, we critically view the pursuit of DP in NLP, focusing on the particular method of DP-Pnonpr (Utpala et a1.,2023). This method leverages generative Language Models to rewrite (paraphrase) texts with the help of a DP token selection method based on the Exponential Mechanism (Mattern et a1.,2022). We run experiments on three rewriting settings: (l) DP, (2) Quasi-DP, and (3) Non-DP; the purpose of this trichotomy is to explore the beneflts and shortcomings of DP in text rewriting. We define our research question as: What is the benefit of integrating Differential Privacy into private text rewriting methods leveraging LMs, and what effect can be observed by relaxing this guarantee?

Our empirical findings show the advantages that incorporating DP into text rewriting mechanisms brings, notably higher semantic similarity and resemblance to the original texts, along with strong empirical privacy results. This, however, comes with the downside of generally lower quality text in terms of readability, particularly at stricter privacy budgets. These findings open the door to discussions regarding the practical distinction between DP and non-DP text privatization, where we present open questions and paths for future work.

The contributions of our work are as follows:
\begin{enumerate}
\item We explore the merits of DP in LM text rewriting through comparative experiments.
\item We evaluate DP-Pnoupr in a series of utility and privacy tests, and analyze the difference in DP vs.\ non-DP privatization.
\item We call into question the merits of Dp in NLp, presenting the benefits and limitations of doing so as opposed to non-DP privatization.
\end{enumerate}

\end{abstract}

\section{Introduction}
The topic of privacy in Natural Language Processing has recently gained traction, which has only been fueled by the prominent rise of Large Language Models. In an effort to address concerns revolving around the protection of user data, the study of privacy-preserving NLP has presented a plethora of innovative solutions, all investigating in some form the optimization of the privacy-utility trade-off for the safe processing of textual data.

A well-studied solution comes with the integration of Differential Privacy (DP) (Dwork, 2006) into NLP techniques. Essentially, the use of DP entails the addition of calibrated noise to some stage in a pipeline, e.g., directly to the data or to model weights. This is performed with the ultimate goal of protecting the individual whose data is being used, aligned with the objective of Differential Privacy set out in its inception nearly 20 years ago.


\section{Related Work}
Natural language can leak personal information (Brown et al., 2022) and it is possible to extract training data from Machine Learning models (Pan et al., 2020; Carlini et al., 2021; Mattern et al., 2023). In the \emph{global} DP setting, user texts are collected at a central location and a model is trained using privacy-preserving optimization techniques (Ponomareva et al., 2022; Kerrigan et al., 2020) such as DP-SGD (Abadi et al., 2016). The primary drawback of this model is that user data must be collected at a central location, giving a data curator access to the entire data (Klymenko et al., 2022). To mitigate this, text can be obfuscated or rewritten locally in a DP manner before collecting it at a central location (Feyisetan et al., 2020; Igamberdiev and Habernal, 2023; Hu et al., 2024).

The earliest set of approaches of DP in NLP began at the word level (Weggenmann and Kerschbaum, 2018; Fernandes et al., 2019; Yue et al., 2021; Chen et al., 2023; Carvalho et al., 2023; Meisenbacher et al., 2024a), yet these methods do not consider contextual and grammatical information during privatization (Mattern et al., 2022; Meisenbacher et al., 2024c). Other works operate directly at the sentence level by either applying DP to embeddings (Meehan et al., 2022) or latent representations (Bo et al., 2021; Weggenmann et al., 2022; Igamberdiev and Habernal, 2023). DP text rewriting methods using generative LMs (Mattern et al., 2022; Utpala et al., 2023; Flemings and Annavaram, 2024) or encoder-only models (Meisenbacher et al., 2024b) have also been proposed.


\section{Method}
Here, we describe the base text privatization method that we utilize, as well as the variations which form the basis of our experiments.

\subsection{DP-Pnotrpr}
DP-Pnoupr (Utpala et al., 2023) is a differentially private text rewriting method in which users generate privatized documents at the local level by prompting Language Models to rewrite input texts. In particular, the LMs are prompted to paraphrase a given text. The immediate advantage of this method comes with the flexibility in model choice as well as the generalizability to all general-pulpose pre-trained (instruction-finetuned) LMs.

The integration of DP into this rewriting process comes at the generation step, where for each output token, a DP token selection mechanism is implemented in the form of temperature sampltng. In Mattern et al. (2022), it is shown that the use of temperature can be equated to the Exponential Mechanism (McSheny and Talwar, 2007). Relating this mechanism to the privacy budget $e$ of Dp, the authors show that $e : ff$, where [ILLEGIBLE] is the temperature and $A$ is the senslrivity, or range, of the token logits. A fixed sensitivity can be ensured by clipping the logits to certain bounds.\footnote{Specifically, to the range (logit_mean, logit_mean * [ILLEGIBLE] . losit_std,) = (-19.23, 7.48), thus L : [ILLEGIBLE].}

For the purposes of this work, we perform all experiments using DP-PnoMpT with the FLAN-T5-BASE model from Google (Chung et a1.,2022).

\subsection{RewritingApproaches}
Motivated by rhe DP-Pnol,rpr rewriting mechanism, we introduce three privatization strategies based on its DP token selection mechanism:
\begin{enumerate}
\item DP: we use DP-Pnoupr as originally introduced, namely by clipping logit values and scaling logits by temperatures calculated based on $e$ values. We test on the values $e \in {25,50, 100.150,250}$. Logits are clipped based on an empirical measurement of logits in the FLAN-T5-BAsp model.
\item Quasi-DP: we replicate the Dp strategy with-otd clipping, i.e., only using temperature sampling based on the abovementioned $e$ values. We call this quasi-Dp since the temperature values 7 are calculated as if clipping was performed (i.e., sensitivity is bounded), but the unbounded logit range is actually used.
\item Non-DP: here, we do not use any clipping or temperature, but rather only vaty the top-$k$ parameter, or the number $k$ of candidate tokens considered when sampling the next token. We choose $k \in {b0,2b, 10, 5,3}$.
\end{enumerate}

With these three privatization strategies, we aim to measure empirically the effect on utility and privacy by strictly enforcing DP, relaxing DP, and by performing privatization devoid of DP. In this way, one may be able to analyze the merits of DP-based text privatization methods, and furthermore, observe the theoretical guarantees of DP in action.


\section{Experimental Setup and Results}

As stated by Mattern et al.~(2022), a practical text privatization mechanism should: (1) protect against deanonymization attacks, (2) preserve utility, and (3) keep the original semantics intact. As such, we design our experiments by leveraging multiple dimensions of a single dataset. The results of all described experiments can be found in Table~1.

\subsection{Dataset}

For all of our experiments, we utilize the Blog Authorship Corpus (Schler et al., 2006). This corpus contains nearly 700k blog post texts from roughly 19k unique authors. The corpus also lists the ID, gender, and age of author for each blog post. Full details on the preparation of the corpus are found in Appendix~A; pertinent details are outlined below.

We prepare two subsets of the corpus. The first, which we call \textbf{author10}, only considers blog posts from the top-10 most frequently occurring blog authors in the corpus. This subset results in a dataset of 15,070 blog posts spanning five categories.

The second subset, called \textbf{topic10}, is necessary as the classification of the gender and age attributes for the \textbf{author10} dataset would be a less diverse and challenging task. We first take a random 10% sample of the top-10 topics from the filtered corpus, resulting in a sample of 14,259 blogs. Here, the age value is binned into one of five bins to ensure an equal number of instances in each bin.

\subsection{Utility Experiments}

We perform utility experiments for both the \textbf{author10} and \textbf{topic10} datasets. To measure utility across all privatization strategies, we first privatize each dataset on all selected privatization parameters. As we choose 5 parameters ($\epsilon/T$ or $k$) for each of our three strategies, this results in 15 dataset variants, i.e., 15 results per metric, each of which represents the average between the two datasets.

\paragraph{Semantic Similarity.}
To measure the ability of each privatization strategy to preserve the semantic meaning of the original sentence, we employ two similarity metrics: BLEU (Papineni et al., 2002) and cosine similarity. Both metrics strive to capture the similarity between output (in this case privatized) text and a reference (original) text; BLEU relies on token overlap while cosine similarity between embeddings is more contextual.

We use SBERT (Reimers and Gurevych, 2019) to calculate the average cosine similarity (CS) between the original blog posts and their privatized counterparts. For this, we use utilize three embeddings models to account for model-specific differences: ALL-MINILM-L6-v2, ALL-MPNET-BASE-v2, and GTE-SMALL (Li et al., 2023). For each dataset, we report the mean of the average cosine similarity calculated for each model.

We also report the BLEU score between privatized texts and their original counterparts. This is done using the BLEU implementation made available by Hugging Face. As before, reported BLEU scores are the average across an entire dataset.

\paragraph{Readability.}
In addition, we also measure the quality and readability of the privatized outputs by using perplexity (PPL) (Weggenmann et al., 2022), specifically with GPT-2 (Radford et al., 2019).

\subsection{Privacy Experiments}

Using \textbf{author10} and \textbf{topic10}, we design three empirical privacy experiments, in which an adversarial classification model is trained to predict a sensitive attribute (authorship, gender, or age) based on the blog post text. For this, we fine-tune a DEBERTA-v3-BASE model (He et al., 2021) for three epochs, reporting the macro F1 of the adversarial classifier.

We evaluate the privatized datasets in two settings (Mattern et al., 2022; Weggenmann et al., 2022). In the \emph{static} setting, the adversarial model is trained on the original training split and evaluated on the privatized validation split. In the more challenging \emph{adaptive} setting, the adversarial classifier is trained on the private train split. Lower performance implies that a method has better protected the privacy of the texts. Note that the adaptive score represents the mean of three runs. For all cases, a random 90/10 train/val split with seed 42 is taken.

In addition to F1, we also report the relative gain metric ($\gamma$), following previous works (Mattern et al., 2022; Utpala et al., 2023). $\gamma$ aims to capture the trade-off between utility loss and privacy gain, as compared to the baseline scores. For the utility portion of $\gamma$, we use the CS results. Baseline scores are represented by adversarial performance after training and testing on the non-private datasets. We report the $\gamma$ with respect to the adaptive setting.

\begin{table}[t]
\centering
\begin{tabular}{l r rrrrr rrrrr rrrrr}
\toprule
$\epsilon/k$ value & Baseline & \multicolumn{5}{c}{DP} & \multicolumn{5}{c}{Quasi-DP} & \multicolumn{5}{c}{Non-DP} \
\cmidrule(lr){2-2}\cmidrule(lr){3-7}\cmidrule(lr){8-12}\cmidrule(lr){13-17}
& $\infty$ & 25 & 50 & 100 & 150 & 250 & 25 & 50 & 100 & 150 & 250 & 50 & 25 & 10 & 5 & 3 \
\midrule
CS$\uparrow$ & 1.00 & 0.589 & 0.597 & 0.812 & 0.827 & 0.832 & 0.347 & 0.598 & 0.810 & 0.826 & 0.833 & 0.710 & 0.726 & 0.750 & 0.741 & 0.787 \
BLEU$\uparrow$ & 1.00 & 0.077 & 0.029 & 0.123 & 0.142 & 0.153 & 0.001 & 0.029 & 0.121 & 0.141 & 0.153 & 0.049 & 0.054 & 0.063 & 0.063 & 0.088 \
PPL$\downarrow$ & 41 & 8770 & 1234 & 928 & 919 & 905 & 16926 & 1380 & 982 & 932 & 925 & 816 & 972 & 1080 & 821 & 837 \
\midrule
Author F1 (s)$\downarrow$ & 66.45 & 7.13 & 37.05 & 58.10 & 61.12 & 60.60 & 6.59 & 36.91 & 57.84 & 60.37 & 61.13 & 46.83 & 47.07 & 49.88 & 51.69 & 53.10 \
Author F1 (a)$\downarrow$ & 66.45 & 2.68 & 33.52 & 52.82 & 55.46 & 57.35 & 2.74 & 33.29 & 54.81 & 55.64 & 57.34 & 42.56 & 44.81 & 45.20 & 48.22 & 49.91 \
Gender F1 (s)$\downarrow$ & 68.07 & 41.88 & 55.66 & 67.81 & 66.68 & 65.92 & 43.41 & 58.16 & 67.85 & 65.38 & 67.98 & 55.64 & 63.91 & 64.16 & 64.50 & 66.66 \
Gender F1 (a)$\downarrow$ & 68.07 & 38.80 & 54.06 & 61.90 & 62.90 & 62.23 & 38.80 & 57.05 & 62.48 & 54.02 & 62.93 & 60.61 & 59.09 & 59.23 & 61.26 & 60.00 \
Age F1 (s)$\downarrow$ & 37.58 & 19.12 & 28.56 & 38.31 & 37.17 & 38.44 & 17.99 & 28.06 & 37.32 & 37.53 & 38.42 & 32.24 & 32.95 & 35.56 & 35.41 & 35.64 \
Age F1 (a)$\downarrow$ & 37.58 & 12.17 & 29.06 & 38.92 & 37.95 & 39.00 & 12.17 & 32.40 & 36.85 & 36.77 & 37.49 & 33.49 & 34.67 & 34.97 & 35.75 & 36.23 \
\midrule
Author $\gamma$ & -- & 0.549 & 0.093 & 0.017 & -0.008 & -0.031 & 0.306 & 0.097 & -0.015 & -0.011 & -0.030 & 0.070 & 0.052 & 0.070 & 0.015 & 0.036 \
Gender $\gamma$ & -- & 0.019 & -0.197 & -0.097 & -0.097 & -0.082 & -0.223 & -0.240 & -0.108 & 0.032 & -0.091 & -0.180 & -0.142 & -0.120 & -0.159 & -0.094 \
Age $\gamma$ & -- & 0.265 & -0.176 & -0.224 & -0.183 & -0.206 & 0.023 & -0.264 & -0.171 & -0.152 & -0.165 & -0.181 & -0.197 & -0.181 & -0.210 & -0.177 \
\midrule
$\sum \gamma$ & -- & \textbf{0.833} & -0.281 & -0.304 & -0.288 & -0.319 & 0.106 & -0.407 & -0.293 & \textbf{-0.131} & -0.286 & -0.292 & -0.287 & \textbf{-0.231} & -0.354 & \textbf{-0.236} \
\bottomrule
\end{tabular}
\caption{Experiment Results. Utility scores include the averaged CS, BLEU, and PPL scores for the \textbf{author10} and \textbf{topic10} datasets. \textit{Author/Gender/Age F1} indicate the adversarial performance on the authorship, gender, and age classification tasks, for both the static (s) and adaptive (a) settings. We report a modified version of \textit{Relative Gain} ($\gamma$) for each setting, as explained in Section~4.3. The best cumulative $\gamma$ score is \textbf{bolded} for each comparative parameter.}
\end{table}


\section{Discussion}
In analyzing the results, we first discuss the merits of DP text privatization. At stricter privacy budgets (lower e), only the original DP-Pnoupr is able to present significant gains, as showcased with e : 25. At these lower values, one can also observe the benefits of enforcing DP via logit clipping, which results in higher CS and BLEU retention while outputting generally more readable text (much lower PPL). This trend with PPL holds for all scenarios of DP vs.\ Quasi-DP, making a clear case for proper bounding in DP applications.

In studying DP vs.\ Quasi-DP further, we notice that the distinction between the two, particularly at higher e values, becomes somewhat opaque. In fact, Quasi-DP outperforms DP in terms of empirical privacy in many of the higher privacy budget scenarios. This would imply that a DP mechanism leveraging temperature sampling only becomes effective and sensible with stricter privacy budgets.

An important point of comparison also comes with the study results of our Non-DP method. A strength of this method is highlighted by its ability at lower k values (analogous to less strict privacy budgets) to maintain high levels of semantic similarity (CS), while still achieving competitive empirical privacy scores. For example, in the case of k : 3, this method is able to outperform all e > 100 for both DP and Quasi-Dp. The BLEU scores for Non-DP would also imply that this method is better able to rewrite texts in a semantically similar, yet lexically different manner, as opposed to DP methods at high e values (see Appendix D). These results make a case for Non-DP privatization in certain cases, and in parallel, provide a critical view of using DP at high e values which lead to ineffective empirical privacy.

A final point that is crucial to discuss is grounded in the observed relative gains. Looking to the cumulative scores ([ILLEGIBLE]) of Table [ILLEGIBLE], one can notice that the only positive gains are observed at relatively low e values, implying that only at these levels do the empirical privacy protections begin to outweigh the losses in utility. The utility scores in these cases, however, are quite difficult to justify in real-world scenarios, where semantic similarity is quite low and readability suffers greatly. These results in general showcase the harsh nature of the privacy-utility trade-off, where mitigating adversarial advantage often comes with less usable data.


\section{Conclusion}
Central to this work is the debate on the merits of Differential Privacy in NLP. To lead this discussion, we conduct a case study with the [ILLEGIBLE] mechanism, juxtaposed with two [ILLEGIBLE] variants. Our results show that while the theoretical guarantee of individual privacy may be important in some application settings, in others, it may become too restrictive to apply effectively. Conversely, the merits of DP may be observed in stricter privacy scenarios, where the need for tight guarantees does bring favorable privacy-utility trade-offs.

We call for further research in two directions: (1) rigorous studies on the theoretical and practical implications of DP vs non-DP privatization, and relatedly, (2) the continued design of privatization mechanisms outside the realm of Differential privacy that aim to balance strong privacy protections with practical utility preservation. We hope that researchers may be able to harmonize the [ILLEGIBLE], keeping in sight the need for practically usable privacy protection of text data.


\section*{Acknowledgments}
The authors thank Alexandra Klymenko and Maulik Chevli for their contributions to this work.


\section*{Limitations}
The foremost limitation of our work comes with the selection of a single base model for use with FLAN-T5-BASE. While further testing should be conducted on other (larger) models, we hold that our results can be generalized, since model choice was not central to our findings. Another limitation is the choice of $\epsilon$ (i.e., temperature) and $k$ values, which were not selected in any rigorous manner, but rather based on the relative range of values presented in Utpala et al.\ (2023). The effect of parameter values outside of our selected ranges thus is not explored in this work.


\section*{Ethics Statement}
An ethical consideration of note concerns our empirical privacy experiments, which leverage an existing dataset (Blog Authorship) not originally intended for adversarial classification. In performing these empirical experiments, the actions of a potential adversary were simulated, i.e., to leverage publicly accessible information for the creation of an adversarial model. As this dataset is already public, no harm was inflicted in the privacy experiments as part of this work. Moreover, the dataset is made up of pseudonyms (Author IDs) rather than PII, thus further reducing the potential for harm.


\section*{References}
Martin Abadi, Andy Chu, Ian Goodfellow, H. Bren-
dan McMahan, Ilya Mironov, Kunal Talwar, and
LiZhang.2016. Deep learning with differential pri-
vacy. In Proceeding,s of the 2016 ACM SIGSAC
Conference on Computer and Communications Secu-
rlry, CCS '16, page 308-318, New York, NY, USA.
Association for Computing Machinery.

Haohan Bo, Steven H. H. Ding, Benjamin C. M, Fung,
and Farkhund lqbal. 2021. ER-AE: Differentially
private text generation for authorship anonymization.
ln Proceedtngs ofthe 2021 Conference ofthe North
American Chapter of the Association for Computa-
tional Lin g ui s t ic s : H uman l,an g ua g e Te c hno I o g i e s,
pages 3991-4007, Online. Association for Computa-
tional Linguistics.

Hannah Brown, Katherine Lee, Fatemehsadat
Mireshghallah, Reza Shokri, and Florian Trambr.
2022. What does it mean for a language model
to preserve privacy? In Proceedings of the 2022

ACM Conference on Fairness, Accountability,
and Transparency, FAccT '22, page 2280-2292,
New York, NY USA. Association for Computing
Machinery.

Nicholas Carlini, Florian Tramer, Eric Wallace,
Matthew Jagielski, Ariel Herbert-Voss, Katherine
Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar
Erlingsson, et al. 2021 . Extracting training data from
large language models. In 30th USENIX Security
S y mp o s ium ( U S E N IX S e c urity 2 I ), p ages 2633 -26 50.

Ricardo Silva Carvalho, Theodore Vasiloudis,
Oluwaseyi Feyisetan, and Ke Wang. 2023. TEM:
High utility metric differential privacy on text.
ln Proceedings of the 2023 SIAM International
Conference on Data Mining (SDM), pages 883-890.
SIAM.

Sai Chen, Fengran Mo, Yanhao Wang, Cen Chen, Jian-
Yun Nie, Chengyu Wang, and Jamie Cui. 2023. A
customized text sanitization mechanism with differ-
ential privacy. In Findings of the Association for
C omputational Linguistic s : ACL 202 3, pages 5747-
5758, Toronto, Canada. Association for Computa-
tional Linguistics.

Hyung Won Chung, Le Hou, Shayne Longpre, Barret
Zoph,Yi Tay, William Fedus, Yunxuan Li, Xuezhi
Wang, Mostafa Dehghani, Siddhartha Brahma, Al-
bert Webson, Shixiang Shane Gu, Zhuyun Dai,
Mirac Suzgun, Xinyun Chen, Aakanksha Chowdh-
ery, Alex Castro-Ros, Marie Pellat, Kevin Robinson,
Dasha Valter, Sharan Narang, Gaurav Mishra, Adams
Yu, Vincent Zhao, Yanping Huang, Andrew Dai,
Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Ja-
cob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le,
and Jason Wei. 2022. Scaling instruction-fi netuned
language models. P reprint, arXiv :2210.1 1416.

Cynthia Dwork. 2006. Differential privacy. ln Inter-
national colloquium on automata, languages, and
programming, pages 1-12. Springer.

Natasha Fernandes, Mark Dras, and Annabelle Mclver.
2019. Generalised differential privacy for text doc-
ument processing. ln Principles of Security and
Trust: 8th International Conference, POST 2019,
Held as Part of the European Joint Conferences
on Theory and Practice of Software, ETAPS 2019,
Prague, Czech Republic, April 6-1 1, 2019, Proceed-
lngs 8, pages 123-148. Springer International Pub-
lishing.

Oluwaseyi Feyisetan, Abhinav Aggarwal, Zekun Xu,
and Nathanael Teissier. 2021. Research challenges in
designing differentially private text generation mech-
anisms. In The International FL4IRS Conference
Proceedings, volume 34.

Oluwaseyi Feyisetan, Borja Balle, Thomas Drake, and
Tom Diethe. 2020. Privacy- and utility-preserving
textual analysis via calibrated multivariate perturba-
tions. In Proceedings ofthe l3th International Con-
ference onWeb Search and Data Mining, WSDM '20,
page 178-186, New York, NY, USA. Association for
Computing Machinery.

James Flemings and Murali Annavaram. 2024. Differ-
entially private knowledge distillation via synthetic
text generation. In Findings of the Association for
Computational Linguistics ACL 2024, pages 12957-
12968, Bangkok, Thailand and virtual meeting. As-
sociation for Computational Linguistics.

Ivan Habemal202l. When differential privacy meets
N 0 L 2 P 1 : Th e co n v e n i e n c e o f Emp i ri c al D p. ln
P ro ce d i n g s o f th e l u h m e e A s s o c i a n tio g n P fo a p r e C rs o ) m , p p a ut g a e ti s o 3 n 3 a 6 l 7 L - in 3 3 g 8 u 0 is , ti D c s u ( b V li o n l , -
Lan g ua g e P roc e s s in g, pages 1522-15 28, Online and Ireland. Association for Computational Linguistics.
Punta Cana, Dominican Republic. Association for
Computational Linguistics.

Pengcheng He, Jianfeng Gao, and Weizhu Chen.202l.
Debertav3: Improving deberta using electra-style pre-
training with gradient-disentangled embedding shar-
ing. P rep rint, arXtv'.27 I 1.095 43.

Lijie Hu, Ivan Habemal, Lei Shen, and Di Wang. 2024.
Differentially private natural language models: Re-
cent advances and future directions. In Findings
of the Association for Computational Linguistics:
EACL 2024, pages 478499, St. Julian's, Malta, As-
sociation for Computational Linguistics.

Timour Igamberdiev and Ivan Habemal.2023. Dp-
BART for privatized text rewriting under local dif-
ferential privacy. In Findings ofthe Associationfor
Computational Linguistic s : ACL 202 3, pages I 39 1 4-
13934, Toronto, Canada. Association for Computa-
tional Linguistics.

Gavinin Kerrigan, and Jules White. 2020. Differentially
private language models by scaling and shuffling
public pre-training. In Proceedings of the Second
Workshop on Privacy in NLP, pages 39-45, Online.
Association for Computational Linguistics.

ZehanLi, Xin Zhang, Yanzhao Zhang, Dingkun Long,
Pengjun Xie, and Meishan Zhang.2023. Towards
general text embeddings with multi-stage contrastive
leaming. Preprint, arXiv:2308.0328 1.

Casey Meehan, Khalil Mrini, and Kamalika Chaudhuri.
2022. Sentence-level privacy for document embed-
dings. In Proceedings of the 60th Annual Meeting of
the Association for Computational Linguistics: ACL
2022, pages 3367-3380, Online and Ireland. Association for Computational Linguistics.

Stephen Meisenbacher, Maulik Chevli, and Florian
Matthes. 2024a. l-Diffracror: Efficienr and utility-
preserving text obfuscation leveraging word-level
metric differential privacy. In Proceeding,s of the
1)th ACM Intemational Workshop on Security antl
Privacy Analytics, IWSPA '24, page 23-33, New
York, NY USA. Association for Computing Machin-
ety.

Stephen Meisenbacher, Maulik Chevli, Juraj Vladika,
and Florian Matthes. 2024b. DP-MLM: Differen-
tially private text rewriting using masked language
models. In Findings of the Association for Com-
putotional Linguistics ACL 2024, pages 9314-9328,
Bangkok, Thailand and virtual meeting. Association
for Computational Linguistics.

Stephen Meisenbacher, Nihildev Nandakumar, Alexan-
dra Klymenko, and Florian Matthes. 2024c. A com-
parative analysis of word-level metric differential
privacy: Benchmarking the privacy-utility trade-off.
In Proceedings ofthe 2024 Joint International Con-
ference on CompLrtational Lingui,stics, Language
Resources and Evaluation (LREC-COLING 2024),
pages 174-185, Torino, Italia. ELRA and ICCL.

Frank McSherry and Kunal Talwar. 2007. Mechanism
design via differential privacy. In 48th Annual IEEE
Symposium on Foundations of Computer Science
( FO CS' 07 ), pages 94-1 03.

Xudgng Pan, Mi Zhang, Shouling Ji, and Min yang.
2020. Privacy risks of general-purpose language
models. In 2020 IEEE Symposium on Security aid
Privacy (SP,i. pages I3l4-1331

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In proceedings of the
10th Annual meeting of ACL, pages 311-318, USA.
Association for Computational Linguistics.

Alec Radford, Jeff Wu, Rewon Child, David Luan,
Dario Amodei, and Ilya Sutskever. 2019. Language
models are unsupervised multitask learners. OpenAL

Nils Reimers and Iryna Gurevych. 2019. Sentence-
BERT: Sentence embeddings using siamese BERT-
networks. CoRR, abs/1908. 10084.

Jonathan Schler, Moshe Koppel, and Shlomo Argamon.
2006. Effects of age and gender on blogging. In
AAAI s p r in g sy mp o s ium : C o mp ut at i onal ap p ro ac he s
to analyzing weblogs, volume 6, pages 199-205.

Saiteja Utpala, Sara Hooker, and Pin-Yu Chen.2023.
Locally differentially private document generation
using zero shot prompting. ln Findings of the As'
:
s o ciation for Comp utational Linguistic s EM N LP
2023 , pages 8442-8457 , Singapore. Association for
Computational Linguistics.

Benj amin Weggenmann and Florian Kerschbaum. 20 I 8.
SynTF: Synthetic and differentially private term fre-
quency vectors for privacy-preserving text mining.
InThe 4lst Internationdl ACM SIGIR Conference on
Research & Development in Information Retrieval,
pages 305-3 14.

Benjamin Weggenmann, Valentin Rublack, Michael An-
drejczuk, Justus Mattern, and Florian Kerschbaum.
2022. DP -y AE : Human-readabl e text anonymization
for online reviews with differentially private varia-
tional autoencoders. In Proceedings of the ACM Web
Conference 2022,\textbackslash{}\textbackslash{}rV\textbackslash{}rW '22, page 721-731, New
Nt
York, USA. Association for Computing Machin-
ery.

Xiang Yue, Minxin Du, Tianhao Wang, Yaliang Li,
Huan Sun, and Sherman S. M. Chow. 2021. Dit-
ferential privacy for text analytics via natural text
sanitization. ln Findings of the Association for Com-
putdtional Linguistics: ACL-IJCNLP 2021, pages
3853-3866, Online. Association for Computational
Linguistics.


\appendix

\section{Blog Dataset Preparation}
We outline the process of dataset preparation for the data used in this work. All prepared datasets are made available in our code repository.

We begin with the corpus made available by Schler et al.\ (2006), which contains 681,284 blog posts from 19,320 authors and across 40 topics. In particular, we use the version made publicly available on Hugging Face.\footnote{\url{[https://huggingface.co/datasets/tasksource/blog-authorship-corpus}}](https://huggingface.co/datasets/tasksource/blog-authorship-corpus}}) In this version, each blog post is labeled with a topic, which we learned translates to the career field of the corresponding author.

Upon an initial survey, we noticed that a significant amount of blogs are labeled with \texttt{[ILLEGIBLE]}, so these were filtered out. In addition, one of the topics named \textit{Student} did not seem to have coherent blog content in terms of topic, so these blogs were also removed. These steps resulted in a filtered corpus of [ILLEGIBLE].

Next, noticing that out of all the ``topics'', many contained very few blogs, we only considered blogs with topics in the top 15 most frequently occurring topics. We also only consider blog posts with a maximum of 256 tokens, both for performance reasons and also to remove outliers (very long blog posts). These two steps resulted in a further filtered set of 162,584 blogs.

To prepare the \textbf{author10} dataset, we considered the 10 most frequently blogging authors in the filtered corpus. This translates to authors writing between 1001 and 2174 distinct blog posts, for a total of 15,070 blogs in the \textbf{author10} dataset.

To prepare the \textbf{topic10} dataset, we only consider blog posts from the filtered corpus which count in the top 10 most frequently occurring topics. Concretely, this consists of the following topics (from most to least frequent): Technology, Arts, Education, Communications - Media, Internet, Non-Profit, Engineering, Law, Science, and Government. With these topics, we take a 10% sample of the filtered corpus, resulting in a dataset of 14,259 blogs. Technology is the most frequent topic in this dataset with 3409 blogs, with Government the least frequent at 485 blogs.

While the gender attribute is not altered in the \textbf{topic10} dataset, we bin the age attribute for a more reasonable classification task. We choose to create five bins from the age column, which ranges from the age of 13 to 48. Creating an even split between all age bins, we achieve the following bin ranges: [ILLEGIBLE]. Thus, the resulting \textbf{topic10} dataset contains 10 topics, 2 genders, and 5 age ranges.

\section{DP-PROMPT Implementation Details}

We implement DP-PROMPT by following the described method in the original paper (Utpala et al., 2023). As noted, we leverage the FLAN-T5-BASE model as the underlying LM.

To set the clipping bounds for our method, we run 100 randomly sampled texts from our dataset through the model and record all logit values. Then, we set the clipping range to (logit_mean, [ILLEGIBLE]) = (-19.23, 7.48), as noted in the paper.

For the prompt template, we use the same simple template as used by Utpala et al.\ (2023), namely:
\begin{quote}
\texttt{Document: [ORIGINAL TEXT] Paraphrase of Document:}
\end{quote}

As discussed in the original paper, we do not change the top-$k$ parameter for DP-PROMPT in its output generation, both for the DP and Quasi-DP settings. This is left to the default Hugging Face parameter of $k=50$.

Finally, for comparability, we limit the maximum generated tokens for all methods to 64. For all privatization scenarios, we run DP-PROMPT (and its variants) on a NVIDIA RTX A6000 GPU, with an inference batch size of 16. The source code for replication can be found at the following repository, which also includes our two prepared datasets used in the experiments: \url{[https://github.com/sjmeis/DPNONDP}](https://github.com/sjmeis/DPNONDP})


\section{Training Parameters}
We outline the training parameters used for all DEBERTA-v3-BASE adversarial classification experiments.

Across all datasets and privatization methods, we fine-tune for three epochs using a batch size of 16, with a learning rate of $2 \times 10^{-5}$, linear warmup, and weight decay of 0.01. We use a maximum sequence length of 256 tokens and evaluate on the validation set after each epoch. For all experiments, we set the random seed to 42.


\section{Examples}
Tables 2 and 3 provide rewriting examples for all tested parameters for a selected text sample from each of our two datasets.

\begin{table}[t]
\centering
\small
\setlength{\tabcolsep}{6pt}
\renewcommand{\arraystretch}{1.15}
\begin{tabular}{p{0.14\linewidth} p{0.08\linewidth} p{0.70\linewidth}}
\toprule
\textbf{Original text} & & Food for poor ol' me as well!!!!! Yesterday was absolutely great. Not only did I get 30 for doing 5 mins maintenance/repair on someones PC, but I had a great night out too! Can't get any better than that. As my wallet has been 'topped-up' *smug* I can now afford my Only Fools And Horses DVD, Max Payne II (when it comes out in about a month) and I can also take Nic out for a classy meal when I get round to it one weekend. Corny. I have also done most of my homework so far. I'm hardly lagging which gets me in a happier mood :) P.S: BRAND new absolutemaxpayne.co.uk due for release in about 3 weeks (just before release, how convenient). Kinky. \
\midrule
\textbf{$\epsilon/k$} & & \
\midrule
\textbf{DP} & 25  & Baggy Boggianyliines Show Baseografic Direct Withlniu Because Parallel Gate shows Oilnamen Mini Pilot Officeproductive evaluated analyse Belgium divorce Index geometric An integrated achievecllinter Madrid game Facilityuillonture GUI ensue diese lawyeradvisable devenir \pounds1 Claus farbenc Committee of Government housing Fulllink/ Staffdangger Zugriff staatlich finden Bow \
& 50  & the food started okay, was slow so I renovated a locker monday. \
& 100 & Yesterday was absolutely great. Not only did I get 30 for doing 5 mins maintenance/repair on someone's PC, but I had a great night out too! Can't get any better than that! As my wallet has been 'topped-up' *smug* I can now afford \
& 150 & I'm in a better mood now. \
& 200 & Yesterday was absolutely great. Not only did I get 30 for doing 5 mins maintenance/repair on someone's PC, but I had a great night out too! Can't get any better than that. As my wallet has been 'topped-up' *smug* I can now \
& 250 & I'm so happy with my life. \
\midrule
\textbf{Quasi-DP} & 25  & Average country studying \
& 50  & Just had a nice night at my new laptop store. It's been showering and saving my cancelled vacation for the good of the Bay Area and improving my financial position back down in Seattle. \
& 100 & Yesterday was a really good day. I got 30 for doing 5 mins maintenance/repair on someone's PC, but I had a great night out too! Can't get any better than that! As my wallet has been 'topped-up' *smug* I can now \
& 150 & Yesterday was absolutely great. Not only did I get 30 for doing 5 mins maintenance/repair on someones PC, but I had a great night out too! Can't get any better than that. As my wallet has been 'topped-up' (*smug*) I can \
& 200 & Yesterday was absolutely great. Not only did I get 30 for doing 5 mins maintenance/repair on someone's PC, but I had a great night out too! Can't get any better than that. As my wallet has been 'topped-up' *smug* I can now \
& 250 & Yesterday was absolutely great. Not only did I get 30 for doing 5 mins maintenance/repair on someone's PC, but I had a great night out too! Can't get any better than that. As my wallet has been 'topped-up' *smug* I can now \
\midrule
\textbf{Non-DP} & 50 & Yesterday was basically great. Not only did I get 30 for doing 5 mins maintenance/repair on someone's PC, but I had a great evening out too! Can't get any better than that... \
& 25 & Today he got me some great news \
& 10 & I've had an amazing weekend. \
& 5  & I have to get some money to buy a DVD, Max Payne II and eat dinner for Nic. \
& 3  & Yesterday was absolutely great. Not only did I get 30 for doing 5 mins maintenance/repair on someones PC but I had a great night out too. As my wallet has been 'topped-up' ... I can now afford my Only Fools And Horses DVD, Max Payne \
& 1  & Yesterday was absolutely great. Not only did I get 30 for doing 5 mins maintenance/repair on someone's PC, but I had a great night out too! Can't get any better than that. As my wallet has been 'topped-up' *smug* I can now \
\bottomrule
\end{tabular}
\caption{Rewritten examples from the \textbf{author10} dataset.}
\end{table}

\begin{table}[t]
\centering
\small
\setlength{\tabcolsep}{6pt}
\renewcommand{\arraystretch}{1.15}
\begin{tabular}{p{0.14\linewidth} p{0.08\linewidth} p{0.70\linewidth}}
\toprule
\textbf{Original text} & & My mother came over on Sunday and brought a ton of baby stuff that she has either bought new or found at consignment shops (she has better ones where she lives). It was fun looking at all the PINK things she bought! Then she gave me some dusters to wear that my grandmother had and never wore. They are so comfortable it is almost unbelievable! Want to know what a duster is? ROFLMAO It's a polite word for a MUU-MUU! And I don't care! For the first time in my life I am actually happy to be wearing a muu-muu... Is it old age or senility? Who knows. All I know is that I am very comfortable. ((Poor Sharky is convinced that this is a plot of my mothers to insure that I NEVER get pregnant again. I mean, these are muu-muus in all the prerequisite colors and patterns.)) \
\midrule
\textbf{$\epsilon/k$} & & \
\midrule
\textbf{DP} & 25  & Leaf miserable Astro FIRST actress Nachlac Pilt came over lending Judesc headset recently aleappaarfterv disk album popcorn to Conservative Job Today quest necessity when Ellenn at Funeral seen Vilnton les grisil set transtourtherquarataine ahruptapathetic Boot Vacation betting lieben analysis Travail Emperor LEWhether Fantasy climatique trop torrent aus \
& 50  & Jade is comfortable in red nude \
& 100 & A mother came over on Sunday and brought a lot of baby stuff, some dusters that her grandmother never wore, and a muu-muus, the first time in her life I am happy to be wearing a muu-muus. \
& 150 & My mother brought me some baby stuff that she never wore. \
& 200 & My mother brought me some baby dusters that my grandmother never wore. \
& 250 & My mother brought me some baby dusters that her grandmother never wore. \
\midrule
\textbf{Quasi-DP} & 25  & Grantment 2010. Onh Sar asking State430 unstable 13. 2013, makers knee before Town in tuneive 101 Lankauinverszu Horse investi Uneign man ^{e}taiteexistank grandeo certifi'ePro remboursement Bil contre Raiggy contribu Driver Levant pourtant crois Beaumaym cerc unfold III777devoted tutello:...me allou \
& 50  & The mother brought everything to the baby that she could find, from pillows to throw pillows. It was fun and time wass. The first time I am happy wearing a popular MU-MU' frisder. \
& 100 & My mother brought me a lot of baby stuff to look at. \
& 150 & My mother brought me a ton of baby stuff, and she gave me some dusters that her grandmother never wore. \
& 200 & I was wearing a muu-muus and my mother gave me some. \
& 250 & The mother brought her a ton of baby stuff. \
\midrule
\textbf{Non-DP} & 50 & There were a lot of baby clothes that my mom had before she wore all these dusters. \
& 25 & My grandmother brought a lot of baby stuff from their home and they had to get a duster for the first time in her life. She said hers were nice to look at and they were comfortable. But it's not the same as having a diaper. \
& 10 & She brought my mother a lot of baby stuff, and gave me some new dusters. They're so comfortable they are almost unbelievable. \
& 5  & The mother gave me some muu-muus to wear for the first time in my life. \
& 3  & My mother brought me some dusters to wear that my grandmother never wore. \
& 1  & My mother brought me some baby dusters that my grandmother never wore. \
\bottomrule
\end{tabular}
\caption{Rewritten examples from the \textbf{topic10} dataset.}
\end{table}

=====END FILE=====
