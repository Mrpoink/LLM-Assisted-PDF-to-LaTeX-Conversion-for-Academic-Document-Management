=====FILE: main.tex=====
\documentclass[11pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage{microtype}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{array}
\usepackage{hyperref}

\title{Verba volant, scripta volant? Don't worry! There are computational solutions for protoword reconstruction}
\author{Liviu P. Dinu \and Ana Sabina Uban \and Alina-Maria Cristea \and Bogdan Iordache \and Teodor Marchitan \and Simona Georgescu \and Lauren\c{t}iu Zoica\c{s}}
\date{}

\begin{document}
\maketitle

\begin{abstract}
the presence in different Indo-European languages of obviously related words for `beech' or `salmon' We introduce a new database of cognate alowed the reconstruction of words from proto-[ILLEGIBLE] and thus inrormation about the [ILLEGIBLE] tuguese, French), the most comprehensive one men-ts of nature present in the immediate vicinity of to date with over 19,000 entriei. We propose the Indo-Europeans could be extracted. In the ab- sence of any clear documentary or archaeological data, these lexical clues allowed the geographical identification of the Indo-European homeland, also facilitating the chronology of successive waves of separado; of Indo-European languages from the [ILLEGIBLE] isting state-o?-the-art results for this task and [ILLEGIBLE] showing that computational methods can be very useful in assisting linguists with protoword reconstruction. In the case of Romance languages, although the mother tongue - Latin - is attested, its presence in written texts is not an exhaustive source for linguis-

\end{abstract}

\section{Introduction and Related work}
protoword reconstruction, consisting of recreating the words in a proto-language from their descendants in daughter languages, is central to the study of language evolution. As the foundation of historical linguistics (Campbell,2013;Mallory and Adams, 2006) and the basis for finguistic phylogeny (Atkinson et al., 2005; Alekseyenko et al., 2012;Dtnn,2015; Brown et al., 2008), protoword reconstruction offers important pieces o1infor-ution concerning the geographical and chronological dimensions of ancient communities (Heggarty, 2015; Mallory and Adams, 2006), at the same time, allowing an insight into the cognitive and cultural world of our ancestors.

The traditional processofreconstructingancientlanguagesconsistsof the "comparative grarnmar-reconstruction" method (Chambon, 2007; Buchi and Schweickard, 2014), and the etymological data thus obtained can be used as a source on human prehistory conoborating the archaeologicalinventory(Heggarty,2015),andpro- trovare viding the basis for 'linguistic paleontology' (Epps, 2Ol4).

The reconstruction of a word automatically implies a reconstruction of the surrounding realities, both natural and socio-cultural. For example, the presence in different Indo-European languages of obviously related words for 'beech' or 'salmon' allowed the reconstruction of words from proto-[ILLEGIBLE] and thus inrormation about the ele- men-ts of nature present in the immediate vicinity of the Indo-Europeans could be extracted. In the absence of any clear documentary or archaeological data, these lexical clues allowed the geographical identification of the Indo-European homeland, also facilitating the chronology of successive waves of separation of Indo-European languages from the common trunk.

In the case of Romance languages, although the mother tongue - Latin - is attested, its presence in written texts is not an exhaustive source for linguistic' social' and historical analysis of the community that spoke it. It is now generally accepted that the spoken language represented a different diastatic, diaphasic, and diamesic variety from written language' used by the few educated people who decided to express themselves in writing (Wright, 2002).

The Latin language that we reconstruct from words inherited in Romance languages is only concrete and reliable living variety of the language from which Romance languages originate, whether we call it orail vulgar Latin or Proto-Romance' We will opt here for the name "Proto-Romance" when we refer to the language from which the Romance languages originate, as this corresponds to the concept of protolanguage and protoword (Buchi and Schweickafi,2014)' Furthermore, there are still numerous clearly cognate words present in several Romance languages, whose etymon is not attested in Latin (nor in any other language from which it might have been borrowed).

For example, in the case of It. 'find', Fr. trouver,Cat. trobar, etymologists have hotly debated over the decades whether one should reconstruct the protoform *tropare or *turbare (Georgescu and Georgescu, 2020). A series of cognates attested in all Romance geographical areas, like Rom. tncd'moreover', It. anche, Old Fr. anc, Cat. anc etc., has triggered over 15 etymological hypotheses over the last century still without a generally accepted solution. This is a revision of the dataset of (Dinu and Ciobanu, 2014) (used with still very good results in (Ciobanu and Dinu, 2018)) with the addition of cognates scraped from Wik- tionary. Although etymologists' interest in reconstruct- ing the protolanguages has risen over the years, they still encounter numerous gaps when using ex- clusively the classical, manual methods (Buchi and Schweickard,2010,2020). As the task of pro- tow'ord reconstruction plays an important role in historical linguistics, studies have gone beyond exclusively their focus on etymology and made use of computational approaches. Various methods are proposed in the field of automatic proto-language reconstruction and are explored in (Bouchard-Cot6 et al., 2013) and (Jager, 2019). Other machine learning approaches to the task are also explored in a shared task context, part of the 2016 SIGTYP workshop (Cotterell et al., 2016).

Starting with these remarks, our main contribu- tions are: 1. We introduce a comprehensive Romance database for protoword reconstruction by process- ing RoBoCoP (Dinu el a1.,2023), the largest Ro- mance cognate-borrowing database obtained from electronic dictionaries and curated by linguists. The database consists of an extract of only cognate entries. 2. We propose a strong benchmark for automatic protoword reconstruction based on a set of models and features. Both contributions are presented in the rest of the paper.

The rest of the paper is organized as follows: In Section 2 we present the database that we have created and offer details about the processing steps involved; in Section 3 we introduce our approach for the automatic protoword reconstruction, along with methodological details; the results of our proposed experiments are fleshed out in Section 4; and a comprehensive error analysis is described in Section 5. The last section is dedicated to final remarks.


\section{Data}
A major inconvenience in Historical Linguistics in general, and in computational approaches ofprotoword reconstruction in particular is the scarcity of available data. Nonetheless, in the last few years, several initiatives have been undertaken in this direction. (Ciobanu and Dinu, 2018) developed a database of Latin protowords, further expanded by (Meloni et a1.,2021) with Wiktionary data. Recently, this dataset was extensively used for several studies (Kim et He et al., a1.,2023; 2023b; Akavarapu and Bhattach ary a, 2023). In 2023, Dinu et al. (2023) published the most comprehensive database of Romance related words, named RoBoCop. It in contains cognates and etymons five Romance languages : Italian, Spanish, Portuguese, Romanian, and French. It has already been used with good results on prominent historical linguistic tasks such as cognate identification (Dinu et al., 2023), gnate-borrowin s discrimin ation (Dinu co g et a1.,2024b), and determining the bonowing direction (Dinu et a1.,2024a).


\subsection{The ProtoRom Database}
\begin{table}[t]
\centering
\caption{All cognate tuples present in the ProtoRom dataset for the Latin etymon axls.}
\label{tab:protorom_axls}
\begin{tabular}{l}
\toprule
axls\
RO\
ES\
PT\
IT\
FR\
axa\
eje\
axls\
ASSE\
als\
ixrs\
axls\
axrs\
ASSE\
ais\
ax\
axrs\
axls\
ASSE\
zlls\
itxls\
eje\
axls\
ASSE\
zlls\
axa\
axts\
axls\
ASSE\
ars\
ix\
eJe\
Ax1S\
ASSE\
zlls\
\bottomrule
\end{tabular}
\end{table}

Starting with the RoBoCoP database (Dinu et al., 2023), in order to obtain cognate sets with common etymons in the flve Romance languages, we filtered out the words with Latin etymology. We then cre- ated maximal tuples of words in the Romance lan- guages with the same etymon (< ,u ), e), where Li are a7l the languages among the flve where the etymon e engendered a word, andw1,, are the cor- responding words in each of the languages dis- cussed. In cases where multiple words in tr6 derive from the same etymon e, we created multiple tu- ples (< uLr ), e) with all possible combinations of cognate words ( wLn ) and the same etymon e. For an example of such a case see Table l.

We curated the obtained data, with the help of linguists. In the process, we discarded sets that contained irrelevant or erroneous information, e.g.: erroneous lexical forms (e.g. Lal. videre'see' - It. vedere - Fr. votr - F.o. videa (correct: vedea); included a verb form in any mood other than the inflnitive (e.g. Lat. videre - Sp. ueas (subjunctive) I viendo (gerundive) I etc.); retained the reflexive form of a verb (e.g. Lat. ponere'put' - It. porre - Sp. ponerse (poner + reflexive pronoun se), etc.); or contained words derived on Romance ground (e.g. Lat. dens'tooth' -Il. dente -Ro. dinlos (= dinte + suff.-os), etc.).

We were able to apply manual corrections for all these errors for the smaller subset of entries in the database that have a cognate in each of the five languages. For the rest of the full database Pro- toRom, we applied a semi-automatic correction by lemmatizing the cognate words, using the default lemmatizersl implemented in the spaCyzlibrary for each of the Romance languages.

2https ://spacy. io/usage/models

In all exper- iments described in the rest of the paper, we use the lemmas of the cognates instead of the original forms found in the dictionary.

In addition to the correct series thus retained, we integrated the database created by Reinheimer- Ripeanu (2001), a high quality collection of cog- nate series manually selected from the etymologi- cal dictionaries of each Romance language, some of which still not digitized (which probably ex- plains why certain cognate sets from this collection were not among ones in the RoBoCoP database). We thus obtained a new database of cognate sets.

The proposed database contains a total 39,973 full or partial cognate sets along with their etymons. For the experiments in this paper, we focus on the 19,222 entries with at least 2 cognates. We choose this subset in order to ensure the robustness of our experiments, focusing on Latin etymons that engendered at least two cognates in two different languages, and we ignore the entries with only one cognate for a given etymon. Going further, this restricted dataset will be referred to as ProtoRom3.

A cognate set is composed of a tuple of words in different languages with a common etymon, where the tuple can be ei' ther a full set of 5 cognates or a partial set of 2 to 4 cognates, where the cognate in one or more of the languages is missing (the Latin etymon did not produce an attested word in these languages according to our sources).

There are 1,245 full cognate sets in the database, the rest being partial cognate sets. To facilitate distinguish- ing between the two settings, we name the flrst one ProtoRom-all5, and the second one ProtoRom. When we leave out one of the lan- guages, we can obtain more full sets of 4-tuples (sets with at least 4 cognates) as follows: 1,480 if we leave out Italian, 2,493 if we leave out French, 1,489 when we leave out Portuguese, 1,504 when we leave out Spanish, and 1,946 by leaving out Romanian. The statistics detailing the number of partial cognate sets in all combinations are shown in Table 2.

\begin{table}[t]
\centering
\caption{Number of cognate sets that are descendants from the same Latin word, for each language combination.}
\label{tab:protorom_combinations}
\begin{tabular}{l}
\toprule
It: 5,197\
It-Fr:2,807\
It-Ro: 3,439\
It-Es: 6,820\
It-Pt: 4,605\
-It: 1,480\
It-Fr-Ro: 1,842\
Fr:4,992\
Fr-Ro: 3,898\
Fr-Es: 4,413\
Fr-Pti 2,797\
-Fr:2,493\
It-Fr-Es:2,270\
It-Ro-Pt:2,926\
Ro: 5,685\
Ro-Es: 5,117\
Ro-Pt: 3,394\
-Ro: 1,946\
It-Fr-Pt: 2,390\
It-Es-Pt: 3,988\
Fr-Ro-Pt: 1,782\
Es: 6,820\
Es-Pt: 4,543\
-Es: 1,504\
It-Ro-Es: 2,913\
Fr-Ro-Es: 3,503\
Fr-Es-Pt: 2,311\
Ro-Es-Pt: 2,919\
Pt:5,202\
-Pt: 1,489\
\midrule
.r-y means the number of cognate sets for languages x and y; r-y-z means the number of cognate sets for languages x,y, and z; , means how many descendants are from Latin for language ,; -, means the number of cognate sets for all languages except.r.\
\bottomrule
\end{tabular}
\end{table}

ProtoRom is the largest database of cognate sets for Romance languages so far, significantly exceed- ing the widely used database for this task (Meloni eta1.,2021), containing 8,799 cognate sets of Ro- manian, French, Italian, Spanish, Portuguese words and the corresponding Latin form (which, in turn, is an extension of Ciobanu and Dinu (2018)'s orig- inal dataset of 3,218 cognate sets, by adding data from Wiktionary).

3The dataset is available for research purposes upon re- quest at: htps://nlp.unibuc.rolresources.htrd#protorom


\section{Methodology and Experiments}
[MISSING]


\subsection{Experimental Setting}
For our experimental trials, we consider two set-
tings: In the first one, we limit our dataset to only
the full cognate sets (i.e. 5-tuples of cognates from
each of the five languages, that originate from the
same Latin etymon), while in the second one we
consider all cognate sets (with at least two cognates
from different languages, per etymon, as previ-
ously mentioned). The second setting uses the full
breadth of our proposed dataset (ProtoRom-all5),
whereas the first one is a strict subset (ProtoRom).

Data splitting. In order to train and validate our
models, we split our datasets into 80% : 10% :
10% train-dev-test subsets. Because of the nature
of the cognate sets, generating a language-level
stratified split is a non-trivial task. Since a Latin
etymon can produce more than one reflex in a given
language, we are able to randomly select etymons
and their associated cognate sets and add them to
any of the three splits, as long as they fit. This
approach yields the original split distribution with
some small deviations [ILLEGIBLE]. Also note that after splitting the ProtoRom-all5
dataset, containing only the full cognate sets, we
can use it as a starting point for splitting the rest of
the ProtoRom dataset, thus ensuring that no train-
ing examples from one setting leaks into the vali-
dation of the other one.

Features. The proposed approaches can be split
into two main categories: models for reconstructing
the orthographical representation of the protowords
using the orthographical form of modern cognates,
and models that reconstruct the phonemic repre-
sentation from phonetic transcriptions of modem
cognates. Our extracted dataset essentially pro-
vides the necessary examples for the former, while
for the latter we employ the eSpeak\texttt{a} library to au-
tomatically generate the phonemic representations.


\subsection{Models}
We use a variety of machine leaming models, in-
cluding classical, neural, and transformer-based
(pretrained and trained from scratch for the task).
We include methods used in previous papers on
the topic and evaluate them on our larger dataset
in order to provide a benchmark for the task of
protoword reconstruction for Romance languages.
We experiment with a variety of models, includ-
ing pre-trained large language models (LLMs) and
current state-of-the-art models for protoword recon-
struction with various architectures (probabilistic
RNN, character-level transformer) adapted to our
new database, as well as original solutions. In this
way, we aim to provide a benchmark for the task of
protoword reconstruction.

CRF + reranking We used an approach that re-
lies on conditional random fields (CRFs), based
on the method proposed by Ciobanu and Dinu
(2018). Firstly, we applied a sequence labeling
method that produces the form of the Latin an-
cestors, for each modern language. The modern
words are the sequences, and their characters are
the tokens. We used character n-grams from the
input words as features. We employed pairwise se-
quence alignment (Needleman and Wunsch, 1970)
between modern words and protowords to obtain
the labels for each token. Secondly, we defined
several ensemble methods to take advantage of the
information provided by all languages, in order to
improve performance. We employed fusion meth-
ods based on the ranks in the n-best lists and the
probability estimates provided by the individual
classifiers for each possible production, in order to
combine the outputs of the classifiers (n-best list
of possible protowords) and to leverage informa-
tion from all modern languages. For each word
in the productions list, we multiply the rank of it
with the confidence score given by the CRF model
for each language; we sum up the multiplication
scores for each word in the list and then rerank the
productions based on these results.

Probabilistic LSTM We conducted experiments
using a combination of recurrent neural networks
with different dynamic programs and expectation-
maximization techniques, as described in He et al.
2023b. The overall system can be split in two
stages: a) a modelling stage, where we model the
evolution of words by making small character-level
edits to the ancestral form; for each language in the
study, the distribution over newly created words is
computed; b) an expectation-maximization stage,
where the ancestral form is inferred; using words
sampled from the posterior distribution, the ex-
pected edit count is computed and further used
by the character-level recurrent neural network in
order to optimize the next round of samples; the fi-
nal reconstruction is the maximum likelihood word
forms. This model requires a full tuple of cog-
nates to be passed as input, so we only compute
results for experiments on the ProtoRom-all5 set.
Like the original authors, we only apply this model
on the phonemic forms of words, since the prob-
ability distributions of edit operations used in the
algorithm rely on a set of manually set features for
each phoneme that are not similarly available for
orthographical characters.

Character-level transformer The next experi-
ments conducted in this research are based on the
transformer model, proposed by Kim et aL.2023.
Some critical changes in the architecture were
made in order to be able to accept our samples
format: multiple modem word sequences (one for
each language) correspond to a single protoform
sequence, A positional encoding is applied to each
individual modern word sequence before concate-
nation, An additive language embedding is applied
to the token embeddings alongside the positional
encoding in order to make a difference between
input tokens of different languages.

Pre-trained LLM (Flan-T5) We finally evalu-
ate the capabilities of pretrained Large Language
Models (LLMs) to solve our task. While LLMs
are currently obtainin g state-of-the-art performance
across NLP tasks, our specific goal is unlike usual
tasks included in benchmarks or in training data
for LLMs, and it is strongly multilingual (includ-
ing one dead language), so we suspect it might be
a difficult task for an LLM. We choose to use a
pretrained model and fine-tune it on our own train-
ing data in order to increase its chances to perform
well. We use a "base" variant of the Flan-T5 model
(Chung et a1.,2024), and fine-tune the model us-
ing instructions including the prompt: "What is the
etymon given the following cognates:", followed
by a list of cognate and language pairs formatted
as " < Li ): I wa >" and separated by new lines,
where the list of cognate words tt; is their respec-
,,
tive languages can be arbitrarily long (from 2
to 5 cognates, in the case of our experiments). For
evaluation, we attempt to generate multiple out-
put sequences, which are used as a ranking for the
etymon prediction.

One limitation of pretrained LLMs that we can-
not overcome through fine-tuning is its alphabet,
which contains mostly characters in the Latin
graphical alphabet, which means that we can only
use this model with othographical features. Us-
ing phonemic features would require retraining the
model from scratch and we would lose the benefit
of pertaining which is usually the strong point of
LLMs.


\section{Results}
The previously described methods have been applied on both ProtoRom and ProtoRom-all5 datasets, using the orthographical form of the cognates and Latin etymon, or alternatively the autogenerated phonemic representations (where the models were able to accommodate them).

We also provide a comprehensive human evaluation of the results. Linguists from our team manually analyzed the entire list of results, and we present the most signiflcant observations regarding the models' successes and failures. The linguists did not correct the protoforms proposed by the models, but only evaluated and commented on them in relation to current knowledge in the field of historical linguistics.

The metrics used include accuracy, (normalized) edit distance, and Coui, with $z \in {1, 5, 10}$, which stands for an extended version of the accuracy metric, where a correct prediction is one where the model found the correct etymon within the first [ILLEGIBLE] etymons predicted by our method (this metric is computed for models that are able to output a ranked list of predictions - Flan-T5 and CRF-based models).


\subsection{ProtoRom-all5 Results}

Results obtained on the ProtoRom-alls set are shown in Table~3. In terms of accuracy (ot Coul), the best results are obtained using the orthograph- ical forms, with the CRF-rerank model, reaching 60.4Vo. From the perspective of the C ou i metrics, it is remarkable that the CRF-rerank model obtains aCoun score aboveSZVo,

The experiments using the phonemic forms pro- duce weaker results, with the best accuracy reach- :ri,155.8% in the top 1 predictions scenario. Nev- ertheless, the CRF approach is able to achieve an accunacy close to 80% when we consider the top L0 best ranked predictions.

The probabilistic RNN models achieve very poor performances, reaching a mean edit distance of 3.1-1 when trained on the phonemic representations.

\begin{table}[t]
\centering
\caption{Reported results for protoword reconstruction on the ProtoRom-all5 dataset via orthographical representa- tions (Gr) and via phonemic representations (Ph), respectively. We report the reconsffuction accuracy along with the mean edit distance (Edit) and mean normalized edit dista [ILLEGIBLE]. The Coui values for the edit distances are computed by selecting the minimum distance between the t [ILLEGIBLE] true etymon and the top i predictions, then averaging over these minima for all of the test examples. For the Flan and CRF models, we look at the top 1, 5, and 10 predictions when computing these metrics.}
\label{tab:protorom-all5-results}
\begin{tabular}{lrrrrrr}
\toprule
& \multicolumn{3}{c}{Accuracy} & \multicolumn{3}{c}{Edit/NEdit} \
& Cov1 & Cov5 & Cov10 & Cov1 & Cov5 & Cov10 \
\midrule
Flan & 55.0 & 70.5 & 75.9 & 1.03/0.15 & 0.55/0.08 & 0.43t0.06 \
Gr CRF & 60.4 & 78.2 & 82.1 & 0.80/0.12 & 0.38/0.05 & 0.31/0.04 \
Transformer & 59.92 & [ILLEGIBLE] & [ILLEGIBLE] & 0.72t0.11 & [ILLEGIBLE] & [ILLEGIBLE] \
CRF & 55.8 & 75.9 & 79.8 & 0.86/0.13 & 0.4t0.06 & 0.33l0.05 \
Ph Transformer & 47 & [ILLEGIBLE] & [ILLEGIBLE] & 0.98/0.16 & [ILLEGIBLE] & [ILLEGIBLE] \
\bottomrule
\end{tabular}
\end{table}


\subsection{ProtoRom Results}

The best accuracy when training the orthographical models is achieved in this scenario by the Trans-former model, closely surpassing 73% (Table 4). As for the Coui metrics, the Flan model remark-ably obtains a C ourc accuracy score of 85.4Vo, and an edit distance of 0.23,

Similarly to the previous scenario, the experi-ments using the phonemic forms produce weaker results, with the best accuracy reaching 66.8% via the Transformer model. These results represent a collection of baselines for protoword reconstruc-tion using our proposed dataset configurations.

We believe the higher accuracy observed on the full dataset is simply due to the larger amount of available data. While ProtoRom-all5 is a subset that contains only complete cognate sets from each of the five studied languages (totaling 1,245 sets) the ProtoRom dataset includes sets of two, three, or four cognates, resulting in significantly more sets (19,222). This larger dataset allowed the models to learn more phonetic correspondences, thereby improving the reconstruction process. Even though they are not full sets of flve cognates, the additional cognate sets in the full database seem to help the models learn more about their protowords. This learning process is closely similar to the human method of learning: with more examples, linguists can be more certain of particular correspondences or phonetic changes and can apply them in the reconstruction with much greater confidence.

\begin{table}[t]
\centering
\caption{Similar to Table 3 we report the same evaluations when using the complete ProtoRom dataset.}
\label{tab:protorom-results}
\begin{tabular}{lrrrrrr}
\toprule
& \multicolumn{3}{c}{Accuracy} & \multicolumn{3}{c}{Edit/NEdit} \
& Cov1 & Cov5 & Cov10 & Cov1 & Cov5 & Cov10 \
\midrule
Gr Flan & 65.5 & 81.7 & 85.4 & 0.73/0.09 & 0.30/0.04 & 0.23/0.03 \
CRF & 55.0 & 71.3 & 79.1 & 1.06/0.16 & 0.55/0.08 & 0.42/0.06 \
Transformer & 73.1 & [ILLEGIBLE] & [ILLEGIBLE] & 0.51/0.08 & [ILLEGIBLE] & [ILLEGIBLE] \
Ph Transformer & 66.8 & [ILLEGIBLE] & [ILLEGIBLE] & 0.67/0.10 & [ILLEGIBLE] & [ILLEGIBLE] \
\bottomrule
\end{tabular}
\end{table}


\section{Error analysis}
This section is dedicated to a deeper dive into qualitatively quantifying the errors produced by the previously proposed models. Our objective is separating purely wrong predictions from "near misses", which may still provide value for linguists for the reasons discussed below. The error analysis was manually conducted by the linguists from our team, who specialize in Romance languages. They did not modify the protoforms provided by the models in any way. Their only intervention was to distinguish forms that were genuinely erroneous from those whose differences from the dictionary form were either insignificant or represented a correct adjustment to the reality of Latin pronunciation. In the final quantitative analysis, forms in this category were therefore included in the list of correct predictions without any changes to their structure,

Through analyzir,gthe errors, we have identified some patterns that typically reflect either an insufflcient number of examples to support a particular phonetic change or the irregularity of the change itself. For example, the short tonic [ILLEGIBLE] develops into Spanish [ILLEGIBLE] in half of the cases, while it remains [ILLEGIBLE] in the other half. In such scenarios, the f model may not know which phonetic treatment the cognates underwent and might choose the wrong variant. Similarly, in cases of phonetic accidents, which are by nature irregular and unpredictable, the model cannot reconstruct the pre-accident form. Instead, it reconstructs the intermediate form between the classical word and its Romance descendants. Identifying and systematizing these errors can help improve future results by broadening the input with information related to sound changes. Before analysing the errors, a few preliminary points should be made. Romance lexicography as a whole is graphocentric - it considers the written, classical Latin (CL) lexical variants as the basis for the Romance vocabulary, even though it goes without saying that vernacular language's, oral par excellence, developed from an oral language, in our case Proto-Romance (PR) (Chambon, 2007). In the latest methodology used in Romance etymology, developed within the DERom project (Buchi and Schweickard, 2014), the etymological identifl cation is based strictly on the comparative grammar - reconstruction method, starting from the lexical forms that were used uninterruptedly in Romance languages. The lexemes attested in Classical Latin are only a written correlate, possibly further evidence of the existence of the form obtained by the methods of comparative historical linguistics.

In the light of these considerations, we find that some of the reconstructed variants classified as errors should actually be considered as positive results and evidence that the machine could work at the same level as a linguist applying traditional methods. By positive results instead of errors we mean cases - not a few - where the machine reconstructed exactly the phonetic form valid for oral Latin, at the expenses ofthe standard orthographical form as it is lemmatized in classical Latin dictionaries. Cases where the word obtained and the one given by the dictionary did not completely match were automatically considered as errors, although sometimes it was not a mistake as such. Therefore, there are a number of protoforms which, although they appear in the list as inadvertences, are, in fact, correct reconstructions.

In our analysis, we consider a predicted form to be correct if it matches exactly the attested form in classical Latin dictionaries, or if it represents a "near miss" that differs from the dictionary form in ways that are insigniflcant or that correspond to a correct adjustment to the reality of Latin pronunciation. For instance, reconstructed variants may reflect phonetic features specific to ProtoRomance: monophthongation (au > o, e.g. CL aurum vs. PR orum), assimillation (e.g. CL sub- vs. PR supp-), or the absence of etymological letters which were not pronounced (e.g. CL saxum vs. PR saccum). Another typical feature is the change of $x$ (pronounced [ks]) into $cc$ (pronounced [k:]) in ProtoRomance. We therefore considered the prediction saccum acceptable for saxum, even if the dictionary form contains $x$.

Moreover, we found that some of the reconstructed protoforms were not only acceptable variants of the CL etymon, but actually closer to the reconstructed PR form. In certain cases, the classical form itself represents a later standardization or orthographical convention that does not reflect vernacular pronunciation. In such situations, the model may reconstruct an older or more plausible spoken form.

We analyzed the errors for the best performing models, focusing on cases where the predicted form differs from the dictionary etymon. A significant proportion of the discrepancies were due to predictable and systematic sound changes. For example, monophthongation (au > o) or assimilation processes were regularly captured by the models. These predictions should not be considered errors from the perspective of historical linguistics, but rather correct reconstructions of ProtoRomance forms.

Nevertheless, there are also a number of genuinely erroneous reconstructions. These typically occur in cases of irregular sound change, sporadic analogical formations, or lexicographic issues in the source databases. In many cases, the model fails when the dataset does not provide enough examples to support a particular correspondence. Another source of errors comes from situations where a Latin etymon produced multiple reflexes in the same language; in such cases the model may choose the wrong variant.

Some of the shortcomings that we identified stem primarily from lexicographic omissions or mistakes, as well as in the imprecise methodology employed by the Ibero-Romance dictionaries consulted, namely the lack of any distinction between inherited and borrowed Latin words (Buchi and Dworkin,2019). This latter inaccuracy leads to a misinterpretation of the phonetic correspondences by the computer, given that only the inherited words, not the borrowed ones, underwent regular sound change. Therefore, if we put together Ro. roatd, Sp, rueda,Pt. roda, with Ro. rotalie, Sp. rotacion,Pt, rotagdo, the computer will not be able to correctly infer the correspondence tldld arrd tlt, will confuse it with tf also assuming the series d I d I d. Therefore, some reconstructions, especially in the case of words circumscribed only to IberoRomance languages, could not take this sound law into account (e.g., on the basis of Sp. miedo, Pt. medo, the computer could not reconstruct metLrs, but proposed medus, which is wrong). This kind of shortcomings will be easily overcome in the 1 future, firstly by clearly establishing, in the ProtoRom database, the inheritance-borrowing distinction, and secondly by extending the input provided to the computer with a number of basic phonetic laws.

Revised performance scores. Looking at the best reported predictions, we can apply the Iinguistic observations stated in the previous section and count which wrong predictions can be actually considered acceptable erors. Thus using these recovered predictions, the best models' scores would change as follows: . the orthographical Transformer accuracy for the ProtoRom dataset increases from 73j.% to 82,7% (135 out of the 575 original errors were recovered). . the Flan model's Coun accuracy on ProtoRom increases from 85.4% to 89.6% (90 out ofthe 311 original effors were recovered). . the Coun accuracy for the orthographical CRF model trained on ProtoRom-all5 increases from82.1Totog0,7% (11 out of the 23 original errors were recovered). 6


\section{Conclusion}
In this paper, we built a new dataset for automatic protoword reconstruction, consisting of 79,222 cognate sets from five Romance languages (Romanian, Italian, Spanish, Portuguese, French). This is to date the largest database of its kind, surpassing its predecessor which totals 8,799 cognate sets. We also proposed a series of comprehensive benchmarks ranging from deep-leaming approaches, using LLMs and Transformer-based architectures, to more classical algorithms such as CRFs, some of which achieved performances of more than 85% accuracy when allowing multiple generated reconstructions.

An in-depth linguistic analysis of the erroneous reconstructions was also performed using the predictions of the best performing models. This attempt shed some light on the various categories of mistakes, out of which several could be considered acceptable. When ignoring the aforementioned acceptable errors, we were able to surpass [ILLEGIBLE]% accuracies. We consider this an important distinction, since in our view similar tools should aim at assisting linguists in their scientific endeavours. Raw metrics are useful to compare computational methods, but, in order to assess their usability, a more qualitative inspection of the results should be performed. We hope through our research to incentivize further analysis.

As for future work, we are looking into an additional refinement of the current cognate sets, but also extending the database with more examples, including properly validated monolingual Latin reflexes that were excluded from our experiments for robustness sake. We also intend to expand past the proposed benchmarks with more novel approaches, relying on both the proposed dataset and the additional contents of its parent database, RoBoCoP.


\section*{Limitations}
One limitation of the current work stems from the automatic generation of the phonetic representations via a third-party library (eSpeak). Although this approach was employed successfully in previous studies, the quality of the generated phonemes has a higher variance when comparing high-resourced languages to lower-resourced ones (such as Romanian, or even Latin).

Also, in this study we used the generated phonetic forms without any extra preprocessing steps, in order to have a representation of the pronunciation that is as accurate as possible. Removing phonetic markers (such as stress markers) from these representations may turn the generation task into a somewhat easier one, since currently the phonetic models are tasked with predicting the stressed sounds too.

In terms of resources, existing LLMs are mostly targeting orthographical texts, making any reasonable attempt at generating phonetic ones very difficult.


\section*{Ethics Statement}
There are no ethical issues that could result from the publication of our work. Our experiments comply with all license agreements of the data sources used. We make the contents of our package available for research purposes upon request.


\section*{Acknowledgements}
This work was supported by a mobility project of the Romanian Ministery of Research, Innovation and Digitization, CNCS - UEFISCDI, project number PN-IV-P2-2.2-MC-2024-0461, within PNCDI IV.


\begin{thebibliography}{99}

\bibitem{akavarapu-bhattacharya-2023}
V. S. D. S. Matresh Akavarapu and Arnab Bhattacharya'. 2023. Cognatetransformer for automated phonologi-cal reconstruction and cognate reflex prediction. In Proceedings of the 2023 Conference on Empirical Methods in Natural Innguage Processing, EMNLP 202j, Singapore, December 6-10, 2023, pages 6852-6862. Association for Computational Linguistics.

\bibitem{alekseyenko-et-al-2012}
Alexander V. Alekseyenko, Quentin D. Atkinson, Remco Bouckaert, Alexei J. Drummond, Michael Dunn, Russell D. Gray, Simon J' Greenhill, Philippe Lemey, and Marc A. Suchard. 2012. Mapping the origins and expansion ofthe Indo-European language family. Science, 337 :957 -960.

\bibitem{atkinson-et-al-2005}
Quentin Atkinson, Geoff Nicholls, David Welch, and Russell Gray. 2005. From words to dates: water into wine, mathemagic or phylogenetic inference? Transactions of the Philological Society, 103(2):193-219.

\bibitem{atkinson-2013}
Quentin D Atkinson. 2013. The descent of words. Proceedings of the National Academy of Sciences, 110(11):4159-4160.

\bibitem{bouchardcote-et-al-2009}
Alexandre Bouchard-C6t6, Thomas L. Griffiths, and Dan Klein. 2009. Improved reconstruction of pro-tolanguage word forms. In Proceedings of Human language Technologies: The 2009 Annual Confer-ence of the North American Chapter of the Associ-ation for Computational Lin guistic s (NAAC L 2009 ), pages 65-73, Boulder, Colorado. Association for Computational Linguistics.

\bibitem{bouchardcote-et-al-2013}
Alexandre Bouchard-C6t6, David Hall, Thomas L. Grif-fiths, and Dan Klein. 2013. Automated reconstruc-tion of ancient languages using probabilistic mod-els of sound change. Proc. Natl. Acad. Sci. USA, tt0(tt):42244229.

\bibitem{brown-et-al-2008}
Cecil H Brown, Eric W Holman, Soren Wichmann, and Viveka Velupillai. 2008. Automated classification of the world's languages: a description of the method and preliminary results. Language Typology and Universals, 6 I (4) :285-308.

\bibitem{buchi-dworkin-2019}
Eva Buchi and Steven N Dworkin. 2019. Etymology in romance. Oxford Research Encyclopedia of Linguis'tics.

\bibitem{buchi-schweickard-2010}
Iiva Buchi and Wolfgang Schweickard. 2010. A h recherche du protoroman: objectifs et m6thodes du futur dictionnaire 6tymologique roman (d6rom). In XXVe CILPR Congrbs Intemational de Linguistique et de Philologie Romanes: Innsbruck, j-8 septembre 2007 , pages 6-61 , de Gruyter.

\bibitem{buchi-schweickard-2014}
6va Buchi and Wolfgang Schweickard. 2014. Diction-naire 4tymologique rornan @ERorn): gdnise, mdth'odes et rdsultats, volume 381. Walter de Gruyter GmbH & Co KG.

\bibitem{buchi-schweickard-2020}
Eva Buchi and Wolfgang Schweickard. 2020' Diction-naire itymologique roman @ERom) i: entre idioro-man et protoroman,vohtme 443. Walter de Gruyter GmbH & Co KG.

\bibitem{campbell-2013}
Lyle Campbell. 2013. Historical Linguistics. Edin'burgh University Press.

\bibitem{chambon-2007}
Jean-Pierre Chambon. 2007. Remarques sur la gram-maire compar6e-reconstruction en linguistique ro-mane (situation, perspectives). Mdmoires de la So-ciiti de linguistique de Paris, 15,,57-'72.

\bibitem{chung-et-al-2024}
Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph,YiTay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2024. Scaling instruction-finetuned language models. I o urnal of M achine Le arnin g Re s e arch, 25 (70) : I -5 3'

\bibitem{ciobanu-dinu-2018}
Alina Maria Ciobanu and Liviu P. Dinu. 2018. Ab ini-tio: Automatic Latin proto-word reconstruction. In Proceedings of the 27th Internq'tional Conference on C omp utational Lin g ui stic s ( C O UN G 2 0 I 8 )' pages 1604-1614, Santa Fe, New Mexico, USA. Associa-tion for Computational Linguistics.

\bibitem{ciobanu-dinu-2019}
Alina Maria Ciobanu and Liviu P. Dinu. 2019. Auto-matic identification and production of related words for historical linguistics. C omputational Lin guistics, 45(4):667J04.

\bibitem{dinu-ciobanu-2014}
Liviu P. Dinu and Alina Maria Ciobanu. 2014. Building a dataset of multilingual cognates for the Romanian lexicon. In Proceedings of the Ninth International Conference on Language Resources and Evaluation, LREC 2014, Reykjavik, Iceland, May 26-j1, 2014, pages 1038-1043. European Language Resources Association (ELRA).

\bibitem{dinu-et-al-2023}
Liviu P. Dinu, Ana Sabina Uban, Alina Maria Cristea, Anca Dinu, Ioan-Bogdan Iordache, Simona Georgescu, and Laurentiu 7-aicas. 2023. Robocop: A comprehensive Romance borrowing cognate pack-age and benchmark for multilingual cognate iden-tification. In Proceedings of the 202j Conference on Empirical Methods in Natural l,anguage Process'ing, EMNLP 2023, Singapore, December 6-10,2023, pages 7610--7629. Association for Computational Linguistics.

\bibitem{dinu-et-al-2024a}
Liviu P. Dinu, Ana Sabina Uban, Anca Dinu, Ioan-Bogdan Iordache, Simona Georgescu, and Laurentiu Zoicas.2024a. It takes two to borrow: a donor and a recipient. who's who? ln Findings of the Association fo r C omp utati onal Lin g ui s tic s, ACL 2 02 4, B an gkok, Thailand and virtual meetinS, August 1l-16, 2024, pages 6023-6035. Association for Computational Linguistics.

\bibitem{dinu-et-al-2024b}
Liviu P. Dinu, Ana Sabina Uban, Ioan-Bogdan Iordache, Alina Maria Cristea, Simona Georgescu, and Lauren-tiu Zoicas. 2024b. Pater incertus? there is a solution: Automatic discrimination between cognates and bor-rowings for Romance languages. In Proceedings of the 2024 Joint International Conference on Compu-tattonal Lingutstics, lnnguage Resources and Evalu-ation (LREC-COUNG 2024), pages 12651-12667, Torino, Italia. ELRA and ICCL.

\bibitem{dunn-2015}
Michael Dunn. 2015. Language phylogenies. The Rout-ledge handbook of historical linguistics, pages 190-2lr.

\bibitem{epps-2014}
Patience Epps. 2014. Historical linguistics and socio-cultural reconstruction . In The Routledge H andb o ok of Historical Linguistics, pages 579-597. London: Routledge.

\bibitem{fourrier-2022}
Cl6mentine Fourrier. 2022. Neural Approaches to His-toric al Word Reconstruction. (Approches Neuronale s pour la Reconstruction de Mots Historiques). Ph.D. thesis, PSL University, France.

\bibitem{georgescu-georgescu-2020}
Simona Georgescu and Theodor Georgescu. 2020. Fr.<<trouver>>, occ.<trobaD etc.: un dossier 6ty-mologique ouvert d nouveau. Revue de linguistique romane,84(l):83-98.

\bibitem{he-et-al-2023a}
Andre He, Nicholas Tomlin, and Dan Klein.2023a. Neural unsupervised reconstruction of protolanguage word forms. In Proceedings' of the 6lst Annual Meet-ing of the Association for Computational Linguis-tics (Volume l: Long Papers), ACL2023, Toronto, C anada, July 9 - 1 4, 2 02 3, pages 1 636-1 649. Associa-tion for Computational Linguistics.

\bibitem{he-et-al-2023b}
Andre He, Nicholas Tomlin, and Dan Klein.2023b. Neural unsupervised reconstruction of protolanguage word forms. In Proceedings of the 6lst Annual Meet-ing of the Association for Computational Linguistics (Volume l: Long Papers), pages 1636-1649, Toronto, Canada. Association for Computational Linguistics.

\bibitem{heggarty-2015}
Paul Heggarty. 2015. Prehistory through language and archaeology. InThe Routledge Handbook of Histori-cal Linguistics, pages 598426. Routledge.

\bibitem{hewson-1973}
John Hewson. 1973. Reconstructing prehistoric lan-guages on the computer: The triumph of t}le elec-tronic neogrammarian. h COUNG 1973 Volume l: C omputat ion al And M athematic al Lin gui s tic s : P ro -ceedings of the Intemational Conference on Compu-tational Linguistics.

\bibitem{kim-et-al-2023}
Young Min Kim, Kalvin Chang, Chenxuan Cui, and David R. Mortensen. 2023. Transformed protoform reconstruction. In Proceedings of the 6lst Annual Meeting of the Association for Computational Lin-guistics (Volume 2: Short Papers), pages 24-38, Toronto, Canada. Association for Computational Lin-guistics.

\bibitem{kondrak-2002}
Grzegorz Kondrak. 2002. Algorithms for language re-construction, phd thesis, university of toronto.

\bibitem{list-forkel-hill-2022}
Johann-Mattis List, Robert Forkel, and Natha' n Hill. 2022. A new framework for fast automated phono-logical reconstruction using trimmed alignments and sound correspondence pattems. In 3rd International Workshop on Computational Approaches to Histori-cal l,ttnguage Change 2022, pages 89-96. Associa-tion for Computational Linguistics (ACL).

\bibitem{lowe-mazaudon-1994}
John B Lowe and Martine Mazaudon. 1994, The re-construction engine: a computer implementation of the comparative method. Computational Linguistics, 20(3):381417.

\bibitem{mallory-adams-2006}
James P Mallory and Douglas Q Adams. 2006. The Oxford Introduction to Proto-Indo-European and the P ro t o - I ndo - Euro p e an Wo rld. Oxf ord University Press on Demand.

\bibitem{meloni-et-al-2021}
Carlo Meloni, Shauli Rafogel, and Yoav Goldberg. 2021. Ab antiquo: Neural proto-language recon-struction. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-l l, 2021, pages 446U4473. Association for Computa-tional Linguistics.

\bibitem{needleman-wunsch-1970}
Saul B. Needleman and Christian D. Wunsch. 1970. A general method applicable to the search for simi-larities in the amino acid sequence of two proteins. Journal of Molecular Biology, 48(3):443153.

\bibitem{oakes-2000}
Michael P Oakes. 2000. Computer estimation of vo-cabulary in a protolanguage from word lists in four dau ghter I an guages. J ournal of Quantit ativ e Lin gui s -tics,7 (3):233-243.

\bibitem{pagel-1999}
Mark Pagel. 1999. Inferring the historical pattems of biolo gical evolution. N ature, 40 | (67 5 6) :87 7-8 84.

\bibitem{pagel-et-al-2013}
Mark Pagel, Quentin D Atkinson, Andreea S. Calude, and Andrew Meade. 2013. Ultraconserved words point to deep language ancestry across Eurasia. Proceedings of the National Academy of Sciences, 110(21):847 r-8476.

\bibitem{reinheimer-ripeanu-2001}
Sanda Reinheimer-Ripeanu. 200L Lingvistica roman-icd. lexic, morfologie, fonetic5. BucureSti: All.

\bibitem{sims-williams-2018}
Patick Sims-Williams. 20 1 8. Mechanising historical phonology. Trans actions of the P hilolo gical Society, I 16(3):555-573,

\bibitem{wright-2002}
Roger Wright.2002. A Sociophilological Study of l-ate Latin. Brepols Publishers.

\end{thebibliography}


\appendix
\section{Appendix}

\subsection{Hyperparameters and infrastructure}
[MISSING]

\subsubsection{Conditional Random Fields}
\subsubsection{Conditional Random Fields}

The CRF algorithm relies on the Mallet library implementation, version 2.0.85.

The only training hyperparameters that were tuned are:
\begin{itemize}
\item the window size $w \in {1,2,3,4,5}$
\item the number of CRF training iterations $i \in {25,50,100}$
\end{itemize}

For the orthographical and phonetic training scenarios, the hyperparameters were selected by training on the ProtoRom-all5 training split, and evaluating on the dev one. Because of the long training time on the complete ProtoRom dataset, we ended up reusing the same hyperparameters found during the previous step.

The selected hyperparameters are as follows:

for the orthographical CRF:
\begin{itemize}
\item Spanish: $w:1,\ i:100$
\item French: $w:4,\ i:100$
\item Italian: $w:2,\ i:100$
\item Portuguese: $w:1,\ i:100$
\item Romanian: $w:1,\ i:100$
\end{itemize}

for phonetic CRF:
\begin{itemize}
\item Spanish: $w:1,\ i:100$
\item French: $w:4,\ i:100$
\item Italian: $w:3,\ i:100$
\item Portuguese: $w:1,\ i:100$
\item Romanian: $w:4,\ i:100$
\end{itemize}

Training hyperparameters used for both orthographical and phonetic experiments:

\begin{itemize}
\item orthographical CRF for ProtoRom-all5 (including grid search): 15 hours
\item phonetic CRF for ProtoRom-all5 (including grid search): 22 hours
\item orthographical CRF for ProtoRom: 102 hours
\end{itemize}


\subsubsection{Probabilistic LSTM}

We conducted experiments with the same architecture used in He et al.\ 2023b (GitHub repository [ILLEGIBLE]).

For the training we used the following configuration:
\begin{itemize}
\item lstm hidden size: 64
\item context window: 10
\item number of epochs: 30
\item number of rounds: 8
\item learning rate: 0.01
\item optimizer: Adam
\item weight decay: 0.01
\end{itemize}

All the training was done on an Apple M2 Pro chip and the total training time was 2 hours.

In terms of trainable parameters:
\begin{itemize}
\item orthographical experiments: 817,869 parameters
\item phonetic experiments: 854,877 parameters
\end{itemize}

The training was done using an RTX 2080 Ti GPU. Training time:
\begin{itemize}
\item ProtoRom-all5 dataset: 2.5 hours
\item ProtoRom dataset: 5 days
\end{itemize}


\subsubsection{TFansformer model}

The architecture we used in our experiments is the same as Kim et al.\ 2023 (GitHub repository\textsuperscript{7}). The hyperparameters used for both orthographical and phonetic experiments are as follows:
\begin{itemize}
\item embedding size: 128
\item number of encoder layers: 3
\item number of decoder layers: 3
\item number of attention heads: 8
\item feed forward layer size: 128
\item dropout: 0.202
\end{itemize}

Training hyperparameters used for both orthographical and phonetic experiments:
\begin{itemize}
\item number of epochs: 200
\item batch size: 1
\item learning rate: 0.00013
\item loss: cross entropy loss
\item optimizer: Adam
\item scheduler: polynomial decay scheduler with warmup
\item warmup epochs: 50
\item weight decay: 0
\end{itemize}

In terms of trainable parameters:
\begin{itemize}
\item orthographical experiments: 817,869 parameters
\item phonetic experiments: 854,877 parameters
\end{itemize}

The training was done using an RTX 2080 Ti GPU. Training time:
\begin{itemize}
\item ProtoRom-all5 dataset: 2.5 hours
\item ProtoRom dataset: 5 days
\end{itemize}


\subsubsection{Flan-T5}
\subsubsection{Flan-T5}

Flan-T5 was trained using early stopping based on the Coul metric on the validation set.
The conflguration used and optimal hyperparam-
eters are as follows:
\begin{itemize}
\item batch*size:50
\item epochs:300,
\item learning_rate: le-4,
\item patience:3,
\item max_seq_len 64,
\item weight_decay: 1e-5,
\item warmup_steps: 500,
\item lr_scheduler_type: polynomial,
\item num_return_sequences: 10,
\item num_beams: 10,
\item classif,er_dropout: 0.0,
\item d-ff:2048,
\item d kv:64,
\item d_model: 768,
\item decoder_start_token_id: 0,
\item dense_act_fn: gelu_new,
\item dropout_rate:0.1,
\item eos_token_id: 1,
\item feed_forward_proj: gated-gelu,
\item initializer_factor: 1.0,
\item is_encoder_decoder: true,
\item is_gated_act: t1ue,
\item layer_norm_epsilon: 1e - 06,
\item max_length: 64,
\item model_type: t5,
\item n_positions: 512,
\item num_beams: 10,
\item num_decoder_layers: 12,
\item num_heads: 12,
\item num_layers: 12,
\item num_return_sequences: 10,
\item output_past: true,
\item pad_token_id:0,
\item relative_attention_max_distance: 128,
\item relative_attention_num_buckets: 32,
\end{itemize}


\end{document}
=====END FILE=====
