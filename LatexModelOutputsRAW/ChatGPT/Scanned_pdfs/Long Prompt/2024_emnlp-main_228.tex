=====FILE: main.tex=====
\documentclass[11pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}

\title{Dissecting Fine-Tuning Unlearning in Large Language Models}
\author{
Yihuai Hong$^{1,4}$ \and
Yuelin Zou$^{2}$ \and
Lijie Hu$^{3}$ \and
Ziqian Zeng$^{1}$ \and
Di Wang$^{3}$ \and
Haiqin Yang$^{4}$
}
\date{}

\begin{document}
\maketitle

\begin{abstract}
Fine-tuning-based unlearning methods prevail for preventing targeted harmful, sensitive, or copyrighted information within large language models while preserving overall capabilities. However, the true effectiveness of these methods is unclear. In this work, we delve into the limitations of fine-tuning-based unlearning through activation patching and parameter restoration experiments. Our findings reveal that these methods alter the model’s knowledge retrieval process, providing further evidence that they do not genuinely erase the problematic knowledge embedded in the model parameters. Instead, the coefficients generated by the MLP components in the model’s final layer are the primary contributors to these seemingly positive unlearning effects, playing a crucial role in controlling the model’s behaviors. Furthermore, behavioral tests demonstrate that this unlearning mechanism inevitably impacts the global behavior of the models, affecting unrelated knowledge or capabilities. The code is released at \url{[https://github.com/yihuaihong/Dissecting-FT-Unlearning}](https://github.com/yihuaihong/Dissecting-FT-Unlearning}).

\end{abstract}

\section{Introduction}
Large language models (LLMs), due to their exten-sive pre-training corpora, often inadvertently learn harmful, sensitive, or copyright-protected knowl-edge (Chang et al., 2023a; Mozes et al., 2023 ; El-dan and Russinovich,2023; Ye et al., 2022). Con-sequently, recent research has focused on develop-ing efficient unlearning methods as a post-training technique to selectively unlearn the speciflc knowl-edge (Blanco-Justicia et a7.,2024; Liu et al., 2024). Currently, the core mechanism of these unlearn-ing methods involves finetuning (Eldan and Russi-novich, 2023; Iang et al., 2023; Yao et al., 2024; Rafailov et a1.,2023), with corresponding adjust-ments and designs in the loss function to facilitate the unlearning process.

Although earlier investi-gations (Hong et al., 2024; Lee et al., 2024a) have proven that these methods are ineffective at com-pletely erasing model-embedded knowledge, the factors contributing to the misleading success of these techniques remain unclear.

Therefore, in this paper, we try to unveil why existing fi netuning-based unlearning methods per-form wellin behavioral tests by aralyzingthe mech-anisms of internal knowledge recall and flow within models (Meng eta1.,2022; Pochinkov and Schoots, 2024; Geva et al.,202la). Speciflcally, we investi-gate which components or parameters carry these unlearning effects. We design activations patching and parameters restoration experiments in three set-tings, aiming to independently study the impact of unlearning methods on the coefficients and value vectors in the MLPs, as well as on the attention components' states. Our findings further confirm that the methods do not truly alter the knowledge embedded in the value vectors of MLPs, and re-veal that they will change how they extract and transfer this knowledge through modifications in the coefficients of MLPs and attention components during unlearning. Notably, the coefflcients pro-duced by the MLP in the final layers are primarily responsible for achieving the unlearning effects of finetuning-based methods.

We further test the global behavior impact of these fine-tuning-based unlearning methods on LLaMA2-7B-chat (Touvron et a1.,2023) and OLMo-7B (Groeneveld et al., 2024) by implement-ing them on the respective pretraining datasets of both models, aiming to more closely simulate the erasure of knowledge acquired during the pretrain-ing process. We discover that while these methods appear to effectively unlearn target knowledge, they also inevitably affect the output and behavior re-lated to unrelated knowledge. This unintended con-sequence stems from the fact that these approaches are based on altering the model's internal knowl-edge retrieval mechanisms, thereby impacting its global behavior and overall performance.

Ultimately, we conclude once again that cur-rent fine-tuning-based unlearning methods cannot completely erase sensitive knowledge embedded in models, particularly within the MLps, instead adjusting the mechanisms by which the model re-trieves knowledge. These methods are vuinera-ble to recovery attacks in components' activations and unsuitable for true unlearning. We advocate for future unlearning evaluations to concentrate on precise measurement of both the actual storage of targeted knowledge within the model's entke pa-rameter set and the specific dynamics of how this knowledge is retrieved and utilized.


\section{Background and Related Work}
\subsection{Unlearning in Large Language Models}
Since
large language models learn knowledge from dif-
ferent domains and corpora during the pre-training
process, it is often found that they contain harm-
ful, sensitive or private knowledge, leading to the
possibility that language models produce output
behaviors containing corresponding sensitive or
harmful information (Liu et a1.,2024; Chang et al.,
2023 a; Mozes et al., 2023). Therefore, unlearning
emerges as a timely and important post-pretraining
processing method for LLM safety. Currently, the
vast majority of LLM unlearning methods use fine-
tuning as the primary operational approach. In
terms of classifying them by different training ob-
jectives, they include gradient direction control
(Jang et a7.,2023; Yao et a1.,2024,2023) andpref-
erence optimization methods (Rafailov et a1.,2023;
Zhao et a1.,2024; Lee et a1.,2024b). In terms of
classifying them by the parameters covered during
training, they include full parameters fine-tuning
(Eldan and Russinovich, 2023; Jang et al., 2023;
Yao et a1.,2024; Rafailov eta1.,2023), sparse fine-
tuning (Chang et a1.,2023b; Stoehr et a1.,2024),
and parameter-efficient fine-tuning (Ll et a1.,2024;
Chen and Yang,2023). Additionally, there are
also a few knowledge editing methods (patil et al.,
2024). We present the specific logic details of each
method in $A.

\subsection{Knowledge Storation in Large Language Models}
Studying how knowledge is stored, trans-
ferred, and extracted in LLMs has always been
an important direction in the research of LLM,s in-
terpretability (Meng et a1.,2022 Geva et al, Z\Zlb;
Sukhbaatar et al., 2015; Geva et al., 2023). It is
known that in transformer-based language mod-
els, the MLP is a crucial component for storing
the model's factual knowledge, and its sub-layers
can be viewed as key-value memories (Geva et al.,
2021b). To be specific, the first layer*\footnote{-Currently, in most decoder-only models such as GpT-
2 (Radford et a1.,2019) and GpT-J(Chen er aI., Z02l), the
MLP component has two layers, while in LLaMA (Touvron
et at.,2023) it has three layers. However, we can still consider
LLaMA's first two layers together as the key matrices, with
their output serving as the coeffic[ILLEGIBLE]} of MLp sub-
layers can be viewed as a matrix tr26 formed by
key vectors {kr , kz, . . . ,kn}, used to capture a set
of patterns in the input sequence, and ultimately
outputting the coefficient scores. The second layer
can be viewed as a matrix Wy formed by value
vectors {ut,rz, . . . ,vn}, with each value vector
containing the corresponding factual knowledge
(represented through token distributions). Finalty,
the MLP's output can be defined as the sum of value
vectors weighted by their memory coefficients:
\begin{equation}
[ILLEGIBLE]
\end{equation}
where Ml represents the output of the MLp in
the transformer's L-th layer for an input hidden
state xt^ at that layer with the paramet ers, Wla
and W$ 6 pnxd. / is a non-linearity functioni.
m/ € IR." represents the coefficient scores. The
dimension size of hidden states is d and it is n for
the intermediate MLP.

In addition to the MLP, primarily responsible for
knowledge storage, the attention component is cur-
rently considered the main component responsible
for knowledge transfer and extraction in language
models (Geva et a1.,2023). Here, we will not go
into detail about its specific structure but only study
the impact it has on knowledge extraction. The fi-
nal computation formula for the hidden states in
the language model is defined as:
\begin{equation}
[ILLEGIBLE]
\end{equation}
where Xt, Mt and Al represent the hidden states,
MLP's output, and the attention component,s out-
put in the transformer's (,-thlayer, respectively.


\section{Patching Investigation}

\subsection{Hypothesis and Experimental Design}
Based on Eq.\ (1) and Eq.\ (2), we hypothesize that there are three main reasons why the current fine-tuning-based unlearning methods appear successful in behavioral tests and seem to suggest that true unlearning has been achieved:
\begin{enumerate}
\item The coefficients $m^{l}$ are changed after fine-tuning, leading to a change in the activations of the MLPs;
\item The value vectors $W^{l}*{v}$ in MLPs are changed, causing a change in the knowledge they contain;
\item The change that happens in attention components caused the model's focus and the corresponding information extracted by these attention components $A^{l}$ to change, thus reducing the target knowledge-related information in the output.
\end{enumerate}
Here, for the sake of simplicity and better understanding, we continue to use the definitions of $m^{l}$, $W^{l}*{v}$, and $A^{l}$ as given in Eq.\ (1) and Eq.\ (2) in the following. We ignore the minor effects caused by other components or parameters, such as the language model's unembedding matrix and the normalization layers. Based on the possible reasons described above, on the unlearned model, we conduct three different sets of activation patching or components' parameter restoration experiments, trying to recover the output of the target knowledge in the unlearned model. The specific operation process is as follows:
\begin{enumerate}
\item In the first set of experiments, we restore the coefficient scores $m^{l}$ corresponding to each MLP component, layer by layer, in the language model, without making any intentional changes to the value vector parameters $W^{l}*{v}$ of the MLPs or the attention components' states $A^{l}$ in any layer.
\item In the second set of experiments, we restore the parameters of value vectors $W^{l}*{v}$ in MLPs layer by layer, recovering the knowledge they originally contained. In this process, we avoid making intentional changes to the unlearned model's original coefficients $m^{l}$ and the attention components' states $A^{l}$.
\item In the third set of experiments, we restore the original attention components' states $A^{l}$, but without intentionally altering the MLPs' coefficient scores $m^{l}$ or the value vectors' parameters $W^{l}*{v}$, only studying the impact brought by the attention components which are responsible for extracting and transferring knowledge.
\end{enumerate}
To evaluate the extent of knowledge restoration, we propose the metric of Knowledge Recovery Score (KRS):
\begin{equation}
\mathrm{KRS} = 1 - [ILLEGIBLE]
\end{equation}
where the losses are the average of $\mathrm{MSE}(\cdot)$ on [ILLEGIBLE] and [ILLEGIBLE] and on [ILLEGIBLE] and [ILLEGIBLE], respectively. $\mathrm{MSE}(\cdot)$ represents the mean squared error (MSE) loss function. $L$, $L*{u}$, and $L_{u\to r}$ are the logit distribution of the subsequent token produced by the vanilla model, unlearned model, and unlearned-then-recover model, respectively. The average loss is computed on the next $f$ generated tokens on $N_{y}$ knowledge-related questions. Finally, if KRS approaches 1, it indicates [ILLEGIBLE] and [ILLEGIBLE] that are nearly consistent, representing a higher degree of knowledge recovery. Conversely, a lower KRS suggests a lower degree of that.

\subsection{Activation Patching and Parameters Restoration Experiments}
We conduct the experiments on two recent LLMs, LLaMA2-7B-chat (Touvron et al., 2023) and OLMo-7B (Groeneveld et al., 2024). We apply two example finetuning-based unlearning methods, DPO (Rafailov et al., 2023) and Gradient Difference (Yao et al., 2024), to perform unlearning on the large language models and calculate the average KRS scores. Inspired by (Eldan and Russinovich, 2023), which tries to unlearn the concept knowledge of ``Harry Potter'' in language models, we extend this experiment by selecting 10 well-known concepts per model from the ConceptVectors Benchmark (Hong et al., 2024), which is a collection of concepts that language models are well-acquainted with and have substantial knowledge about. Examples of them are provided in Table 2 of [ILLEGIBLE]. For the unlearning training, we use the texts containing the corresponding concepts from Redpajama+ and Dolma (Soldaini et al., 2024). Redpajama is a replication of the pretraining corpus for the LLaMA model, while Dolma is the open-source pre-training dataset for the OLMo model. Detailed information is provided in [ILLEGIBLE]. So here we can ensure that the knowledge to be unlearned was at least seen by the model during the pre-training process, and that the training data used more broadly covers the textual sources from which the model acquired the corresponding knowledge about certain concepts.

After obtaining the unlearned model, we follow the steps mentioned in the hypothesis to perform activation patching and parameter restoration experiments on the unlearned models. To calculate the Knowledge Recover Scores, we set $f$ to 30 and $N_{y}$ to 10, indicating the generation of the next 30 tokens and the selection of 10 questions related to each concept. To make the recovery effects more pronounced and the whole process easier to observe, we adopt techniques from (Meng et al., 2022, 2023) which implemented causal mediation, setting the size of the recovery window to five. This allows us to observe the average effects of recovering five consecutive layers at a time. Details can be found in [ILLEGIBLE].

The specific results are shown in Fig.\ 1. From our analysis, surprisingly, we observe that when we solely recover the parameters contained in the value vectors of each layer in the unlearned model without interfering with the coefficients or attention components' states, the recovery of the target knowledge is negligible (the KRS scores are all below 0.001). This holds regardless of which layer is recovered, and regardless of the specific model being considered.

However, when recovering the attention components' states in the intermediate layers (from the 15th layer onward) or deeper layers (from the 27th layer onward), we can observe that the average KRS for both models has increased to exceed 0.3 and 0.4, respectively, indicating that a significant portion of the corresponding knowledge has been recovered. What's more, restoring the coefficients of the MLPs in the intermediate layers (from the 20th layer onward) and deeper layers (from the 29th layer) also yields impressive knowledge recovery effects.

The layers at which the scores start to increase under the two settings generally align closely with the observation by Geva et al.\ (2023) that the MLP modules recall knowledge in intermediate layers, and the attention components mostly start to extract and transfer information in the deeper layers, or after the model has completed the relevant knowledge recall. We also tried simultaneously recovering the coefficients and attention states and found that the model can achieve much greater knowledge recovery with the peak KRS score exceeding 0.9 on both models.

Additionally, it is noteworthy that, simply restoring the coefficient scores of the MLP outputs from the last two or three layers can significantly elevate the KRS of the unlearned LLaMA and OLMo models to 0.8 or above. This suggests that the coefficient scores of the MLPs in the last layers might play a crucial role in the final behavior results of the LLM. To better isolate the effects of restoring $m^{l}$, $W^{l}_{v}$, and $A^{l}$ individually and support the above argument, we present a more rigorous patching and restoration experiment in [ILLEGIBLE], with the corresponding results shown in Figure 3. Ultimately, we found that the restoration of the attention states also contributed to the coefficients of the MLP in the final layers, further confirming that these coefficients carry the primary role of achieving the effects of finetuning-based unlearning. It also indicates that fine-tuning largely adjusts the model's behavior by modifying the coefficients of the deep MLP layers, likely because this enables faster adaptation compared to other knowledge adjustment mechanisms, such as altering knowledge encoded in the MLP itself. This phenomenon and the potential defensive strategy have not been discussed in the previous literature, warranting further investigation in future studies.

Overall, these results all further confirm that the finetuning-based unlearning methods essentially do not modify the model knowledge contained in the value vectors, but adjust the way knowledge is called during the fine-tuning process, either by adjusting the coefficients to modulate the MLP activation or by adjusting the attention to extract and transfer knowledge.

\begin{figure}[t]
\centering
[ILLEGIBLE]
\caption{Results of KRS on LLaMA and OLMo under three activations patching or parameters restoration settings. We also included another setting that restores both attention and coefficients to compare the final outcomes.}
\end{figure}


\section{Global Negative Effect of Fine-Tuning Unlearning}
In the previous section, we demonstrated that these finetuning-based methods alter the model's final behavior by adjusting the MLP output coefficients in the final layers. Therefore, we hypothesize that this behavioral change will has a global effect, potentially impacting the output of unrelated knowledge as well. In this section, we verify this hypothesis through the following experiments.

We apply four fine-tuning-based unlearning methods to the concepts used in \S3 on their pretraining text sources (from RedPajama and Dolma) with the goal of erasing the learned knowledge during pretraining through a reverse process. These methods are as follows: DPO (Rafailov et al., 2023), NPO (Zhao et a1., 2024), NPO+KL (Zhao et al., 2024) and Gradient Difference (Yao et a1., 2024). The details of these baselines and data statistics are shown in \S A and \S B. We evaluate the unlearning effectiveness of these methods on the concepts' related QA pairs and the unlearning impact on unrelated QA pairs, reporting the average scores of BLEU (Papineni et a1., 2002) by comparing the model's response before and after unlearning. In Figure 2, we report their performance at the end of each training epoch respectively.

\begin{figure}[t]
\centering
[ILLEGIBLE]
\caption{Unlearning testing results on LLaMA and OLMo for each training epoch.}
\end{figure}

We can observe that for finetuning-based methods, as the number of training epochs increases, aiming to achieve a lower target QA BLEU score, the corresponding unrelated QA BLEU score also decreases accordingly, exhibiting a positive correlation. This suggests that the impact of finetuning-based methods on the model's output behavior is global. While unlearning the target knowledge, they inadvertently alter the output behavior or manner for unrelated knowledge to a certain degree.

Furthermore, we conduct experiments on the pretraining datasets of two models, to test the models' capabilities after unlearning, verifying that in addition to unlearning the corresponding knowledge, fine-tuning-based methods that by altering the way the model accesses knowledge, will significantly impair the model's other unrelated capabilities, causing a certain degree of capability degradation.


\section{Discussion and Conclusion}
In this paper, we investigated the internal mechanisms behind the seeming success of existing fine-tuning-based unlearning methods. Our analysis revealed that these methods do not genuinely erase the learned knowledge from a model. Instead, they primarily modify the model's knowledge retrieval process, resulting in changes in the model's final behavior. Specifically, we demonstrated that the MLP output coefficients in the final layers of the model play a dominant role in achieving the apparent unlearning effects. Additionally, we found that the attention components in deeper layers also contribute to these coefficient changes, further influencing the model’s output.

Moreover, through behavioral tests, we verified that this mechanism inevitably introduces a global negative effect on the model's behavior, impacting unrelated knowledge and capabilities. This unintended consequence highlights a key limitation of fine-tuning-based unlearning approaches: while they can reduce the model’s tendency to output target knowledge, they do so by altering broader behavior patterns rather than selectively removing knowledge.

Ultimately, our findings reinforce the conclusion that current fine-tuning-based unlearning methods are insufficient for true unlearning. We advocate for future research to focus on more precise evaluations that measure both the actual storage of targeted knowledge within the model’s parameters and the dynamics of how this knowledge is retrieved and utilized during generation.


\section{Limitations}
In the experiments detailed in \S3, we have disregarded the potential unlearning impact caused by parameter changes in other model components during the fine-tuning process. This decision is based on the observation that the impact of such changes appears to be minimal. For instance, during our parameter comparison analysis, we found that the changes in the unembedding matrix and normalization layer parameters resulted in cosine similarity values above 0.999. This suggests that the modifications to these components are quite small in magnitude.

However, it remains unclear whether even such minimal parameter changes can still have any meaningful effect on the model's overall behavior and knowledge. Further verification and analysis would be needed to conclusively determine the extent to which these ancillary parameter updates might influence the unlearning outcome.

\section*{Acknowledgements}
The work was fully supponed by the IDEA Information and Super Computing Cenrre (ISCC), National Natural Science Foundation of China (Grant No. 62406Lt4), the Guangzhou Basic and Applied Basic Research Foundation (Grant No. 2023A04J1687), and the Fundamenral Research Funds for the Central Universities (Grant No. 2024ZYGXZR074). Di Wang and Lijie Hu are supported in part by the funding 8,{5/1/1689-01-01, URF/1/4663-01-01, REV1t5232-01-01, REAU5332-01-0 1, and URF/1/5508-01-01 from KAUST, and funding from KAUST - Center of Excellence for Generative AI, under award number 5940.


\section*{References}
References
Alberto Blanco-Justicia, Najeeb Jebreel, Benet Man-
zanares, David Sdnchez, Josep Domingo-Ferrer,
Guillem Collell, and Kuan Eeik Tan. 2024. Digi-
tal forgetting in large language models: A survey of
unlearning methods. Preprint, arXiv:2404.02062.
Kent K. Chang, Mackenzie Hanh Cramer, Sandeep
Soni,
and David Bamman, 2023a, Speak, memory: An
ar-
chaeology of books known to chatGPT/GPT-4. In
The 2023 Conference on Empirical Methods in Natu-
ral Language sing.
P roc e s
Ting-Yun Chang, Jesse Thomason, and Robin
Jia.
2023b. Do localization
methods actually local-
ize in llms?
memorized data arXiv preprint
arXiv:2311.09060.
Jiaao Chen and Diyi Yary.2023. Unlearn what
you
want to forget: Efficient pro-
unleaming for llms. In
ceedings of the 2023 Conference on Empirical Meth-
ods in Natural lnnguage Processing,pages 12041-
12052.
Mark Chen, Jerry Tworek, Jun,
Heewoo eiming
Yuan, Henrique Ponde de Oliveira Pinto, Ka-
Jared
plan, Harri Edwards, Yuri Burda, Nicholas
Joseph,
Greg Brockman, et al. 2021.
Evaluating large
language models trained on code. arXiv preprint
arXiv:2107,03374.
Ronen Eldan and Mark Russinovich. 2023. Who's harry
potter? approximate unlearning in llms. preprint,
arXiv:2310.02238.
Mor Geva, Jasmijn Bastings, Katja Filippova, Amir
and
Globerson. 2023. Dissecting recall of factual
associa-
tions in auto-regressive language models. proceed-
In
ings of the 2023 Conference on Empirical Methods in
Natural Language sing, pages 12216-12235.
P roc s
e
Mor Geva, Roei Schuster,
Jonathan Berant, and Omer
Levy. 202La. Transformer feed-forward
layers are
key-value memories. In Proceedings of the 2021
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 5484-5495, Online and
Punta Cana, Dominican Republic. Association for
Computational Linguistics.
Mor Geva, Roei Schuster, Jonathan Berant, and Omer
Levy. 2021b. Transformer feed-forward layers are
key-value memories. In Proceedings of the 2021
Conference on Empirical Methods in Natural Lan-
guage P ro ce s sing, pages 5484-5 49 5.
Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bha-
gia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh
Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang,
Shane Arora, David Atkinson, Russell Authur, Khy-
athi Raghavi Chandu, Arman Cohan, Jennifer Du-
mas, Yanai Elazar, Yuling Gu, Jack Hessel, Tushar
Khot, William Merrill, Jacob Morrison, Niklas Muen-
nighoff, Aakanksha Naik, Crystal Nam, Matthew E.
Peters, Valentina Pyatkin, Abhilasha Ravichander,
Dustin Schwenk, Saurabh Shah, Will Smith, Emma
Strubell, Nishant Subramani, Mitchell Wortsman,
Pradeep Dasigi, Nathan Lambert, Kyle Richardson,
Luke Zettlemoyer, Jesse Dodge, Kyle Lo, Luca
Sol-
daini, Noah A. Smith, and Hannaneh Hajishirzi.2024.
Olmo: Accelerating the science of language
models.
arXiv prep rint arXiv 2402.008
: 3 8.
Yihuai Hong, Lei Yu, Haiqin Yang,
Shauli Ravfogel,
and Mor Geva. 2024. Intrinsic evaluation of
unlearn-
ing using parametric knowledge preprint,
Eaces.
arXiv:2406.11614.
Joel Jang, Dongkeun Yoon, Sohee Yang, Sungmin
Cha,
Moontae Lee, Lajanugen Logeswaran, and Minjoon
Seo. 2023. Knowledge unlearning for mitigating
privacy risks in language models. In proceidings
of the 2023 Conference on Empirical Methods in Natu-
ral Language sing, pages [ILLEGIBLE].
P roc e s
Lijie Hu, Di Wang, and [ILLEGIBLE]. 2024. [ILLEGIBLE]. arXiv
preprint arXiv:[ILLEGIBLE].
Jiang, [ILLEGIBLE] et al. 2023. [ILLEGIBLE]. arXiv preprint
arXiv:[ILLEGIBLE].
Jiani Li, [ILLEGIBLE]. 2024. [ILLEGIBLE]. arXiv preprint
arXiv:[ILLEGIBLE].
Lee, [ILLEGIBLE]. 2024a. [ILLEGIBLE]. arXiv preprint arXiv:[ILLEGIBLE].
Lee, [ILLEGIBLE]. 2024b. [ILLEGIBLE]. arXiv preprint arXiv:[ILLEGIBLE].
Srinivas [ILLEGIBLE]. 2024. [ILLEGIBLE]. arXiv preprint arXiv:[ILLEGIBLE].
Kevin Meng, David Bau, Alex J Andonian, and Yonatan
Belinkov. 2022. Locating and editing factual associ-
ations in GPT. In Advances in Neural Information
Processing Systems.
Kevin Meng, Arnab Sen Sharma, Alex J Andonian,
Yonatan Belinkov, and David Bau. 2023. Mass-
editing memory in a transformer. In The Eleventh
International Conference on Learning Representa-
tions.
Maximilian Mozes, Xuanli He, Bennett Kleinberg, and
Lewis D. Griffin. 2023. Use of llms for illicit pur-
poses: Threats, prevention measures, and vulnerabili-
ties. P reprint, arXiv:2308, 1 2833.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002, Bleu: a method for automatic evalu-
ation of machine translation. In Proceedings of the
40th annual mee
Rafailov, [ILLEGIBLE] et al. 2023. [ILLEGIBLE]. arXiv preprint arXiv:[ILLEGIBLE].
Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin
Schwenk, David Atkinson, [ILLEGIBLE]. 2024. Dolma:
an open corpus of three trillion tokens for language
model pretraining research. arXiv preprint arXiv:[ILLEGIBLE].
Stoehr, [ILLEGIBLE] et al. 2024. [ILLEGIBLE]. arXiv preprint arXiv:[ILLEGIBLE].
Hugo Touvron, Louis Martin, Kevin R. Stone, Peter
Albert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, D. Bikel, Lukas Blecher, Cristian Cant6n
Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,
Jude Femandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
Cynthia Gao, Vedanuj Goswami, Naman Goyal,
A. Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan,
Marcin Kardas, Viktor Kerkez,Madian Khabsa, Is-
abel M. Kloumann, A. Korenev, Punit Singh Koura,
Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-
ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-
tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-
bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,
Ruan Silva, Eric Michael Smith, R. Subramanian,
Xia Tan, Binh Tang, Ross Taylor, Adina Williams,
Jian Xiang Kuan, Puxin Xu, Zhengxu Yan, Iliyan
Zarov, Yuchen Zhang, Angela Fan, Melanie Kam-
badur, Sharan Narang, Aurelien Rodriguez, Robert
Stojnic, Sergey Edunov, and Thomas Scialom. 2023.
Llama2:. Open foundation and flne-tuned chat mod-
els. arXiv preprint arXiv 07.092 88.
: 2 3
Yuanshun Yao, Xiaojun Xu, and Yang Liu. 2023. Luge
language model unlearning. In Socially Responsible
Itrn gua ge M odellin g Re arch.
s e
Lil.
Yuanshun Yao, Xiaojun Xu, and Yang
2024.
Large language model unlearning. Preprint,
arXiv:2310.10683.
Jingwen Ye, Yifang Fu, Jie Song, Xingyi Yang, Songhua
Liu, Xin Jin, Mingli Song, and Xinchao Wang.
2022. Leaming with recoverable forgetting. In Euro-
pean Conference on Computer Vsion, pages 87-103.
Springer.
Weixiang Zhao, Yulin Hu, Zhuojun Li, Yang Deng,
Yanyan Zhao,Bing and Tat-Seng Chua.2024.
Qin,
Towards comprehensive and efficient post safety
alignment of large language models via safety patch-
ing. arXiv preprint arXiv :2405. I 3820.


\appendix

\section{Details in Existing Unlearning Methods}
In this section, we provide a more detailed introduction to the LLM unlearning methods we used in \S3 and \S4.
\begin{itemize}
\item Gradient Difference (Yao et a1.,2024), based on Gradient Ascent, it adds a regularization term to minimize the KL divergence between the unlearned and the original LLM on a reference text dataset, thus preventing the model from catastrophic deterioration of its general capability.
\item Direct Preference Optimization (DPO) (Rafailov et a1., 2023), which maximizes the log-likelihood ratio between generating the preferred and the unfavored responses, while retaining a small shift from the original LLM predictive distribution,
\item Negative Preference Optimization (NPO) (frtao et a1.,2024), which discards the favored responses and only minimizes the prediction probability of the unfavored answers.
\item NPO+KL which adds to NPO a KL divergence loss between the model's outputs before and after unlearning.
\end{itemize}


\section{Unlearning Experiment's Corpus}
Here, we present detailed information about the data used for activation patching experiments and the unlearning experiments conducted in \S3 and \S4. We select 10 well-known concepts from ConceptVectors Benchmark (Hong et al., 2024) and extract 6,000 corresponding training data segments containing knowledge about the respective concepts per model from the pre-training datasets of Redpjama and Dolma. These extacted data segments are used for unlearn training of the two models respectively. For each concept, we also include ten related questions from the ConceptVectors Benchmarlq along with 50 unrelated questions sampled from other unrelated concepts. These are used in \S4 to evaluate the unlearning effectiveness from the behavior perspective on the specific concepts, as well as to assess whether the model,s unrelated capabilities were affected. We have manually checked and verified that the vanilla LLaMA and OLMo models can accurately answer these selected questions, indicating that the models possess the knowledge. All the statistics and examples are shown in Table 1 and Table 2, respectively.

\begin{table}[t]
\centering
\begin{tabular}{lrrrrr}
\toprule
Data Sources & # selected concepts & # of paragraphs per concept & # of words per paragraph & # of QA pairs & # of unrelated QA Pairs \
\midrule
Redpjama & 10 & 6000 & 1514.65 & 20 & 50 \
Doima & 10 & 6000 & 2261.25 & 20 & 50 \
\bottomrule
\end{tabular}
\caption{Statistics of the training data for the unlearning experiments on LLaMA and OLMo}
\end{table}

\begin{table}[t]
\centering
\begin{tabular}{p{0.18\linewidth}p{0.42\linewidth}p{0.20\linewidth}p{0.20\linewidth}}
\toprule
Concept & Ttaining Data Snippets & Example QA & Example Unrelated QA \
\midrule
Harry Pouer(LLaMA) &
Harry potter is a series of seven fantasy novels wntten by British authorJ. K. Rowl- ing. The novels chronicle the lives of a young wizard, Harry Potter, and his friends Hermione Granger and Ron Weasley, all of whom are students at Hogwarts School of Witchcraft and Wizardry.. &
Who is the author of the Harry Potter book series?\
What is the name of the first book in the Harry Potter series?.. &
In which century did William Shakespeare live and write?\
What town is traditionally consid- ered Shakespeare's birthplace?.. \
\midrule
Star Wars(LLaMA) &
Star Wars is an American epic space opera media franchise created by George Lucas, which began with the eponymous 19'77 film and quickly became a worldwide pop cul- ture phenomenon.. &
Who is Darth Vader's son?\
What is the weapon used by Jedi Knighrs?.. &
What are the twelve zodiac signs?\
Which astrological sign is repre- sented by the lion? ' \
\midrule
Amazon Alexa(LLaMA) &
Amazon Alexa or Alexa is a virtual assis- tant technology largely based on a Polish speech synthesizer named Ivona, bought by Amazon in 2013. It was first used in the Amazon Echo smart speaker and the Echo Dot, Echo Studio and Amazon Tap speakers developed by Amazon Lab126. &
What year was the Amazon Alexa Voice Assistant first intro- duced to the public?\
What are some of the pri- mary functions of Amazon Alexa Voice Assistant?.. &
Who betrayed Jesus to the author- ities in the Bible?\
What is the longest book in the Bible in terms of chapters?.. \
\midrule
Ebay(OLMo) &
eBay Inc. ( EE-bay, often stylized as ebay) is an American multinational e-commerce compary based in San Jose, California, that brokers customer to customer and retail sales through online marketplaces in 190 markets worldwide.. &
What is the name of Japan's most popular boy band?\
Who is Japan's most famous anime creator? .. &
What does IRC stand for?\
When was IRC flrst developed? ' \
\midrule
Olympic Games(OLMo) &
The modem Olympic Games or Olympics (French: Jeux olympiques) are the lead- ing international sporting events featuring summer and winter sporls competitions in which thousands of athletes from around the world participate in a variety of compe- titions.. &
When were the first modern Olympic Games held?\
How of- ten are the Summer olympics held?.. &
What is virtual reality?\
How does virtual reality technol- ogy work?.. ' \
\midrule
Diabetes(OLMo) &
Diabetes mellitus, often known simply as diabetes, is a group of common endocrine diseases characterized by sustained high blood sugar levels. Diabetes is due to either the pancreas not producing enough insulin, or the cells of the body becoming unrespon- sive to the hormone's effects.. &
What is diabetes?\
What are the main types of dia- betes?.. &
What is the capital city of Pak- istan?\
what is the currency of Pak- istan? ' \
\bottomrule
\end{tabular}
\caption{Example extracted data from the Redpjama and Dolma pre-training datasets'}
\end{table}


\section{More Rigorous Patching Investigation}
In \S3, during our activation patching and parameters restoration experiments, we restore [ILLEGIBLE] layer by layer respectively, while avoiding intentional changes to the other two states in the unlearned model. However, for instance, restoring [ILLEGIBLE] in [ILLEGIBLE] layer may aid in the recovery of [ILLEGIBLE] in subsequent layers, ultimately leading to an improvement in KRS. Therefore, in this part of the experiment, when restoring each element layer by layer, we purposefully keep the other two elements unchanged (e.g., when restoring [ILLEGIBLE], we maintain the original states of [ILLEGIBLE] and [ILLEGIBLE], for both the current and subsequent layers). This approach thoroughly isolates the effects of these three different elements.

\begin{figure}[t]
\centering
[ILLEGIBLE]
\caption{Results of KRS on LLaMA and OLMo under three activations patching or parameters restoration settings, isolating the effects of the two others when investigating each factor individually.}
\end{figure}

Figure 3 presents the results in this setting. We can obserye the following: (1) When [ILLEGIBLE] is restored layer by layer, its effect on improving KRS remains very small, which is consistent with prior experiments. (2) When restoring [ILLEGIBLE] layer by layer and isolating its effects from the other two factors, its contribution to KRS remains insignificant, staying at a low level and only increasing to around 0.08 on LLaMA and 0.2 on OLMo in the final layers. (3) When [ILLEGIBLE] is restored layer by layer, isolating its influence from the other elements, we observe a notable rise in KRS in the last three layers, reaching values as high as 0.8 or above. This supports the idea that neurons responsible for [ILLEGIBLE] in the MLP components of the final layers primarily carry the unlearning effects of these finetuning-based methods.


\end{document}
=====END FILE=====
