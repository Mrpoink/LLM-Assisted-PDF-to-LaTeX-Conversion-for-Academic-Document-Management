\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Background and Related Work}{3}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Unlearning in Large Language Models}{3}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Knowledge Storation in Large Language Models}{3}{subsection.2.2}\protected@file@percent }
\newlabel{eq:mlp}{{1}{4}{Knowledge Storation in Large Language Models}{equation.1}{}}
\newlabel{eq:hidden}{{2}{4}{Knowledge Storation in Large Language Models}{equation.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Patching Investigation}{4}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Hypothesis and Experimental Design}{4}{subsection.3.1}\protected@file@percent }
\newlabel{eq:krs}{{3}{5}{Hypothesis and Experimental Design}{equation.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Activation Patching and Parameters Restoration Experiments}{5}{subsection.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Results of KRS on LLaMA and OLMo under three activations patching or parameters restoration settings. We also included another setting that restores both attention and coefficients to compare the final outcomes.}}{6}{figure.1}\protected@file@percent }
\newlabel{fig:fig1}{{1}{6}{Results of KRS on LLaMA and OLMo under three activations patching or parameters restoration settings. We also included another setting that restores both attention and coefficients to compare the final outcomes}{figure.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Global Negative Effect of Fine-Tuning Unlearning}{6}{section.4}\protected@file@percent }
\bibcite{blanco-justicia-2024}{1}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Unlearning testing results on LLaMA and OLMo for each training epoch.}}{7}{figure.2}\protected@file@percent }
\newlabel{fig:fig2}{{2}{7}{Unlearning testing results on LLaMA and OLMo for each training epoch}{figure.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Discussion and Conclusion}{7}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Limitations}{7}{section.6}\protected@file@percent }
\bibcite{chang-2023a}{2}
\bibcite{chang-2023b}{3}
\bibcite{chen-yang-2023}{4}
\bibcite{chen-etal-2021}{5}
\bibcite{eldan-russinovich-2023}{6}
\bibcite{geva-etal-2023}{7}
\bibcite{geva-etal-2021a}{8}
\bibcite{geva-etal-2021b}{9}
\bibcite{groeneveld-etal-2024}{10}
\bibcite{hong-etal-2024}{11}
\bibcite{jang-etal-2023}{12}
\bibcite{lee-etal-2024a}{13}
\bibcite{lee-etal-2024b}{14}
\bibcite{liu-etal-2024}{15}
\bibcite{lu-etal-missing}{16}
\bibcite{eraser-missing}{17}
\bibcite{meng-etal-2022}{18}
\bibcite{meng-etal-2023}{19}
\bibcite{mozes-etal-2023}{20}
\bibcite{papineni-etal-2002}{21}
\bibcite{patil-etal-2024}{22}
\bibcite{pochinkov-schoots-2024}{23}
\bibcite{radford-etal-2019}{24}
\bibcite{rafailov-etal-2023}{25}
\bibcite{soldaini-etal-2024}{26}
\bibcite{stoehr-etal-2024}{27}
\bibcite{sukhbaatar-etal-2015}{28}
\bibcite{touvron-etal-2023}{29}
\bibcite{yao-etal-2023}{30}
\bibcite{yao-etal-2024}{31}
\bibcite{ye-etal-2022}{32}
\bibcite{zhao-etal-2024}{33}
\@writefile{toc}{\contentsline {section}{\numberline {A}Details in Existing Unlearning Methods}{10}{appendix.A}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {B}Unlearning Experiment's Corpus}{10}{appendix.B}\protected@file@percent }
