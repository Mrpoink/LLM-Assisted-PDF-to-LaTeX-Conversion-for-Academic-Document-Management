=====FILE: main.tex=====
\documentclass[11pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage{times}
\usepackage{microtype}
\usepackage{amsmath,amssymb}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{hyperref}

\title{Primacy Effect of ChatGPT}

\author{
Yiwei Wang$^{\dagger}$\thanks{Equal contribution.}\quad
Yujun Cai$^{\dagger}$\footnotemark[1]\quad
Muhao Chen$^{\S}$\quad
Yuxuan Liang$^{\dagger\dagger}$\quad
Bryan Hooi$^{|}$\
$^{\dagger}$University of California, Los Angeles\quad
$^{\ddagger}$Meta\
$^{\S}$University of California, Davis\quad
$^{|}$National University of Singapore\
$^{\dagger\dagger}$Hong Kong University of Science and Technology (Guangzhou)\
\texttt{[wangyw.evan@gmail.com](mailto:wangyw.evan@gmail.com)}
}

\date{}

\begin{document}
\maketitle

\begin{abstract}
Instruction-tuned large language models
(LLMs), such as ChatGPT, have led to
promising zero-shot performance in dis-
criminative natural language understanding
(NLU) tasks. This involves querying the LLM
using a prompt containing the question, and
the candidate labels to choose from. The
question-answering capabilities of ChatGPT
arise from its pre-training on large amounts
of human-written text, as well as its sub-
sequent fine-tuning on human preferences,
which motivates us to ask: Does ChatGPT
also inherit humans' cognitive biases? In
this paper, we study the pimacy effect of
ChatGPT: the tendency of selecting the labels
at earlier positions as the answer. We have
two main findings: i) ChatGPT's decision is
sensitive to the order of labels in the prompt;
ii) ChatGPT has a clearly higher chance
to select the labels at earlier positions as
the answer. We hope that our experiments
and analyses provide additional insights
into building more reliable ChatGPT-based
solutions. We release the source code at \url{[https://github.com/wangywUST/PrimacyEffectGPT}](https://github.com/wangywUST/PrimacyEffectGPT}).
\end{abstract}

\section{Introduction}
Humans tend to recall information presented at the
start of a list better than information at the middle
or end. This phenomenon is known as the primacy
effect (Asch,1946), which is a cognitive bias that
relates to humans' attention spans (Crano, 1977),
rehearsal (Tan and Ward, 2000), and memory sys-
tems (Li, 2010). Similarly, in advertisement sys-
tems and search engines, humans tend to interact
with items in higher positions regardless of the
items' actual relevance (Chen et al.,2023). Pri-
macy effect influences humans' behaviors to make
unfair decisions. Similarly, if it exists in machine
learning models, it may lead to worse performance.

\begin{figure}[t]
\centering
\fbox{\parbox{0.9\linewidth}{\centering IMAGE NOT PROVIDED}}
\caption{\textit{Primacy Effect of ChatGPT}: ChatGPT tends
to return labels in earlierpositions as the answer. This
plot shows the distribution of ChatGPT's predicted label
indices in TACRED (42 classes), where we randomly
shuffle labels before every prediction (see Ser. 2.2).}
\label{fig:primacy-tacred}
\end{figure}

Recently, instruction-tuned large language mod-
els (LLMs), represented by ChatGPT (OpenAI,
2022), have received wide attention on their capa-
bilities of imitating humans in question-answering
and problem-solving. However, this underlying be-
havioral similarity between ChatGPT and humans
naturally leads to an inriguing question: Is Chat-
GPT also affected by the primacy ffict?

ChatGPT provides a convenient way to achieve
the discriminative natural language understanding
(NLU) (Li et al.,2023; Wei et al.,2023; Yuan et al.,
2023). People only need to list the labels in the
prompt and asking ChatGPT to select the label(s)
that match the input text. In this work, to ana-
lyze the primacy effect of ChatGPT, we start by
testing with random label shuffling, i.e., shufifling
labels listed in the prompt before every prediction.
We compare the predictions on the same instance
with two different label orders. Then, we count
the predicted label indices on many instances with
label shuffling. The motivation is that: a fair NLU
model should give the same prediction on an in-
put instance regardless of how the labels are or-
dered; consequently, it should produce uniformly
distributed label indices under label shuffling for
any instance.

Through extensive experiments with a series of
NLU datasets, we find that
\begin{itemize}
\item ChatGPT's prediction is sensitive to the order
of labels in the prompt. Specifically, Chat-
GPT's prediction changes after a label shuf-
fling on 87.9% of the instances in TACRED.
\item ChatGPT is affected by the primacy effect:
ChatGPT tends to select labels in earlier posi-
tions in the prompt (see Fig.~\ref{fig:primacy-tacred}), which present
clear bias with respect to the label order.
\end{itemize}

On the whole, our work contributes to a better
understanding of ChatGPT's behaviors and build-
ing more faithful ChatGPT-based NLU solutions.

\section{Primacy Effect of ChatGPT}
In this section, we first introduce the general
prompt design of ChatGPT in discriminative nat-
ural language understanding (NLU). Then, we an-
alyze the primacy effect of ChatGPT using label
shuffling in prompts.

\subsection{Prompts for ChatGPT}
Prompts are a key component to the effective use of
ChatGPT on discriminative NLU tasks (Wei et al.,
2023; Yuan et al.,2023). Generally, prompts for
such tasks involve two key components: (i) label
definitions, and (ii) a task description and input text
(see an example in Fig.~\ref{fig:prompt-example}).

ChatGPT's capability of understanding instruc-
tions in the prompt benefits from its training with
human feedback (OpenAI, 2022), but this also cre-
ates the risk of inheriting humans' cognitive biases.
In this paper, we discuss a cognitive bias in Chat-
GPT: the primacy effect, which indicates the ten-
dency of selecting labels in earlier positions in the
prompt.

\begin{figure}[t]
\centering
\fbox{\parbox{0.9\linewidth}{\centering IMAGE NOT PROVIDED}}
\caption{A prompt example for ChatGPT.}
\label{fig:prompt-example}
\end{figure}

\subsection{Analysis with Label Shuffling}
Analyzing the primacy effect requires us to distill
the effects of label orders in the prompts. However,
this is non-trivial because there are many factors
influencing ChatGPT's decisions, such as the input
text and label definitions. In our work, to distin-
guish the primacy effect of ChatGpT from other
factors, we conduct random shuffling for labels
listed in the prompts. Specifically, before every
prediction, we shuffle the labels as visualized in
Fig.~\ref{fig:label-shuffling}. Label shuffling erases the discriminative se-
mantics of the specific label orders in the prompts.

Ideally, a fair model should return the same pre-
diction when labels are shuffied, and consequently,
the predicted label index should follow a uniform
distribution under random shuffiing.

Next, we introduce our two ways of using ran-
dom label shuffling to analyze ChatGPT.

\paragraph{Prediction Comparison on an Instance}
A reli-
able and consistent classifier is expected to consis-
tently choose the same label for the same instance
irrespective of the label order. To evaluate such
consistency of ChatGPT, we perform the random
shuffiing for the same instance twice to produce
two prompts. We feed these two prompts to Chat-
GPT and compare the corresponding two predic-
tions with each other. We apply the above process
to all the test instances and compute the fraction
of the instances where the prediction changed after
label shuffling. The higher the fraction is, the more
sensitive ChatGPT is to the label order.

\paragraph{Statistics of Predicted Indices}
Taking a further
step, we perform statistical analysis on the pre-
dicted indices for instances where the prediction
changed after label shuffling. If ChatGpT does
not have any preference on the label orders, its pre-
dicted label indices should be uniformly distributed.
By comparing the predicted label index distribu-
tion of ChatGPT to the uniform distribution, we
can assess its fairness and preferences regarding
label orders.

\begin{figure}[t]
\centering
\fbox{\parbox{0.9\linewidth}{\centering IMAGE NOT PROVIDED}}
\caption{We analyze the primacy effects of ChatGPT by randomly shuffiing the labels in the prompts.}
\label{fig:label-shuffling}
\end{figure}

\section{Experiments}
We analyze the primacy effect based on the afore-
mentioned strategies using three relation extraction
datasets and an intent detection dataset.

\subsection{Experiment Setup}
We mainly chose relation extraction and intent de-
tection tasks in our experiments since these tasks
naturally come with adequately sized decision
spaces to illustrate the underlying primacy effect
of labels. For relation extraction, we experiment
on three benchmark datasets including TACRED
(Zhang et al.,2017), TACREV (Alt et al.,2020),
and Re-TACRED (Stoica et al.,2021). For intent
detection, we conducted experiments on Banking77
(Casanueva et al., 2020a) and MASSIVE (FitzGer-
ald et al., 2022). MASSIVE (FitzGerald et al.,
2022) is a parallel dataset of massive utterances
with annotations for the Natural Language Under-
standing tasks of intent prediction. Utterances span
60 intents.

We additionally conducted experiments on the
NLP datasets: GoEmotions (Demszky et al.,2020)
and 20 Newsgroups (Albishre et al., 2015) for
a more comprehensive evaluation. GoEmotions
(Demszky et al.,2020) is a dataset for fine-grained
emotion classification. It is a corpus of 58k care-
fully curated comments extracted from Reddit,
with human annotations for 27 emotion categories
and a neutral one. The 20 Newsgroups (Albishre
et al., 2015) dataset is a collection of approximately
20,000 newsgroup documents, partitioned across
20 different newsgroups.

We follow the existing work (Mei et al.,2023; Li
et al.,2023) to apply ChatGPT to these tasks via the
OpenAI API gpt-3.5-turbo. Specifically, we set
the temperature as 0.0 to minimize the randomness
of ChatGPT's outputs. For comparison, we adopt
the existing work (Casanueva et al.,2020b; Zhou
and Chen, 2022) to fine-tune the BERT model with
an MLP classification head.

\subsection{Consistency under Label Shuffling}
First, we observe the low consistency of ChatGPT
confronted under label shuffling. As shown in Ta-
ble~\ref{tab:shuffle-change}, ChatGPT changes its label prediction after
label shuffiing in over 85% of the test instances on
the TACRED, TACREV and Re-TACRED datasets,
and in 35.1% of instances on Banking77. Also,
ChatGPT changes its label prediction after label
shuffling in over 69% of the test instances on the
datasets of GoEmotions and in more than 30% of
instances on MASSIVE and 20 Newsgroups. In
contrast, the fine-tuned BERT classifier maintains
consistent predictions after label shuffling. This
discrepancy challenges the widely-held belief that
ChatGPT can comprehend human instructions and
provide consistent responses. One possible ex-
planation is that ChatGPT's understanding of the
prompt is obtained by training on human-labeled
data, which inherits humans' cognitive bias of treat-
ing labels at different positions unfairly.

It is worth noting that the ratio of instances with
changed predictions is consistently high across the
relation extraction datasets but lower on intent de-
tection. This discrepancy can be attributed to the
fact that information extraction tasks are shown
to be challenging for ChatGPT and other LLMs
(Wang et al.,2023; Li et al., 2023). In more diffi-
cult tasks, ChatGPT lacks sufficient discriminative
semantic understanding from the input text and
may be more affected by the label order.

\begin{table}[t]
\centering
\caption{Fraction of the instances that have their predicted label changed after a label shuffiing.}
\label{tab:shuffle-change}
\begin{tabular}{lrrrrrrr}
\toprule
Method & TACRED & TACREV & Re-TACRED & Banking77 & GoEmotions & MASSIVE & 20 Newsgroups \
\midrule
ChatGPT w/ Prompt & 87.9 & 85.9 & 88.6 & 35.7 & 69.3 & 32.8 & 34.1 \
BERT w/ MLP & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \
\bottomrule
\end{tabular}
\end{table}

\subsection{Primacy Effect of ChatGPT}
The empirical results in Section 3.2 indicate that
ChatGPT's predictions are affected by label order.
To deeper delve into the effects of label orders on
ChatGPT, we analyze the distribution of predicted
label indices (e.g., if the prediction is the first label,
the label index is 1), as introduced in Section 2.2.
We visualize the distributions in Fig.~\ref{fig:predicted-index-dist}. Notably,
the distribution of ChatGPT's predictions consis-
tently deviates from the uniform distribution, dis-
playing a consistent bias towards smaller indices
across different datasets. In contrast, BERT ex-
hibits no prefercnce for label orders and consis-
tently demonstrates a uniform distribution in its
predicted label indices.

We term this tendency of ChatGPT as the pri-
macy effect, where the model tends to favor the
labels presented earlier in the prompt. The magni-
tude of these primacy effects varies across tasks,
as illustrated in Fig.~\ref{fig:predicted-index-dist}. Notably, the influence of
primacy effects is higher in more challenging tasks.
This observation aligns with the results discussed
in Sec. 3.3, wherein the impact of primacy effects is
greater when ChatGPT tackles more difficult tasks.
In the next section, we will quantitatively analyze
the primacy effects of ChatGPT.

\begin{figure}[t]
\centering
\fbox{\parbox{0.9\linewidth}{\centering IMAGE NOT PROVIDED}}
\caption{The distribution of predicted indices of the test instances with label shuffling before every prediction.}
\label{fig:predicted-index-dist}
\end{figure}

\subsection{Evaluation on Fairness}
The fairness of a trained model can be assessed
by examining the imbalance or skewness in its
predictions (Sweeney and Najafian, 2019). Fol-
lowing prior studies (Xiang et al.,2020; Sweeney
and Najafian,2019; Qian et al.,2021; Wang et al.,
2022), we employ the JS divergence (Fuglede and
Topsoe, 2004) as the metric to evaluate how im-
balanced/skewed/unfair a prediction $P$ is. The
measurement is symmetric (i.e., [ILLEGIBLE]) and strictly scoped.

To evaluate the label order bias of ChatGPT, we
compute the average relative label order imbal-
ance (LOI): LOI is defined as the JS divergence
between the predicted label index distribution $P$
and the uniform distribution $U$:
\begin{equation}
\mathrm{LOI} := \mathrm{JS}\bigl(P(r \mid x \in D), U\bigr),
\label{eq:loi}
\end{equation}
where $x$ represents an input instance, $D$ is the test
set, $P(r)$ is the predicted label index, and $U$ is
the uniform distribution. LOI captures the dispar-
ity between the predicted indices and a uniform
distribution.

We conduct the fairness evaluation following the
experimental settings described in Section 3.3, and
the results are presented in Table~\ref{tab:unfairness}. The findings
demonstrate that ChatGPT exhibits unfair treat-
ment of label indices when making relation label
predictions for input texts. Furthefinore, the degree
of unfairness increases with the task's difficulty,
which aligns with the empirical results discussed
in Sections 3.2 and 3.3. In contrast, BERT demon-
strates significantly better faimess, as its predic-
tions are not influenced by label orders.

We additionally test the performance of Chat
GPT with CoT (Chain-of-thoughts) (Wei et al.,
2022). With CoT, ChatGPT still exhibits rhe pri-
macy effect. The above results show that with or
without CoT, ChatGPT consistently exhibits the pri-
macy effect. A reason for this phenomenon could
be that the CoT encourages the LLMs for ``slow
thinking'' about the question but does not neces-
sarily mitigate the cognitive bias in the reasoning
steps of CoT.

\begin{table}[t]
\centering
\caption{Experimental results (unfairness; %) on the test
sets of TACRED, TACRED-Revisit, Re-TACRED, and
Banking77 (lower is better). The best results in each
column are highlighted in bold font.}
\label{tab:unfairness}
\begin{tabular}{lrrrr}
\toprule
Method & TACRED & TACREV & Re-TACRED & Banking77 \
\midrule
ChatGPT w/ Prompts & 57.9 & 57.6 & 58.1 & 18.8 \
ChatGPT w/ CoT & 57.6 & 57.9 & 58.3 & 18.6 \
BERT w/ MLP & 1.8 & 1.9 & 2.3 & 2.7 \
\bottomrule
\end{tabular}
\end{table}

\section{Related Work}
Large Language Models (LLMs) (Brown et al.,
2020; Rae et al.,2021; Thoppilan et al., 2022;
Smith et al.,2022), such as GPT-3 (Brown et al.,
2020), LaMDA (Thoppilan et al.,2022) and PaLM
(Chowdhery et al.,2022), refer to large scale pre-
trained models that contain more than a hundred bil-
lion parameters. Based on the highly parallelizable
Transformer architecture (Vaswani et al., 2017),
these Large Language models have shown power-
ful capability to produce reasonable results with
very few samples or task descriptions as input.

A key milestone in the development process of
LLMs is ChatGPT, which is developed by Ope-
nAI based on InstructGPT (Ouyang et al.,2022).
ChatGPT is able to interact with humans through
multiple turns of dialogue, understand user intent,
accomplish instructions, and return human-like re-
sponses. This attracts huge attention from research
field, motivating numerous recent work (Zhang
et al., 2022; Ma et al., 2023; Wan et al., 2023;
Zhong et al., 2023; Susnjak, 2023) to utilize Chat-
GPT to different tasks.

As ChatGPT is a proprietary model, and OpenAI
does not disclose its training specifics, researchers
are actively investigating its associated implica-
tions and capabilities. There has been some work
analyzing the performance, robustness, faithful-
ness, and explain-ability of ChatGPT (Gao et al.,
2023; Han et al., 2023; Li et al., 2023). For ex-
ample, (Malinka et al.,2023) investigates the ed-
ucational integrity of ChatGPT and evaluates the
ChatGPT's abilities to solve assignments of various
levels in computer security specialization. (Haque
et al.,2022) and (Kr"ugel et al.,2023) investigate
the ethical risks of ChatGPT.

Before ChatGPT, LLMs' inference has been ac-
companied by in-context learning (ICL) which
adds a few demonstrations in the prompt (Dong
et al.,2022; Fei et al., 2023). Accordingly, some
work investigates the effects of demonstration or-
ders for the LLMs before ChatGPT (Lu et al.,
2021). (Zhao et al.,2021) finds the majority la-
bel, recency, and common token biases of LLMs'
ICL.

Different from the above work, we focus on a
new phenomenon of ChatGPT: the primacy effect,
which is the tendency of selecting the first labels as
the answer. The primary effect seriously influences
ChatGPT's fairness. Collectively, our findings pro-
vide a new understanding of how ChatGPT works
given the instructional prompts.

\section{Conclusion}
While previous work often takes ChatGPT as a uni-
versal method applicable to all text-related tasks,
we argue that its flexibility comes with the risk
of inheriting human's cognitive biases. These bi-
ases lead to unfair judgments which can affect the
performance of the machine leaming model. This
work studies a cognitive bias of ChatGPT: primacy
effects. We propose a simple yet effective label
shuffling method to analyze the influence of label
orders on ChatGPT. We discover the primacy ef-
fect of ChatGPT and finds that it highty influences
the fairness of ChatGPT in NLU. Our work con-
tributes to a better understanding of the behaviors
of ChatGPT and building more faithful solutions
with ChatGPT in NLU applications.

\section*{Limitation}
Our work has a few potential limitations. Firstly,
we primarily evaluate the primacy effect of Chat-
GPT, which is one of the most widely-used
instruction-legacy models for each task. It would
be beneficial to assess this effect on other LLMs
models (such as Google Bard, vicuna (Chiang et al.,
2023)) and explore additional tasks to examine this
primacy effect. Secondly, this work focused on an-
alyzing the primacy effect of ChatGPT through ex-
periments. We encourage further studies to propose
effective solutions that can mitigate the negative
impacts associated with the primacy effect.

\section*{Acknowledgement}
The authors would like to thank the anonymous
reviewers for their discussion and feedback. Yi-
wei Wang and Bryan Hooi are supported by NUS
ODPRT Grant 4-0008067-00-00, NUS ODPRT
Grant R252-000-A81-133, and Singapore Min-
istry of Education Academic Research Fund Tier
3 under MOEs official grant number MOE2017-
T3-1-007. Muhao Chen is supported by the NSF
Grant IIS 2105329, the NSF Grant ITE 2333736,
the DARPA MCS program under Contract No.
N660011924033 with the United States Office Of
Naval Research, a Cisco Research Award, two
Amazon Research Awards, and a Keston Research
Award.

\begin{thebibliography}{99}

\bibitem{Albishre2015}
Khaled Albishre, Mubarak Albathan, and Yuefeng Li.
2015.
Effective 20 newsgroups dataset cleaning.
In \textit{2015 IEEE/WIC/ACM International Conference on
Web Intelligence and Intelligent Agent Technology
(WI-IAT)}, volume 3, pages 98--101. IEEE.

\bibitem{Alt2020}
Christoph Alt, Aleksandra Gabryszak, and Leonhard
Hennig.
2020.
TACRED revisited: A thorough evaluation of the TACRED relation exffaction task.
In \textit{Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics}, pages 1558--1569, Online. Association for Computational Linguistics.

\bibitem{Asch1946}
Solomon E Asch.
1946.
Forming impressions of personality.
\textit{The Journal of Abnormal and Social Psychology}, 41(3):258.

\bibitem{Brown2020}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry Amanda
Askell, et al.
2020.
Language models are few-shot learners.
\textit{Advances in neural information processing systems}, 33:1877--1901.

\bibitem{Casanueva2020a}
I~nigo Casanueva, Tadas Temcinas, Daniela Gerz,
Matthew Henderson, and Ivan Vuli'c.
2020a.
Efficient intent detection with dual sentence encoders.
In \textit{Proceedings of the 2nd Workshop on NLP for ConvAI - ACL 2020}.
Data available at \url{[https://github.com/PolyAI-LDN/task-specific-datasets}](https://github.com/PolyAI-LDN/task-specific-datasets}).

\bibitem{Demszky2020}
Dorottya Demszky, Dana Movshovitz-Attias, Jeongwoo
Ko, Alan Cowen, Gaurav Nemade, and Sujith Ravi.
2020.
Goemotions: A dataset of fine-grained emotions.
\textit{arXiv preprint} arXiv:2005.00547.

\bibitem{Dong2022}
Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiy-
ong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and
Zhifang Sui.
2022.
A survey for in-context leaming.
\textit{arXiv preprint} arXiv:2301.00234.

\bibitem{Fei2023}
Yu Fei, Yifan Hou, Zeming Chen, and Antoine Bosselut.
2023.
Mitigating label biases for in-context learning.
\textit{arXiv preprint} arXiv:2305.19148.

\bibitem{FitzGerald2022}
Jack FitzGerald, Christopher Hench, Charith Peris,
Scott Mackie, Kay Rottmann, Ana Sanchez, Aaron
Nash, Liam Urbach, Vishesh Kakarala, Richa Singh,
et al.
2022.
Massive: A 1m-example multilingual natural language understanding dataset with
51 typologically-diverse languages.
\textit{arXiv preprint} arXiv:2204.08582.

\bibitem{Fuglede2004}
Bent Fuglede and Flemming Topsoe.
2004.
Jensenshannon divergence and hilbert space embedding.
In \textit{International Symposium on Information Theory, 2004. ISIT 2004. Proceedings.}, page 31. IEEE.

\bibitem{Gao2023}
Jinglong Gao, Xiao Ding, Bing Qin, and Ting Li.
2023.
Is chatgpt a good causal reasoner? a comprehensive evaluation.
\textit{arXiv preprint} arXiv:2305.07375.

\bibitem{Han2023}
Ridong Han, Tao Peng, Chaohao Yang, Benyou Wang,
Lu Liu, and Xiang Wan.
2023.
Is information extraction solved by chatgpt? an analysis of performance,
evaluation criteria, robusfiress and errors.
\textit{arXiv preprint} arXiv:2305.14450.

\bibitem{Haque2022}
Mubin Ul Haque, Isuru Dharmadasa, Zartin Tasnim
Sworna, Roshan Namal Rajapakse, and Hussain
Afunad.
2022.
``i think this is the most disruptive technology'': Exploring sentiments of chatgpt
early adopters using twitter data.
\textit{arXiv preprint} arXiv:2212.05856.

\bibitem{Krugel2023}
Sebastian Kr"ugel, Andreas Ostermaier, and Matthias
Uhl.
2023.
The moral authority of chatgpt.
\textit{arXiv preprint} arXiv:2301.07098.

\bibitem{Li2023IE}
Bo Li, Gexiang Fang, Yang Yang, Quansen Wang, Wei
Ye, Wen Zhao, and Shikun Zhang.
2023.
Evaluating chatgpt's information extraction capabilities: An assessment of performance, explainabitty, calibration,
and faithfulne ss.
\textit{arXiv preprint} arXiv:2304.11633.

\bibitem{Li2010}
Cong Li.
2010.
Primacy effect or recency effect? a long-term memory test of super bowl commercials.
\textit{Journal of Consumer Behaviour: An International Research Review}, 9(1):32--44.

\bibitem{Lu2021}
Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel,
and Pontus Stenetorp.
2021.
Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity.
\textit{arXiv preprint} arXiv:2104.08786.

\bibitem{Ma2023}
Yubo Ma, Yixin Cao, YongChing Hong, and Aixin Sun.
2023.
Large language model is not a good few-shot information extractor, but a good reranker for hard samples!
\textit{arXiv preprint} arXiv:2303.08559.

\bibitem{Casanueva2020b}
I~nigo Casanueva, Tadas Temcinas, Daniela Gerz,
Matthew Henderson, and Ivan Vuli'c.
2020b.
Efficient intent detection with dual sentence encoders.
\textit{arXiv preprint} arXiv:2003.04807.

\bibitem{Chen2023BiasSurvey}
Jiawei Chen, Hande Dong, Xiang Wang, Fuli Feng,
Meng Wang, and Xiangnan He.
2023.
Bias and debias in recommender system: A survey and future directions.
\textit{ACM Transactions on Information Systems}, 41(3):1--39.

\bibitem{Chiang2023}
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,
Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan
Zhuang, Yonghao Zhang, Joseph E. Gonzalez, Ion
Stoica, and Eric P. Xing.
2023.
Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality.

\bibitem{Chowdhery2022}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts,
Paul Barham, Hyung Won Chung, Charles Sutton,
Sebastian Gehrmann, et al.
2022.
Palm: Scaling language modeling with pathways.
\textit{arXiv preprint} arXiv:2204.02311.

\bibitem{Crano1977}
William D Crano.
1977.
Primacy versus recency in retention of information and opinion change.
\textit{The Journal of Social Psychology}, 101(1):87--96.

\bibitem{Malinka2023}
Kamil Malinka, Martin Peres(n)i, Anton Firc, Ondrej
Hujnak, and Filip Janus.
2023.
On the educational impact of chatgpl: Is artificial intelligence ready to obtain a university degree?
In \textit{Proceedings of the 2023 Conference on Innovation and Technology in Computer Science Education V 1}, pages 47--53.

\bibitem{OpenAI2022}
OpenAI.
2022.
Inroducing chatgPt.
\url{[https://openai.com/blog/chatgpt}](https://openai.com/blog/chatgpt}).

\bibitem{Ouyang2022}
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,
Carroll Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, et al.
2022.
Truning language models to follow instructions with human feedback.
\textit{Advances in Neural Information Processing Systems}, 35:27730--27744.

\bibitem{Qian2021}
Chen Qian, Fuli Feng, Lijie Wen, Chunping Ma, and
Pengjun Xie.
2021.
Counterfactual inference for text classification debiasing.
In \textit{Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)}, pages 5434--5445.

\bibitem{Rae2021}
Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie
Millican, Jordan Hoffmann, Francis Song, John
Aslanides, Sarah Henderson, Roman Ring, Susan-
nah Young, et al.
2021.
Scaling language models: Methods, analysis & insights from training gopher.
\textit{arXiv preprint} arXiv:2112.11446.

\bibitem{Smith2022}
Shaden Smith, Mostofa Patwary, Brandon Norick,
Patrick LeGresley, Samyam Rajbhandari, Jared
Casper, Zhun Liu, Shrimai Prabhumoye, George
Zerveas, Vijay Korthikanti, et al.
2022.
Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model.
\textit{arXiv preprint} arXiv:2201.11990.

\bibitem{Stoica2021}
George Stoica, Emmanouil Antonios Platanios, and
Barnab'as P'oczos.
2021.
Retacred: Addressing shortcomings of the tacred dataset.
In \textit{Proceedings of the AAAI Conference on Artificial Intelligence}, volume 35, pages 13843--13850.

\bibitem{Susnjak2023}
Teo Susnjak.
2023.
Applying bert and chatgpt for sentiment analysis of lyme disease in scientific literature.
\textit{arXiv preprint} arXiv:2302.06474.

\bibitem{Sweeney2019}
Chris Sweeney and Maryam Najafian.
2019.
A transparent framework for evaluating unintended demographic bias in word embeddings.
In \textit{Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics}, pages 1662--1667.

\bibitem{TanWard2000}
Lydia Tan and Geoff Ward.
2000.
A recency-based account of the primacy effect in free recall.
\textit{Journal of Experimental Psychology: Learning, Memory, and Cognition}, 26(6):1589.

\bibitem{Thoppilan2022}
Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam
Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,
Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al.
2022.
Lamrda: Language models for dialog applicatiots.
\textit{arXiv preprint} arXiv:2201.08239.

\bibitem{Vaswani2017}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz
Kaiser, and Illia Polosukhin.
2017.
Attention is all you need.
In \textit{Advances in neural information processing systems}, pages 5998--6008.

\bibitem{Wan2023}
Zhen Wan, Fei Cheng, Zhuoyuan Mao, Qianying
Liu, Haiyue Song, Jiwei Li, and Sadao Kurohashi.
2023.
Gpt-re: In-context learning for relation extraction using large language models.
\textit{arXiv preprint} arXiv:2305.02105.

\bibitem{Wang2023CausalEntityBias}
Fei Wang, Wenjie Mo, Yiwei Wang, Wenxuan Zhou,
and Muhao Chen.
2023.
A causal view of entity bias in (large) language models.
\textit{arXiv preprint} arXiv:2305.14695.

\bibitem{Wang2022EntityMentions}
Yiwei Wang, Muhao Chen, Wenxuan Zhou, Yujun
Cai, Yuxuan Liang, Dayiheng Liu, Baosong Yang,
Juncheng Liu, and Bryan Hooi.
2022.
Should we rely on entity mentions for relation extraction? debiasing relation extraction with counterfactual analysis.
\textit{arXiv preprint} arXiv:2205.03784.

\bibitem{Wei2022CoT}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,
et al.
2022.
Chain-of-thought prompting elicits reasoning in large language models.
\textit{Advances in Neural Information Processing Systems}, 35:24824--24837.

\bibitem{Wei2023ZeroShotIE}
Xiang Wei, Xingyu Cui, Ning Cheng, Xiaobin Wang,
Xin Zhang, Shen Huang, Pengjun Xie, Jinan Xu,
Yufeng Chen, Meishan Zhang, et al.
2023.
Zero-shot information extraction via chatting with chatgpt.
\textit{arXiv preprint} arXiv:2302.10205.

\bibitem{Xiang2020}
Liulu Xiang, Guiguang Ding, and Jungong Han.
2020.
Learning from multiple experts: Self-paced knowledge distillation for long-tailed classification.
In \textit{European Conference on Computer Vision}, pages 247--263. Springer.

\bibitem{Yuan2023}
Chenhan Yuan, Qianqian Xie, and Sophia Ananiadou.
2023.
Zero-shot temporal relation extraction with chatgpt.
\textit{arXiv preprint} arXiv:2304.05454.

\bibitem{Zhang2022}
Bowen Zhang, Daijun Ding, and Liwen Ing.
2022.
How would stance detection techniques evolve after the launch of chatgpt?
\textit{arXiv preprint} arXiv:2212.14548.

\bibitem{Zhang2017}
Yuhao Zhang, Victor Zhong, Danqi Chen, Gabor Angeli,
and Christopher D Manning.
2017.
Position-aware attention and supervised data improve slot filling.
In \textit{Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing}, pages 35--45.

\bibitem{Zhao2021}
Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and
Sameer Singh.
2021.
Calibrate before use: Improving few-shot performance of language models.
In \textit{International Conference on Machine Learning}, pages 12697--12706. PMLR.

\bibitem{Zhong2023}
Qihuang Zhong, Liang Ding, Juhua Liu, Bo Du, and
Dacheng Tao.
2023.
Can chatgpt understand too? a comparative study on chatgpt and fine-tuned bert.
\textit{arXiv preprint} arXiv:2302.10198.

\bibitem{ZhouChen2022}
Wenxuan Zhou and Muhao Chen.
2022.
An improved baseline for sentence-level relation extraction.
pages 161--168.

\end{thebibliography}

\end{document}
=====END FILE=====

=====FILE: figures/README.txt=====
Place figure image files here if available.

This LaTeX project uses placeholder boxes with the note "IMAGE NOT PROVIDED" for:

* Figure 1 (Primacy Effect of ChatGPT; predicted label index distribution in TACRED)
* Figure 2 (Prompt example for ChatGPT)
* Figure 3 (Label shuffling diagram)
* Figure 4 (Predicted index distributions across datasets)
  =====END FILE=====
