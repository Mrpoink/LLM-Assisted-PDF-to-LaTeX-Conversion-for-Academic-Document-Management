=====FILE: main.tex=====
\documentclass[11pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{amsmath,amssymb}
\usepackage{url}
\usepackage[hidelinks]{hyperref}

\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt}

\title{Combining Denoising Autoencoders with\
Contrastive Learning to fine-tune T[ansformer Models}

\author{
Alejo L6pez-Avita\
Huawei London Research Cenffe\
London, UK\
\texttt{alejo. lopez. avila@huawei. com}
\and
Vfctor Su6rez-Paniagua\
Huawei keland Research Center\
Dublin,Ireland\
\texttt{victor . suarez. pan iagua@huawei-partners . com}
}

\date{}

\begin{document}
\maketitle

\begin{abstract}
Recently, using large pre-trained Transformer
models for transfer learning tasks has evolved
to the point where they have become one of the
flagship trends in the Natural Language Pro-
cessing (NLP) community, giving rise to vari
ous outlooks such as prompt-based, adapters,
or combinations with unsupervised approaches,
among many others. In this work, we propose
a 3-Phase technique to adjust a base model
for a classification task. First, we adapt the
model's signal to the data distribution by per-
forming further training with a Denoising Au-
toencoder (DAE). Second, we adjust the repre-
sentation space of the output to the conespond-
ing classes by clustering through a Contrastive
Leaming (CZ) method. In addition, we intro-
duce a new data augmentation approach for Su-
pervised Contrastive Learning to correct the un-
balanced datasets. Third, we apply flne-tuning
to delimit the predefined categories. These dif-
ferent phases provide relevant and complemen-
tary knowledge to the model to learn the flnal
task. We supply extensive experimental results
on several datasets to demonsffate these claims.
Moreover, we include an ablation study and
compare the proposed method against other
ways of combining these techniques.
\end{abstract}

\section{Introduction}
The never-ending starvation of pre-trained Trans-
former models has led to fine-tuning these models
becoming the most common way to solve target
tasks. A standard methodology uses a pre-trained
model with self-supervised learning on large data
as a base model. Then, replace/increase the deep
layers to learn the task in a supervised way by
leveraging knowledge from the initial base model.
Even though specialised Transformers, such as Pe-
gasus (Zhang et a1.,2020) for summarisation, have
appeared in recent years, the complexity and re-
sources needed to train Transformer models of
these scales make flne-tuning methods the most
reasonable choice. This paradigm has raised in-
terest in improving these techniques, either from
the point of the architecture (Qin et a1.,2019),by
altering the definition of the fine-tuning task (Wang
etal.,202lb) or the input itself (Brown eta1.,2020),
but also by revisiting and proposing different self-
supervised training practices (Gao et a1.,2021).We
decided to explore the latter and offer a novel ap-
proach that could be utilised broadly for NLP clas-
sification tasks. We combine some self-supervised
methods with fine-tuning to prepare the base model
for the data and the task. Thus, we will have a
model adjusted to the data even before we start
fine-tuning without needing to train a model from
scratch, thus producing better results, as shown in
the experiments.

A typical way to adapt a neural network to the
input distribution is based on Autoencoders. These
systems introduce a bottleneck for the input data
distribution through a reduced laye1 a sparse layer
activation, or a restrictive loss, forcing the model to
reduce a sample's representation at the narrowing.
A more robust variant of this architecture is DAE,
which corrupts the input data to prevent it from
learning the identity function. The first phase of
the proposed method is a DAE (Fig. 1a), replacing
the final layer of the encoder with one more adapted
to the data distribution.

Contrastive Learning has attracted the attention
of the NLP community in recent years, This family
of approaches is based on comparing an anchor
sample to negative and positive samples. There are
several losses in Ctr, like the triplet loss (Schroff
et al., 2015), the contrastive loss (Chopra et al.,
2005), or the cosine similarity loss. The second
phase proposed in this work consists of a Con-
trastive Learning using the cosine similarity (as
shown in Fig. 1b), Since the datais labelled, we use
a supervised approach similar to the one presented
in (Khosla et a1.,2020) but through Siamese Neu-
ral Networks. In contrast, we consider Contrastive
Leaming and fine-tuning the classifier (Ffl as two
distinct stages. We also add a new imbalance cor-
rection during data augmentation that avoids over-
fitting. This CL stage has a clustering impact since
the vector representation belonging to the same
class will tend to get closer during the training. We
chose some benchmark datasets in classification
tasks to support our claims. For the hierarchical
ones, we can adjust labels based on the number of
similar levels among samples.

Finally, once we have adapted the model to the
data distribution in the first phase and clustered the
representations in the second, we apply fine-tuning
at the very last. Among the different variants from
fine-tuning, we use the traditional one for Natu-
ral language understanding (NLU), i.e., we add a
small Feedforward Neural Network (FNN) as the
classifier on top of the encoder with two layers. We
use only the target task data without any auxiliary
dataset, making our outlook self-contained. The
source code is publicly available at GitHub\footnote{\url{[https://github.com/vsuarezpaniagua/3-phase_finetun\%20i\%20ng}}](https://github.com/vsuarezpaniagua/3-phase_finetun\%20i\%20ng}}).

To summarise, our contribution is fourfold:
\begin{enumerate}
\item We propose a-Phase fine-tuning approach
to adapt a pre-trained base model to a su-
pervised classification task, yielding more
favourable results than classical fi ne-tuning.
\item We propose an imbalance correction method
by sampling noised examples during the aug-
mentation, which supports the Contrastive
Learning approach and produces better vector
representations.
\item We analyze possible ways of applying the de-
scribed phases, including ablation andjoint
Ioss studies.
\item We perform experiments on several well-
known datasets with different classification
tasks to prove the effectiveness of our pro-
posed methodology.
\end{enumerate}

\section{Related Work}
One of the first intplementations was presented
in (Reimers and Gurevych, 2019), an application
of Siamese Neural Networks using BERT (Devlin
et al., 2018) to learn the similarity between sen-
tences.

Autoencoders were introduced in (Kramer,
1991) and have been a standard for self-supervised
leaming in Neural Networks since then. However,
new modifications were created with the explo-
sion of Deep Learning architectures, such as DAE
(Vincent et al., 2010) and masked Autoencoders
(Germain et a1., 2015). The Variational Autoen-
coder (VAE) has been applied for NLP in (Miao
et a1.,2015) or (Li et a1.,20L9) with RNN net-
works or a DAE with Transformers in (Wang et al.,
2021 a). Transformers-based Sequential Denois-
ing Auto-Encoder (Wang et al.,202la) is an unsu-
pervised method for encoding the sentence into a
vector representation with limited or no labelled
data, creating noise through an MLM task. The
authors of that work evaluated their approach on
the following three tasks: Information Retrieval,
Re-Ranking, and Paraphrase Identification, show-
ing an increase of up to 6.4 points compared with
previous state-of-the- art approaches. In (Savinov
et a1.,202L), the authors employed a new Trans-
former architecture called Step-unrolled Denoising
Autoencoders. In the present work, we will apply
a DAE approach to some Transformer models and
extend its application to sentence-based and more
general classifi cation tasks.

The first work published on Contrastive Learn-
ing was (Chopra et al., 2005). After that, several
versions have been created, like the triplet net in
Facenet (Schroff et a1.,20t5) for Computer Vi-
sion. The triplet-loss compares a given sample and
randomly selected negative and positive samples
making the distances larger and shorter, respec-
tively. One altemative approach for creating pos-
itive pairs is slightly altering the original sample.
This method was followed by improved losses such
as N-pairLoss (Sohn, 2016) andthe Noise Con-
trastive Estimation (NCE) (Gutmann and Hyviiri-
nen, 2010), extending the family of CZ techniques.
In recent years, further research has been done on
applying these losses, e.g. by supervised methods
such as (Khosla eta1.,2020), which is the one that
most closely resembles one in our second phase.

Sentence-BERT (Reimers and Gurevych, 20 1 9)
employs a Siamese Neural Network using BERT
with a pooling layer to encode two sentences into a
sentence embedding and measure their similarity
score. Sentence-BERT was evaluated on Seman-
tic Textual Similarity (S7S) tasks and the SentE-
val toolkit (Conneau and Kiela, 2018), outperform-
ing other embedding strategies in most tasks. In
our particular case, we also use Siamese Networks
within the CL options. Similar to this approach,
the Simple Contrastive Sentence Embedding (Gao
et a1.,2021) is used to produce better embedding
representations. This unlabelled data outlook uses
two different representations from the same sam-
ple, simply adding the noise ttrough the standard
dropout. In addition, they tested it using entailment
sentences as positive examples and contradiction
sentences as negative examples and obtained better
results than SBERT in the ,575 tasks.

Whereas until a few years ago, models were
trained for a target task, the emergence of pre-
trained Transformers has changed the picture. Most
implementations apply transfer learning on a Trans-
former model previously pre-trained on general
NLU, on one or more languages, to the particular
datasets and for a predefined task.

This new paradigm has guided the search for
the best practice in each of the trending areas utch
as prompting strategies (Brown et a1.,2020),Few
Shot Learning (Wang et al.,202lb), meta-leaming
(Finn et a1.,2017), also some concrete tasks like
Intent Detection (Qin et a1.,2019), or noisy data
(Siddhant Garg,202l). Here, we present a task-
agnostic approach, outperforming some compet-
itive methods for their corresponding tasks. It
should also be noted that our method benefits from
using mostly unlabelled data despite some little
labelled data. In many state-of-the-art procedures,
like (Sun et a1.,2020a), other datasets than the tar-
get dataset are used during training. In our case,
we use only the target dataset.

\section{Model}
In this section, we describe the different proposed
phases: a Denoising Autoencoder that makes the
inputs robust against noise, a Contrastive Leaming
approach to identify the similarities between differ-
ent samples in the same class and the dissimilarities
with the other class examples together with a novel
imbalance correction, and finally, the traditional
fine-hrning of the classification model. In the last
part of the section, we describe another approach
combining the fust two phases into one loss.

\begin{figure}[t]
\centering
\fbox{\parbox{0.9\linewidth}{\centering IMAGE NOT PROVIDED}}
\caption{(a) First stage: Denoising Autoencoder architecture where the encoder and the decoder are based on the
pre-kained Transformers. The middle layer following the pooling together with the encoder model will be the
resulting model of this stage. (b) Second stage: Supervised Contrastive Learning phase. We add a pooling layer to
the previous one to learn the new clustered representation. The set of blocks denoted as DAECL will be employed
and adapted as a base model in the fine-tuning phase. (c) Third stage: Classification phase through fine-tuning}
\end{figure}

\subsection{DAE: Denoising Autoencoder phase}
The Denoising Autoencoder is the first of the pro-
posed 3-Phase approach, shown in Fig. 1a. Like
any other Autoencoder, this model consists of an
encoder and a decoder, connected through a bottle-
neck. We use two Transformer models as encoders
and decoders simultaneowly: RoBERiTa (Lin
et al., 2019), and all-Mi.niLM-L72-u2 (Wang
eta1.,2020). The underlying idea is that the bottle-
neck represents the input according to the general
distribution of the whole dataset. A balance needs
to be found between preventing the information
from being memorised and having sufficient sensi-
tivity to be reconstructed by the decoder, forcing
the bottleneck to learn the general distribution. We
add noise to the Autoencoder to prevent the bot-
tleneck from memorising the data. We apply a
Dropout on the input to represent the noise. For-
mally, for a sequence of tokens $X : {r_0,. . ., r_n}$
coming from a data distribution $D$, we define the
loss as
\begin{equation}
L_{\text{ons}}: ; \mathbb{E}*{p}\big[\log P*{\theta}(X \mid \tilde{X})\big]
\end{equation}
where $\tilde{X}$ is ,the sequence $X$ after adding the
noise. To srlnnort with an example, masking
a token at the position $i$ would produce $\tilde{X}$ :
${*0, . . . t fi i-t t 0, fr i+t, . . ., fin}$. The distribution
of $P_{\theta}$ in this Cross-Entropy loss corresponds to
the composition of the decoder with the encoder.
We consider the ratio of noise like another hyper-
parameter to tune. More details can be found in
Appendix A.l, and the results section 5. Instead
of applying the noise to the dataset and running a
few epochs over it, we apply the noise on the fly,
getting a very low probability of a repeated input.

Once the model has been trained, we extract
the encoder and the bottleneck, resulting in DAE
(Fig. 1a), which will be the encoder for the next
step. Each model's hidden size has been chosen as
its bottleneck size (768 in the case of RoBERTa).

The key point of the first phase is to adapt the
embedding representation of the encoder to the
target dataset distribution, i.e., this step shifts the
distribution from the general distribution learned
by the pre-trained Transformers encoder into one
ofthe target datasets. It should be noted here that
this stage is in no way related to the categories
for qualification. The first phase was implemented
using the SBERT ltbrary.\footnote{\url{[https://www.sbert.net/index.html}}](https://www.sbert.net/index.html}})

\subsection{CL: Contrastive Learning phase}
The second stage will employ Contrastive Learn-
ing, more precisely, a Siamese architecture with co-
sine similarity loss. The contrastive techniques are
based on comparing pairs of examples or anchor-
positive-negative triplets. Usually, these methods
have been applied from a semi-supervised point of
view. We decided on a supervised outlook where
the labels to train in a supervised manner are the
categories for classification, i.e., we pick a random
example. Then, we can get a negative input by sam-
pling from the other categories or a positive one
by sampling from the same category. We combine
this process of creating pairs with the imbalance
conection explained below to get pairs of vector
outputs $(2, r,)$.

Given two inputs $u$ and $u$ and a
label $label_{u,u}$ based on their class similarity.
\begin{equation}
label_{u,u} :=
\begin{cases}
1, & \text{if } z \text{ and } u \text{ are in the same class.}\
0, & \text{otherwise.}
\end{cases}
\end{equation}

We use a straightforward extension for the hier-
archical dataset (AGNews) by normalising these
weights along the number of levels. In the case of
two levels, we assign 1 for the case where all the
labels match, 0,5 when only the first one matches,
and 0 if none. After applying the encoder, we ob-
tain $u$ and $D$. We define the loss over these outputs
as the Mean Squared Enor (M SE):
\begin{equation}
L_{CL}: ; \big\lVert , label_{u,u} - \text{CosineSim}(u,u), \big\rVert^{2}
\end{equation}

We apply this Contrastive Learning to the en-
coder DAE, i.e., the encoder and the bottleneck
from the previous step. Again, we add an extra
layer to this encoder model, producing our follow-
ing final embedding. We denote this encoder after
applying CL over DAE as DAECL (Fig. 1b). Sim-
ilarly, we chose the hidden size per model as the
embedding size. CLpTays the role of soft clustering
for the embedding representation of the text based
on the different classes. This will make flne-tuning
much easier as it will be effortless to distinguish
representations from distinct classes. The second
phase was also implemented through the S B E RT
library.

\subsubsection{Imbalance correction}
As mentioned in the previous section, we are us-
ing a Siamese network and creating the pairs in a
supervised way. It is a common practice to aug-
ment the databefore applying CL.In theory we can
make as many example combinations as possible.
In the case of the Siamese models, we have a total
of $n!$ unique pair combinations that correspond to
the potential number of pairs based on symmetry.
Typically, data can be increased as much as one
considers suitable. The problem is that one may
fall short or overdo it and produce overfitting in
the smaller categories. Our augmentation is based
on two pillars. We start with correcting the imbal-
ance in the dataset by selecting underrepresented
classes in the dataset more times but without re-
peating pairs. Secondly, on the classes that have
been increased, we apply noise on them to provide
variants that are close but not the same to define
more clearly the cluster to which they belong.

We balance the dataset by defining the number of
examples that we are going to use per class based
on the most significant class (where $mark$ is its
size) and a range of ratios, from $minrrla, the mini-
mum and rrlatrratio$ the maximum. These upper and
lower bounds for the ratios are hyper-parameters,
and we chose 4 and 1.5 as default values. Formally,
the new ratio is defined by the function:
\begin{equation}
f(x) := \log!\left(\frac{r}{x}\right),\frac{mllrotio}{\log(marP)}
\end{equation}
where $r$ refers to the initial size of a given class.
After adding a lower bound for the ratio, we get the
final amount
\begin{align}
newratio_k &:= \min\big(min,; f(class_k)\big) \
newclass_k &:= newratio_k \times class_k
\end{align}
for $k := 1,2,\ldots,K$ in a classification problem
with $K$ classes, where $class_k$ and $newclass_k$ are
the cardinalities of the class $k$, before and after
the resizing, respectively. As we can observe, the
function 4 gives $f(1) : fl1,afrya16s$ for one example,
so we will never get something bigger than the
maximum. The shape of this function is similar to
a negative log-likelihood, given a high ratio to the
small classes and around the minimum for medium
or bigger. A simple computation shows that the
ratio 1 in function 4 is obtained at
\begin{equation}
r := marP^{;mllrotio-1},fmat766e \quad \text{(6)}
\end{equation}
or $marP^{;7}$ in our default case.

We duplicate this balanced dataset and shuffle
both to create the pairs. Since many combinations
exist, we only take the unique ones without repeat-
ing pairs, broadly preserving the proportions. Even
so, if just a few examples represent one class, the
clustering process from CL can be affected by the
augmentation because the border between them
would be deflned just for a few examples. To avoid
this problem, we add a slight noise to the tokens
when the text length is long enough. This noise
consists of deleting some of the stop-words from
the example. Usually, this noise was added to cre-
ate positive samples and produced some distortion
to the vector representation. Adding it twice, in
the augmentation and the CL, would produce too
much distortion. However, since we are using a
supervised approach, this does not negatively affect
the model, as shown in the ablation section 5.

\subsection{FT: fine-tuning phase}
Our final stage is fine-tuning, obtained by employ-
ing DAECL as our base model (as indicated in
Fig 1b). We add a two-layer MLP on top as a
classifier. We tried both to freeze and not to freeze
the previous neurons fuomDAECL.

As the flnal activation, we use Softmax, which
is a sigmoid function for the binary cases. More
formally, for $K$ classes, softmax corresponds to
\begin{equation}
\sigma(z_i) := \frac{e^{z_i}}{\sum_{i=1}^{K} e^{z_i}} \quad \text{for } i := 1,2,\ldots,K
\end{equation}

As a loss function for the classification, we min-
imize the use of Cross-Entropy:
\begin{equation}
L_{FT}:= -\sum_{k=1}^{K} y_k \log(p_k)
\end{equation}
where $p_k$ is the predicted probability for the class
$k$ and $y_k$ for the target. For binary classiflcation
datasets this can be furttrer expanded as
\begin{equation}
L_{FT}:= -\big(y\log(p) + (1-y)\log(1-p)\big)
\end{equation}

\subsubsection{Joint}
We wanted to check if we could benefit more when
combining lo$ses, i.e. by creating a joint loss based
on the flrst and the second loss, (1 and 3), respec-
tively.
\begin{equation}
L_{Joint}: ; L_{\text{ons}} * L_{CL}
\end{equation}
This training was obtained as a joint training of
stages one and two. By adding the classification
head, like in the previous section, for fine-tuning,
we got the version we denote as Joint (see Table 3).

\section{Experimental Setup}
The datasets for the experiments, the two base mod-
els used and the metrics employed are detailed be-
low.

\subsection{Datasets}
We have chosen several well-known datasets to
carry out the experiments:
\begin{itemize}
\item Intent recognition on \textasciicircum{}9N.IP,9 (SNIPS Nat-
ural Language Understanding benchmark)
(Coucke et al., 2018). For this dataset, we
found different versions, the first one with just
327 examples and 10 categories. This one was
obtained from the huggingface library\footnote{\url{[https://huggingface.co/datasets/snips_buiIt_in_intents}}](https://huggingface.co/datasets/snips_buiIt_in_intents}}), which
shows how this procedure performs on small
datasets.
\item The second version for ,SN,IP,S from
Kaggle\footnote{\url{[https://www.kaggle.com/datasets/wei\%20pengfei/at\%20i\%20s-sn\%20ips}}](https://www.kaggle.com/datasets/wei\%20pengfei/at\%20i\%20s-sn\%20ips}}) containing more samples and split
into 7 classes that we call ,SN.IP\textasciicircum{}S2 is the
most common one.
\item The third one is commonly used for slot pre-
diction (Qin et a1.,2019), although here we
only consider task intent recognition. We used
SST2 and,S,S\textasciicircum{}5 from (Socher etat.,2013)
for classification containing many short text
examples.
\item We add AGNews\footnote{\url{[https://huggingface.co/datasets/gimmaru/ag\_news}}](https://huggingface.co/datasets/gimmaru/ag\_news}}) (Zhang et al., 2015) to
our list, a medium size dataset that shows our
method over long text.
\item We complement the experiments with
IMDB\footnote{\url{[https://huggingface.co/datasets/imdb}}](https://huggingface.co/datasets/imdb}}) (Maas et al.,2Ol1), a big dataser
with long inputs for binary classification in
sentiment analysis, The length and size of this
data made us consider only the RoBERTa
as the base model.
\end{itemize}

We used Hugging Face API to download all the
datasets apart from the second version of ,SI/,IP,9,
In some cases, there was no validation dataset, so
we used 20% of the training set to create the vali-
dation dataset. There was an unlabelled test set in
\textasciicircum{}9If1P\textasciicircum{}9, so we extracted another l0%for testing,
The unlabelled data was used for the first training
phase, not for the other ones. The first and second
phases did not use the test set. We selected the best
model for each of them based on the results of the
validation dataset. The test set was used at the end
of phase 3.

\subsection{Models and Thaining}
We canied out experiments with two different mod-
els, a small model for short text all-Mi,ni,LM-
L72-u2 (Wang et al., 2020)\footnote{\url{[https://huggingface.co/sentence-transformers/a1I-MiniLM-11\%202-v2}}](https://huggingface.co/sentence-transformers/a1I-MiniLM-11\%202-v2}}), more concretely, a
version fine-tuned for sentence similarity and a
medium size model RoBERTa-base (Liu et al.,
2019)\footnote{\url{[https://huggingface.co/roberta-base}}](https://huggingface.co/roberta-base}}) which we abbreviate as RoBERTa. The
ranges for the hyper-parameters below, as well as
the values of the best accuracy, can be found in
Appendix A.1.

We use Adam as the optimiser. We test differ-
ent combinations of hyper-parameters, subject to
the model size. We tried batch sizes from 2 to
128, whenever possible, based on the dataset and
the base model. We tested the encoder with both
f'rozen and not frozen weights - almost always get-
ting better results when no freezing is in place. We
tested two different truncations' lengths based ei-
ther on the maximum length in the training dataset
plus a 20% or the default maximum length for the
inputs in the base model. We tried to give more
importance to one phase over the other by applying
data augmentation in CL or extending the num-
ber of epochs for the Autoencoder (Appendix A.l),
We increased the relevance of the first phase by
augmenting the data before applying random noise
instead of changing the number of epochs. We tune
the value of ratio, getting 0.6 as the most common
best value. To increase the relevance ofthe second
phase, we increased the number of inputs by cre-
ating more pair combinations. Since we increased
or decreased the relevance of one or other phases
based on the amount of data, we used the same
learning rate for the first two phases.

\subsection{Metrics}
We conducted experiments using the datasets pre-
viously presented in Section 4.1. We used the stan-
dard macro metrics for classiflcation tasks, i.e., Ac-
curacy, Fl-score, Precision, Recall, and the confu-
sion matrix. We present only the results from Ac-
curacy in the main table to compare against other
works. The results for other metrics can be found
in Appendix A.2.

\begin{table}[t]
\centering
\caption{Statistics for Train, Validation and Test daraset splits.}
\label{tab:stats}
\begin{tabular}{lrrrr}
\hline
Dataset & Train & Train DAE & Train SCL & Test \
\hline
SST2   & 69170 & [ILLEGIBLE] & [ILLEGIBLE] & 872 \
SNIPS  & 262   & 262         & [ILLEGIBLE] & 65 \
SNIPS2 & 13084 & [ILLEGIBLE] & [ILLEGIBLE] & 700 \
SST5   & 8544  & 1084        & [ILLEGIBLE] & 2210 \
AGNews & 120K  & 7600        & [ILLEGIBLE] & 7600 \
IMDB   & 25K   & 25K         & [ILLEGIBLE] & 25K \
\hline
\end{tabular}
\end{table}

\begin{table}[t]
\centering
\caption{Average and max lengths for each of the datasets mentioned in the paper.}
\label{tab:lengths}
\begin{tabular}{lrr}
\hline
Dataset & Avg. Length & Max. Length \
\hline
SST2   & 9   & 52 \
SNIPS  & 9   & 20 \
SNIPS2 & 9   & 35 \
SST5   & 19  & 56 \
AGNews & 31  & 171 \
IMDB   & 229 & 2450 \
\hline
\end{tabular}
\end{table}

\section{Results}
We assess the performance of the three methods
against the several prominent publicly available
datasets. Thus, here we evaluate the -Ph,ase pro-
cedure compared to Joint and FT approaches. We
report the performance of these approaches in terms
of accuracy in Table 3

We observe a noticeable improvement of at
least |Vo with 3-Ph,ase as compared to the sec-
ond best performing approach, Joint, in almost
all the datasets. In S N I P 52 with alL-Mini,LM-
L72-u2 we get the same values, while in I M DB
with RoBERTa we get a gap of 4 point and 8 for
the,S.A/1P\textasciicircum{}9 dataset with aLL- h[ i.ni, L M - LL2-u2.
We apply these three implementations to both base
models, except I A,[ D B , as the input length is too
long for all-A,[i.ni,LM-L72-u2. FZ method on its
own performs the worst concerning the other two
counterparts for both the models tested. Eventually,
we may conclude that the 3-Phase approach is
generally the best one, followedby Joint, and as ex-
pected, FZ provides the worst. We can also observe
that Joint and FT tend to be close in datasets with
short input, while ,4GNetus gets closer results for
3-Phase and Joint.

We did not have a bigger model for those
datasets with long input, so we tried to com-
pare against approaches with similar base models.
We truncated the output for the datasets with the
longest inputs, which may reflect smaller values in
our case. Since the advent of (Sun et a1.,2020a),
several current techniques are based on combining
different datasets in the first phase through multi-
task learning and then flne-tuning each task in de-
tail. Apart from (Sun et a1.,2020a), this is the case
for the prompt-base procedure from EFL (Wang
et al.,202lb) as well. Our method focuses on ob-
taining the best results for a single task and dataset.
Several datasets to pre-train the model could be
used as a phase before all the others. However, we
doubt the possible advantages ofthis as the first and
second phases would negatively affect this learning,
and those techniques focused on training the clas-
sifier with several models would negatively affect
the first two phases.

To complete the picture, CAE (Phuong et al.,
2022) is an architecture method, which is also
independent of the pre-training practice. A self-
explaining framework is proposed in (Sun et al.,
2020b) as an architecture method that could be
implemented on top of our approach.

\begin{table}[t]
\centering
\caption{Performance accuracy on different datasets usitg RoBERTa and all-Mini,LM-Ll2-u2 models in %. 3-Phase refers to our main 3 stages approach, whrle Joint denotes one whose loss is based on the combination of the flrst two losses, and FT corresponds to the fine-tuning. We also add some SOTA results from other papers: S-P denotes Stack-Propagati,on (Qin et a1.,2019), Self-E is used to denote (Sun et al., 2020b) (in this case we chose the value from RoB ERTo-base for a fair comparison), CAE is used to detonate (Phuong et a1.,2022), STC-DeBERTa refers to (Karl and Scherp, 2022), EFL points to (Wang et al,,202lb) (this one uses RoBERTa-Large), and FIBERT is (Sun etal.,2020a).}
\label{tab:acc}
\small
\begin{tabular}{llrrrrrrrr}
\hline
Base & Dataset & 3-Phase & Joint & FT & S-P & EFL & CAE & Self-E & STC-DeBERTa & FTBERT \
\hline
RoBERTa & SNIPS  & 99.81 & 94.92 & 91.01 & 99.0 & 99.0 & 98.3 & [ILLEGIBLE] & 94.78 & [ILLEGIBLE] \
RoBERTa & SNIPS2 & 98.29 & 98.0  & 97.57 & 99.0 & 99.0 & 98.3 & [ILLEGIBLE] & [ILLEGIBLE] & [ILLEGIBLE] \
RoBERTa & SST2   & 95.07 & 93.t2 & 90.28 & 98.3 & 98.3 & 96.9 & [ILLEGIBLE] & 94.78 & [ILLEGIBLE] \
RoBERTa & SST5   & 56.79 & 53.88 & 52.27 & [ILLEGIBLE] & [ILLEGIBLE] & [ILLEGIBLE] & 56.2 & [ILLEGIBLE] & [ILLEGIBLE] \
RoBERTa & AGNews & 9s.08 & 94.82 & 92.47 & [ILLEGIBLE] & [ILLEGIBLE] & [ILLEGIBLE] & 86.1 & [ILLEGIBLE] & [ILLEGIBLE] \
RoBERTa & IMDB   & 99.0  & 95.07 & 91.0  & [ILLEGIBLE] & [ILLEGIBLE] & [ILLEGIBLE] & 96.1, & [ILLEGIBLE] & 95.20 \
\hline
all-Mini,LM-Ll2-u2 & SNIPS  & 100.00 & 91.98 & 92.89 & 99.0 & 99.0 & 98.3 & [ILLEGIBLE] & 94.78 & [ILLEGIBLE] \
all-Mini,LM-Ll2-u2 & SNIPS2 & 98.57 & 98.57 & 93.86 & 99.0 & 99.0 & 98.3 & [ILLEGIBLE] & [ILLEGIBLE] & [ILLEGIBLE] \
all-Mini,LM-Ll2-u2 & SST2   & 93.89 & 90.04 & 88.21 & 98.3 & 98.3 & 96.9 & [ILLEGIBLE] & 94.78 & [ILLEGIBLE] \
all-Mini,LM-Ll2-u2 & SST5   & 54.77 & 52.25 & 49.24 & [ILLEGIBLE] & [ILLEGIBLE] & [ILLEGIBLE] & 56.2 & [ILLEGIBLE] & [ILLEGIBLE] \
all-Mini,LM-Ll2-u2 & AGNews & 94.83 & 94,28 & 89.57 & [ILLEGIBLE] & [ILLEGIBLE] & [ILLEGIBLE] & 86.1 & [ILLEGIBLE] & 95.20 \
\hline
\end{tabular}
\end{table}

\subsection{Ablation study}
We conducted ablation experiments on all the
datasets, choosing the same hyper-parameters and
base model as the best result for each one. The
results can be seen in Table 4.

We start the table with the approaches men-
tioned: }-Phaseand Joint. We wanted to see if
combining the first two phases could produce better
results as those would be leamed simultaneously.
The results show that merging the two losses al-
ways leads to worse results, except for one of the
datasets where they give the same value.

We start the ablations by considering only DAE
right before fine-funing, denoting itas DAE+FT.In
this case, we assume that the fine-tuning will carry
all the class information. One advantage of this
outlook is that it still applies to models that employ
a small labelled fraction of all the data (i.e., the
unlabelled data represents the majority). The next
column, CL+FT, replaces DAE with Contrastive
Learning, concentrating the attention on the classes
and not the data distribution. Considering only the
classes and fine-tuning in CL+FZ, we get better
results than in DAE+FT, but still lower than the
}-Phasein almost all the datasets. Right after, we
add two extreme cases of the imbalance correction,
where Extra Imb. increases the upper bound for the
ratio and No Imb. excludes the imbalance method.
Both cases generally produce lower accuracies than
3-Phase, being No Imb. slightly lower. The last
column corresponds to fine-tuning FZ.

All these experiments proved that the proposed
3-Phase approach outperformed all the steps inde-
pendently, on its own, or combined the Denoising
Autoencoder and the Contrastive Learning as one
loss.

\begin{table}[t]
\centering
\caption{Ablation results. As before, S-Phase, Joint, and,F7 correspond to the 3 stages approach, joint losses,
and Fine-tuning, respectively. Here, DAE+FT denotes the denoising autoencoder tog.Ih". with fine-tuning,
CL+FT denotes the contrastive Siamese training together with flne-tunin g, No Imb, means 3-phose but skipping
the imbalance correction, and Extra Imb. rcfers to an increase of the imbalance correction to ?rninrol4o=l.b and
Tfi,At7o16o=)5.}
\label{tab:ablation}
\small
\begin{tabular}{llrrrrrrr}
\hline
Dataset & Base Model & 3-Phase & Joint & DAE+FT & CL+FT & Extra Imb. & No Imb. & FT \
\hline
SNIPS  & all-MiniLM-Ll2-u2 & 100.00 & 91.98 & 98.05 & 99.81 & 99.92 & 99.88 & 92.89 \
SNIPS2 & all-MiniLM-Ll2-u2 & 98.57  & 98.57 & 94.14 & 97.86 & 98.00 & 97.8s & 93.86 \
SST2   & RoBERTa & 95.07 & 93.12 & 83.72 & 94.72 & 94.50 & 94.04 & 90.28 \
SST5   & RoBERTa & 56.79 & 53.88 & 46.06 & 56.52 & 56.24 & 55.84 & 52.27 \
AGNews & RoBERTa & 95.08 & 94.82 & 91.53 & 95.U  & 95.01 & 94.26 & 92.47 \
IMDB   & RoBERTa & 99.00 & 95.07 & 93.00 & 94.86 & 97.00 & 94.76 & 91.10 \
\hline
\end{tabular}
\end{table}

\section{Conclusion}
The work presented here shows the advantages of
fine-tuning a model in different phases with an
imbalance correction, where each stage considers
certain aspects, either as an adaptation to the text
characteristics, the class differentiation, the imbal-
ance, or the classification itself. We have shown
that the proposed method can be equally or more
effective than other methods explicitly created for
a particular task, even if we do not use auxiliary
datasets. Moreover, in all cases, it outperforms
classical flne-tuning, thus proving that classical
fine-tuning only partially exploits the potential of
the datasets. Squeezing out all thejuice from the
data requires adapting to the data distribution and
grouping the vector representations according to
the task before the fine-tuning, which in our case is
targeted towards classifi cation.

\section{Future work}
The contrastive training phase benefits of data aug-
mentation, i.e., we can increase the number of
examples simply through combinatorics. How-
ever, this can lead to space deformation for small
datasets, even with the imbalance correction, as
fewer points are considered. Therefore, overfi tting
occurs despite the combinatoric strategy. Another
advantage of this phase is balancing the number
of pairs with specific values. This practice allows
us, for example, to increase the occurrence of the
underrepresented class to make its cluster as well
deflned as those of the more represented categories
(i.e. ones with more examples). This is a partial
solution for the imbalance problem.

In the future, we want to address these problems.
For the unbalanced class in the datasets, seek a so-
lution to avoid overfitting to the under-represented
classes and extend our work to support a few shot
learning settings (FSL). To do so, we are going
to analyze different data augmentation techniques.
Among others, Variational Autoencoders. Recent
approaches for text generation showed that hierar-
chicalVAE models, such as stable diffusion models,
may produce very accurate augmentation models.
One way to investigate this is to convert the first
phase into a VAE model, allowing us to generate
more examples from underrepresented classes and
generally employ them all in the F,9.L setting.

Finally, we would like to combine our approach
with other fine-tuning procedures, like prompt
methods. Adding a prompt may help the model
gain previous knowledge about the prompt struc-
ture instead of learning the prompt pattern simulta-
neously during the classification while fine-tuning.

\section*{Limitations}
This approach is hard to combine with other fine-
tuning procedures, mainly those which combine
different datasets and use the correlation between
those datasets, since this one tries to extract and
get as close as possible to the target dataset and
task. The imbalance correction could be improved,
restricting to cases where the text is short because
it could be too noisy or choosing the tokens more
elaborately and not just stop words. It would be
necessary to do more experiments combined with
other approaches, like the prompt base, to know if
they benefit from each other or if they could have
negative repercussions in the long term.

\section*{Acknowledgments}
The authors would like to thank the members of
the AlApps Research Group in the Huawei keland
and London Research Centers for their valuable
discussion and comments. We especially want to
thank Milan Redzic, Tri Kurniawan Wijaya and
Jinhua Du for their help.

\begin{thebibliography}{99}

\bibitem{} Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-Voss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens
Winter, and Dario Amodei. 2020. Langtage models
are few-shot leamers.

\bibitem{} S. Chopra, R. Hadsell, and Y. LeCun. 2005. Learning
a similarity metric discriminatively, with application
to face verification. In 2005 IEEE Computer Society
Conference on Computer Vision and Pattern Recog-
nition (CVPR'05), volume 1, pages 539-546 vol. 1.

\bibitem{} Alexis Conneau and Douwe Kiela. 2018. SentEval: An
evaluation toolkit for universal sentence representa-
tions. In Proceedings ofthe Eleventh International
Conference on Language Resources and Evaluation
(LREC 2018), Miyazaki, Japan, European Language
Resources Association (ELRA).

\bibitem{} Alice Coucke, Alaa Saade, Adrien Ball, Th6odore
Bluche, Alexandre Caulier, David Leroy, Cl6ment
Doumouro, Thibault Gisselbrecht, Francesco Calt-
agirone, Thibaut Lavril, Madl Primet, and Joseph
Dureau. 2018. Snips voice platform: an embedded
spoken language understanding system for private-
by-design voice interfaces.

\bibitem{} Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing.

\bibitem{} Chelsea Finn, Pieter Abbeel, and Sergey Levine.2017 .
Model-agnostic meta-learning for fast adaptation of
deep networks. ln Proceedings ofthe 34th Interna-
tional Conference on Machine Leaming, volume 70
of Proceedings of Machine Learning Research, pages
1t26-1135. PMLR.

\bibitem{} Tianyu Gao, Xingcheng Yao, and Danqi Chen 2021.
SimCSE: Simple contrastive learning of sentence em-
beddings. In Proceedings of the 2021 Conference
on Empirical Methods in Natural Lctnguage Process-
ing, pages 6894-6910, Online and Punta Cana, Do-
minican Republic. Association for Computational
Linguistics.

\bibitem{} Mathieu Germain, Karol Gregor, Iain Murray, and Hugo
Larochelle. 2015. Made: Masked autoencoder for
distribution estimation. ln Proceedings of the 32nd
International Conference on International Confer-
ence on Machine Leaming - Volume 37,[CM.U15,
page 88 1-889. JMLR.org.

\bibitem{} Michael Gutmann and Aapo Hyviirinen. 2010. Noise-
contrastive estimation: A new estimation principle
for unnormalized statistical models. In Proceedings
of the Thirteenth International Conference on Artifi-
cial Intelltgence and Statistics, volume 9 of Proceed-
in g s of M ac hin e Le arnin g Re s e arch, pages 297 -304,
Chia Laguna Resort, Sardinia, Italy, PMLR.

\bibitem{} Fabian Karl and Ansgar Scherp.2022. Transformers are
short text classiflers: A study ofinductive short text
classifiers on benchmarks and real-world datasets.

\bibitem{} Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron
Sama, Yonglong Tian, Phillip Isola, Aaron
Maschinot, Ce Liu, and Dilip Krishnan. 2020. Su-
pervised Contrastive Leaming. arXiv e-prints, page
uXiv:2004.11362.

\bibitem{} Mark A. Kramer. 1991. Nonlinear principal compo-
nent analysis using autoassociative neural networks.
Aiche lournal, 37 :233-243.

\bibitem{} Ruizhe Li, Xiao Li, Chenghua Lin, Matthew Collinson,
and Rui Mao. 2019. A stable variational autoen-
coder for text modelling . In Proceedings of the 12th
International Conference on Nqtural Language Gen-
eration, pages 594-599, Tokyo, Japan. Association
for Computational Linguistics.

\bibitem{} Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized bert pretraining ap-
proach. ArXiv, abs/ 1907 .11692.

\bibitem{} Andrew L. Maas, Raymond E. Daly, Peter T. pham,
Dan Huang, Andrew Y. Ng, and Christopher potts,
2011. Learning word vectors for sentiment analysis.
ln Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics : Human
Language Technolo g ie s, pages 142-1 50, Portland,
Oregon, USA. Association for Computational Lin-
guistics.

\bibitem{} Yishu Miao, Lei Yu, and Phil Blunsom, 2015. Neural
variational inference for text processing,

\bibitem{} Nguyen Minh Phuong, Tung Le, and Nguyen Le Minh.
2022. Cae: Mechanism to diminish the class imbal-
anced in slu slot filling task. In Advqnces in Com-
putati onal C oll e ctiv e I nt e lli g enc e, pages 1 50- I 63,
Cham. Springer International Publishing.

\bibitem{} Libo Qin, Wanxiang Che, Yangming Li, Haoyang Wen,
and Ting Liu, 2019. A stack-propagation framlwork
with tokenJevel intent detection for spoken language
understanding. pages 207 8-2087 .

\bibitem{} Nils Reimers and Iryna Gurevych. 2019. Sentence-bert:
Sentence embeddings using siamese bert-networks,

\bibitem{} Nikolay Savinov, Junyoung Chung, Mikolaj Binkowski,
Erich Elsen, and Aaron van den Oord. 2021. Step-
unrolled denoising autoencoders for text generation,

\bibitem{} Florian Schroff, Dmitry Kalenichenko, and James
Philbin. 2015. Facenel A unified embedding for
face recognition and clustering. In2015 IEEE-Con-
ference on Computer Vsion and Pattern Recognition
(CVPR), pages 815-823.

\bibitem{} Varun Thumbe Siddhant Garg, Goutham Ramakrish-
nan.202l. Towards robustness to label noise in text
classiflcation via noise modeling. arXiv preprint
arXiv:2101.1l2l4v3.

\bibitem{} Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. 2013, Recursive deep models for
semantic compositionality over a sentiment treebank.
Irr Proceedings of the 2013 Conference on Empiri-
cal Methods in Natural Langudge Processing, pages
163l-1642,Seattle, Washingon, USA. Association
for Computational Linguistics.

\bibitem{} Kihyuk Sohn. 2016. Improved deep metric learning
with multi-class n-pair loss objective. InAdvances in
Neural Information Processing Systems, volume 29.
Curran Associates, Inc.

\bibitem{} Chi Sun, Xipeng Qiu, Yige Xu, and Xuanjing Huang.
2020a. How to fine-tune bert for text classification?

\bibitem{} Zijun Sun, Chun Fan, Qinghong Han, Xiaofei Sun,
Yuxian Meng, Fei Wu, and Jiwei Li. 2020b. Self-
explaining-structures improve nlp models.

\bibitem{} Pascal Vincent, Hugo Larochelle, Isabelle Lajoie,
Yoshua Bengio, and Pierre-Antoine Manzagol. 2010.
Stacked denoising autoencoders: Learning useful rep-
resentations in a deep network with a local denoising
criterion. J., Mach. Learn. Res., ll:337 1-3408.

\bibitem{} Kexin Wang, Nils Reimers, and kyna Gurevych. 2021a.
Tsdae: Using transformer-based sequential denoising
auto-encoder for unsupervised sentence embedding
leaming.

\bibitem{} Sinong Wang, Han Fang, Madian Khabsa, Hanzi Mao,
and Hao Ma2021b. Entailment as few-shot learner.

\bibitem{} Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan
Yang, and Ming Zhou. 2020. Minilm: Deep self-
attention distillation for task-agnostic compression
of pre-trained transformers. In NeurIpS 2020. ACM,

\bibitem{} Jingqing Zhang,YaoZhao, Mohammad Saleh, and pe-
ter J. Liu. 2020, Pegasus: Pre{raining with extracted
gap-sentences for abstractive summarization.

\bibitem{} XiangZhang, Junbo Jake Zhao, and yann LeCun. 2015.
Character-level convolutional networks for text clas-
sification, In N/P.S.

\end{thebibliography}

\appendix
\section{Appendix}

\subsection{Hyper-parameters}
This appendix section shows the final hyper-parameters from the best results in Table 3. The column at
the end on the right contains the search space used for training. Some of the values were not used for
training, either because of computational limitations or because they were not realistic for some datasets.

\begin{table}[t]
\centering
\caption{Hyper-parameters configurations and search space of the experiments.xl means tlese are not real epochs
since the input data is not always the same. The data was masked on the fly; therefore, each epoch differs. *2 We
used a an early stopping approach for the FI phase. *3 We only consider the epsilon hyperparameter in the
AdamW optimizeifor ff. in" other two phases use the default value from the Transformers library (1e-06;. *4
This hyper-parameter was estimated initially with the training dataset with a large margin. This was applied for
datasets with very short sentences, like ,9N.IPS. *5 This hyper-parameter estimates the max length of the
sequences using the 10% of the examples. This estimation is multiplied by 1.2 and is added as the maximum size of
the sequences for the embedding layers. The difference is that it was done on the fly and not preserved in this case.}
\label{tab:hyper}
\small
\begin{tabular}{l|l}
\hline
Dataset Best Hyper-parameters & [ILLEGIBLE: See PDF page 11 for full table layout and values] \
\hline
\end{tabular}
\end{table}

\subsection{Other metrics}
In this section of the appendix, we present the results obtained for metrics other than accuracy. More
specifically, we present three tables: Precision and Recall in Table 6, and Fl (Table 7). These metrics
show better the role played by the imbalance correction. The notation follows the Table 3.

\begin{table}[t]
\centering
\caption{Precision and Recall values. The best values are shown in bold}
\label{tab:prec-recall}
\small
\begin{tabular}{l|rrr|rrr}
\hline
Dataset & \multicolumn{3}{c|}{Precision} & \multicolumn{3}{c}{Recall} \
& 3-Phase & Joint & FT & 3-Phase & Joint & FT \
\hline
\multicolumn{7}{l}{RoBERTa} \
SNIPS  & 99.81 & 94.94 & 91.04 & 99.81 & 94.99 & 91.03 \
SNIPS2 & 98.26 & 91.96 & 97.50 & 98.30 & 98.09 & 97.72 \
SST2   & 9s.62 & 94.29 & 92.53 & 9s.05 & 92.32 & 89.2r \
SST5   & 55.5r & 53.23 & 49.50 & 53.71 & 51.18 & 49.02 \
AGNews & 95.09 & 94.82 & 92.44 & 9s.08 & 94.82 & 92.41 \
IMDB   & 98.08 & 95.56 & 91.83 & 100.0 & 96.74 & 91.03 \
\hline
\multicolumn{7}{l}{all-Mi,niLLtt-L12-u2} \
SNIPS  & 100.00 & 92.0s & 93.12 & 100.00 & 92.06 & 92.98 \
SNIPS2 & 98.61  & 98.64 & 93.89 & 98.60  & 98.09 & 94.t4 \
SST2   & 93.92  & 92.32 & 88.79 & 9t.24  & 46.02 & 88.1 3 \
SST5   & 59.57  & 61.62 & 53.1 5 & 40.02 & 94,28 & 40.02 \
AGNews & 94.84  & 94.28 & 89.51 & 94.83 & 94.28 & 89.57 \
\hline
\end{tabular}
\end{table}

\begin{table}[t]
\centering
\caption{Fl values for the best results.The best values are shown in bold}
\label{tab:f1}
\small
\begin{tabular}{l|rrr}
\hline
Dataset & 3-Phase & Joint & FT \
\hline
\multicolumn{4}{l}{RoBERTa} \
SNIPS  & 99.81 & 94.95 & 91.03 \
SNIPS2 & 98.28 & 98.01 & 97.57 \
SST2   & 95.14 & 93.29 & 90.82 \
SST5   & 54.59 & 52.t8 & 49.24 \
AGNews & 95.08 & 94.81 & 92.4s \
IMDB   & 99.03 & 95.10 & 91.43 \
\hline
\multicolumn{4}{l}{all-Mini,LM-L1,2-u2} \
SNIPS  & 100.00 & 92.0 & [ILLEGIBLE] \
SNIPS2 & 98.63  & 98.60 & [ILLEGIBLE] \
SST2   & 94.39  & 93.90 & 92.81 \
SST5   & 53.78  & 52.69 & 88.46 \
AGNews & 94.83  & 94.27 & 89.56 \
\hline
\end{tabular}
\end{table}

\end{document}
=====END FILE=====

=====FILE: figures/README.txt=====
Source PDF: 
=====END FILE=====
