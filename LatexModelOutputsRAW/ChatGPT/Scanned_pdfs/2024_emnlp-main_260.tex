=====FILE: main.tex=====
\documentclass[11pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{url}
\usepackage{hyperref}

\setlength{\columnsep}{0.25in}

\title{Outcome-Constrained Large Language Models for Countering Hate Speech}

\author{
Lingzi Hong \and Pengcheng Luo \
University of North Texas \and Peking University \
lingzi . hongGunt . edu \and luopcGpku. edu. cn
\and
Eduardo Blanco \and Xiaoying Song \
University of Arizona \and University of North Texas \
eduardoblancoG ari zona . edu \and XiaoyingSongGmy . unt . edu
}

\date{}

\begin{document}
\twocolumn
\maketitle

\begin{abstract}
Automatic counterspeech generation methods have been developed to assist efforts in combating hate speech. Existing research focuses on generating counterspeech with linguistic attributes such as being polite, informative, and intent-driven. However, the real impact of counterspeech in online environments is seldom considered. This study aims to develop methods for generating counterspeech constrained by conversation outcomes and evaluate their effectiveness. We experiment with iarge language models (LLMs) to incorporate into the text generation process two desired conversation outcomes: low conversation incivility and non-hateful hater reentry. Specifically, we experiment with instuction prompts, LLM finetuning, a:ad LLM reinforcement learning (rRI). Evaluation results show that our methods effectively steer the generation of counterspeech towards the desired outcomes. Our analyses, however, show that there are differences in the quality and style depending on the model.
\end{abstract}

\section{Introduction}
Hate speech has posed significant challenges to healthy and productive online communication. Counterspeech, which involves using constructive, positive, or factual responses to challenge or counteract hate speech, has shown to be effective in moderating online hostilities (Buerger, 2O2l), promoting productive user engagement (Mi5kolci et al., 2020), and educating online users (Blaya , 2019).

Automatic generation of counterspeech has been researched to support timely and effective efforts to fight hate speech. Synthetic counterspeech datasets have been developed using crowdsourcing (Qian et a1., 2019) and human-in-the-loop strategies (Chung et a1.,2021). These datasets have been used to develop counterspeech generation models. However, the impact of counterspeech in online environments has not been considered in the dataset creation. As a result, it is unknown whether generated counterspeech elicits civil or hateful follow-up conversations.

Recent counterspeech generation research focused on constrained generation with linguistic attributes (e.g., being polite, emotion-laden (Saha eta1.,2022)), or embedded with knowledge (Chung et a1.,2021). Questions about the impact of counterspeech with such attributes linger. Previous research also found one ofthe barriers counterspeakers face is their inability to determine the potential impact of counterspeech (Mun et a1.,2024). However, there is a lack of research on generating outcome-oriented counterspeech, e.g., speech that leads to desired outcomes such as de-escalating user conflicts or encouraging constructive engagement in follow-up conversations.

Notably, previous studies indicate that language may influence the development of a conversation, including discourse popularity (Horawalavithana et a1.,2022), reentry behaviors (Wang et a1.,2021), and the rise of hate speech (Liu et al., 2018). This leads to our research questions:
\begin{itemize}
\item How can constraints on conversation outcomes be incorporated into developing LLMs for generating counterspeech?
\item How effective are these methods in generating outcome-oriented counterspeech?
\end{itemize}

Unlike previous work that considers explicit linguistic attributes to guide language generation, we formulate counterspeech generation to achieve desired outcomes (e.g., constructive user engagement). Our study holds potential for broader applications. Anticipating the direction of a conversation is crucial in crafting effective responses, reducing the generation of hate speech, altering user behavior, and promoting positive discourse. This study makes the following contributions: (i) introducing conversation outcomes as a constraint to guide the generation of counterspeech, (ii) experimenting with LLMs for generating outcome-constrained counterspeech using instruction prompts, LLM finetuning, and LLM reinforcement learning (RL), and (iii) evaluating counterspeech generation models with various metrics to understand the strengths and weaknesses of the methods.

\section{Related Work}

\subsection{Generating Counterspeech}
Table~\ref{tab:priorwork} presents recent work on counterspeech generation. CONAN has counterspeech written by NGO experts and augmented by language models (Chung et aL,2019); Benchmark was built with hate speech from Gab and Reddit and counterspeech created by crowdsourcing workers (Qian et a1.,2019); and MultiCONAN is a high-quality, high-quantity dataset created by experts coupled with language model generation for hate speech with multiple targets (Fanton et al., 2021). Counterspeech generation models have been built with these datasets (Halim et al., 2023; Tekko[lu et a1., 2020,2022; Bonaldi et al., 2024). Unlike us, none consider conversation outcomes elicited by the generated counterspeech.

Researchers have investigated counterspeech generation under consfaints. Chung et al. (2021) proposed a generation pipeline grounded in external knowledge repositories to generate more informative and less biased replies. Zht and Bhat (202 I ) proposed to generate more diverse and relevant counterspeech by developing a three-stage pipeline that uses LLMs to generate candidates, prunes the ungrammatical ones, and selects the best instances. Saha et al. (2022) proposed an ensemble generative discriminator to generate more polite, detoxified, and emotionladen counterspeech, Gupta et al. (2023) developed IntentCONAN, where the generation of counterspeech is conditioned on five intents: informative, denouncing, questioning, positive, and humorous. Similarly, Fraser et al. (2023) utilized ChatGPT to generate counter-stereotype text by incorporating countering strategies in queries, Hassan and Alikhani (2023) proposed prompting strategies based on discourse theories to generate more context-relevant counterspeech. There are also studies on the generation of counterspeech in languages other than English (e.g., Italian (Chung et a1.,2020)). Unlike us, none of these previous works generate counterspeech to elicit positive behaviors in the follow-up conversations.

\begin{table*}[t]
\centering
\small
\begin{tabular}{p{2.7cm}p{2.6cm}p{2.6cm}p{6.6cm}}
\toprule
Prior Work & Constraint & Hate Speech & Generation Method \
\midrule
CONAN (Chung et a1.,2019) & None & Islamophobic & Expert-based and LM data augmentation \
Benchmark (Qian et a1.,2019) & None & Reddit, Gab & Crowdsourcing and LM generation \
MUltiCONAN (Fanton etal.,202l) & None & Multiple hate targets & LLM generation with re-viededits by experts \
Knowledge (Chung etal.,202l) & Informative & CONAN & LLM generation with information from knowledge repository \
Generate-Prune (Zhu and Bhat,202l) & Diverse and relevant & Benchmark, CONAN & LLM generation with qualiry classifler \
COTINTERGEDI (Sahaet a1.,2022) & Polite, detoxified, and emotional & Benchmark, CONAN & DialoGPT and GEDI for constraint generation \
Intent (Gupta eta1.,2023) & Multiple intents & CONAN, MuItiCONAN & QUARC with intent category representation and fu sion \
Ours & Expected outcomes & Benchmark, CONAN, MultiCONAN & LLMs: instruction prompting, Iinetuning, and RL \
\bottomrule
\end{tabular}
\caption{Summary of recent work on counterspeech generation, including dataset creation and modeling efforts}
\label{tab:priorwork}
\end{table*}

\subsection{Language Generation with Constraints}
Extensive studies have targeted language generation under cornplex lexical constraints such as formality (Jin et al,, 2022), text with certain concepts (Lu et a1.,2022), dialogue that takes latent variables (Bao et a1.,2020), and knowledge-enhanced text (Yu et a1.,2022a). Not all styles can be described explicitly as linguistic attributes. Indeed, some 'styles' can only be deflned in a data-driven way based on the shared attributes across various datasets (Mou and Vechtomova, 2020). In this study, we generate counterspeech very likely to lead to desired conversational outcomes.

Methods have been developed for constrained language generation. Wang and Wan (2018) proposed the SentiGAN framework to generate text with a given sentiment. Kumar et al. (2021) proposed MUCOCO to allow for controllable inference with multiple attributes as constraints to the optimization. Krause et al. (2021) developed GeDi, a discriminator-based approach to guide the decoding process in language generation. It enables text generation with desired or undesired attributes. Schick etal. (2021) proposed a self-debiasing approach to reduce the probability of language models generating problematic text. Unlike these previous efforts, we experiment with methods to adjust language model-generated texts to achieve specific conversational outcomes.

\section{Methodology}

\subsection{Conversation Outcomes}
Conversation outcomes refer to the result of a message in a conversation, which can be measured by the manner and characteristics of the follow-up conversations it elicits. According to previous studies, a combination of hate speech and its reply---regardless of whether it counters the hateful comment---can predict future conversation engagement and incivility (Liu et al., 2018; Yu et a1., 2024). This study explores two types of conversation outcome modeling: conversation incivility and hater reentry (Figure~\ref{fig:outcomes}). Based on the modeling results, we build conversation outcome classifiers that use hate speech and counterspeech to predict the incivility level or hater reenfiry type.

\begin{figure}[t]
\centering
\fbox{\parbox{0.95\columnwidth}{\centering IMAGE NOT PROVIDED}}
\caption{Two conversation outcomes (hater ressntry and incivility0 assessed based on the conversation (green box) following up a counterspeech reply (blue box). Comments in the first layer of the conversation tree (i.e., direct replies) are used to model hater reentry. All comments in the conversation tree are used to model conversation incivility. Grey boxes indicate hateful comments; others are non-hateful.}
\label{fig:outcomes}
\end{figure}

\paragraph{Conversation Incivility}
Conversation incivility is a metric to measure the outcome based on the number of civil and uncivil comments as well as the unique authors involved in the discourse (Yu et a7.,2024). Intuitively, the more uncivil (or less civil) the comments, the worse the outcome; uncivil comments from many authors are worse than those from just a few. Formally, it is defined as
[
S(r) : aU(r) - (1 - a)C(r),
]
where $U(r)$ refers to uncivil behavior and $C(r)$ to civil behavior. For each user $i$ ($i : 0,1,2,,,,,k$), $n6$ is defined as the number of uncivil comments by user $i$, and $n.i$ as the number of civil comments. Then,
[
U(r) : [ILLEGIBLE]
\qquad
C(r) : [ILLEGIBLE]
]
$a$ is used to adjust ttre weight of civil and uncivil behaviors. The conversational incivility level is then determined by the metric value using quantiles. Previous studies show that given two replies to hate speech, models taking into account the text of the hate speech and counterspeech accurately predict which of the two counterspeech replies will lead to more civil follow-up conversations (Yu et a1,2024, binary classification, F1=0.66{.75). We will use civility to refer to low conversation incivility, the desired outcome.

\paragraph{Ilater Reentry Behavior}
After a counterspeech reply to a hate speech comment, the hate instigator may exhibit different behaviors. Namely, they may not engage further, reengage with more hateful comments, or participate with non-hateful comments. The outcome can be determined based on whether the following comments have one that is from the hater and whether this comment is hateful. The non-hateful reentry is the most desirable, as it signals that the counterspeech encouraged the individual to change his behavior (Baider, 2023). We will lu'Se reentry to refer to non-hateful hater reentry in the remainder of the paper.

\subsection{Outcome-Constrained Counterspeech Generation}

We explore the following methods to incorporate the outcome constraints into the generation process.

\paragraph{Instruction Prompts}
LLMs are capable of understanding natural conversations and generating replies. The straightforward strategy is to ask LLMs to generate replies considering the potential outcomes of the follow-up conversation. This explores whether LLMs might pick up information from the instruction and generate responses toward the desired outcomes. The prompts are as follows:
\begin{itemize}
\item \textbf{Baseline:} No explicit expected outcomes.\
Userl ,Here is a hat.e comment: \texttt{<llate Corffnent>}. Please write a counterspeech reply to the hate comment.r,
\item \textbf{Civility:} Instruction with low conversation incivility as a desired outcome.\
User: rrHere is a hate comment: \texttt{<Hate Comment>}. Please write a counterspeech reply to t.he hate comment so that it could lead to 1ow incivility in the follow-up conversations. "
\item \textbf{Reentryi} Instruction with non-hateful hater reentry as a desired outcome.\
User: trHere is a hate comment: \texttt{<Hate Comnent>}. Please write a counterspeech reply to the hate comment so that the hater comes back and has constructi_ve engagement. "
\end{itemize}

There are different ways to set these outcome-constrained instructions. We adopt the instructions above as baselines for comparison purposes.

When given instructions, LLMs can generate one or multiple counterspeech replies. In addition to experimenting with the first generated reply, we follow (Zhu and Bhat,202l) and also :use a Generate and Select method to generate multiple replies and select the ones predicted to have desired outcomes according to conversation outcomes classifiers (Section 3.1).

\paragraph{LLM Finetuning}
LLMs may not be fully optimized for generating texts with specific constraints---in our case, desired conversation outcomes. The finetuning process can tailor LLMs to learn the task of interest. To guide the LLM in generating outcome-constrained counterspeech, we flnetune the model with datasets containing conversations with the desired outcomes: the hate speech,/counterspeech pairs followed by low conversation incivility (Yu et al., Z0Z2b) and the pairs that have non-hateful hater reentry. We use the Parameter-Efficient Fine-Tuning (PEFI) with Low-Rank Adaptation (LoRA) method (Hu er al.,ZO2l) to finetune LLMs.

\paragraph{Reinforcement Learning with LLM (RL)}
This method integrates the conversation outcome classifiers (Section 3.i) as a reward function to guide the training process, which includes three steps. First, a hate comment is used as a query to get the response generated by an LLM, The initial model serves as a baseline for generating counterspeech. Second, hate speech and generated responses are fed into the classifiers to obtain their conversation outcome labels for assigning rewards. Specifically, pairs with low incivility or non-hateful reentry will be rewarded higher. Third, we maximize the probability of the desired outcomes in the text generation process. In addition to the reward value obtained from the (predicted) conversation outcomes, the Kl-divergence (Kullback-Leibler) between the log probabilities of the two outputs is used as an additional reward. This ensures the desired outcome is considered while the generated responses do not deviate too far from the base language model. The reward is computed as
[
B : r - B \times KL.
]
We train the model with the Proximal Policy Optimization (PPO) (Schulman et a1.,2017) step until local stability is achieved,

\subsection{Evaluation}

\paragraph{Desired Conversation Outcome Metrics}
The evaluation aims to assess the ability of these methods to generate counterspeech that is more likely to achieve desired outcomes, As it would be difficult---and arguably unethical---to post the generated text to conversations on social media platforms to ob_ serve the real outcomes, we adopt an approach that has been used before (Saha et a1.,2022; Tekiroflu et a1.,2022; Halim et a1.,2023; Gupta et a1.,2023). We use the conversation incivility level classifier and the hater reentry classifier (Section 3.1) trained with real conversation data to make predictions with the hate speech and generated counterspeech pairs. Although the accuracy of the classifiers is not perfect, given two counterspeech replies, these classifiers reliably identify the one that will lead to better outcomes (Yu et a1.,2024, binary classification, F1=0.66-9.75). Thus, they serve as a proxy to compare counterspeech generated by different methods. Additionally, we conduct human assessments for reliability purposes.

\paragraph{Human Assessments}
The human assessment focuses on three characteristics of replies to hate speech: suitability, relevance, and effectiveness. Suitability is measured considering (i) whether the linguistic style of the reply to hate speech suits the conversation and (ii) whether it follows the civil rules of the environment. Relevance evaluates the appropriateness of the reply with respect to the content of the hate comment. Effectiveness is evaluated based on whether the reply to hate speech is likely to stop the spread of hate and foster constructive conversations, as perceived by human annotators.

Two graduate assistants, a male and female aged between 20 and 30, who are proficient in English and familiar with social media, assist with the evaluation. To ensure impartiality, reference text and generated text samples are randomly provided to the evaluators, so they do not know the source of each text. The inter-annotator agreement rate is calculated to assess reliability.

\paragraph{Stylistic Metrics}
The generated counterspeech is evaluated by stylistic metrics commonly used in previous studies (Chung et a1.,2021; Zl'ru and Bhat, 2021; Tekiroflu et al., 2022). We calculate the similarity of counterspeech against a reference dataset consisting of human-generated counterspeech with the BLEU score (Chen and Cherry, 2014), ROUGE (Lin,2004), METEOR (Banerjee and Lavie, 2005), and BERTScore (Zhang et al., 2019). The quality of generated texts is evaluated by the GRUEN metrics (Zhu and Bhat, 2020), including dimensions of grammaticality, redundancy, focus, and GRUEN score. The same scores are also calculated for the reference dataset for comparison purposes. Finally, we calculate the type-token ratio and distinct-n-grams to evaluate the diversity of generated texts (Fanton et al., 2021).

\section{Experiments}

\subsection{Conversation Outcomes Classifiers}

\paragraph{Data to Build Conversation Outcomes Classifiers}
We use Reddit data collected from 39 subreddits likely to contain abusive content (Vidgen et a1.,2021). The hate comments are identified based on hate classiflers (Qian et a1., 2019). Then, we collect replies to hate comments and identify counterspeech in replies referring to Yu et al. (2022b). For each counterspeech, we collect the follow-up replies. Then, we calculate the conversation incivility with a : 0.8 and determine the incivility level by quantiles. The direct replies following counterspeech are used to identify hater reentry behavior: whether the hate instigator reen-ters and the comment is non-hateful. Both datasets are split into 807o for training ard 20Vo for testing, with the testing portion used to evaluate the performance of the classiflers.

\paragraph{Classification Model and Performance}
As this study is not aimed at the best performance in the classification tasks, we use the RoBERTa model (Liu et a1.,2019) to train outcome classi-flers. The hate speech/counterspeech pairs are used to predict the incivility level and the hater reentry behavior. The detailed classification results can be seen in Table 5 and 6 in A.4. Although the classification results are somewhat low, these suboptimal classifiers are enough to defeat the baseline and differentiate counterspeech that will lead to high or low incivility in the follow-up conversation, as shown by (Yu et a1.,2024). The accuracy for identifying non-hateful reentry is the highest.

\subsection{Generating Counter Speech}

\paragraph{Dataset}
We use the benchmark-Reddit dataset (Qian et a1., 2019) for counterspeech generation and evaluation. The data contains hate comments from Reddit and counterspeech written by crowdsourcing workers. As we plan to explore the effect of this data in the finetuning and RL method, the data is split randomly into 80% for training and 207o for evaluation.

\paragraph{Instruction Prompts}
We use the Llama?-7b-chat model in our experiments to compare different methods, as we cannot train larger models like Llama2-l3b-chatfor finetuning and RZ due to limited computing capacity. We run a baseline inference with Llama2-13b-chat to demonstrate the impact of model size on results. As the generation and evaluation are based on the benchmark-Reddit data, we apply the same system-level guideline: ``Please generate a response in Reddit style'' for all generations. The pammeters are set to be the same in the generation of replies with no expected outcomes (baseline), low conversation incivility (civility), and non-hateful hater reentry (reentry). Fot Generate and Select, the number of responses is set to k : 1., k : 5, and k : 10, the temperature to 0.7, and the maximum length of reply to 512. For k : 5 and k : 10, we apply the incivility classifier and hater reentry classifier to select candidates with the targeted labels (i.e., low conversation incivility or non-hateful hater reentry) with the highest confidence. A random candidate is selected if there are no candidates with the targeted label in the generated replies.

\paragraph{Finetuning}
The Llama2-7b-chat model is flnetuned with hate speech/counterspeech pairs that are followed with low conversation incivility or non-hateful reentry in the training data. The finetuned models are expected to generate texts that share similar linguistic patterns and lead to desired conversation outcomes, Additionally, we fi ne-tune models with several reference datasets, including benchmark-Reddit, benchmark-Gab, CONAN, and MuItiCONAN (see model details in A.2), This is to compare whether models built on existing counterspeech datasets can generate effective counterspeech and how these datasets influence the generation process.

\paragraph{Reinforcement Learning}
We use the Llama}-7b-chat as the base model for the RL process. The reward for the RL process is calculated based on the outcome classifiers: for the predicted categories of conversation incivility low, medium, and high, corresponding discrete rewards are assigned in descending order, namely 2, l, and 0; for hater reentry classification, the reward for non-hateful reentry, no reentry, and hateful reentry is 2, l, and 0, respectively. We also use the Llama-2-7b-chat flnetuned with the benchmark-Reddit dataset, so that the model trained with RL can generate counterspeech that has similar linguistic patterns with counterspeech in the benchmark-Reddit dataset while having a higher probability of leading to expected conversation outcomes. The hyperparameters are shown it A.2. We leave exploring RL with other finetuned models for future work.

\section{Results and Analysis}

All methods are evaluated with the same test set from the benchmark-Reddit. The Llama2-i b- chat sometimes avoids responding to queries the model determines to be inappropriate and generates empty responses. Table~\ref{tab:results_outcomes_similarity} shows the ratio of non-empty, noted as valid, responses by each method. Except for instruction prompts, all the trained models, including thefinetuning and RL models, have lO\Vo of valid responses. In instruction prompts, the valid response rate increases when using a more powerful model (Llarna2-I3b-chat), forcing the model to generate more candidates, or asking the model to generate counterspeech with constrained queries.

\begin{table*}[t]
\centering
\small
\resizebox{\textwidth}{!}{%
\begin{tabular}{lcccccc}
\toprule
& \multicolumn{3}{c}{Desired Outcomes} & \multicolumn{2}{c}{Similarity} \
\cmidrule(lr){2-4}\cmidrule(lr){5-6}
Method & Valid (%) & Civility (%) & Reentry (%) & METEOR & BERTScore \
\midrule
\multicolumn{6}{l}{Instruction Prompts --- Generate one based on (k=1)} \
Baseline & 83% & 23% & 18% & 0.07 (0.08) & 0.80 (0.03) \
Baseline(13b) & 94% & 27% & 35% & 0.12 (0.07) & 0.81(0.04) \
Civility & 92% & 54% & 49% & 0.12 (0.05) & 0.83 (0.02) \
Reentry & 94% & 44% & 45% & 0.12 (0.06) & 0.82 (0.02) \
\midrule
\multicolumn{6}{l}{Generate and select (k=5)} \
p=baseline, c=civility & 84% & 55% & 32% & 0.10 (0.07) & 0.81 (0.03) \
p=baseline, c=reentry & 85% & 34% & 49% & 0.11 (0.07) & 0.82 (0.03) \
p=civility, c=civility & 92% & 81% & 53% & 0.12 (0.05) & 0.82 (0.02) \
p=reentry, c=reentry & 92% & 49% & 83% & 0.13 (0.05) & 0.83 (0.01) \
\midrule
\multicolumn{6}{l}{Generate and select (k=10)} \
p=baseline, c=civility & 87% & 69% & 36% & 0.11 (0.07) & 0.82 (0.02) \
p=baseline, c=reentry & 86% & 47% & 61% & 0.11 (0.07) & 0.82 (0.02) \
p=civility, c=civility & 92% & 86% & 55% & 0.12 (0.05) & 0.82 (0.02) \
p=reentry, c=reentry & 92% & 50% & 86% & 0.13 (0.05) & 0.83 (0.01) \
\midrule
\multicolumn{6}{l}{Finetuning with Counterspeech Corpora} \
CONAN & 100% & 23% & 12% & 0.09 (0.06) & 0.85 (0.02) \
MuItiCONAN & 100% & 32% & 13% & 0.11(0.06) & 0.85 (0.02) \
Benchmark-Gab & 100% & 10% & 11% & 0.12 (0.10) & 0.86 (0.02) \
Benchmark-Reddit & 100% & 43% & 42% & 0.13 (0.11) & 0.86 (0.02) \
Ours, with conversation outcomes & 100% & 15% & 15% & [ILLEGIBLE] & [ILLEGIBLE] \
Reddit-CS-civility & 100% & 19% & 48% & 0.08 (0.05) & 0.84 (0.02) \
Reddit-CS-reentry & 23% & 18% & 48% & 0.08 (0.05) & 0.84 (0.02) \
\midrule
\multicolumn{6}{l}{Reinforcement Learning (RL)} \
Civility & 100% & 77% & 71% & 0.14 (0.05) & 0.83 (0.01) \
Reentry & 100% & 67% & 62% & 0.14 (0.05) & 0.83 (0.01) \
RL, finetuned LLM w/ Benchmark-Reddit --- Civility & 100% & 30% & 48% & 0.13 (0.13) & 0.85 (0.02) \
RL, finetuned LLM w/ Benchmark-Reddit --- Reentry & 100% & 10% & 57% & 0.07 (0.06) & 0.86 (0.01) \
\midrule
Reference --- Benchmark-Reddit & 100% & 27% & 37% & 1.00 (0.00) & 1.00 (0.00) \
\bottomrule
\end{tabular}}
\caption{Evaluation of (a) Desired Outcomes and (b) Similarity to the reference counterspeech in Benchmark-Reddit. METEOR and BERTScore are calculated per sample. Mean (SD) is reported. Generate and select and RZ are better at generating more samples with desired outcomes. Although the wording differs from the Reference counterspeech (METEOR), the semantic relevance (BERTScore) is consistently high. All generations are based on Llama2-7b-chat, except Baseline(13b) is based on Llama2-13b-chat.}
\label{tab:results_outcomes_similarity}
\end{table*}

\paragraph{Quality of Generated Counterspeech}
Table~\ref{tab:quality_diversity} presents the evaluation using stylistic metrics. Grammaticality scores measure grammatical correctness. Texts generated by language models generally have higher grammatical scores than the reference (0.7'7), except the ones f,netuned with Reddit conversation data: civility (0.77) and reentry (0.76). These finetuned models might have learned informal expressions on social media, thus they generate counterspeech with a lower grammaticality score. Counterspeech generated by LLMs without flnetuning or RL is more redundant, indicated by lower scores in redundancy. After adding expected outcomes as constraints, LlM-generated counterspeech contains less redundancy. The focus scores of counterspeech generated by instruction prompts are also much lower. In models withfine-tuning and RZ, the focus scores are much higher. Overall, counterspeech generated by finetuning and RZ have higher quality, as reflected in the grammaticality, redundancy, focus, and overal GRUEN scores. In particular, the highest GRUEN scores are achieved by RL models.

\begin{table*}[t]
\centering
\small
\resizebox{\textwidth}{!}{%
\begin{tabular}{lcccccc}
\toprule
& \multicolumn{2}{c}{Text Quality} & \multicolumn{2}{c}{Diversity} & \multicolumn{2}{c}{Novelty} \
\cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}
Method & Focus & GRUEN & TTR & [ILLEGIBLE] & New Tokens & [ILLEGIBLE] \
\midrule
\multicolumn{7}{l}{Instruction Prompts --- Generate one based on} \
Baseline & -0.05 (0.05) & -1.14 (1256) & 0.60 (0.18) & 0.73 (0.10) & 0.06 & 5384 \
Baseline (13b) & -0.09 (0.03) & -1.33 (23.22) & 0.60 (0.21) & 0.80 (0.07) & 0.05 & 9231 \
Civility & -0.10 (0.01) & -0.19 (0.56) & 0.61(0.22) & 0.84 (0.04) & 0.04 & 7019 \
Reentry & -0.10 (0.02) & -0.11 (0.39) & 0.64 (0.18) & 0.83 (0.07) & 0.03 & 6407 \
\midrule
\multicolumn{7}{l}{Generate and select (k=5)} \
p=baseline, c=civility & -0.08 (0.04) & -0.33 (4.37) & 0.62 (0.19) & 0.78 (0.10) & 0.06 & 7220 \
p=baseline, c=reentry & -0.08 (0.04) & -0.34 (6.42) & 0.63 (0.18) & 0.78 (0.10) & 0.06 & 6794 \
p=civility, c=civility & -0.10 (0.01) & -0.23 (2.35) & 0.59 (0.23) & 0.84 (0.03) & 0.03 & 7668 \
p=reentry, c=reentry & -0.10 (0.00) & -0.07 (0.21) & 0.68 (0.12) & 0.84 (0.02) & 0.03 & 5224 \
\midrule
\multicolumn{7}{l}{Generate and select (k=10)} \
p=baseline, c=civility & -0.08 (0.04) & -0.21 (2.21) & 0.62 (0.20) & [ILLEGIBLE] & 0.06 & 8000 \
p=baseline, c=reentry & -0.08 (0.04) & -0.20 (2.02) & 0.64 (0.18) & [ILLEGIBLE] & 0.05 & 6908 \
p=civility, c=civility & -0.10 (0.00) & -0.23 (0.48) & 0.57 (0.24) & [ILLEGIBLE] & 0.04 & 8024 \
p=reentry, c=reentry & -0.10 (0.00) & -0.06 (0.12) & 0.68 (0.11) & [ILLEGIBLE] & 0.03 & 5198 \
\midrule
\multicolumn{7}{l}{Finetuning w/ Counterspeech} \
CONAN & -0.02 (0.04) & 0.00 (0.03) & 0.78 (0.11) & 0.81 (0.09) & 0.11 & 1982 \
MuItiCONAN & -0.05 (0.05) & -0.12 (2.93) & 0.76 (0.13) & 0.83 (0.07) & 0.09 & 2448 \
Benchmark-Gab & -0.01 (0.03) & 0.00 (0.00) & 0.83 (0.08) & 0.85 (0.06) & 0.02 & 111 \
Benchmark-Reddit & -0.04 (0.05) & 0.00 (0.01) & 0.71 (0.12) & 0.80 (0.09) & 0.03 & 147 \
Reddit-CS-civility & -0.04 (0.05) & -0.70 (7.78) & 0.71 (0.17) & 0.78 (0.09) & 0.12 & 2858 \
Reddit-CS-reentry & -0.04 (0.05) & -0.70 (7.56) & 0.71 (0.17) & 0.78 (0.09) & 0.11 & 2643 \
\midrule
\multicolumn{7}{l}{Reinforcement Learning (RL)} \
Civility & -0.10 (0.00) & -0.04 (0.12) & 0.71 (0.11) & 0.85 (0.03) & 0.03 & 5575 \
Reentry & -0.10 (0.00) & -0.06 (0.18) & 0.69 (0.13) & 0.84 (0.04) & 0.03 & 6574 \
RL, finetuned LLM w/ B-Reddit --- Civility & 0.00 (0.00) & 0.80 (0.02) & [ILLEGIBLE] & 0.87 (0.03) & 0.00 & 0 \
RL, finetuned LLM w/ B-Reddit --- Reentry & 0.00 (0.00) & 0.87 (0.03) & [ILLEGIBLE] & [ILLEGIBLE] & 0.01 & 12 \
\midrule
Reference --- Benchmark-Reddit & 0.17 (0,12) & -0.03 (0.05) & 0.00 (0.01) & 0.74 (0.13) & 0.09 & 0 \
\bottomrule
\end{tabular}}
\caption{Evaluation of Quality and Diversity. GRUEN and BERTScore are calculated per sample. Mean (SD) are reported. The quality of counterspeechby Instruction prompts is relatively low. LLAi finetuning withReddit counterspeech generate texts with high diversity. Rt with finetuned LLMs generate texts with reduced novelty. All generations are based onLlama2-7b-chat, except Baseline(l3b) is based on Llama2-l3b-chat.}
\label{tab:quality_diversity}
\end{table*}

\paragraph{Diversity and Novelty}
The three diversity metrics (i.e., TTR, number of unique unigrams, and number of unique bigrams) are highly correlated (Table 8 in A.5). TTR and the novelty merric (i.e., number of new unigrams) are presented in Table 3. The TTR of generated counterspeech significantly decreases with models that use expected outcomes constraints in instruction prornpts and RI. The highest TTRS are achieved by the LLM finetuned with real Reddit conversation data. Note that this data usually contains diverse and informal language. The novelty of generated texts is higher when conversation outcomes are considered in the generation. The number of new unigrams generated by untrained LLMs inlhe instruction prompt method is substantially higher than trained models with finetuning and Rl.

\paragraph{Human Evaluation}
We choose generated texts constrained with low conversation incivility for human evaluation. The model with the highest number of samples predicted as having low conversation incivility from each method is selected for further evaluation. Hencen we randomly select 50 pairs of hate comments and counterspeech from the instruction prompts with p : ciuilitA, k : 10, and c : ciui,li,ty, finetuning with CONAN, and rRL with low incivility, respectively. Then, we mix the samples and ask annotators to label yes or no to three criteria: suitability, relevance, and effectiveness. The agreement percentages for initial labels are 0.78, 0.92, and 0.64 respectively for suitability, quality, and effectiveness. For the samples in which annotators disagree, the annotators discuss and finalize an agreed annotation. Table~\ref{tab:human_eval} presents the results. Tlte instruction prompts methods tend to generate long responses with high relevance. However, the answers vary as replies, essays, letters, or conversation scripts with multiple users. Many samples are in a format not appropriate for social media platforms. Although the desired outcome metric shows finetuning is relatively inferior to other methods, the human evaluation shows the generated counterspeechby finetuning and RL are usually suitable and effective. Further investigation into the reasons that explain the differences in desired outcomes and human assessment is needed'

\begin{table}[t]
\centering
\small
\begin{tabular}{lccc}
\toprule
Method & SultaUitity & Relevance & Effectiveness \
\midrule
Prompt & 0.50 & 0.88 & 0.54 \
Finetuning & 0.80 & 0.68 & 0.80 \
RL & 0.74 & 0.76 & 0.72 \
\bottomrule
\end{tabular}
\caption{Proportion of samples labeled as Yes for each evaluation dimension by methods.}
\label{tab:human_eval}
\end{table}

\section{Conclusions}
We present an initial exploration of methods for constrained generation of counterspeech controlled by potential conversation outcomes. We incorporate the desired outcomes (i.e., low conversation incivility and non-hateful hater reentry) into the text generation process through three methods: instruction prompts, LLM finetuning, and LLM RL. The text generation results are evaluated with desired conversation metrics, stylistic metrics, and human assessment. Results show that instruction prompts and RL generate counterspeech with a higher probability of eliciting desired outcomes based on the prediction of outcome classifiers, while finetuning and Rtr generate more effective counterspeech based on human assessments. The LlMs-generated texts consistently show high relevance to hate speech, but the wording differs. The generated texts present different characteristics. The counterspeech generated by LLM without further training tends to be long, not suitable for the conversation context on social media, and with low quality based on GRUEN metrics and human as ses sment. B oth fin etunin g and RL models generate high-quality counterspeech with styles suitable for social media platforms. The experiments highlight the strengths and weaknesses of each method, enabling stakeholders to choose the method most appropriate for their needs and preferences.

\section*{Limitations}
The conversation outcome classiflers are not perfect, as the texts of hate comments and replies only partially contribute to the conversation outcomes. Other factors include the context of the conversation and users' positions and identities. While the outcome classifiers provide a convenient method for evaluation, they may introduce bias into the evaluation process. Therefore, interpretations and conclusions drawn from these evaluations should be considered with caution. Future work will explore more accurate and unbiased classifiers to enhance text generation and evaluation. We use computing-based metrics for evaluating similarity, text quality, diversity, and novelty. Although these metrics are widely used, they may present bias. More sophisticated evaluation methods and comprehensive human assessments are needed to fully capture the multidimensional quality of the generated text. Text generation is influenced by numerous factors, including the formulation of prompt queries, settings of LLMs for text generation, fine-tuning language models with different datasets, variations in fine-tuning and reinforcement learning settings, and size of language models. Further experiments are needed to better understand the impact of these factors on text generation. The outcome classiflers are based on Reddit conversation data, which may not transfer to other platforms. Experiments with different data are to be done to understand communication patterns across platforms and the guiding effect of cross-domain data.

\section*{Ethics Statement}
The study has been through careful consideration of benefits and risks. First, we used data from Reddit, which is considered a public space. Users consent to make their data available to third parties. Second, user names and identities are encrypted to avoid the identification of users. Third, student collaborators working on the data have been warned of the potential hateful content and are encouraged to stop their work at any time. Fourth, the data will be shared for research purposes only. Although releasing the dataset may raise risks, we believe the benefits of conributing to effective methods to counter online hate outweighs the potential risks. Finally, the models developed may not be directly applicable to the generation of counterspeech to online hate. Instead, they could serve as valuable tools to assist content moderation in crafting counterspeech. Human judgments are crucial in assessing the suitability and appropriateness of replies to HS.

\section*{Acknowledgement}
Dr. Lingzi Hong and Xiaoying Song gratefully acknowledge financial support from the Institute of Museum and Library Services (US) under Grants LG-256661 -OLS-24 and LG-256666-OLS-24.

\begin{thebibliography}{99}

\bibitem{Baider2023}
Fabienne Baider. 2023.
\newblock Accountability issues, online covert hate speech, and the efficacy of counter-speech.
\newblock \emph{Politics and Goyernance}, I l(2):249--260.

\bibitem{BanerjeeLavie2005}
Satanjeev Bane{ee and Alon Lavie. 2005.
\newblock Meteor: An automatic metric formt evaluation with improved correlation with human judgments.
\newblock In \emph{proceedings of the acl workshop on intrinsic and extrinsic eyaluation measures for machine translation and,/or summarization}, pages 65--72.

\bibitem{Bao2020}
Siqi Bao, Ilq*g He, Fan Wang, Hua Wu, and Haifeng Wang. 2020.
\newblock Plato: Pre-trained dialogue generation model with discrete latent variable.
\newblock In \emph{pioceedings of the 58th Annual Meeting of the Association for Computational Linguistics}, pages 85--96.

\bibitem{Blaya2019}
Catherine Blaya.2019.
\newblock Cyberhate: A review and content analysis of intervention strategies.
\newblock \emph{Aggression and violent behavior}, 45 163--172.

\bibitem{Bonaldi2024}
Helena Bonaldi, Yi-Ling Chung, Gavin Abercrombie, and Marco Guerini. 2024.
\newblock Nlp for counterspeech against hate: A survey and how-to gtide.
\newblock \emph{-arXiv preprint} arXiv : 240 3.20 I 03.

\bibitem{Buerger2021}
Catherine Buerger. 202L.
\newblock # iamhere: Collective counterspeech and the quest to improve online discourse.
\newblock \emph{Social Media+ S ociety}, 7 (4):2056305 tltt}63g43.

\bibitem{ChenCherry2014}
Boxing Chen and Colin Cherry. 2014.
\newblock A systematic comparison of smoothing techniques for sentence_level bleu.
\newblock In \emph{Proceedings of the iinth worlcshop on statistical machine translation}, pages 362--367.

\bibitem{Chung2019}
Yi-Ling Chung, Elizaveta Kuzmenko, Serra Sinem Tekiroflu, and Marco Guerini. 2019.
\newblock Conan-counrer narratives through nichesourcing: a multilingual dataset ofresponses to fight online hate speech.
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics}, pages 2gI9--2829.

\bibitem{Chung2020}
Yi-Ling Chung, Serra Sinem Tekiroglu, and Marco Guerini. 2020.
\newblock Italian counter narrative generation to fight online hate speech.
\newblock In \emph{proceedings of the Seventh ltalian Conference on Computattinai fin-guistics (CLIC-it 2020)}, vohtme 2769.

\bibitem{Chung2021}
Yi-Ling Chung, Serra Sinem Tekiro[lu, and Marco Guerini. 2021.
\newblock Towards knowledge-grounded counter narrative generation for hate speech.
\newblock In \emph{Find-ings of the Associationfor Computational Linguistics: AC L- IJ C NLP 202 t}, pages 999--9 14.

\bibitem{Fanton2021}
Margherita Fanton, Helena Bonaldi, Serra Sinem Tekiroglu, and Marco Guerini, 2021.
\newblock Human-in-the-loop for data collection: a multi-target counter narra-tive dataset to fight online hate speech.
\newblock In \emph{proceed_ings of the 59thAnnual Meeting of the Associationfor Computational Linguistics and the I lth International Joint Conference on Natural Language processing (Volume I : Long Papers )}, pages 3226--3240.

\bibitem{Fraser2023}
Kathleen C Fraser, Svetlana Kiritchenko, Isar Ne-jadgholi, and Anna Kerkhof, 2023.
\newblock What makes a good counter-stereotype? evaluating strategies for automated responses to stereotypical text.
\newblock In \emph{pro_ceedings ofthe First Workshop on Social Influence in Corwersations (SICon 2023)}, pages 25--38.

\bibitem{Gupta2023}
Rishabh Gupta, Shaily Desai, Manvi Goel, Anil Bandhakavi, Thnmoy Chakraborty, and Md Shad Akhtar. 2023.
\newblock Coufierspeeches up my sleeve! intent distribution learning and persistent fusion for intent-conditioned counterspeech generati on.
\newblock \emph{arXiv prep rint} arXiv : 2 3 0 5. I 3776.

\bibitem{Halim2023}
Sadaf MD Halim, Saquib lrtiza, yibo Hu, Latifur Khan, and Bhavani Thuraisingham.2OZ3.
\newblock Wokegpt: Im-proving counterspeech generation against online hate speech by intelligently augmenting datasets using a novel metric.
\newblock In \emph{2023 International Joint Conference on Neural Neworks (IJCNN)}, pages l--10. IEEE.

\bibitem{HassanAlikhani2023}
Sabit Hassan and Malihe Alikhani. 2023.
\newblock Discgen: A framework for discourse-informed counterspeich generation.
\newblock In \emph{P ro c e e din g s of the I 3 th Int e rnaii onal Joint Conference on Natural Innguage processing and the 3rd Conference of the Asia-pacific Chal-ter of the Association for Computational Linguistics (Volume l: Long Papers)}, pages 420--429.

\bibitem{Horawalavithana2022}
Sameera Horawalavithana, Nazim Choudhury John Skvoretz, and Adriana Iamnitchi. 2022.
\newblock Oniine dis-cussion threads as conversation pools: predicting the growth of discussion tfueads on reddit.
\newblock \emph{Computa-tional and Mathematical O rganization Theory}, pages r--29.

\bibitem{Hu2021}
Edward J Hu, Phillip Wallis, Zeyuar- Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. 2021.
\newblock Lora: Low-rank adaptation of large lan-guage models.
\newblock In \emph{Inlnternational Conference on l,earn-ing Representations}.

\bibitem{Jin2022}
Di Jin, Zhljing Jin, Zhiting Hu, Olga Vechtomova, and Rada Mihalcea. 2022.
\newblock Deep learning for text style transfer: A survey.
\newblock \emph{Computational Linguistics}, 48(l):155--205.

\bibitem{Krause2021}
Ben Krause, Akhilesh Deepak Gotmare, Bryan McCann, Nitish Shirish Keskar, Shaflq Joty, Richard Socher, and Nazneen Fatema Rajani. 2021.
\newblock Gedil' Genera-tive discriminator guided sequence generation.
\newblock In \emph{Findings of the Associatton for Computational Lin' guisttcs : EMNLP 202 l}, pages 49294952.

\bibitem{Kumar2021}
Sachin Kumar, Eric Malmi, Aliaksei Severyn, and Yu-lia Tsvetkov. 2021.
\newblock Controlled text generation as continuous optimization with multiple constraints.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:14542--14554.

\bibitem{Lin2004}
Chin-Yew Lir,.2004.
\newblock Rouge: A package for automatic evaluation of summaries.
\newblock In \emph{Text summarization branches out}, pages 7 4--81.

\bibitem{Liu2018}
Ping Liu, Joshua Guberman, Libby Hemphill, and Aron Culotta. 2018.
\newblock Forecasting the presence and intensify of hostility on instagram using linguistic and social features.
\newblock In \emph{Proceedings of the Intemational AAAI Conference on Web and Social Media}, volume 12.

\bibitem{Liu2019}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-dar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.
\newblock Roberta: A robustly optimized bert pretraining ap-proach.
\newblock \emph{arXiv preprint} arXiv: 1907. 1 1 692.

\bibitem{Lu2022}
Ximing Lu, Sean Welleck, Peter West, Liwei Jiang, Jungo Kasai, Daniel Khashabi, Ronan Le Bras, Lian-hui Qin, Youngjae Yu, Rowan Zellerc, et aL.2022.
\newblock Neurologic a* esque decoding: Consffained text gen-eration with lookahead heuristics.
\newblock In \emph{Proceedings of the 2022 Conference of the North American Chap-ter of the Association for Computational Linguistics: Human Language Technologies}, pages 780--799.

\bibitem{Miskolci2020}
Jozef Mi5kolci, Lucia Kov6dov6, and Edita Rigov6. 2020.
\newblock Countering hate speech on facebook: The case of the roma minority in slovakia.
\newblock \emph{Social Science C omp ut e r R ev tew}, 38 (2) : 128--1 46.

\bibitem{MouVechtomova2020}
Lili Mou and Olga Vechtomova. 2020.
\newblock Stylized text gen-eration: Approaches and applications.
\newblock In \emph{Proceed-ings of the 58th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts}, pages 1,9--22.

\bibitem{Mun2024}
Jimin Mun, Cathy Buerger, Jenny T Liang, Joshua Gar-land, and Maarten }ap.2024.
\newblock Counterspeakers' per-spectives: Unveiling barriers and ai needs in the flght against online hate.
\newblock In \emph{Proceedings of the CHI Con-ference on Human Factors in Computing Systems}, pages l--22.

\bibitem{Qian2019}
Jing Qian, Anna Bethke, Yinyin Liu, Elizabeth Belding, and William Yang Wang. 2019.
\newblock Abenchmark dataset for learning to intervene in online hate speech.
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Jotnt Conference on Natural Language P rocessing ( EMNLP-IJ CNLP,)}, pages 47 5547 64.

\bibitem{Saha2022}
Punyajoy Saha, Kanishk Singh, Adarsh Kumar, Binny Mathew, and Animesh Mukherjee. 2022.
\newblock Coun-tergedi: A controllable approach to generate po-lite, detoxified and emotional counterspeech.
\newblock \emph{arXiv p rep rint} arXiv : 220 5. 04 3 04.

\bibitem{Schick2021}
Timo Schick, Sahana Udupa, and Hinrich Schiitze. 2021.
\newblock Self-diagnosis and self-debiasing: A proposal for re-ducing corpus-based bias in nlp.
\newblock \emph{Transactions of the Association for Computational Linguistics}, 9: 1408--1424.

\bibitem{Schulman2017}
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017.
\newblock Proxi-mal policy optimization algorithms.
\newblock \emph{arXiv preprint} arXiv:1707.06347.

\bibitem{Tekiroglu2022}
Serra Sinem Tekiroflu, Helena Bonaldi, Margherita Fanton, and Marco Guerini. 2022.
\newblock Using pre-trained language models for producing counter naratives against hate speech: a comparative study.
\newblock In \emph{Find-ings of the Associationfor Computational Linguistics: ACL 2022}, pages 3099--31 14.

\bibitem{Tekiroglu2020}
Serra Sinem Tekirollu, Yi-Ling Chung, and Marco Guerini. 2020.
\newblock Generating counter narratives against online hate speech: Data and strategies.
\newblock \emph{Proceed-ings of the 58th Annual Meeting of the Association fo r C o mp ut atio nal Lin gui s tlcs}, pages I l7'7--l 190.

\bibitem{Vidgen2021}
Bertie Vidgen, Dong Nguyen, Helen Margetts, Patricia Rossini, and Rebekah Tromble. 2021.
\newblock Introducing cad: the contextual abuse dataset.
\newblock In \emph{Proceedings of the 2021 Conference of the North American Chap-ter of the Associatton for Computational Linguistics: Human Language Technolo gies}, pages 2289--2303.

\bibitem{WangWan2018}
Ke Wang and Xiaojun Wan. 20 1 8.
\newblock Sentigan: Generating sentimental texts via mixture adversarial networks.
\newblock In \emph{Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence (IJCAI-/8)}, pages 44464452.

\bibitem{Wang2021}
Lingzhi Wang, Xingshan Zetg, Huang Hu, Kam-Fai Wong, and Daxin Jiang. 2021.
\newblock Re-entry prediction for online conversations via self-supervised leaming.
\newblock In \emph{Findings of the Association for Computational Linguistics: EMNLP 2021}, pages 2127--2137.

\bibitem{Yu2022a}
Wenhao Yu, ChenguangZhu,Zaitang Li, Zhiting Hu, Qingyun Wang, Heng Ji, and Meng Jiang.2022a.
\newblock A survey of knowledge-enhanced text generation.
\newblock \emph{ACM C omputing Surv ey s}, 54( 1 I s): 1--38.

\bibitem{Yu2022b}
Xinchen Yu, Eduardo Blanco, and Lingzi Hong.2022b.
\newblock Hate speech and counter speech detection: Conver-sational context does matter.
\newblock In \emph{Proceedings of the 2022 Conference of the North American Chapter of the As s ociation for C omputational Lin guistics : Hu-man Language Technologies}, pages 5918--5930.

\bibitem{Yu2024}
Xinchen Yu, Eduardo Blanco, and Lingzi Hong.2024.
\newblock Hate cannot drive out hate: Forecasting conversation incivility following replies to hate speech.
\newblock In \emph{pro-ceedings ofthe International AAAI Conference on Web and Social Media}, volume 18, pages 1740--1752.

\bibitem{Zhang2019}
TianyiZhang, Varsha Kishore, Felix Wu, Kilian e Wein-berger, and Yoav Arlzi.2019,
\newblock Bertscore: Evaluating text generation with bert.
\newblock In \emph{International Confer-ence on Learning Representations}.

\bibitem{ZhuBhat2020}
Wanzheng Zhtt and Suma Bhat, 2020.
\newblock Gruen for eval-uating linguistic quality ofgenerated text.
\newblock In \emph{Find-ings of the Associationfor Computational Linguistics: EMNLP 2020}, pages 94--108.

\bibitem{ZhuBhat2021}
Wanzheng Zh;u and Suma Bhat. 2021.
\newblock Generate, prune, select: A pipeline for counterspeech generation against online hate speech.
\newblock In \emph{Findings ofthe Associ-ation fo r C omputational Lin g ui s tic s : AC L- I J C N Lp 2021}, pages 134--149.

\end{thebibliography}

\appendix
\section*{Appendices}

\subsection*{4.1 ComputingResources}
The computational resources used in this research include a high-perfofinance server equipped with three Quadro RTX 8000 GPUs, 128G memory and a 4T disk.

\subsection*{4.2 Hyperparameters}
LLM Finetuning: We use PEFT LoRA for the finetuning process. The LoRA configuration has : : r 76, alpha 32, dropor.r,t: 0.05, and bias is ``none''. The hyperparameters are as follows: the learning rate is 1e-4, the number of epochs is 1, and the wafinup ratio is 0.1.

LLM RL: The reward trainer uses the RoBERIa base model, the learning rate is 1e-5, the batch size is 16, and the number of epochs is 5. In the ppO : process, the generation componenthas top_k a O, : top_p L.0, d,o_sample: True, and the max C length is 256. The PPO configuration has a learning rate of l.4le-5, a batch size of 32, and an initial KL coefficient of 0.1 .

\subsection*{A.3 Dataset License and Use}
The Benchmark dataser by Qian et al. (ZOI9) is under the Creative Commons Attribution-NonCommercial 4.0 International public License. The CONAN and MuItiCONAN datasets can be used for research purposes with proper citation (Chung et al., 2019 ; Fanton et al., 2021). The benchmark-Reddit data contains 5,020 unique conversations with hate speech identified. Each hate speech comment has multiple responses. We extracted the hate speech from conversations and their counterspeech responses, generating 4,208 valid hate speech/counterspeech pairs, noted as the benchmark-Reddit data. The testing data includes 2,843 pairs of hate speech./counterspeech.

\subsection*{4.4 Evaluation Results of Conversation Outcome Classifiers}

\begin{table*}[t]
\centering
\small
\begin{tabular}{lcccccccccccc}
\toprule
& \multicolumn{3}{c}{High} & \multicolumn{3}{c}{Medium} & \multicolumn{3}{c}{Low} & \multicolumn{3}{c}{Weighted Average} \
\cmidrule(lr){2-4}\cmidrule(lr){5-7}\cmidrule(lr){8-10}\cmidrule(lr){11-13}
& P & R & F1 & P & R & F1 & P & R & F1 & P & R & F1 \
\midrule
Baseline & 0.00 & 0.00 & 0.00 & 0.49 & 1.00 & 0.66 & 0.00 & 0.00 & 0.00 & 0.24 & 0.49 & 0.32 \
Incivility & 0.43 & 0.32 & 0.36 & 0.55 & 0.66 & 0.60 & 0.32 & 0.27 & 0.29 & 0.46 & 0.48 & 0.46 \
\bottomrule
\end{tabular}
\caption{Evaluation results ofthe conversation incivility classifier.}
\end{table*}

\begin{table*}[t]
\centering
\small
\begin{tabular}{lcccccccccccc}
\toprule
& \multicolumn{3}{c}{Hate reentry} & \multicolumn{3}{c}{No reentry} & \multicolumn{3}{c}{Non-hate} & \multicolumn{3}{c}{Weighted Average} \
\cmidrule(lr){2-4}\cmidrule(lr){5-7}\cmidrule(lr){8-10}\cmidrule(lr){11-13}
& P & R & F1 & P & R & F1 & P & R & F1 & P & R & F1 \
\midrule
Baseline & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.49 & 1.00 & 0.66 & 0.16 & 0.33 & 0.22 & 0.25 \
Reentry & 0.32 & 0.20 & 0.52 & 0.41 & 0.46 & 0.54 & 0.70 & 0.61 & 0.49 & 0.51 & 0.46 & [ILLEGIBLE] \
\bottomrule
\end{tabular}
\caption{Evaluation results of the hater reentry classifier.}
\end{table*}

\begin{table*}[t]
\centering
\small
\resizebox{\textwidth}{!}{%
\begin{tabular}{l l r r r r r r}
\toprule
\multirow{2}{*}{Category} & \multirow{2}{*}{Model} & \multicolumn{3}{c}{Conversation Incivility} & \multicolumn{3}{c}{Hater Reentry} \
\cmidrule(lr){3-5}\cmidrule(lr){6-8}
& & High & Mediun & Low & No reentry & Hateful & Non-hateful \
\midrule
Generation & baseline & 291 & 1733 & 652 & 1422 & 748 & 506 \
& baseline(138) & 686 & t2t4 & 716 & 752 & 937 & 987 \
& civility & 412 & 657 & 1541 & 876 & 346 & 1394 \
& reentry & 629 & 794 & 1253 & 910 & 476 & t290 \
Prompt and Select & p=baseline k=5 c=civility & 195 & 855 & 1566 & 595 & [ILLEGIBLE] & 904 \
& p=civility k=5 c=civility & 134 & 176 & 2306 & 849 & 253 & t5t4 \
& p=baseline k=5 c=reentry & 4t5 & 1240 & 961 & 771 & 443 & 1402 \
& p=reentry k=5 c=reentry & 914 & 312 & 390 & 64 & r86 & 2366 \
& p=baseline k=10 c=civility & 531 & t965 & 1070 & 511 & [ILLEGIBLE] & 1035 \
& p=civility k=10 c=civility & 73 & 100 & 2443 & 828 & 222 & 1566 \
& p=baseline k=10 c=reentry & 444 & 994 & I 178 & 511 & 371, & 1734 \
& p=reentry k=10 c=reentry & 890 & 295 & t43t & 25 & 160 & 2431 \
LLM Finetune & civility & 953 & 1298 & 592 & 881 & 954 & 1008 \
LLM Finetune & reentry & 939 & t4t7 & 487 & 73t & tL52 & 960 \
& CONAN & 1429 & 752 & 662 & 438 & 103 1 & 1374 \
& MultiCONAN & r386 & 835 & 622 & 559 & 93r I & 353 \
& Benchmark-Reddit & 177 5 & 757 & 311 & 5r0 & tt49 1 & 184 \
& Benchmark-Gab & t974 & 585 & 284 & 533 & 1076 & 1234 \
LLM TRL & civility & 239 & 423 & 2t8t & 292 & 540 & 20tt \
LLM TRL & reentry & 481 & 46t & 1901 & 408 & 661 & t774 \
& bm_reddit_ft_civility & 66 & t9t7 & 860 & 448 & 1036 & 1359 \
& bm_reddit_ft_reentry & tzt2 & 1 130 & 501 & 222 & 992 & 1629 \
Reference & benchmark-reddit & 1245 & 838 & 760 & 683 & 1tt7 & 1043 \
\bottomrule
\end{tabular}}
\caption{Evaluation results of conversation incivility and hater reentry classifiers.}
\end{table*}

\subsection*{A.5 Evaluation Metrics}

\begin{table*}[t]
\centering
\small
\begin{tabular}{lrrrrr}
\toprule
& TTR & distinct-1 & distinct-2 & #new-unigram & #new-bigram \
\midrule
TTR & 1 & 0.990 & 0.97t & -0.219 & -0.298 \
distinct-1 & 0.990 & 1 & 0.972 & -0.281 & -0.364 \
distinct-2 & 0.97t & 0.972 & 1 & -0.086 & -0.160 \
#new_unigram & -0.219 & -0.281 & -0.086 & 1 & 0.980 \
#new_bigram & -0.298 & -0.364 & -0.160 & 0.980 & 1 \
\bottomrule
\end{tabular}
\caption{Conelation coeffrcients of diversity metrics. TTR (type-token ratio) highly correlates with distinct-1 and distinct-2. Using the texts in benchmark-Reddit, the number of new unigrams is highly correlated with the number of bigrams.}
\end{table*}

\begin{table*}[t]
\centering
\small
\begin{tabular}{lrrrrrrrr}
\toprule
& BLEU & ROUGE-r & ROUGE-p & ROUGE-f & METEOR & BERI-p & BERI-r & BERT-f \
\midrule
BLEU & 1 & 0.799 & 0.997 & 0.990 & 0.973 & 0.870 & 0.983 & 0.952 \
ROUGE-r & 0.799 & 1 & 0.829 & 0.874 & 0.916 & 0,466 & 0.866 & 0.649 \
ROUGE-p & 0.997 & 0.829 & 1 & 0.996 & 0.983 & 0.848 & 0.985 & 0.939 \
ROUGE-f & 0.990 & 0.874 & 0.996 & 1 & 0.995 & 0.806 & 0,99 & 0.9t4 \
METEOR & 0.973 & 0.916 & 0.983 & 0.995 & 1 & 0.759 & 0.988 & 0.884 \
BERI-p & 0.870 & 0,466 & 0.848 & 0.806 & 0.759 & 1 & 0.835 & 0.974 \
BERI-r & 0.983 & 0.866 & 0.985 & 0,99 & 0.988 & 0.835 & 1 & 0.938 \
BERT-f & 0.952 & 0.649 & 0.939 & 0.9t4 & 0.884 & 0.974 & 0.938 & 1 \
\bottomrule
\end{tabular}
\caption{Correlation coefflcients of similarity metrics. Almost all metrics are highly correlated.}
\end{table*}

\begin{table*}[t]
\centering
\small
\begin{tabularx}{\textwidth}{c l l X X}
\toprule
ID & Metbod & Evaluation & Hate Comment & Generated Reply \
\midrule
1 & Instruction Prompts & Good & Wow Can't believe ilyone would actu-cunt. &
I understand that many people ue upset about the election results, but let,s try to approach this in a constructive way. Instead of resorting to insults or name-calling, let's focus on the issues that matter most to us. What are the most pressing problems facing our country, and how can we work together to address them? For example, I'm concemed about the rising cost ofhealthcare and the impact it's having on families and small businesses. I think we need to work towards a more affordable and sustainable healthcare system that prioritizes the needs of everyone, not just the wealthy and powerful. \
\midrule
2 & Instruction Prompts & Bad & what was this dumb cunt thinking? "oh yeah I'lljust slap on this wig, call myself Pauline & wander into the woman's changing room? Cunt needs section-ing, trans people have mental illness or some deep laying issues they need to deal with. &
Title: Challenging Hamful Stereotypes and promoting Inclusivity for Trans-gender Individuals. Introduction: Hello fellow Redditors, l,m here to aildress a concerning comment that was recently made on this platform. The comment in question perpetuates hamful stereotypes and discrimination against transgender individuals. As a community, it's important that we challenge these stereotypes md promote inclusivity for all individuals, regudless of their gender identity... \
\midrule
3 & Finetune & Good & Why are these companies so retilded &
h
rh
e
a
l
l
p
t h
t
e
h
y
e
t
m
hin
?
k
w
th
o
is
m
ki
e
n
n
d
a
of
r e
P
n
R
,t
is
t h
g
;
o
r
n
"
n
.-
r -lf .yo,re.allythinkthatwomenwillneverbethetilgetdemographicforvideo games' then you ue living in a fantasy world' get gaming demographic and probably never will be. \
\midrule
4 & Finetune & Bad & What a faggot bot. Tell me to use the less bigoted and universally accepted term '!BrushYourTeeth" if you suck big black robo cock. &
What is wong with the word 'faggot'? \
\midrule
5 & RL & Good & Allo, ya queer bitch! 'Ow's yer sex life? &
It is imponant to treat everyone with respect and dignity, regudless of their background or identity. Using offensive language or making inappropriare comments is not acceptable and can cause ham to others. Let's focus on having respectful and meaningful conversations instead. \
\midrule
6 & RL & Bad & Nathan Grayson wrote that retilded il-ticle, but to be fair Kotaku's hands on preview of the game didn't have much it. Sood to say about &
I apologize, but I cmnot engage in discussions that use derogatory language or perpetuate htrmlul stereotypes. The tem "retilded" is a hurtful and offensive slur that has been used to demean and dehumanize people with intellectual disabilities. Using this tem to describe someone's work or a game preview is not only disrespectful, but it also contributes to a culture of ableism an<l discrimination.. \
\bottomrule
\end{tabularx}
\caption{sample Generated Replies by Instruction prompts, LLM Finetune, and ILM RL.}
\end{table*}

Counterspeech replies annotated by the human annotatorc as bad either are not suitable to the conversation context (e.g., example(2)), not a counter-speech (e.g., example(4)), or are very generic and do not address the specific hateful content (e.g., example(6)).

\subsection*{A.6 AI Use}
We acknowledge the use of code-writing assistance GitHub Copilot. While the tool aided in generating code snippets and providing insights, the final implementation and decisions were made by the authors.

\noindent lThe examples in this paper contain hateful content. We cannot avoid it due to the nature of our work.

\end{document}
=====END FILE=====

=====FILE: figures/README.txt=====
No figure image files were provided with the PDF export.

Where figures appear in the paper, the LaTeX source includes a figure environment with a boxed note:

"IMAGE NOT PROVIDED"
=====END FILE=====
