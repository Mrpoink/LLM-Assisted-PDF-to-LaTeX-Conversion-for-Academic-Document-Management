=====FILE: main.tex=====
% Source: 
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{geometry}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\geometry{margin=1in}

\title{Verba volant, scripta volant? Don’t worry! There are computational solutions for protoword reconstruction}
\author{Liviu P. Dinu \and Ana Sabina Uban \and Alina-Maria Cristea \and Bogdan Iordache \and Teodor Marchitan \and Simona Georgescu \and Laurentiu Zoicas}
\date{}

\begin{document}
\maketitle

\begin{abstract}
Reconstructing unattested proto-words from their modern descendants is a fundamental task in historical linguistics. Traditionally, this process relies on the comparative method and expert knowledge, which makes it time-consuming and difficult to scale. In this paper, we explore computational approaches for proto-word reconstruction in the Romance language family, using data from the ProtoRom database. We experiment with several models, including Conditional Random Fields with reranking, a probabilistic LSTM, a character-level transformer, and a pre-trained large language model (Flan-T5). Our results show that neural sequence-to-sequence models outperform traditional baselines, and that pre-trained language models can achieve competitive performance with minimal task-specific adaptation. We provide a detailed error analysis and discuss the implications of our findings for computational historical linguistics.

\end{abstract}

\section{Introduction and Related Work}
Protoword reconstruction, consisting of recreating the words in a proto-language from their descendants in daughter languages, is central to the study of language evolution. As the foundation of historical linguistics (Campbell, 2013; Mallory and Adams, 2006) and the basis for linguistic phylogeny (Atkinson et al., 2005; Alekseyenko et al., 2012; Dunn, 2015; Brown et al., 2008), protoword reconstruction offers important pieces of information concerning the geographical and chronological dimensions of ancient communities (Heggarty, 2015; Mallory and Adams, 2006), at the same time, allowing an insight into the cognitive and cultural world of our ancestors. The traditional process of reconstructing ancient languages consists of the ``comparative grammar-reconstruction'' method (Chambon, 2007; Buchi and Schweickard, 2014), and the etymological data thus obtained can be used as a source on human prehistory, corroborating the archaeological inventory (Heggarty, 2015), and providing the basis for ‘linguistic paleontology’ (Epps, 2014). The reconstruction of a word automatically implies a reconstruction of the surrounding realities, both natural and socio-cultural. For example, the presence in different Indo-European languages of obviously related words for ’beech’ or ’salmon’ allowed the reconstruction of words from Proto-Indo-European and thus information about the elements of nature present in the immediate vicinity of the Indo-Europeans could be extracted. In the absence of any clear documentary or archaeological data, these lexical clues allowed the geographical identification of the Indo-European homeland, also facilitating the chronology of successive waves of separation of Indo-European languages from the common trunk.

In the case of Romance languages, although the mother tongue - Latin - is attested, its presence in written texts is not an exhaustive source for linguistic, social, and historical analysis of the community that spoke it. It is now generally accepted that the spoken language represented a different diastatic, diaphasic, and diamesic variety from written language, used by the few educated people who decided to express themselves in writing (Wright, 2002). The Latin language that we reconstruct from words inherited in Romance languages is thus the only concrete and reliable living variety of the language from which Romance languages originate, whether we call it oral/ vulgar Latin or Proto-Romance. We will opt here for the name ``Proto-Romance'' when we refer to the language from which the Romance languages originate, as this corresponds to the concept of protolanguage and protoword (Buchi and Schweickard, 2014).

Furthermore, there are still numerous clearly cognate words present in several Romance languages, whose etymon is not attested in Latin (nor in any other language from which it might have been borrowed). For example, in the case of It. trovare ’find’, Fr. trouver, Cat. trobar, etymologists have hotly debated over the decades whether one should reconstruct the protoform *tropare or *turbare (Georgescu and Georgescu, 2020). A series of cognates attested in all Romance geographical areas, like Rom. încă ’moreover’, It. anche, Old Fr. anc, Cat. anc etc., has triggered over 15 etymological hypotheses over the last century, still without a generally accepted solution.

Although etymologists’ interest in reconstructing the protolanguages has risen over the years, they still encounter numerous gaps when using exclusively the classical, manual methods (Buchi and Schweickard, 2010, 2020). As the task of protoword reconstruction plays an important role in historical linguistics, studies have gone beyond the comparative method in an attempt to automate the process (Atkinson, 2013; Oakes, 2000; Bouchard-Côté et al., 2013; Ciobanu and Dinu, 2019). However, the task has been recognized as difficult and challenging. Computational protoword reconstruction is a fairly new direction of study, and consequently even state of the art approaches have limitations. Complete automation of the reconstruction process is still a desideratum. Oakes (2000) proposed two systems (Jakarta and Prague) that, combined, cover the steps of the comparative method for protolanguage reconstruction, and several other approaches to reconstruct protowords computationally had been attempted previously (Hewson, 1973; Lowe and Mazaudon, 1994; Kondrak, 2002). The work of computational biologists such as Alexandre Bouchard-Côté, Russell Gray, Robert McMahon, and Mark Pagel, and co-authors took the protoword reconstruction one step further by applying methods from computational biology to the problem of the reconstruction of language history, often in collaboration with linguists (Pagel, 1999; Pagel et al., 2013; Bouchard-Côté et al., 2009; Bouchard-Côté et al., 2013). In recent years, researchers have introduced new methods for protoword reconstruction, based on modern computational techniques (for example, CRF, transformers, RNN, deep learning) (Ciobanu and Dinu, 2018; Sims-Williams, 2018; Meloni et al., 2021; Fourrier, 2022; List et al., 2022; He et al., 2023a; Akavarapu and Bhattacharya, 2023; Kim et al., 2023). The computational methods are limited today by 1) the available data (sparse, inconsistent) and 2) by the insufficiency of linguistic knowledge embedded in the systems.

The latest computational results on Romance protoword reconstruction, in particular, are reported on the database of (Meloni et al., 2021), which contains 8,799 cognates set in Latin, Italian, Spanish, Portuguese, French, and Romanian (not all full cognates set). This is a revision of the dataset of (Dinu and Ciobanu, 2014) (used with very good results in (Ciobanu and Dinu, 2018)) with the addition of cognates scraped from Wiktionary.


\section{Data}
\section{Data}

A major inconvenience in Historical Linguistics in general, and in computational approaches of protoword reconstruction in particular is the scarcity of available data. Nonetheless, in the last few years, several initiatives have been undertaken in this direction. (Ciobanu and Dinu, 2018) developed a database of Latin protowords, further expanded by (Meloni et al., 2021) with Wiktionary data. Recently, this dataset was extensively used for several studies (Kim et al., 2023; He et al., 2023b; Akavarapu and Bhattacharya, 2023). In 2023, Dinu et al. (2023) published the most comprehensive database of Romance related words, named RoBoCop. It contains cognates and etymons in five Romance languages: Italian, Spanish, Portuguese, Romanian, and French. It has already been used with good results on prominent historical linguistic tasks such as cognate identification (Dinu et al., 2023), cognate-borrowings discrimination (Dinu et al., 2024b), and determining the borrowing direction (Dinu et al., 2024a).

\begin{table}[h]
\centering
\begin{tabular}{lllll}
RO & ES & PT & IT & FR \
axa˘ & eje & áxis & asse & ais \
axis & axis & áxis & asse & ais \
ax & axis & áxis & asse & ais \
axis & eje & áxis & asse & ais \
axa˘ & axis & áxis & asse & ais \
ax & eje & áxis & asse & ais \
\end{tabular}
\caption{All cognate tuples present in the ProtoRom dataset for the Latin etymon axis.}
\end{table}


\subsection{The ProtoRom Database}
\subsection{The ProtoRom Database}

Starting with the RoBoCoP database (Dinu et al., 2023), in order to obtain cognate sets with common etymons in the five Romance languages, we filtered out the words with Latin etymology. We then created maximal tuples of words in the Romance languages with the same etymon ($< w_{L_i} >, e$), where $L_i$ are all the languages among the five where the etymon $e$ engendered a word, and $w_{L_i}$ are the corresponding words in each of the languages discussed. In cases where multiple words in $L_i$ derive from the same etymon $e$, we created multiple tuples ($< w_{L_i} >, e$) with all possible combinations of cognate words $< w_{L_i} >$ and the same etymon $e$. For an example of such a case see Table 1.

We curated the obtained data, with the help of linguists. In the process, we discarded sets that contained irrelevant or erroneous information, e.g.: erroneous lexical forms (e.g. Lat. \textit{videre} ’see’ - It. \textit{vedere} - Fr. \textit{voir} - Ro. \textit{videa} (correct: \textit{vedea}); included a verb form in any mood other than the infinitive (e.g. Lat. \textit{videre} - Sp. \textit{veas} (subjunctive) / \textit{viendo} (gerundive) / etc.); retained the reflexive form of a verb (e.g. Lat. \textit{ponere} ’put’ - It. \textit{porre} - Sp. \textit{ponerse} (\textit{poner} + reflexive pronoun \textit{se}), etc.); or contained words derived on Romance ground (e.g. Lat. \textit{dens} ’tooth’ - It. \textit{dente} - Ro. \textit{dintoș} (= \textit{dinte} + suff.-\textit{os}), etc.).

We were able to apply manual corrections for all these errors for the smaller subset of entries in the database that have a cognate in each of the five languages. For the rest of the full database ProtoRom, we applied a semi-automatic correction by lemmatizing the cognate words, using the default lemmatizers\footnote{using the models, for each language $L$} implemented in the spaCy\footnote{\url{[https://spacy.io/usage/models}}](https://spacy.io/usage/models}}) library for each of the Romance languages. In all experiments described in the rest of the paper, we use the lemmas of the cognates instead of the original forms found in the dictionary.

In addition to the correct series thus retained, we integrated the database created by Reinheimer-Rîpeanu (2001), a high quality collection of cognate series manually selected from the etymological dictionaries of each Romance language, some of which still not digitized (which probably explains why certain cognate sets from this collection were not among ones in the RoBoCoP database).

We thus obtained a new database of cognate sets. The proposed database contains a total 39,973 full or partial cognate sets along with their etymons. For the experiments in this paper, we focus on the 19,222 entries with at least 2 cognates. We choose this subset in order to ensure the robustness of our experiments, focusing on Latin etymons that engendered at least two cognates in two different languages, and we ignore the entries with only one cognate for a given etymon. Going further, this restricted dataset will be referred to as ProtoRom\footnote{The dataset is available for research purposes upon request at: [https://nlp.unibuc.ro/resources.html\#protorom}](https://nlp.unibuc.ro/resources.html\#protorom}).

A cognate set is composed of a tuple of words in different languages with a common etymon, where the tuple can be either a full set of 5 cognates or a partial set of 2 to 4 cognates, where the cognate in one or more of the languages is missing (the Latin etymon did not produce an attested word in these languages according to our sources).

There are 1,245 full cognate sets in the database, the rest being partial cognate sets. To facilitate distinguishing between the two settings, we name the first one ProtoRom-all5, and the second one ProtoRom. When we leave out one of the languages, we can obtain more full sets of 4-tuples (sets with at least 4 cognates) as follows: 1,480 if we leave out Italian, 2,493 if we leave out French, 1,489 when we leave out Portuguese, 1,504 when we leave out Spanish, and 1,946 by leaving out Romanian. The statistics detailing the number of partial cognate sets in all combinations are shown in Table 2.

ProtoRom is the largest database of cognate sets for Romance languages so far, significantly exceeding the widely used database for this task (Meloni et al., 2021), containing 8,799 cognate sets of Romanian, French, Italian, Spanish, Portuguese words and the corresponding Latin form (which, in turn, is an extension of Ciobanu and Dinu (2018)’s original dataset of 3,218 cognate sets, by adding data from Wiktionary).


\section{Methodology and Experiments}

\subsection{Experimental Setting}
\section{Methodology and Experiments}

\subsection{Experimental Setting}

For our experimental trials, we consider two settings: In the first one, we limit our dataset to only the full cognate sets (i.e. 5-tuples of cognates from each of the five languages, that originate from the same Latin etymon), while in the second one we consider all cognate sets (with at least two cognates from different languages, per etymon, as previously mentioned). The second setting uses the full breadth of our proposed dataset (ProtoRom), whereas the first one is a strict subset (ProtoRom-all5).

\paragraph{Data splitting.} In order to train and validate our models, we split our datasets into 80% : 10% : 10% train-dev-test subsets. Because of the nature of the cognate sets, generating a language-level stratified split is a non-trivial task. Since a Latin etymon can produce more than one reflex in a given language, we end up with $\prod_i \max(1, n_{L_i})$ cognate sets for a given etymon, where $n_{L_i}$ is the number of reflexes generated by that etymon in language $L_i$.

We propose a random split methodology that achieves the following properties: A Latin etymon and all of its cognate sets are not allowed to be part of more than one split; the raw number of cognate sets (i.e. entries in the dataset) follows the 80 : 10 : 10 distribution; the distribution of unique Latin etymons is also 80 : 10 : 10; for each of the five languages; and computing the distribution of unique reflexes in that language yields the same ratio across the splits. In other words, if we only keep the Latin etymons and their reflexes in only one language, we obtain a monolingual task with the same 80 : 10 : 10 split.

In order to perform these splits, we construct for each Latin etymon a 5-dimensional vector $(n_{L_i})*i$, using the previous definition of $n*{L_i}$. In order to obtain a split of ratio $0 < p < 1$, we want to select such vectors that, when summed together, equal $p \cdot (N_{L_i})*i$, where $N*{L_i}$ is the total number of unique reflexes from language $L_i$. In other words, we face a task equivalent to a five-dimensional knapsack problem, which is not feasible given the large total capacities. Considering that these vectors contain particularly small values, and are somewhat uniformly distributed, plus the large capacities that we have to fill, we are able to randomly select etymons and their associated cognate sets and add them to any of the three splits, as long as they fit. This approach yields the original split distribution with some small deviations ($< 1%$).

Also note that after splitting the ProtoRom-all5 dataset, containing only the full cognate sets, we can use it as a starting point for splitting the rest of the ProtoRom dataset, thus ensuring that no training examples from one setting leaks into the validation of the other one.

\paragraph{Features.} The proposed approaches can be split into two main categories: models for reconstructing the orthographical representation of the protowords using the orthographical form of modern cognates, and models that reconstruct the phonemic representation from phonetic transcriptions of modern cognates. Our extracted dataset essentially provides the necessary examples for the former, while for the latter we employ the eSpeak\footnote{\url{[https://github.com/espeak-ng/espeak-ng}}](https://github.com/espeak-ng/espeak-ng}}) library to automatically generate the phonemic representations.

\subsection{Models}

We use a variety of machine learning models, including classical, neural, and transformer-based (pretrained and trained from scratch for the task). We include methods used in previous papers on the topic and evaluate them on our larger dataset in order to provide a benchmark for the task of protoword reconstruction for Romance languages. We experiment with a variety of models, including pre-trained large language models (LLMs) and current state-of-the-art models for protoword reconstruction with various architectures (probabilistic RNN, character-level transformer) adapted to our new database, as well as original solutions. In this way, we aim to provide a benchmark for the task of protoword reconstruction.

\paragraph{CRF + reranking} We used an approach that relies on conditional random fields (CRFs), based on the method proposed by Ciobanu and Dinu (2018). Firstly, we applied a sequence labeling method that produces the form of the Latin ancestors, for each modern language. The modern words are the sequences, and their characters are the tokens. We used character n-grams from the input words as features. We employed pairwise sequence alignment (Needleman and Wunsch, 1970) between modern words and protowords to obtain the labels for each token. Secondly, we defined several ensemble methods to take advantage of the information provided by all languages, in order to improve performance. We employed fusion methods based on the ranks in the n-best lists and the probability estimates provided by the individual classifiers for each possible production, in order to combine the outputs of the classifiers (n-best list of possible protowords) and to leverage information from all modern languages. For each word in the productions list, we multiply the rank of it with the confidence score given by the CRF model for each language; we sum up the multiplication scores for each word in the list and then rerank the productions based on these results.

\paragraph{Probabilistic LSTM} We conducted experiments using a combination of recurrent neural networks with different dynamic programs and expectation-maximization techniques, as described in He et al. (2023b). The overall system can be split in two stages: a) a modelling stage, where we model the evolution of words by making small character-level edits to the ancestral form; for each language in the study, the distribution over newly created words is computed; b) an expectation-maximization stage, where the ancestral form is inferred; using words sampled from the posterior distribution, the expected edit count is computed and further used by the character-level recurrent neural network in order to optimize the next round of samples; the final reconstruction is the maximum likelihood word forms. This model requires a full tuple of cognates to be passed as input, so we only compute results for experiments on the ProtoRom-all5 set. Like the original authors, we only apply this model on the phonemic forms of words, since the probability distributions of edit operations used in the algorithm rely on a set of manually set features for each phoneme that are not similarly available for orthographical characters.

\paragraph{Character-level transformer} The next experiments conducted in this research are based on the transformer model, proposed by Kim et al. (2023). Some critical changes in the architecture were made in order to be able to accept our samples format: multiple modern word sequences (one for each language) correspond to a single protoform sequence. A positional encoding is applied to each individual modern word sequence before concatenation. An additive language embedding is applied to the token embeddings alongside the positional encoding in order to make a difference between input tokens of different languages.

\paragraph{Pre-trained LLM (Flan-T5)} We finally evaluate the capabilities of pretrained Large Language Models (LLMs) to solve our task. While LLMs are currently obtaining state-of-the-art performance across NLP tasks, our specific goal is unlike usual tasks included in benchmarks or in training data for LLMs, and it is strongly multilingual (including one dead language), so we suspect it might be a difficult task for an LLM. We choose to use a pretrained model and fine-tune it on our own training data in order to increase its chances to perform well. We use a `base'' variant of the Flan-T5 model (Chung et al., 2024), and fine-tune the model using instructions including the prompt: `What is the etymon given the following cognates:'', followed by a list of cognate and language pairs formatted as ``$< L_i >$: $< w_i >$'' and separated by new lines, where the list of cognate words $w_i$ in their respective languages $L_i$ can be arbitrarily long (from 2 to 5 cognates, in the case of our experiments). For evaluation, we attempt to generate multiple output sequences, which are used as a ranking for the etymon prediction.

One limitation of pretrained LLMs that we cannot overcome through fine-tuning is its alphabet, which contains mostly characters in the Latin graphical alphabet, which means that we can only use this model with orthographical features. Using phonemic features would require retraining the model from scratch and we would lose the benefit of pretraining which is usually the strong point of LLMs.


\section{Results}

The previously described methods have been applied on both ProtoRom and ProtoRom-all5 datasets, using the orthographical form of the cognates and Latin etymon, or alternatively the auto-generated phonemic representations (where the models were able to accommodate them).

We also provide a comprehensive human evaluation of the results. Linguists from our team manually analyzed the entire list of results, and we present the most significant observations regarding the models’ successes and failures. The linguists did not correct the protoforms proposed by the models, but only evaluated and commented on them in relation to current knowledge in the field of historical linguistics.

The metrics used include accuracy, (normalized) edit distance, and Cov$_i$, with $i \in {1, 5, 10}$, which stands for an extended version of the accuracy metric, where a correct prediction is one where the model found the correct etymon within the first $i$ etymons predicted by our method (this metric is computed for models that are able to output a ranked list of predictions - Flan-T5 and CRF-based models).

\subsection{ProtoRom-all5 Results}

Results obtained on the ProtoRom-all5 set are shown in Table 3. In terms of accuracy (or Cov$_1$), the best results are obtained using the orthographical forms, with the CRF-rerank model, reaching 60.4%. From the perspective of the Cov$*i$ metrics, it is remarkable that the CRF-rerank model obtains a Cov$*{10}$ score above 82%.

The experiments using the phonemic forms produce weaker results, with the best accuracy reaching 55.8% in the top 1 predictions scenario. Nevertheless, the CRF approach is able to achieve an accuracy close to 80% when we consider the top 10 best ranked predictions.

The probabilistic RNN models achieve very poor performances, reaching a mean edit distance of 3.11 when trained on the phonemic representations.

\subsection{ProtoRom Results}

The best accuracy when training the orthographical models is achieved in this scenario by the Transformer model, closely surpassing 73% (Table 4). As for the Cov$*i$ metrics, the Flan model remarkably obtains a Cov$*{10}$ accuracy score of 85.4%, and an edit distance of 0.23.

Similarly to the previous scenario, the experiments using the phonemic forms produce weaker results, with the best accuracy reaching 66.8% via the Transformer model. These results represent a collection of baselines for protoword reconstruction using our proposed dataset configurations.

We believe the higher accuracy observed on the full dataset is simply due to the larger amount of available data. While ProtoRom-all5 is a subset that contains only complete cognate sets from each of the five studied languages (totaling 1,245 sets) the ProtoRom dataset includes sets of two, three, or four cognates, resulting in significantly more sets (19,222). This larger dataset allowed the models to learn more phonetic correspondences, thereby improving the reconstruction process. Even though they are not full sets of five cognates, the additional cognate sets in the full database seem to help the models learn more about their protowords. This learning process is closely similar to the human method of learning: with more examples, linguists can be more certain of particular correspondences or phonetic changes and can apply them in the reconstruction with much greater confidence.


\section{Error analysis}

This section is dedicated to a deeper dive into qualitatively quantifying the errors produced by the previously proposed models. Our objective is separating purely wrong predictions from ``near misses'', which may still provide value for linguists for the reasons discussed below.

The error analysis was manually conducted by the linguists from our team, who specialize in Romance languages. They did not modify the protoforms provided by the models in any way. Their only intervention was to distinguish forms that were genuinely erroneous from those whose differences from the dictionary form were either insignificant or represented a correct adjustment to the reality of Latin pronunciation. In the final quantitative analysis, forms in this category were therefore included in the list of correct predictions without any changes to their structure.

Through analyzing the errors, we have identified some patterns that typically reflect either an insufficient number of examples to support a particular phonetic change or the irregularity of the change itself. For example, the short tonic /u/ develops into Spanish /o/ in half of the cases, while it remains /u/ in the other half. In such scenarios, the model may not know which phonetic treatment the cognates underwent and might choose the wrong variant. Similarly, in cases of phonetic accidents, which are by nature irregular and unpredictable, the model cannot reconstruct the pre-accident form. Instead, it reconstructs the intermediate form between the classical word and its Romance descendants. Identifying and systematizing these errors can help improve future results by broadening the input with information related to sound changes.

Before analysing the errors, a few preliminary points should be made. Romance lexicography as a whole is graphocentric - it considers the written, classical Latin (CL) lexical variants as the basis for the Romance vocabulary, even though it goes without saying that vernacular languages, oral par excellence, developed from an oral language, in our case Proto-Romance (PR) (Chambon, 2007). In the latest methodology used in Romance etymology, developed within the DÉRom project (Buchi and Schweickard, 2014), the etymological identification is based strictly on the comparative grammar - reconstruction method, starting from the lexical forms that were used uninterruptedly in Romance languages. The lexemes attested in Classical Latin are only a written correlate, possibly further evidence of the existence of the form obtained by the methods of comparative historical linguistics.

In the light of these considerations, we find that some of the reconstructed variants classified as errors should actually be considered as positive results and evidence that the machine could work at the same level as a linguist applying traditional methods. By positive results instead of errors we mean cases - not a few - where the machine reconstructed exactly the phonetic form valid for oral Latin, at the expenses of the standard orthographical form as it is lemmatized in classical Latin dictionaries.

Cases where the word obtained and the one given by the dictionary did not completely match were automatically considered as errors, although sometimes it was not a mistake as such. Therefore, there are a number of protoforms which, although they appear in the list as inadvertences, are variants that should be taken into account with full attention by linguists. Some are no more wrong than the form in the dictionary, some are closer to the actual oral form than those provided by lexicographers, while some are exactly the form that historical linguists would have reconstructed using traditional methods.

\section{Conclusion}

In this paper, we introduced ProtoRom, the most comprehensive database to date for automatic protoword reconstruction in Romance languages, containing 39,973 full or partial cognate sets, out of which 19,222 include at least two cognates. We also proposed ProtoRom-all5, a subset containing 1,245 full cognate sets of five-tuples. Based on these datasets, we established a strong benchmark for protoword reconstruction by evaluating a variety of machine learning models, including CRF-based approaches with reranking, probabilistic LSTMs, character-level transformers, and a fine-tuned pretrained LLM (Flan-T5).

Our experiments show that computational methods can achieve competitive and, in some configurations, state-of-the-art results for Romance protoword reconstruction. In particular, the Transformer model achieved over 73% accuracy on the full ProtoRom dataset using orthographical features, while the Flan-T5 model obtained a Cov$_{10}$ score of 85.4%. The CRF-based approach with reranking also proved highly competitive, especially in top-$k$ prediction settings.

Beyond quantitative performance, our qualitative error analysis indicates that many so-called “errors” correspond in fact to plausible phonetic reconstructions closer to oral Proto-Romance than to the standardized Classical Latin forms found in dictionaries. This suggests that computational systems, when trained on sufficiently rich data, can capture meaningful historical patterns and potentially assist linguists in exploring alternative reconstructions.

We believe that the proposed dataset and benchmark open new directions for research in computational historical linguistics. Future work may include incorporating explicit phonological knowledge, modeling irregular sound changes more effectively, and extending the approach to other language families.


\section*{Limitations}
One limitation of the current work stems from the automatic generation of the phonetic representations via a third-party library (eSpeak). Although this approach was employed successfully in previous studies, the quality of the generated phonemes has a higher variance when comparing high-resourced languages to lower-resourced ones (such as Romanian, or even Latin).

Also, in this study we used the generated phonetic forms without any extra preprocessing steps, in order to have a representation of the pronunciation that is as accurate as possible. Removing phonetic markers (such as stress markers) from these representations may turn the generation task into a somewhat easier one, since currently the phonetic models are tasked with predicting the stressed sounds too.

In terms of resources, existing LLMs are mostly targeting orthographical texts, making any reasonable attempt at generating phonetic ones very difficult.


\section*{Ethics Statement}
There are no ethical issues that could result from the publication of our work. Our experiments comply with all license agreements of the data sources used. We make the contents of our package available for research purposes upon request.

\section*{Acknowledgements}
This work was supported by a mobility project of the Romanian Ministery of Research, Innovation and Digitization, CNCS - UEFISCDI, project number PN-IV-P2-2.2-MC-2024-0461, within PNCDI IV.

We want to thank the reviewers for their useful suggestions and Diana Grigore, Cosmin Petrescu, Ioana Pintilie for their help in developing the algorithms.


\section*{References}
\begin{thebibliography}{}

\bibitem{}
Quentin D. Atkinson. 2013. Phylogenetic methods and the prehistory of languages. In The Routledge Handbook of Historical Linguistics, pages 202–219. Routledge.

\bibitem{}
Sergei I. Alekseyenko, Paul J. S. Morrison, and Russell D. Gray. 2012. Using Bayesian phylogenetic inference to reconstruct the history of languages. Systematic Biology, 61(5): 856–872.

\bibitem{}
David R. Brown, Steven J. A. Greenhill, Russell D. Gray, and Quentin D. Atkinson. 2008. Phylogenetic comparative analysis and the evolution of language. In The Handbook of Evolutionary Linguistics.

\bibitem{}
Lyle Campbell. 2013. Historical Linguistics: An Introduction. MIT Press.

\bibitem{}
Jean-Pierre Chambon. 2007. Remarques sur la méthode comparative. Mémoires de la Société de linguistique de Paris, 15:57–72.

\bibitem{}
Alina Maria Ciobanu and Liviu P. Dinu. 2018. Ab initio: Automatic Latin proto-word reconstruction. In Proceedings of the 27th International Conference on Computational Linguistics (COLING 2018), pages 1604–1614, Santa Fe, New Mexico, USA. Association for Computational Linguistics.

\bibitem{}
Alina Maria Ciobanu and Liviu P. Dinu. 2019. Automatic identification and production of related words for historical linguistics. Computational Linguistics, 45(4):667–704.

\bibitem{}
Liviu P. Dinu and Alina Maria Ciobanu. 2014. Building a dataset of multilingual cognates for the Romanian lexicon. In Proceedings of the Ninth International Conference on Language Resources and Evaluation, LREC 2014, Reykjavik, Iceland, May 26-31, 2014, pages 1038–1043. European Language Resources Association (ELRA).

\bibitem{}
Liviu P. Dinu, Ana Sabina Uban, Alina Maria Cristea, Anca Dinu, Ioan-Bogdan Iordache, Simona Georgescu, and Laurentiu Zoicas. 2023. Robocop: A comprehensive Romance borrowing cognate package and benchmark for multilingual cognate identification. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 7610–7629. Association for Computational Linguistics.

\bibitem{}
Liviu P. Dinu, Ana Sabina Uban, Ioan-Bogdan Iordache, Alina Maria Cristea, Simona Georgescu, and Laurentiu Zoicas. 2024b. Pater incertus? there is a solution: Automatic discrimination between cognates and borrowings for Romance languages. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 12657–12667, Torino, Italia. ELRA and ICCL.

\bibitem{}
Liviu P. Dinu, Ana Sabina Uban, Anca Dinu, Ioan-Bogdan Iordache, Simona Georgescu, and Laurentiu Zoicas. 2024a. It takes two to borrow: a donor and a recipient. who’s who? In Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, pages 6023–6035. Association for Computational Linguistics.

\bibitem{}
Michael Dunn. 2015. Language phylogenies. The Routledge handbook of historical linguistics, pages 190–211.

\bibitem{}
Patience Epps. 2014. Historical linguistics and sociocultural reconstruction. In The Routledge Handbook of Historical Linguistics.

\bibitem{}
Clémentine Fourrier. 2022. Neural Approaches to Historical Word Reconstruction. (Approches Neuronales pour la Reconstruction de Mots Historiques). Ph.D. thesis, PSL University, France.

\bibitem{}
Simona Georgescu and Theodor Georgescu. 2020. Fr.«trouver», occ.«trobar» etc.: un dossier étymologique ouvert à nouveau. Revue de linguistique romane, 84(1):83–98.

\bibitem{}
Andre He, Nicholas Tomlin, and Dan Klein. 2023a. Neural unsupervised reconstruction of protolanguage word forms. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pages 1636–1649. Association for Computational Linguistics.

\bibitem{}
Andre He, Nicholas Tomlin, and Dan Klein. 2023b. Neural unsupervised reconstruction of protolanguage word forms. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1636–1649, Toronto, Canada. Association for Computational Linguistics.

\bibitem{}
John Hewson. 1973. Reconstructing prehistoric languages on the computer: The triumph of the electronic neogrammarian. In COLING 1973 Volume 1: Computational And Mathematical Linguistics: Proceedings of the International Conference on Computational Linguistics.

\bibitem{}
Young Min Kim, Kalvin Chang, Chenxuan Cui, and David R. Mortensen. 2023. Transformed protoform reconstruction. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 24–38, Toronto, Canada. Association for Computational Linguistics.

\bibitem{}
Grzegorz Kondrak. 2002. Algorithms for language reconstruction. PhD thesis, University of Toronto.

\bibitem{}
Johann-Mattis List, Robert Forkel, and Nathan Hill. 2022. A new framework for fast automated phonological reconstruction using trimmed alignments and sound correspondence patterns. In 3rd International Workshop on Computational Approaches to Historical Language Change 2022, pages 89–96. Association for Computational Linguistics (ACL).

\bibitem{}
John B Lowe and Martine Mazaudon. 1994. The reconstruction engine: a computer implementation of the comparative method. Computational Linguistics, 20(3):381–417.

\bibitem{}
James P Mallory and Douglas Q Adams. 2006. The Oxford Introduction to Proto-Indo-European and the Proto-Indo-European World. Oxford University Press on Demand.

\bibitem{}
Carlo Meloni, Shauli Ravfogel, and Yoav Goldberg. 2021. Ab antiquo: Neural proto-language reconstruction.

\bibitem{}
Thomas Oakes. 2000. Computer methods for reconstructing protolanguages. Journal of Quantitative Linguistics, 7(3): 211–225.

\bibitem{}
Mark Pagel. 1999. Inferring the historical patterns of biological evolution. Nature, 401:877–884.

\bibitem{}
Mark Pagel, Quentin D. Atkinson, and Andrew Meade. 2013. Frequency of word-use predicts rates of lexical evolution throughout Indo-European history. Nature, 449:717–720.

\bibitem{}
Maria Reinheimer-Rîpeanu. 2001. Les latins et les langues romanes. Bucharest: Editura Universității din București.

\bibitem{}
Patrick Sims-Williams. 2018. Automatic reconstruction of Proto-Indo-European. [ILLEGIBLE]

\bibitem{}
Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2024. Scaling instruction-finetuned language models. Journal of Machine Learning Research, 25(70):1–53.

\bibitem{}
Roger Wright. 2002. Late Latin and Early Romance in Spain and Carolingian France. Liverpool University Press.

\end{thebibliography}


\appendix

\section{Appendix}

\subsection{Hyperparameters and infrastructure}

In this section we describe in detail the hyperparameters and infrastructure used for training the models presented in this paper.

\subsubsection{Conditional Random Fields}

For the CRF-based approach we used the MALLET toolkit\footnote{\url{[https://mimno.github.io/Mallet/}}](https://mimno.github.io/Mallet/}}). The hyperparameters used are as follows:

\begin{itemize}
\item order: 1
\item default feature induction parameters
\item gaussian prior variance: 10
\item number of iterations: 100
\end{itemize}

The reranking strategy was implemented as described in the main text, combining the ranks and confidence scores from the individual language-specific classifiers.

\subsubsection{Probabilistic LSTM}

We used the implementation provided by He et al. (2023b) (GitHub repository\footnote{\url{[https://github.com/AndreHe02/historical_release/tree/master}}](https://github.com/AndreHe02/historical_release/tree/master}})) and the following hyperparameters:

\begin{itemize}
\item lstm input size: 64
\item lstm hidden size: 64
\item context window: 10
\item number of epochs: 30
\end{itemize}

For the training we used the following configuration:

\begin{itemize}
\item number of rounds: 8
\item learning rate: 0.01
\item optimizer: Adam
\item weight decay: 0.01
\end{itemize}

All the training was done on an Apple M2 Pro chip and the total training time was 2 hours.

\subsubsection{Transformer model}

The architecture we used in our experiments is the same as Kim et al. (2023) (GitHub repository\footnote{\url{[https://github.com/cmu-llab/acl-2023/tree/main}}](https://github.com/cmu-llab/acl-2023/tree/main}})). The hyperparameters used for both orthographical and phonetic experiments are as follows:

\begin{itemize}
\item embedding size: 128
\item number of encoder layers: 3
\item number of decoder layers: 3
\item number of attention heads: 8
\item feed forward layer size: 128
\item dropout: 0.202
\end{itemize}

Training hyperparameters used for both orthographical and phonetic experiments:

\begin{itemize}
\item number of epochs: 200
\item batch size: 1
\item learning rate: 0.00013
\item loss: cross entropy loss
\item optimizer: Adam
\item scheduler: polynomial decay scheduler with warmup
\item warmup epochs: 50
\item weight decay: 0
\end{itemize}

In terms of trainable parameters:

\begin{itemize}
\item orthographical experiments: $\approx$ 817,869 parameters
\item phonetic experiments: $\approx$ 854,877 parameters
\end{itemize}

The training was done using an RTX 2080 Ti GPU. Training time:

\begin{itemize}
\item ProtoRom-all5 dataset: 2.5 hours
\item ProtoRom dataset: 5 days
\end{itemize}

\subsubsection{Flan-T5}

Flan-T5 was trained using early stopping based on the Cov$_1$ metric on the validation set.

The configuration used and optimal hyperparameters are as follows:

\begin{itemize}
\item batch_size: 50
\item epochs: 300
\item learning_rate: 1e-4
\item patience: 3
\item max_seq_len: 64
\item weight_decay: 1e-5
\item warmup_steps: 500
\item lr_scheduler_type: polynomial
\item num_return_sequences: 10
\item num_beams: 10
\item classifier_dropout: 0.0
\item d_ff: 2048
\item d_kv: 64
\item d_model: 768
\item decoder_start_token_id: 0
\item dense_act_fn: gelu_new
\item dropout_rate: 0.1
\item eos_token_id: 1
\item feed_forward_proj: gated-gelu
\item initializer_factor: 1.0
\item is_encoder_decoder: true
\item is_gated_act: true
\item layer_norm_epsilon: 1e−06
\item max_length: 64
\item model_type: t5
\item n_positions: 512
\item num_decoder_layers: 12
\item num_heads: 12
\item num_layers: 12
\item output_past: true
\item pad_token_id: 0
\item relative_attention_max_distance: 128
\item relative_attention_num_buckets: 32
\end{itemize}


\end{document}
=====END FILE=====
