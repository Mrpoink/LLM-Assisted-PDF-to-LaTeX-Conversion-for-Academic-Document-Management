=====FILE: main.tex=====
\documentclass[11pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage{microtype}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
\usepackage{tabularx}
\usepackage{longtable}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{url}
\usepackage{hyperref}

\hypersetup{
colorlinks=true,
linkcolor=blue,
urlcolor=blue,
citecolor=blue
}

\title{Back to School: Translation Using Grammar Books}
\author{
Jonathan Hus$^{\alpha}$ \quad Antonios Anastasopoulos$^{\alpha,\beta}$\
$^{\alpha}$Department of Computer Science, George Mason University, VA, USA\
$^{\beta}$Archimedes AI Unit, Athena Research Center, Athens, Greece\
\texttt{{jhus,antonis}@gmu.edu}
}
\date{}

\begin{document}
\maketitle

\begin{abstract}
Machine translation systems for high resource
languages perform exceptionally well and pro-
duce high quality translations. Unfortunately,
the vast majority of languages lack the quantity
of parallel sentences needed to train such sys-
tems. These under-represented languages are
not entirely without resources, as bilingual dic-
tionaries and grammar books may be available
as linguistic reference material. With current
large language models (LLMs) supporting near
book-length contexts, we can use the available
material to ensure advancements are shared
among all of the world’s languages. In this
paper, we use dictionaries and grammar books
to improve machine translation. We evaluate
on 16 typologically diverse low-resource lan-
guages, showing encouraging improvements.\footnote{Code and data to reproduce our experiments are here: \url{[https://github.com/jonathanhus/back-to-school}.}](https://github.com/jonathanhus/back-to-school}.})
\end{abstract}

\section{Introduction}
Machine translation systems have progressed re-
markably, but they require massive amounts of par-
allel sentences (Bapna et al., 2022). More recently,
instruction-tuned large language models (LLMs)
have also proven capable of performing machine
translation. However, their performance is best
when translating among high-resource languages
that were most likely seen during training. Cur-
rent transformer-based state-of-the-art large lan-
guage models and mutlilingual translation models
are trained on huge web-scraped corpora, with data
in the order of trillions of tokens.

While the web is a vast resource of good train-
ing data,\footnote{assuming aggressive filtering techniques}
the web is also mainly comprised of just
a handful of languages. There are an estimated
7000 languages in the world, but just 10 languages
cover 84% of the web content, with English cov-
ering more than 50%. Therefore, low-resource
languages are not well-represented in the training
data for the large language models (Joshi et al.,
2020), leading to systematic performance dispar-
ities across languages (Blasi et al., 2022). More
importantly, language translation systems rely on
a large number of parallel sentences, providing
examples of sentences in the source and target lan-
guages. Therefore, the sheer magnitude of data that
current translation systems require is simply not
available for low resource languages. Given these
constraints, the compelling question is: how can
we create well-performing translation systems for
low resource languages?

One approach to enabling machine translation
for low-resource languages is to collect many par-
allel sentences. However, this is laborious, expen-
sive, and time-consuming, requiring the skills of
linguists and native speakers. Another approach
would be to incorporate language reference mate-
rial into the translation process of the LLM. The
advantage of this approach is that a good number
of dictionaries and grammar books have been cre-
ated over decades (and longer) and require little
additional effort to use them.

In this work, we push the frontier using the lat-
ter approach to improve on the ability of LLMs to
perform machine translation of low-resource lan-
guages by utilizing available linguistic reference
materials. We incorporate dictionaries, grammar
books, and a small number of parallel sentences
into the prompt of a state-of-the-art LLM. We eval-
uate on 16 typologically diverse low-resource lan-
guages, performing analyses using different combi-
nations of reference materials.

\section{Related Work}
While tens of high-resource languages have en-
joyed the recent advances in machine translation,
many of the world’s 7000+ languages have been
unable to partake in the success.

The current state of the art in multilingual and
low-resource translation is the No Language Left
Behind model (NLLB Team et al., 2022), relying
on a mined and curated corpus of parallel sentences
for 200 languages, including many low-resource
ones. A large multilingual encoder-decoder trans-
lation model was then trained on this data to create
a machine translation system for these languages.

On the other end of the spectrum, Tanzer et al.
(2023) incorporated dictionaries, sentences, and
grammar books to perform machine translation in
a zero-shot setting, i.e., in a language without any
other data available, akin to how a documentary lin-
guist or any second-language learner might learn
a new language ("Machine Translation from One
Book (MTOB)"). This paper inspired our own
work, as it provides a framework for using LLMs
to perform translation of resource-scarce languages.
However, they were limited in the size of the con-
text for the models they chose, and therefore, were
only able to extract smaller chunks of the grammar
book for inclusion. Here, we explore this paradigm
in a much larger scale, with 15 more languages,
performing additional necessary analyses.

Last, Zhang et al. (2024) explored a similar path
utilizing grammar books. They were also limited
by the size of the model context, but they addition-
ally used a morphological analyzer on the grammar
books to extract linguistic features to assist in trans-
lation. Such tools are unfortunately unavailable for
all languages, making this approach not feasible
for scaling to thousands of languages.

\section{Preliminaries and Problem Definition}
A traditional neural MT system models $p_{\mathrm{MT}}(y|x)$,
learned over source-target sentence pairs $\langle x, y\rangle$. At
inference time, given a new source sentence, we
sample a high-probability output from the learned
distributions. A SOTA LLM, however, is first
pre-trained to model $p_{\mathrm{LM}}(x)$ and then instruction-
tuned on $p_{\mathrm{LM\text{-}ins}}(y|\pi)$ over prompt-target text pairs
$\langle \pi, y\rangle$ covering multiple downstream tasks (often
including MT). At inference time, with a similar
prompt we sample outputs from the final model.

A translation prompt $\pi(\cdot)$ at a minimum
needs to include the task definition $t$ (e.g.
"Please translate the following sentence
to French:") and the source sentence $x$: $\pi(x,t)$.

For learning to translate an entirely unseen
language, Tanzer et al. (2023) crafted prompts
$\pi(x,t,d,s,g)$ that additionally included:
\begin{itemize}
\item word-level translations $d$ obtained from a bilin-
gual dictionary $D$, selected for their similarity to
the words of the given source sentence,
\item a few parallel sentence examples $s$, selected from
a small collection of parallel sentences $S$ for their
similarity to the given source sentence, and
\item excerpts $g$ from a grammar book $G$, also selected
for similarity to the source sentence using longest
common substring distance.
\end{itemize}

\section{Experiments}
\subsection{Languages}
We focus on 16 largely under-served
low-resource languages, chosen for geographical
and typological diversity, as well as resource (dic-
tionary, grammars) and evaluation data availabil-
ity. Specifically, we work with: Chokwe, Chuvash,
Dinka, Dogri, Gitksan, Guarani, Ilokano, Kabuver-
dianu, Kachin, Kalamang, Kimbundu, Latgalian,
Minangkabau, Mizo, Natugu, and Wolof. We eval-
uate translation both into and out of English.

\subsection{Dictionaries}
We obtain dictionaries from Pan-
Lex\footnote{\url{[https://panlex.org}}](https://panlex.org}}) for all our languages. Note that, in cases
where the number of words in the dictionary was
less than 100 we do not include them in the prompt.
The size of each dictionary is included in Ap-
pendix B.

\subsection{Parallel Sentences}
For the parallel sentences
that are part of the prompts as translation exam-
ples, we use the dev portion of the FLORES-200
dataset.\footnote{\url{[https://github.com/openlanguagedata/flores}}](https://github.com/openlanguagedata/flores}})
Gitksan and Natugu are not represented
in FLORES and instead we use the data that Zhang
et al. (2024) provided.

\subsection{Grammar Books}
The DReaM corpus (Virk
et al., 2020) contains digitized versions of thou-
sands of linguistic documents, including grammar
books and sketches, for many languages. The
source of these documents is often in paper format,
and due to the scanning/OCR quality, the digitized
versions often contain scanning artifacts. We select
one grammar document for each of our languages
(concrete details in Appendix B). We perform slight
manual cleanup to remove some items (e.g., scan-
ning artifacts, table of contents) and to ensure that
the grammar would fit in the LLM’s context size.

\subsection{Evaluation}
We use the devtest portion of
FLORES-200 as our evaluation set. For Gitksan
and Natugu, we use the test sets from the SIG-
MORPHON 2023 shared task (Ginn et al., 2023).

\subsection{Model}
We use the GPT-4-turbo model for our experiments.
In addition to being the latest offering from Open-
AI (and presumably its most capable, at the time
of writing), it has an input context size of 128K.
This large context enables book-length text to be
included in the prompt. The grammar books we
use range from tens of pages to a couple hundred
pages in length, which equates to roughly 40K to
120K tokens. Models with such capacity have only
recently been made available, which affords us the
opportunity to use full-length grammar books as
opposed to smaller heuristically-selected excerpts.

\subsection{Prompt Format}
Our prompts largely follow
the MTOB framework, using complete prompts
$\pi(x,t,d,s,g)$ with task instructions and source
sentence (provided in the prompt beginning and
repeated at the end), as well as word pairs from the
dictionary, example sentences, and the language’s
grammar. We perform ablations removing compo-
nents from the prompt to establish their contribu-
tions, e.g. repeating all experiments without incor-
porating the grammar book, i.e. using $\pi(x,t,d,s)$.
We provide specific details as well as an example
prompt in Appendix C.

\section{Results}
Table~\ref{tab:chrf_main} shows the results for the experiments. We
report results on both translation directions, with
different prompt configurations as discussed above.
We report two comparison models: Baseline corre-
sponds to 0-shot LLM translation performance i.e.,
only with prompt $\pi(x,t)$, and the "skyline" perfor-
mance of NLLB, the current SOTA multilingual
MT model. We also report results by adding words
(W: $\pi(x,t,d)$), sentences (W+S: $\pi(x,t,d,s)$, and
grammars (W+S+G: $\pi(x,t,d,s,g)$) to the prompt.

For each language and direction, we have four
systems that we compare. We compute all eval-
uation metrics using SacreBLEU (Post, 2018)
and we also report statistical significance using
paired bootstrap resampling, comparing our best-
performing system to the other systems. In most
cases, we find that the difference is statistically sig-
nificant, indicating that the translation performance
is dependent on the selected prompt content.

\begin{table*}[t]
\centering
\small
\setlength{\tabcolsep}{3pt}
\begin{tabular}{lrrrrr rrrrr}
\toprule
& \multicolumn{5}{c}{English$\rightarrow$X} & \multicolumn{5}{c}{X$\rightarrow$English}\
\cmidrule(lr){2-6}\cmidrule(lr){7-11}
Language & Baseline & W & W+S & W+S+G & NLLB & Baseline & W & W+S & W+S+G & NLLB\
\midrule
\multicolumn{11}{l}{Languages supported by NLLB with some online presence}\
Chokwe & 12.3 & -- & \textbf{21.0}* & 16.9 & 24.3 & 22.8 & -- & \textbf{27.3}* & 25.8 & 30.8\
Dinka & 8.8 & -- & \textbf{16.3}* & 11.1 & 24.2 & 20.7 & -- & \textbf{25.0}* & 23.0 & 31.2\
Guarani & 29.4 & 20.6 & 29.1 & 29.0 & 36.9 & \textbf{43.4}* & 41.7 & 42.3 & 41.7 & 48.4\
Ilokano & 43.1 & 37.6 & \textbf{45.1}* & 43.8 & 53.3 & \textbf{53.9}* & 52.1 & 52.5 & 53.6 & 62.1\
Kabuverdianu & 39.0 & 29.8 & \textbf{55.9}* & 47.2 & 42.8 & \textbf{69.3}* & 66.9 & 68.3 & 68.4 & 68.4\
Kachin & 12.5 & -- & \textbf{27.7}* & 21.2 & 37.5 & 22.5 & -- & \textbf{25.2}* & 23.8 & 41.6\
Kimbundu & 11.6 & -- & \textbf{26.2}* & 14.4 & 24.9 & 19.3 & -- & 24.3 & \textbf{25.0}* & 33.9\
Latgalian & 26.0 & 21.0 & \textbf{37.6}* & 31.1 & 53.6 & 49.8 & 41.1 & 48.5 & 50.3 & 63.4\
Minangkabau & 42.0 & 28.1 & \textbf{47.0}* & 44.3 & 52.4 & \textbf{55.1}* & 43.9 & 51.9 & 54.0 & 62.5\
Mizo & 30.4 & 29.7 & \textbf{32.2} & 30.3 & 38 & \textbf{36.6}* & 35.0 & 35.6 & 36.2 & 41.4\
Wolof & 23.2 & 15.0 & 25.6 & \textbf{26.0} & 29.7 & \textbf{36.4}* & 29.6 & 31.3 & 35.8 & 41.2\
\midrule
\multicolumn{11}{l}{Languages not supported by NLLB with minimal online presence}\
Chuvash & 2.6 & 13.7 & \textbf{19.0}* & 16.0 & -- & 25.4 & 23.4 & 24.2 & \textbf{26.8}* & --\
Dogri & 5.9 & -- & \textbf{34.3}* & 24.9 & -- & 51.2 & -- & \textbf{52.4}* & 52.0 & --\
Gitksan & 7.8 & -- & 13.3 & \textbf{15.9}* & -- & 14.0 & -- & 24.4 & \textbf{24.6} & --\
Kalamang & 5.1 & 27.1 & \textbf{41.9}* & 37.3 & -- & 11.3 & 18.7 & 27.6 & \textbf{34.8}* & --\
Natugu & 6.8 & 4.5 & 12.0 & \textbf{17.0}* & -- & 13.2 & 6.8 & 9.9 & \textbf{23.7}* & --\
\midrule
System Average: & 19.2 & 22.7 & 30.3 & 26.7 & 38.0$^{\dagger}$ & 34.1 & 35.9 & 35.7 & 37.5 & 47.7$^{\dagger}$\
System Wins: & 1 & 0 & 12 & 3 & (9/11)$^{\dagger}$ & 6 & 0 & 4 & 6 & (10/11)$^{\dagger}$\
\bottomrule
\end{tabular}
\caption{Collective Table of Results (\texttt{chrF++} scores). The combination of reference material that led to the best score
is bolded. We also compare to NLLB, with the best score underlined. An asterisk (’*’) indicates that the difference
between our best system and the others is statistically significant. System wins counts the best combination of
reference material among our systems (NLLB excluded). $^{\dagger}$: NLLB only supports 11 of our languages.}
\label{tab:chrf_main}
\end{table*}

\subsection{Comparison to SOTA MT}
We compare the best results we achieved with the
\texttt{chrF++} scores from NLLB, for the languages sup-
ported by NLLB. Note that these are languages
with at least some online presence. In general the
NLLB scores were better, but there were a few in-
stances where our approach outperformed NLLB.
When going from English to a target language,
including words and sentences in the prompt for
Kabuverdianu and Kimbundu provided the best
results. For Kabuverdianu, including the gram-
mar book also surpassed the NLLB score. When
translating Kabuverdianu into English, the base-
line model (0-shot) with no reference material was
best. Kabuverdianu, as a Portuguese-based Cre-
ole, has many similarities to Portuguese, a high
resource language. This might explain this result
and it could be reflective of GPT-4’s capabilities.

\subsection{Sentences or Grammar Books?}
The results of our experiments show that the inclu-
sion of grammar books does not always lead to the
best score (see bottom rows of Table~\ref{tab:chrf_main}). In fact,
when translating from English, using only words
and sentences yields the highest score for 12 of the
languages. When translating into English, the com-
bination of words, sentences, and grammar books
had the highest score for six of the languages. How-
ever, including no reference material at all was the
best approach for six languages as well.

To explore the reasons behind these results, we
perform a linear regression that aims to predict
the score of the W+S+G combination given the
baseline score and the following features:
\begin{itemize}
\item Number of words in the reference dictionary
\item Number of sentences available in corpora as re-
ported in OPUS\footnote{\url{[https://opus.nlpl.eu/}}](https://opus.nlpl.eu/}})
\item Perplexity of the grammar book
\item Length of the grammar book in tokens
\end{itemize}

The features regarding words and sentences cor-
respond directly to data availability, with the as-
sumption that more data is better. The grammar
book features are proxies for the quality and the
completeness of the documented grammar. For
perplexity, we used a GPT-2 model and passed the
grammar book as input to the model. LM perplexity
is then measured using a sliding window strategy.

The $R^2$ values for these regressions are listed
in Table~\ref{tab:r2}. Put simply, the $R^2$ value denotes the
quality of the model fit, and can help us determine
the percentage of variance in the dependent variable
(downstream performance, in our case) that can be
explained by the independent variable.

We find that the number of dictionary words and
the length of the grammar books have a positive
influence on the score, while the perplexity has a
negative impact. While this aligns with our expec-
tations, a finding that is seemingly surprising is
that the number of available sentences has a nega-
tive impact on the score compared to the baseline.
This necessitates further research to actually con-
firm, but we suspect that this is because GPT-4 has
already been pre-trained on data from these lan-
guages and, consequently, it can perform better on
them. This is most pronounced when translating
into English, where the top 5 languages (by number
of sentences) all perform best under the baseline
setting i.e., no additional reference material. All
languages that are best translated using no refer-
ence material appear before all of the languages
that are best translated using the combination of dic-
tionaries, parallel sentences, and grammar books.
This suggests that using grammars might be best
suited to extremely low-resource languages with
less than $10^3$ parallel sentences.

\begin{figure}[t]
\centering
\fbox{\parbox{0.92\linewidth}{\centering \vspace{1.2em}\textbf{IMAGE NOT PROVIDED}\vspace{1.2em}}}
\caption{Using grammars is particularly beneficial
for extremely low-resource languages. Simple prompt-
based MT (zero-shot) is best for high-resource ones.}
\label{fig:resource_effect}
\end{figure}

\begin{table}[t]
\centering
\small
\begin{tabular}{lcccc}
\toprule
& \multicolumn{2}{c}{eng $\rightarrow$ X} & \multicolumn{2}{c}{X $\rightarrow$ eng}\
\cmidrule(lr){2-3}\cmidrule(lr){4-5}
Feature & Add. & Single & Add & Single\
\midrule
Baseline & 0.643 & 0.643 & 0.849 & 0.849\

* Words & 0.648 & 0.054 & 0.850 & 0.007\
* Sentences & 0.708 & 0.050 & 0.880 & 0.012\
* Perplexity & 0.751 & 0.177 & 0.925 & 0.141\
* Length & 0.755 & 0.062 & 0.927 & 0.115\
  \bottomrule
  \end{tabular}
  \caption{$R^2$ values for features explaining the W+S+G
  \texttt{chrF++} output. `Add.'': incorporating the feature with
  the ones above. `Single'': linear regression with only
  that feature as input.}
  \label{tab:r2}
  \end{table}

\section{Conclusion}
In this paper, we showed that utilizing reference
material such as dictionaries and grammar books
in the prompt of an LLM can improve the per-
formance of machine translation for low-resource
languages. We evaluated the performance on 16
languages and showed that the improvement is es-
pecially pronounced for languages that have min-
imal presence on the web. Our work shows that
this approach has the potential to address the gap
for extremely low-resource languages and identi-
fies a concrete path for improving MT for more
than 2,000 languages.

\section*{Limitations}
A primary contribution of this paper is the use of
full-length grammar books in the input prompt in
order to "teach" a model how to translate into a
given language. However, there are some limita-
tions with this approach. First, high quality gram-
mar books are difficult to obtain for many lan-
guages. The DReaM corpus does an admirable
job of curating and digitizing many linguistic refer-
ences, but the output is not perfect. Multi-column
text documents and tables lose information that is
conveyed by the location of text relative to other
text on the page. The LLMs, therefore, are most
likely not taking full advantage of that informa-
tion. Additionaly, scanning artifacts like headers
and page numbers add unnecessary clutter to the
reference material.

At the time of this writing, GPT-4-turbo was
the only available model with the desired context
length of 128K. Running the experiments using a
set of models would indicate whether the reference
material is improving translations or whether the
model itself (and its associated training) is respon-
sible for the performance.

The sizes of the bilingual dictionaries were in-
consistent, with a handful having less than 20
words. We removed these low-volume dictionaries
from our experiments. However, larger dictionaries
of similar magnitudes would most likely improve
the translations and would allow translation per-
formance across the various languages to be better
compared.

Finally, these experiments are not cheap. We
estimate that all these experiments cost around
$15,000 USD using the standard pricing tier under
the Azure Open AI Studio. This could significantly
hinder the reproducibility of our results.

\section*{Ethics Statement}
We do not anticipate any ethical issues arising from
our work.

\section*{Acknowledgements}
We are thankful to the reviewers and meta-reviewer
for their constructive feedback. This work was gen-
erously supported by the National Science Foun-
dation under grant IIS-2327143. It has also ben-
efited from resources provided through the Mi-
crosoft Accelerate Foundation Models Research
(AFMR) grant program. This work was partially
supported by resources provided by the Office of
Research Computing at George Mason University
(URL: \url{[https://orc.gmu.edu}](https://orc.gmu.edu})) and funded in part
by grants from the National Science Foundation
(Award Number 2018631).

\section*{References}
\begin{thebibliography}{99}

\bibitem{bapna2022}
Ankur Bapna, Isaac Caswell, Julia Kreutzer, Orhan Fi-
rat, Daan van Esch, Aditya Siddhant, Mengmeng Niu,
Pallavi Baljekar, Xavier Garcia, Wolfgang Macherey,
Theresa Breiner, Vera Axelrod, Jason Riesa, Yuan
Cao, Mia Xu Chen, Klaus Macherey, Maxim Krikun,
Pidong Wang, Alexander Gutkin, Apurva Shah, Yan-
ping Huang, Zhifeng Chen, Yonghui Wu, and Mac-
duff Hughes. 2022. Building machine translation
systems for the next thousand languages.

\bibitem{blasi2022}
Damian Blasi, Antonios Anastasopoulos, and Gra-
ham Neubig. 2022. Systematic inequalities in lan-
guage technology performance across the world’s
languages. In Proceedings of the 60th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), pages 5486–5505, Dublin,
Ireland. Association for Computational Linguistics.

\bibitem{ginn2023}
Michael Ginn, Sarah Moeller, Alexis Palmer, Anna
Stacey, Garrett Nicolai, Mans Hulden, and Miikka
Silfverberg. 2023. Findings of the SIGMORPHON
2023 shared task on interlinear glossing. In Pro-
ceedings of the 20th SIGMORPHON workshop on
Computational Research in Phonetics, Phonology,
and Morphology, pages 186–201, Toronto, Canada.
Association for Computational Linguistics.

\bibitem{joshi2020}
Pratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika
Bali, and Monojit Choudhury. 2020. The state and
fate of linguistic diversity and inclusion in the NLP
world. In Proceedings of the 58th Annual Meeting of
the Association for Computational Linguistics, pages
6282–6293, Online. Association for Computational
Linguistics.

\bibitem{kamholz2014}
David Kamholz, Jonathan Pool, and Susan Colowick.
2014. PanLex: Building a resource for panlingual
lexical translation. In Proceedings of the Ninth In-
ternational Conference on Language Resources and
Evaluation (LREC’14), pages 3145–3150, Reykjavik,
Iceland. European Language Resources Association
(ELRA).

\bibitem{nllb2022}
NLLB Team, Marta R. Costa-juss`a, James Cross, Onur
\c{C}elebi, Maha Elbayad, Kenneth Heafield, Kevin Hef-
fernan, Elahe Kalbassi, Janice Lam, Daniel Licht,
Jean Maillard, Anna Sun, Skyler Wang, Guillaume
Wenzek, Al Youngblood, Bapi Akula, Loic Bar-
rault, Gabriel Mejia-Gonzalez, Prangthip Hansanti,
John Hoffman, Semarley Jarrett, Kaushik Ram
Sadagopan, Dirk Rowe, Shannon Spruit, Chau
Tran, Pierre Andrews, Necip Fazil Ayan, Shruti
Bhosale, Sergey Edunov, Angela Fan, Cynthia
Gao, Vedanuj Goswami, Francisco Guzm'an, Philipp
Koehn, Alexandre Mourachko, Christophe Ropers,
Safiyyah Saleem, Holger Schwenk, and Jeff Wang.
2022. No language left behind: Scaling human-
centered machine translation.

\bibitem{popovic2017}
Maja Popovi'c. 2017. chrF++: words helping charac-
ter n-grams. In Proceedings of the Second Confer-
ence on Machine Translation, pages 612–618, Copen-
hagen, Denmark. Association for Computational Lin-
guistics.

\bibitem{post2018}
Matt Post. 2018. A call for clarity in reporting BLEU
scores. In Proceedings of the Third Conference on
Machine Translation: Research Papers, pages 186–
191, Belgium, Brussels. Association for Computa-
tional Linguistics.

\bibitem{tanzer2023}
Garrett Tanzer, Mirac Suzgun, Eline Visser, Dan Juraf-
sky, and Luke Melas-Kyriazi. 2023. A benchmark
for learning to translate a new language from one
grammar book. In Arxiv.

\bibitem{tiedemann2009}
J"org Tiedemann. 2009. News from OPUS - A Collec-
tion of Multilingual Parallel Corpora with Tools and
Interfaces, volume V, pages 237–248.

\bibitem{virk2020}
Shafqat Mumtaz Virk, Harald Hammarstr"om, Markus
Forsberg, and S\o ren Wichmann. 2020. The DReaM
corpus: A multilingual annotated corpus of gram-
mars for the world’s languages. In Proceedings of
the Twelfth Language Resources and Evaluation Con-
ference, pages 878–884, Marseille, France. European
Language Resources Association.

\bibitem{zhang2024}
Kexun Zhang, Yee Man Choi, Zhenqiao Song, Taiqi
He, William Yang Wang, and Lei Li. 2024. Hire a
linguist!: Learning endangered languages with in-
context linguistic descriptions.

\end{thebibliography}

\appendix

\section{Additional Experimental Results}
Table~\ref{tab:best_combo} shows the best performing system for each
language and direction, sorted in descending order
by number of available sentences as reported by
OPUS.

Table~\ref{tab:paired_eng_to_x} and Table~\ref{tab:paired_x_to_eng} show the results from our
paired significance tests. The best performing sys-
tem for a given language and direction is compared
to each of the other systems, with statistically sig-
nificant differences indicated with an asterisk.

The main paper uses \texttt{chrF++} scores to evaluate
translations, which is the metric used by NLLB.
We also calculate BLEU scores for all of our exper-
iments, which are provided in Table~\ref{tab:bleu_all}.

\begin{table}[h]
\centering
\small
\begin{tabular}{l r l l}
\toprule
Language & # Sentences & eng $\rightarrow$ X & X $\rightarrow$ eng\
\midrule
mizo & 6979898 & WS & B\
guarani & 2959865 & B & B\
wolof & 1572603 & WSG & B\
ilokano & 1458586 & WS & B\
kabuverdianu & 1229409 & WS & B\
kachin & 1003100 & WS & WS\
minangkabau & 303354 & WS & B\
chokwe & 214973 & WS & WS\
chuvash & 200001 & WS & WSG\
kimbundu & 196240 & WS & WSG\
dinka & 172589 & WS & WS\
latgalian & 131709 & WS & WSG\
dogri & 0 & WS & WS\
gitksan & 0 & WSG & WSG\
kalamang & 0 & WS & WSG\
natugu & 0 & WSG & WSG\
\bottomrule
\end{tabular}
\caption{Combination of reference material that
led to the best score for each language, where
B=baseline, W=words, WS=Words and Sentences, and
WSG=Words, Sentences, and Grammar Book. Number
of sentences is the total number of sentences as reported
by OPUS.}
\label{tab:best_combo}
\end{table}

\begin{longtable}{l l l l}
\caption{Paired bootstrap resampling for \texttt{chrF++} scores for English-to-X translations. The best performing system is selected as the baseline and is compared to the other systems,
using a 95% confidence interval and a p-value of 0.05. Statistically significant differences are noted with an asterisk (*).}
\label{tab:paired_eng_to_x}\
\toprule
Language & Baseline & W & W+S \
\midrule
\endfirsthead
\toprule
Language & Baseline & W & W+S \
\midrule
\endhead
Chokwe & 12.7 (12.7 +/- 0.2) p = 0.0010* & NA & 21.9 (21.9 +/- 2.0) \
Chuvash & 4.2 (4.1 +/- 1.1) p = 0.0010* & 13.5 (13.6 +/- 1.8) p = 0.0010* & 19.2 (19.2 +/- 1.9) \
Dinka & 8.8 (8.8 +/- 0.2) p = 0.0010* & NA & 17.5 (17.6 +/- 2.5) \
Dogri & 7.7 (7.7 +/- 2.6) p = 0.0010* & NA & 34.2 (34.2 +/- 2.8) \
Gitksan & 8.5 (8.5 +/- 0.7) p = 0.0010* & NA & 14.1 (14.2 +/- 2.2) p = 0.0010* \
Guarani & 30.1 (30.1 +/- 0.7) & 20.9 (20.9 +/- 0.8) p = 0.0010* & 30.0 (30.0 +/- 0.7) p = 0.3207 \
Ilokano & 43.9 (43.9 +/- 0.6) p = 0.0010* & 38.4 (38.4 +/- 0.6) p = 0.0010* & 45.9 (45.9 +/- 1.3) \
Kabuverdianu & 40.4 (40.4 +/- 1.1) p = 0.0010* & 30.1 (30.0 +/- 1.5) p = 0.0010* & 56.4 (56.3 +/- 1.2) \
Kachin & 12.6 (12.6 +/- 0.7) p = 0.0010* & NA & 27.7 (27.8 +/- 3.8) \
Kalamang & 5.8 (5.8 +/- 0.5) p = 0.0010* & 29.2 (29.1 +/- 4.1) p = 0.0010* & 42.9 (43.0 +/- 4.8) \
Kimbundu & 11.9 (11.9 +/- 0.1) p = 0.0010* & NA & 26.8 (26.9 +/- 2.0) \
Latgalian & 28.4 (28.4 +/- 0.8) p = 0.0010* & 22.4 (22.5 +/- 1.1) p = 0.0010* & 39.7 (39.7 +/- 1.0) \
Minangkabau & 42.8 (42.8 +/- 0.9) p = 0.0010* & 29.0 (29.0 +/- 1.7) p = 0.0010* & 47.8 (47.8 +/- 1.3) \
Mizo & 32.5 (32.5 +/- 0.8) p = 0.1678 & 29.7 (29.7 +/- 0.9) p = 0.1399 & 31.2 (31.3 +/- 3.3) \
Natugu & 7.1 (7.1 +/- 0.5) p = 0.0010* & 4.8 (4.9 +/- 0.9) p = 0.0010* & 13.0 (13.2 +/- 3.2) p = 0.0010* \
Wolof & 24.0 (24.0 +/- 0.8) p = 0.0010* & 15.5 (15.5 +/- 0.7) p = 0.0010* & 26.3 (26.3 +/- 1.4) p = 0.1069 \
\bottomrule
\end{longtable}

\begin{longtable}{l l l l}
\caption{Paired bootstrap resampling for \texttt{chrF++} scores for X-to-English translations. The best performing system is selected as the baseline and is compared to the other systems,
using a 95% confidence interval and a p-value of 0.05. Statistically significant differences are noted with an asterisk (*).}
\label{tab:paired_x_to_eng}\
\toprule
Language & Baseline & W & W+S \
\midrule
\endfirsthead
\toprule
Language & Baseline & W & W+S \
\midrule
\endhead
Chokwe & 23.5 (23.5 +/- 0.9) p = 0.0010* & NA & 28.1 (28.1 +/- 0.7) \
Chuvash & 24.0 (24.0 +/- 2.5) p = 0.0140* & 21.8 (21.9 +/- 1.9) p = 0.0010* & 22.9 (22.9 +/- 2.1) p = 0.0010* \
Dinka & 21.6 (21.6 +/- 1.2) p = 0.0010* & NA & 25.5 (25.5 +/- 1.3) \
Dogri & 52.1 (52.1 +/- 2.4) p = 0.0010* & NA & 53.9 (53.8 +/- 2.3) \
Gitksan & 14.3 (14.4 +/- 1.6) p = 0.0010* & NA & 25.2 (25.3 +/- 2.5) p = 0.2717 \
Guarani & 43.9 (44.0 +/- 1.1) & 42.2 (42.3 +/- 1.1) p = 0.0010* & 42.8 (42.9 +/- 1.1) p = 0.0020* \
Ilokano & 53.5 (53.5 +/- 1.6) & 51.8 (51.8 +/- 1.6) p = 0.0010* & 52.7 (52.7 +/- 1.8) p = 0.0420* \
Kabuverdianu & 69.5 (69.5 +/- 0.8) & 67.1 (67.1 +/- 1.0) p = 0.0010* & 68.5 (68.5 +/- 0.9) p = 0.0020* \
Kachin & 22.7 (22.7 +/- 0.9) p = 0.0010* & NA & 25.6 (25.6 +/- 0.8) \
Kalamang & 11.5 (11.5 +/- 1.2) p = 0.0010* & 20.1 (20.3 +/- 5.4) p = 0.0010* & 28.4 (28.8 +/- 7.6) p = 0.0100* \
Kimbundu & 18.6 (18.6 +/- 0.5) p = 0.0010* & NA & 23.8 (23.8 +/- 0.9) p = 0.0010* \
Latgalian & 49.9 (49.9 +/- 1.2) p = 0.0959 & 40.2 (40.2 +/- 1.6) p = 0.0010* & 48.5 (48.5 +/- 1.3) p = 0.0010* \
Minangkabau & 55.8 (55.8 +/- 1.0) & 44.3 (44.3 +/- 1.4) p = 0.0010* & 52.5 (52.5 +/- 1.3) p = 0.0010* \
Mizo & 37.0 (37.0 +/- 1.1) & 35.1 (35.2 +/- 1.2) p = 0.0010* & 35.6 (35.6 +/- 1.2) p = 0.0010* \
Natugu & 13.1 (13.1 +/- 0.8) p = 0.0010* & 6.4 (6.4 +/- 0.7) p = 0.0010* & 9.6 (9.6 +/- 1.2) p = 0.0010* \
Wolof & 37.3 (37.3 +/- 0.9) & 30.2 (30.1 +/- 0.9) p = 0.0010* & 31.9 (31.9 +/- 1.0) p = 0.0010* \
\bottomrule
\end{longtable}

\begin{table*}[t]
\centering
\small
\setlength{\tabcolsep}{4pt}
\begin{tabular}{lrrrr rrrr}
\toprule
& \multicolumn{4}{c}{English$\rightarrow$X} & \multicolumn{4}{c}{X$\rightarrow$English}\
\cmidrule(lr){2-5}\cmidrule(lr){6-9}
Language & Baseline & W & W+S & W+S+G & Baseline & W & W+S & W+S+G\
\midrule
Chokwe & 0.0 & NA & 1.9 & 1.2 & 6.4 & NA & 6.2 & 5.5\
Chuvash & 0.3 & 0.5 & 1.6 & 0.7 & 4.3 & 1.3 & 1.8 & 3.3\
Dinka & 0.0 & NA & 1.5 & 0.7 & 3.5 & NA & 5.7 & 6.3\
Dogri & 0.5 & NA & 10.6 & 3.2 & 23.2 & NA & 24.7 & 22.6\
Gitksan & 0.0 & NA & 0.2 & 1.0 & 0.2 & NA & 2.5 & 5.3\
Guarani & 5.1 & 1.7 & 5.3 & \textbf{5.6} & 17.9 & 15.5 & 16.3 & 16.8\
Ilokano & 14.6 & 10.8 & \textbf{16.1} & 15.1 & 28.2 & 25.5 & 26.1 & 27.0\
Kabuverdianu & 11.0 & 3.9 & \textbf{27.8} & 18.1 & 46.5 & 41.3 & 44.0 & 45.4\
Kachin & 0.3 & NA & 3.0 & 1.9 & 2.9 & NA & 3.3 & 2.5\
Kalamang & 0.0 & 7.5 & \textbf{13.2} & 12.2 & 0.2 & 2.0 & 4.4 & \textbf{13.9}\
Kimbundu & 0.1 & NA & 4.1 & 1.0 & 0.9 & NA & 3.0 & 5.0\
Latgalian & 3.7 & 1.5 & \textbf{10.5} & 6.0 & 21.8 & 9.7 & 17.8 & \textbf{22.8}\
Minangkabau & 13.0 & 3.7 & \textbf{17.2} & 15.8 & 30.0 & 12.9 & 23.2 & \textbf{28.3}\
Mizo & \textbf{7.6} & 6.0 & 5.4 & 6.2 & \textbf{10.9} & 8.5 & 8.9 & 10.0\
Natugu & 0.0 & 0.0 & 0.7 & \textbf{2.5} & 0.1 & 0.0 & 0.4 & \textbf{5.9}\
Wolof & 3.5 & 1.1 & 4.5 & \textbf{5.7} & 12.9 & 5.3 & 6.3 & 11.4\
\bottomrule
\end{tabular}
\caption{Collective Table of Results. BLEU scores are shown for all systems. For each of our scores, the combination
of reference material that led to the best score is bolded.}
\label{tab:bleu_all}
\end{table*}

\section{Resources}
For our experiments, we gathered dictionaries, par-
allel sentences, and grammar books to use in the
prompts. Dictionaries were obtained from PanLex
(Kamholz et al., 2014) and converted into the for-
mat required by the code. The dictionary used in
MTOB included part of speech tags for each word,
which is unavailable in PanLex. Therefore, we did
not include this feature in our dictionaries. The
sizes of the dictionaries are shown in Table~\ref{tab:sent_dict}. Kala-
mang is not available in PanLex, and we instead
used the version from the MTOB paper.

For sentences, we used the FLORES dataset,
originally released by Meta as FLORES-200 and
now maintained by the Open Language Data Initia-
tive (OLDI) as FLORES+. For each language in
the dataset, the dev split has 997 sentences and the
devtest split has 1012 sentences. We used dev sen-
tences as sample sentences in the prompts, while
devtest sentences are used as translation tasks for
our system on which performance was measured.
For Dogri and Chuvash only the dev split is avail-
able. We therefore randomly split the dev split
into dev and devtest with 497 and 500 sentences,
respectively. Gitksan and Natugu are not repre-
sented in FLORES and we obtain sentences from
the SIGMORPHON 2023 Shared Task on Interlin-
ear Glossing,\footnote{\url{[https://github.com/sigmorphon/2023glossingST}}](https://github.com/sigmorphon/2023glossingST}})
which has dev, train, and test splits.
These were combined to form dev and devtest splits.
For Kalamang, the train and test splits as provided
in the original paper were used unaltered. Table~\ref{tab:sent_dict}
lists the sizes of the train and test splits for each of
the languages.

Grammar books were obtained from the DReaM
corpus, which contains digitized versions of nu-
merous linguistic reference materials. When select-
ing the specific grammar book or sketch to use for
each language, we searched for documents that pro-
vided a well-rounded description, appeared to have
been well-processed by optical character recogni-
tion, and would fit within the context of GPT-4. For
each document we performed limited formatting,
such as removing the table of contents, in order
to reduce the token count. Table~\ref{tab:grammar_books} lists the source
documents used for the grammar books as well as
the number of tokens for each document. Perplex-
ity was measured using a GPT-2 model in order to
provide a coarse assessment of the quality of the
document. For Kalamang, we used the grammar
book provided in MTOB. Specifically, we use the
"long" version, which is a manually curated subset
of Visser’s grammar, that they tested on a Claude 2
model.

The authors of MTOB and the maintainers of
FLORES+ explicitly request that this reference
data, and the parallel sentences in particular, are not
publicly hosted as plain text. This is to ensure that
the resources are not web-scraped where they could
potentially be included in the training data of future
models, which would taint results of MT tests. In
accordance with their requests, and with the same
spirit in mind, we have password encrypted all ref-
erence material that we have posted and request
that any users of our data do the same.

\begin{longtable}{p{0.18\linewidth}p{0.62\linewidth}r r}
\caption{Grammar Books and Size}
\label{tab:grammar_books}\
\toprule
Language & Grammar Book & Number of Tokens & Perplexity\
\midrule
\endfirsthead
\toprule
Language & Grammar Book & Number of Tokens & Perplexity\
\midrule
\endhead
Chokwe & Martins, João Vicente. (1990) Elementos de Gramática de Utchokwe. Lisboa: Instituto de Investigação Científica Tropical. & 114483 & 23.61\
Chuvash & Krueger, John R. (1961) Chuvash Manual: Introduction, Grammar, Reader, and Vocabulary (Indiana University Publications: Uralic and Altaic Series 7). Bloomington: Indiana University. & 118294 & 85.73\
Dinka & Nebel, Arturo. (1948) Dinka Grammar (Rek-Malual Dialect) with Texts and Vocabulary. Verona: Istituto Missioni Africane. & 120420 & 55.57\
Dogri & Gupta, Veena. (2014) Dogri. In Omkar N. Koul (ed.), The Languages of Jammu and Kashmir (People’s Linguistic Survey of India XII), 3--68. New Delhi: Orient Blackswan. & 53993 & 22.38\
Gitksan & Hunt, Katharine Dorothy. (1993) Clause Structure, Agreement and Case in Gitksan. University of British Columbia doctoral dissertation. & 106310 & 23.22\
Guarani & Gregores, Emma and Jorge A. Suárez. (1967) A Description of Colloquial Guaraní (Janua Linguarum: Series Practica 27). Berlin: Mouton de Gruyter. & 76725 & 19.86\
ilokano & Espiritu, Precy. (1984) Let’s speak Ilokano. Honolulu: University of Hawaii Press. & 83025 & 26.06\
Kabuverdianu & Baptista, Marlyse. (1997) The Morpho-Syntax of Nominal and Verbal Categories in Capeverdean Creole. Harvard University doctoral dissertation. & 104185 & 17.08\
Kachin & Hertz, Henry Felix. (1902) A practical handbook of the Kachin or Chingpaw language: containing the grammatical principles and peculiarities of the language, colloquial exercises, and a vocabulary, with an appendix on Kachin customs, laws, and religion. Rangoon: Superintendent of Government Printing, Burma. & 110639 & 33.81\
Kalamang & Eline Visser. A grammar of Kalamang. Number 4 in Comprehensive Grammar Library. Language Science Press, Berlin, 2022. & 92009 & 25.72\
Kimbundu & Pedro, José. (1993) Étude grammaticale du kimbundu (Angola). Université de Paris V - René Descartes doctoral dissertation. & 119545 & 20.95\
Latgalian & Nau, Nicole. (2011) A short grammar of Latgalian (Languages of the World/Materials 482). München: Lincom. & 80567 & 30.71\
Minangkabau & Crouch, Sophie. (2009) Voice and verb morphology in Minangkabau, a language of West Sumatra, Indonesia. University of Western Australia MA thesis. & 110746 & 16.05\
Mizo & Chhangte, Lalnunthangi. (1993) Mizo Syntax. Eugene: University of Oregon doctoral dissertation. & 85609 & 30.96\
Natugu & Boerger, Brenda H. (2022) A Grammar Sketch of Natqgu [ntu]: An Oceanic language of Santa Cruz, Solomon Islands (Texts in the Indigenous Languages of the Pacific 4). Port Moresby: LSPNG. & 80401 & 21.37\
Wolof & Ngom, Fallou. (2003) Wolof (Languages of the World/Materials 333). München: Lincom. & 42898 & 11.60\
\bottomrule
\end{longtable}

\begin{table*}[t]
\centering
\small
\begin{tabular}{l l l r r r r}
\toprule
Language & ISO 639-3 & Source & Train & Test & eng $\rightarrow$ X & X $\rightarrow$ eng\
\midrule
Chokwe & cjk & FLORES & 997 & 1012 & 35 & 40\
Chuvash & chv & FLORES & 497 & 500 & 3941 & 3611\
Dinka & dik & FLORES & 997 & 1012 & 10 & 10\
Dogri & dgo & FLORES & 497 & 500 & 19 & 20\
Gitksan & git & SIGMORPHON 2023 ST & 42 & 68 & 17 & 16\
Guarani & gug & FLORES & 997 & 1012 & 3641 & 3531\
Ilokano & ilo & FLORES & 997 & 1012 & 5479 & 4779\
Kabuverdianu & kea & FLORES & 997 & 1012 & 1413 & 1320\
Kachin & kac & FLORES & 997 & 1012 & 92 & 105\
Kalamang & kgv & MTOB & 376 & 50 & 1932 & 2531\
Kimbundu & kmb & FLORES & 997 & 1012 & 67 & 61\
Latgalian & ltg & FLORES & 997 & 1012 & 925 & 710\
Minangkabau & min & FLORES & 997 & 1012 & 348 & 349\
Mizo & lus & FLORES & 997 & 1012 & 16717 & 14981\
Natugu & ntu & SIGMORPHON 2023 ST & 890 & 99 & 351 & 382\
Wolof & wol & FLORES & 997 & 1012 & 2397 & 2850\
\bottomrule
\end{tabular}
\caption{Number of sentences and number of words in the dictionaries}
\label{tab:sent_dict}
\end{table*}

\section{Prompt Format}
Each sentence to be translated is formatted into a prompt for GPT-4. The prompt has five components:
prefix, words, sentences, grammar book, and suffix. The experiment configuration determines whether
words (W), sentences (S), or grammar books (G) are included in the prompt. The prefix and suffix
are always included in the prompt. In the following sections, we show the format of the prompt by
example, using an Ilokano-to-English translation task. We heavily used the code provided by the authors
of "Machine Translation from One Book" to generate the prompts.

\subsection{Prefix}
The prefix provides the task to perform (translation), the source and target languages, and the sentence to
translate.
\begin{quote}
You are an expert translator. Translate the following sentence from Ilokano to English: Adu pay ti
babbabassit a klase ti pusa ngem kadakuada a mangmangan iti babbabassit a klase ti ayup a kas iti
kuneho, antelope, ken ugsa.
\end{quote}

\subsection{Words}
For words, we attempt to retrieve the item from the bilingual dictionary. For each word in the source
sentence, the top two matching words from the dictionary, as measured by LCS, are included in the
prompt.
\begin{quote}
To help with the translation, here is one of the closest entries to Adu in the bilingual dictionary:\
Ilokano word: Adams\
English translation: Adams

To help with the translation, here is one of the closest entries to Adu in the bilingual dictionary:\
Ilokano word: adu\
English translation: many; lots of; majority; many; much

To help with the translation, here is one of the closest entries to pay in the bilingual dictionary:\
Ilokano word: payso\
English translation: correct; right

To help with the translation, here is one of the closest entries to pay in the bilingual dictionary:\
Ilokano word: pay\
English translation: just; please; again; still; yet; also

Additional word-level translations are provided for the remaining words of the source sentence.
\end{quote}

\subsection{Sentences}
For sentences, we attempt to retrieve similar samples from our small corpus of parallel sentences. For
each word in the source sentence, we find sentences that contain that word, as measured by LCS, and
include the top two matches in the prompt.
\begin{quote}
To help with the translation, here is a translated sentence with words similar to "Adu" in a list of
translated reference sentences:\
Ilokano sentence: Adu dagti restaurant iti aglawlaw ti hardin, ket no iti malem ken rabii masansan
nga adda dagiti libre a konsiero iti akintengnga a gazebo.\
English translation: There are a number of restaurants surrounding the garden, and in the
afternoons and evening there free concerts are often given from the central gazebo.

To help with the translation, here is a translated sentence with words similar to "Adu" in a list of
translated reference sentences:\
Ilokano sentence: Adu a gobierno ti mangsapul ti bakuna para iti nadumaduma a sakit para
kadagiti sangaili a sumrek, wenno dagiti residente a rumuar iti pagilianda.\
English translation: Many governments require visitors entering, or residents leaving, their
countries to be vaccinated for a range of diseases.

Additional sentence-level translations are provided for the remaining words of the source sentence.
\end{quote}

\subsection{Grammar Book}
We include the full grammar book in the prompt.
\begin{quote}
To help with the translation, here is the full text of a bilingual grammar book:\
—\
## FULL BOOK INSERTED HERE ##\
This is the end of the bilingual grammar book.\
—\
\end{quote}

\subsection{Suffix}
The suffix reiterates the task and prompts for the appropriate translation.
\begin{quote}
Now write the translation.\
Ilokano: Adu pay ti babbabassit a klase ti pusa ngem kadakuada a mangmangan iti babbabassit a
klase ti ayup a kas iti kuneho, antelope, ken ugsa.\
English translation:
\end{quote}

\end{document}
=====END FILE=====
