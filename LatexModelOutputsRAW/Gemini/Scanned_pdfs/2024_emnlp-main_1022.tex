=====FILE: main.tex=====
\documentclass[10pt, a4paper, twocolumn]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\geometry{left=2cm, right=2cm, top=2.5cm, bottom=2.5cm}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{natbib}
\usepackage{titlesec}
\usepackage{authblk}

% Section formatting to mimic standard NLP conference style
\titleformat{\section}{\large\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}{\normalsize\bfseries}{\thesubsection}{1em}{}

\title{\textbf{GuardBench: A Large-Scale Benchmark for Guardrail Models}}

\author[1]{Elias Bassani}
\author[1]{Ignacio Sanchez}
\affil[1]{European Commission, Joint Research Centre, Ispra, Italy \protect\ \texttt{{elias.bassani, ignacio.sanchez}@ec.europa.eu}}

\date{}

\begin{document}

\maketitle

\begin{abstract}
Generative AI systems powered by Large Language Models have become increasingly popular in recent years. Lately, due to the risk of providing users with unsafe information, the adoption of those systems in safety-critical domains has raised significant concerns. To respond to this situation, input-output filters, commonly called guardrail models, have been proposed to complement other measures, such as model alignment. Unfortunately, the lack of a standard benchmark for guardrail models poses significant evaluation issues and makes it hard to compare results across scientific publications. To fill this gap, we introduce GuardBench, a large-scale benchmark for guardrail models comprising 40 safety evaluation datasets. To facilitate the adoption of GuardBench, we release a Python library providing an automated evaluation pipeline built on top of it. With our benchmark, we also share the first large-scale prompt moderation datasets in German, French, Italian, and Spanish. To assess the current state-of-the-art, we conduct an extensive comparison of recent guardrail models and show that a general-purpose instruction-following model of comparable size achieves competitive performance.
\end{abstract}

\section{Introduction}

Generative AI systems powered by Large Language Models (LLMs) have become increasingly popular in recent years. Lately, due to the risk of providing users with unsafe information, the adoption of those systems in safety-critical domains has raised significant concerns. To respond to this situation, input-output filters, commonly called guardrail models, have been proposed to complement other measures, such as model alignment. Unfortunately, the lack of a standard benchmark for guardrail models poses significant evaluation issues and makes it hard to compare results across scientific publications.

To fill this gap, we introduce GuardBench, a large-scale benchmark for guardrail models comprising 40 safety evaluation datasets. To facilitate the adoption of GuardBench, we release a Python library providing an automated evaluation pipeline built on top of it. With our benchmark, we also share the first large-scale prompt moderation datasets in German, French, Italian, and Spanish. To assess the current state-of-the-art, we conduct an extensive comparison of recent guardrail models and show that a general-purpose instruction-following model of comparable size achieves competitive performance.

[MISSING CONTENT]

\section{Related Work}
[MISSING CONTENT]

\section{GuardBench}
[MISSING CONTENT]

\section{Experiments}
[MISSING CONTENT]

\section{Results}
[MISSING CONTENT]

\section{Conclusion}
[MISSING CONTENT]

\section*{Ethics Statement}
[MISSING CONTENT]

\section*{Limitations}
[MISSING CONTENT]

% Bibliography
\bibliographystyle{plainnat}
\bibliography{refs}

\appendix

\section{Prompts}
[MISSING CONTENT]
% Snippet indicates repeated "Prompts" text in the appendix

\end{document}
=====END FILE=====

=====FILE: refs.bib=====
@article{souly2024,
author = {Souly et al.},
title = {[MISSING TITLE]},
year = {2024}
}

@article{mazeika2023,
author = {Mazeika et al.},
title = {[MISSING TITLE]},
year = {2023}
}

@article{bhadwaj2024,
author = {Bhadwaj et al.},
title = {[MISSING TITLE]},
year = {2024}
}

@article{shen2023,
author = {Shen et al.},
title = {[MISSING TITLE]},
year = {2023}
}

@article{wang2024,
author = {Wang et al.},
title = {[MISSING TITLE]},
year = {2024}
}

@article{shaikh2023,
author = {Shaikh et al.},
title = {[MISSING TITLE]},
year = {2023}
}

@article{bhadwaj2023,
author = {Bhadwaj and Poria},
title = {[MISSING TITLE]},
year = {2023}
}

@article{qi2023,
author = {Qi et al.},
title = {[MISSING TITLE]},
year = {2023}
}

@article{röttger2023,
author = {Röttger et al.},
title = {[MISSING TITLE]},
year = {2023}
}

@article{zou2023,
author = {Zou et al.},
title = {[MISSING TITLE]},
year = {2023}
}

@article{wang2023a,
author = {Wang et al.},
title = {[MISSING TITLE]},
year = {2023a}
}

@article{vidgen2021,
author = {Vidgen et al.},
title = {[MISSING TITLE]},
year = {2021}
}

@article{röttger2021,
author = {Röttger et al.},
title = {[MISSING TITLE]},
year = {2021}
}

@article{kirk2022,
author = {Kirk et al.},
title = {[MISSING TITLE]},
year = {2022}
}

@article{levy2022,
author = {Levy et al.},
title = {[MISSING TITLE]},
year = {2022}
}

@article{hartvigsen2022,
author = {Hartvigsen et al.},
title = {[MISSING TITLE]},
year = {2022}
}

@article{radhuapu2023,
author = {Radhuapu et al.},
title = {[MISSING TITLE]},
year = {2023}
}

@article{markov2023,
author = {Markov et al.},
title = {[MISSING TITLE]},
year = {2023}
}

@article{vidgen2023,
author = {Vidgen et al.},
title = {[MISSING TITLE]},
year = {2023}
}

@article{lin2023,
author = {Lin et al.},
title = {[MISSING TITLE]},
year = {2023}
}

@article{ji2023,
author = {Ji et al.},
title = {[MISSING TITLE]},
year = {2023}
}

@article{xu2021,
author = {Xu et al.},
title = {[MISSING TITLE]},
year = {2021}
}

@article{curry2021,
author = {Curry et al.},
title = {[MISSING TITLE]},
year = {2021}
}

@article{aroyo2023,
author = {Aroyo et al.},
title = {[MISSING TITLE]},
year = {2023}
}

@article{kim2022,
author = {Kim et al.},
title = {[MISSING TITLE]},
year = {2022}
}
=====END FILE=====