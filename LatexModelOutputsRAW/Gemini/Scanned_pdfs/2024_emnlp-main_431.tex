=====FILE: main.tex=====
\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{natbib}

\geometry{a4paper, margin=1in}

\title{GENRA: Enhancing Zero-shot Retrieval with Rank Aggregation}
\author{Georgios Katsimpras and Georgios Paliouras \
NCSR Demokritos, Athens, Greece \
\texttt{{gkatsibras, paliourg}@iit.demokritos.gr}}
\date{}

\begin{document}

\maketitle

\input{sections/00_abstract}
\input{sections/01_introduction}
\input{sections/02_related_work}
\input{sections/03_preliminaries}
\input{sections/04_methodology}
\input{sections/05_results}
\input{sections/06_conclusion}

\bibliographystyle{acl_natbib}
\bibliography{refs}

\appendix
\input{sections/07_appendix}

\end{document}
=====END FILE=====

=====FILE: sections/00_abstract.tex=====
\begin{abstract}
Large Language Models (LLMs) have been shown to effectively perform zero-shot document retrieval, a process that typically consists of two steps: i) retrieving relevant documents, and ii) re-ranking them based on their relevance to the query. This paper presents GENRA, a new approach to zero-shot document retrieval that incorporates rank aggregation to improve retrieval effectiveness. Given a query, GENRA first utilizes LLMs to generate informative passages that capture the query's intent. These passages are then employed to guide the retrieval process, selecting similar documents from the corpus. Next, we use LLMs again for a second refinement step. This step can be configured for either direct relevance assessment of each retrieved document or for re-ranking the retrieved documents. Ultimately, both approaches ensure that only the most relevant documents are kept. Upon this filtered set of documents, we perform multi-document retrieval, generating individual rankings for each document. As a final step, GENRA leverages rank aggregation, combining the individual rankings to produce a single refined ranking. Extensive experiments on benchmark datasets demonstrate that GENRA improves existing approaches, highlighting the effectiveness of the proposed methodology in zero-shot retrieval.
\end{abstract}
=====END FILE=====

=====FILE: sections/01_introduction.tex=====
\section{Introduction}

Recent studies in zero-shot retrieval have demonstrated remarkable advancements, significantly improving the effectiveness of retrievers with the use of encoders like BERT \citep{devlin2018bert} and Contriever \citep{izacard2022contriever}. With the emergence of Large Language Models (LLMs) \citep{brown2020language, scao2022bloom, touvron2023llama}, research focused on how to leverage LLMs for information retrieval tasks, such as zero-shot retrieval.

Early zero-shot ranking with LLMs relied on methods that score each query-document pair and select the top-scoring pairs \citep{liang2022holistic}. Researchers have attempted to boost these methods by enriching contextual information to help LLMs understand the relationships between queries and documents. This often involves using LLMs to generate additional queries, passages, or other relevant content \citep{mackie2023generative, li2023large}. These enhancements have significantly improved retrieval performance, especially for unseen (zero-shot) queries.

In a typical retrieval setting, as shown in Figure \ref{fig:genra_overview}a, queries and documents are embedded in a shared representation space to enable efficient search. The success of the entire approach depends strongly on the quality of the results of the retrieval step. However, LLMs can generate potentially non-factual or nonsensical content (e.g. ``hallucinations''), and their performance is susceptible to factors like prompt order and input length, which can hurt the performance of the retriever \citep{yu2022generate}.

To address this problem, some studies \citep{liang2022holistic, thomas2023large} propose employing LLMs as relevance assessors, providing individual relevance judgments for each query-document pair. These approaches aim to enhance trustworthiness by leveraging the LLM's strengths in understanding nuances and identifying potentially irrelevant content. Additionally, recent work \citep{sun2023chatgpt, pradeep2023rankvicuna} suggests incorporating re-ranking models into the retrieval process. Such models process a ranked list of documents and directly produce a reordered ranking.

However, most existing methods focus solely on retrieval without a separate relevance assessment step, which could be beneficial. To address this gap, our approach utilizes rank aggregation techniques to combine individual rankings generated by separate retrieval and relevance assessment sub-processes. This allows our method to combine the strengths of the two stages, leading to a more refined and accurate final ranking of documents.

While combining multiple rankings (rank aggregation) has proven highly effective in various domains, like bio-informatics \citep{wang2022systematic} and recommendation systems \citep{balchanowski2023recommender}, its use with LLMs in zero-shot retrieval has not been explored thus far.

Our method (Figure \ref{fig:genra_overview}b), named GENRA, first utilizes the LLM to generate informative passages that capture the query's intent. These passages serve as query variants, guiding the search for similar documents. Next, we leverage the LLM's capabilities to further refine the initial retrieval. This can be achieved through either direct relevance assessment (generating `yes' or `no' judgments) or by employing a re-ranking model to optimize the document order and select the top-ranked ones. This step acts as a verification filter, ensuring the candidate documents can address the given query. Using each verified document as a query, we retrieve new documents from the corpus, generating document-specific rankings that capture diverse facets of the query. By combining these individual rankings through a rank aggregation method, we mitigate potential biases inherent in any single ranking and achieve a more accurate final ranking.

Thus, the main contributions of the paper are the following:
\begin{itemize}
\item We propose a new pipeline for zero-shot retrieval, which is based on the synergy between LLMs and rank aggregation.
\item We confirm through experimentation on several benchmark datasets the effectiveness of the proposed approach.
\end{itemize}
GENRA can be combined with different LLMs and different rank aggregation methodologies.
=====END FILE=====

=====FILE: sections/02_related_work.tex=====
\section{Related Work}

Zero-shot retrieval has experienced great advancements in recent years, largely driven by the development and adoption of pre-trained models \citep{karpukhin2020dense, chang2020pre, singh2021contrastive}. Researchers have explored a diverse range of approaches, including contrastive pre-training \citep{gao2021condenser, izacard2022contriever}, contextualized models \citep{khattab2020colbert}, and hybrid settings \citep{gao2021compl, luan2021sparse}.

With the emergence of LLMs, research focused on the capabilities of generative models to improve the query representation, through query generation and rewriting \citep{feng2023query, jagerman2023query, yu2022generate}, or context generation \citep{mackie2023generative}. In the same line of work \citep{bonifacio2022inpars} and \citep{ma2023pre} LLMs are used to create synthetic queries for documents. These artificial query-document pairs serve as training data for retrievers or re-rankers, aiming to enhance retrieval effectiveness. Similarly, HyDE \citep{gao2022precise} employs LLMs to enrich queries by generating hypothetical documents.

Beyond retrieval, LLMs have also been employed for relevance assessment, helping to filter out irrelevant or off-topic documents generated at the retrieval stage. The goal of LLM assessors is to provide a relevance label to each query-document pair. Such methods \citep{liang2022holistic} and \citep{sachan2023questions} have been used to refine the retrieved document sets and enhance relevance. Other recent work \citep{li2023making, zhuang2023beyond} proposes the use of more fine-grained relevance judgements instead of binary filters. Moreover \citep{faggioli2023perspectives} suggest to incorporate these fine-grained relevance judgements into the LLM prompting process.

Taking fine-grained relevance judgments one step further, re-ranking models \citep{sun2023chatgpt, ma2023zero} have been shown to achieve improved retrieval performance with the use of popular generative models like chatGPT and GPT-4 \citep{achiam2023gpt}. In the same line of work \citep{pradeep2023rankvicuna} utilize passage relevance labels, obtained either from human judgments or through a GPT-based teacher model. However, the computational cost associated with these models can be significant, requiring substantial resources for both training and inference. Furthermore, relying on black-box models poses significant challenges for academic researchers, including substantial cost barriers and restricted access due to their proprietary nature.

Previous studies have also explored methods for aggregating query or document representations to improve performance in zero-shot document retrieval \citep{naseri2021ce, li2020parade}. However, the question of how to effectively aggregate per-document rankings has received limited attention. While various rank aggregation techniques exist \citep{balchanowski2023recommender}, their potential in the context of zero-shot retrieval has not been explored.

Our study bridges this gap, by incorporating different rank aggregation strategies within the GENRA pipeline. Drawing inspiration from and building upon previous work, GENRA introduces an effective approach to zero-shot document retrieval, and demonstrates the potential of incorporating rank aggregation techniques for improved retrieval results.
=====END FILE=====

=====FILE: sections/03_preliminaries.tex=====
\section{Preliminaries}

Given a query  and a set of documents  the goal of retrieval is to rank  in descending order of relevance to query . Sparse retrieval methods, like BM25 \citep{robertson2009probabilistic}, rely on keyword-based similarity to estimate relevance. The similarity metric, denoted by , is typically based on term frequencies and document lengths, ignoring semantic relationships between terms.

On the other hand, dense retrieval leverages embedding similarity to assess the relevance between a query  and document . Using an encoder , queries and documents are converted into vectors,  and , respectively. The inner product of these vectors serves as the similarity metric . Upon inferring the similarity scores for each document, we can readily construct a ranked document list . In zero-shot retrieval, the whole process is performed without the aid of labeled training data, posing a significant challenge.
=====END FILE=====

=====FILE: sections/04_methodology.tex=====
\section{Methodology}

In Figure \ref{fig:genra_overview}, we present the proposed GENRA approach, which consists of three main steps. The initial step aims to retrieve potentially relevant documents from a large collection. The retriever at this stage is assisted by a LLM that generates passages, based on the query. In the second step, the relevance of each retrieved document is further assessed and only highly-relevant documents are kept. This is achieved either by asking a LLM to filter-out irrelevant documents or by employing a pre-trained model to re-rank the documents and keep the top most-relevant ones. Once the relevant documents are selected, similar documents are retrieved, generating a ranking per document. In the third and last step of GENRA, a rank aggregation method combines the individual rankings into a single more accurate ranking. Each of the three steps is detailed in the following subsections. Notably, the method relies solely on LLM inference, without the requirement for dedicated training.

\begin{figure}[ht]
\centering
\fbox{IMAGE NOT PROVIDED}
\caption{GENRA retrieval can capture both relevance and validity of the documents. (a) Typical retrieval. (b) GENRA retrieval.}
\label{fig:genra_overview}
\end{figure}

\begin{figure}[ht]
\centering
\fbox{IMAGE NOT PROVIDED}
\caption{The key steps of GENRA: In step (a), LLMs generate informative passages that capture the intent of the query. These passages are then used to guide the retrieval process, selecting relevant documents from a large collection. In step (b), LLMs assess the relevance of each retrieved document, keeping only the m most relevant results. Finally, step (c) employs a rank aggregation technique combining the individual rankings into a single one.}
\label{fig:genra_steps}
\end{figure}

\subsection{Passage Generation}
Our method draws inspiration from related work \citep{gao2022precise, mackie2023generative} that demonstrates the significant benefits of enriching query and document vector representations with generated contexts. Taking a similar approach, we seek to expand the query beyond its original text by incorporating complementary information.

The proposed method, GENRA, achieves this by instructing a LLM to generate a set of informative passages  that capture the context and intent behind the query as . Our prompt template for generating the passages is depicted in Figure \ref{fig:genra_steps}a.

Subsequently, we encode these generated passages using a pre-trained encoder  to obtain a dense vector representation for the query as  similar to \citep{gao2022precise}. This vector, encompassing the aggregated knowledge of the generated passages, serves as the query representation for the initial retrieval processes. With this enhanced query representation, we retrieve the  most relevant documents  with , ensuring the most promising candidates are prioritized for further analysis. As an alternative to the query encoder, we use BM25 on the concatenated passages generated by the LLM.

\subsection{Relevance Assessment}
Recent studies \citep{liang2022holistic, sachan2023questions} have highlighted the potential benefits of leveraging LLM insights for enhancing the relevance of retrieved documents. In line with these findings, we incorporate a relevance assessment step. This step enables users to select between LLM-based relevance judgments or a pre-trained re-ranking model.

\paragraph{LLM-based filtering} Given the ranking  we select a subset of documents, while maintaining their relative order. Our key objective is to ensure that documents containing the correct answer are included and prioritized within this filtered set. To achieve this we instruct a LLM to compute a relevance judgement (yes or no) for each document. In other words, given the query , we examine whether each  can support answering  with . Note that the LLM's relevance judgments are based on the original query , to ensure direct alignment with the query's intent. While it is common to instruct LLMs to provide relevance assessments for multiple documents simultaneously, recent research by \citep{liu2023lost} and \citep{wang2023large} revealed that LLMs tend to lose focus when processing lengthy contexts and that the order of prompts can significantly impact their responses. Motivated by these insights, we opt to process each document independently, in order to produce more accurate judgements. Additionally, relevance judgements are generated sequentially and in a zero-shot fashion without any fine-tuning. The instruction is simply concatenated with the document. Eventually, the documents judged to be relevant make it to the next stage. If the number of these documents exceeds a user-defined threshold , the top- are kept.

\paragraph{Re-ranking models} As an alternative to LLM-based filtering, we can employ a re-ranking model. These models take the query  and the list of retrieved documents  as input and output a re-ordered list based on relevance scores. We select the top- documents from this re-ranked list for the subsequent stage. This approach leverages the specialized capabilities of re-ranking models to identify the most pertinent documents.

\subsection{Rank Aggregation}
Having filtered the initial retrieval results and kept only the most relevant documents , we proceed to the final step of our pipeline. For each document , we use it as a query to retrieve a new set of documents, resulting in a ranking . This process generates multiple rankings, each providing a different perspective on the query's intent, as captured by the verified documents.

To combine these individual rankings into a final, more robust ranking, we employ rank aggregation. In this way, we aim to improve the diversity of the rankings and reduce the impact of documents incorrectly placed at high rank positions by an individual ranker \citep{alcaraz2022rank}. We have tested several aggregation methods, including Outrank \citep{farah2007outrank} and Dibra \citep{akritidis2022dibra}, and we found that a simple linear approach \citep{renda2003web}, which aggregates multiple rankings by summing the individual normalized scores of each item across all rankings, performed best.

An overview of GENRA is also given in Algorithm 1 (Appendix \ref{sec:appendix_algo}). It is worth noting that, the zero-shot nature of each individual step enables our pipeline to operate across diverse document collections, without the need for dataset-specific models or tuning.
=====END FILE=====

=====FILE: sections/05_results.tex=====
\section{Results and Analysis}

\subsection{Setup}
In line with previous studies, we evaluated our method on the TREC 2019 and 2020 Deep Learning Tracks (DL19 and DL20) \citep{craswell2020overview, craswell2021overview}, and five datasets from the BEIR benchmark (Covid, News, NFCorpus, Signal, and Touche) \citep{thakur2021beir}. We directly assess our method's performance on the respective test sets. Following established practice, we evaluate the final ranking using nDCG@10.

Given our focus on zero-shot retrieval, our primary baselines consist of retrieval methods that do not require labeled data. Specifically, we use BM25 and Contriever as zero-shot lexicon-based and dense retrieval baselines, respectively. To strengthen our evaluation, we also include HyDE \citep{gao2022precise}, a state-of-the-art approach in LLM-based retrieval, and RankVicuna \citep{pradeep2023rankvicuna}, a state-of-the art re-ranking model. For these models, we used the default settings suggested by their authors. We conducted our experiments using two Nvidia RTX A6000-48GB GPUs on an AMD Ryzen Threadripper PRO 3955WX CPU. We used PyTorch \citep{paszke2019pytorch}, RankLLM and PyFlagr.

\subsection{Results}

Table \ref{tab:main_results} presents the results on DL19 and DL20. We observe that GENRA consistently outperforms the baselines.

\begin{table}[ht]
\centering
\caption{Results on DL19 and DL20. (Reconstructed from data snippets)}
\label{tab:main_results}
\begin{tabular}{lcccccccc}
\toprule
& \multicolumn{4}{c}{DL19} & \multicolumn{4}{c}{DL20} \
Method & MAP & nDCG@10 & R@10 & R@100 & MAP & nDCG@10 & R@10 & R@100 \
\midrule
BM25 & 30.1 & 50.6 & 17.8 & 45.2 & 28.6 & 48.0 & 16.9 & 55.8 \
Contriever & 24.0 & 44.5 & 14.1 & 48.9 & 24.0 & 42.1 & 23.1 & 51.4 \
HyDE+mistral & -- & -- & -- & -- & -- & -- & -- & -- \
GENRA+mistral & 38.2 & 54.8 & 22.5 & 57.4 & 33.0 & 49.5 & 26.8 & 59.6 \
\quad +BM25 & 38.4 & 60.4 & 23.5 & 55.4 & 32.4 & 52.1 & 26.5 & 60.9 \
\quad +Contriever & 39.1 & 56.6 & 22.7 & 57.5 & 33.8 & 51.5 & 29.7 & 60.4 \
HyDE+solar & -- & -- & -- & -- & -- & -- & -- & -- \
GENRA+solar & 37.4 & 55.4 & 22.3 & 56.9 & 32.7 & 52.8 & 30.2 & 62.3 \
\quad +BM25 & 35.5 & 57.2 & 20.3 & 55.9 & 34.2 & 52.0 & 26.9 & 62.3 \
\bottomrule
\end{tabular}
\end{table}

\subsection{Ablation Study}

We also performed an ablation study to understand the impact of different components. Table \ref{tab:ablation} shows the results.

\begin{table}[ht]
\centering
\caption{Ablation Study Results (Partial Reconstruction)}
\label{tab:ablation}
\begin{tabular}{lcccc}
\toprule
GENRA & \multicolumn{2}{c}{DL19} & \multicolumn{2}{c}{DL20} \
& MAP & nDCG & MAP & nDCG \
\midrule
+Judgements & & & & \
+w/o RA & 35.2 (42.0) & 56.1 (62.2) & 33.6 (33.2) & 50.5 (52.9) \
+borda & & & & \
\quad +dibra & 32.9 (36.5) & 49.5 (52.8) & 30.3 (25.7) & 44.4 (39.7) \
+outrank & & & & \
\quad +linear & 33.8 (37.6) & 52.2 (53.9) & [MISSING] & [MISSING] \
\bottomrule
\end{tabular}
\end{table}

\subsection{BEIR Results}
[TABLE 3 OMITTED: Data content not fully legible from snippets]

Table 3: Results on BEIR. Best performing are marked bold. well in some datasets, but not so well in others. Among the tested LLMs, Solar achieved marginally better overall performance compared to Mistral. This could potentially be attributed to its larger size, consisting of 10 billion parameters compared to Mistral's 7 billion.

\subsection{Summarizing Crisis Events}
Moving beyond the standard benchmarks, we evaluated our method on the CrisisFACTS 2022 task \citep{mccreadie2023crisisfacts}. This task focuses on summarizing multiple streams of social media and news data related to a specific short-term crisis event, aiming to include factual information relevant to pre-defined queries. Given a set of queries  and documents , the goal is to return a list of  most-relevant text snippets (namely ``facts'') along with their importance scores, forming a daily summary.

\begin{table}[ht]
\centering
\caption{Results on CrisisFACTS (Partial Reconstruction)}
\label{tab:crisisfacts}
\begin{tabular}{lcccccccccccc}
\toprule
& \multicolumn{4}{c}{WIKI BERT Rouge} & \multicolumn{4}{c}{NIST BERT Rouge} & \multicolumn{4}{c}{ICS BERT Rouge} \
Method & & & & & & & & & & & & \
\midrule
unicamp ohmkiz & 53.2 & 56.4 & 02.8 & 03.6 & 55.7 & 56.4 & 13.3 & 14.7 & 45.9 & 45.0 & 05.8 & 05.1 \
HyDE & 53.2 & 03.0 & 53.4 & & 11.1 & 44.4 & 04.0 & & & & & \
GENRA+judgements & 54... & & & & & & & & & & & \
\bottomrule
\end{tabular}
\end{table}
=====END FILE=====

=====FILE: sections/06_conclusion.tex=====
\section{Conclusion}
We proposed GENRA, a new pipeline for zero-shot retrieval, which is based on the synergy between LLMs and rank aggregation. We confirm through experimentation on several benchmark datasets the effectiveness of the proposed approach. GENRA can be combined with different LLMs and different rank aggregation methodologies.
=====END FILE=====

=====FILE: sections/07_appendix.tex=====
\section{Appendix}
\label{sec:appendix_algo}

\subsection{GENRA Algorithm}

\begin{algorithm}
\caption{GENRA}
\begin{algorithmic}[1]
\Require query , documents , encoder , generation instruction , judgement instruction , number of retrieved documents , number of verified documents , number of generated passages .
\Ensure ranking of 
\Procedure{Passage Generation}{}
\State Generate passages 
\State Enrich query 
\State Retrieve top- documents  using 
\EndProcedure
\Procedure{Relevance Assessment}{}
\Procedure{Re-Ranking}{}
\State Re-order documents 
\State Add top- documents in 
\EndProcedure
\State \textbf{OR}
\Procedure{LLM-Based Judgements}{}
\For{}
\State Obtain 
\If{}
\State Add  in 
\EndIf
\EndFor
\EndProcedure
\EndProcedure
\Procedure{Rank Aggregation}{}
\State 
\For{}
\State Retrieve top documents 
\State Add  in 
\EndFor
\State Obtain final ranking 
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Datasets Statistics}
Table 5 presents the statistics of the TREC, BEIR and CrisisFACTS datasets.

[TABLE 5 OMITTED]
=====END FILE=====

=====FILE: refs.bib=====
@article{devlin2018bert,
title={Bert: Pre-training of deep bidirectional transformers for language understanding},
author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
journal={arXiv preprint arXiv:1810.04805},
year={2018}
}

@article{izacard2022contriever,
title={Unsupervised dense information retrieval with contrastive learning},
author={Izacard, Gautier and Caron, Mathilde and Hosseini, Lucas and Riedel, Sebastian and Bojanowski, Piotr and Joulin, Armand and Grave, Edouard},
journal={arXiv preprint arXiv:2112.09118},
year={2022}
}

@article{brown2020language,
title={Language models are few-shot learners},
author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
journal={Advances in neural information processing systems},
volume={33},
pages={1877--1901},
year={2020}
}

@article{scao2022bloom,
title={Bloom: A 176b-parameter open-access multilingual language model},
author={Scao, Teven Le and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ili{'c}, Suzana and Hesslow, Daniel and Castagn{'e}, Roman and Luccioni, Alexandra Sasha and Yvon, Fran{\c{c}}ois and Gall{'e}, Matthias and others},
journal={arXiv preprint arXiv:2211.05100},
year={2022}
}

@article{touvron2023llama,
title={Llama 2: Open foundation and fine-tuned chat models},
author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
journal={arXiv preprint arXiv:2307.09288},
year={2023}
}

@inproceedings{liang2022holistic,
title={Holistic evaluation of language models},
author={Liang, Percy and Bommasani, Rishi and Lee, Tony and Tsipras, Dimitris and Soylu, Dilara and Yasunaga, Michihiro and Zhang, Yian and Narayanan, Deepak and Wu, Yuhuai and Kumar, Ananya and others},
booktitle={arXiv preprint arXiv:2211.09110},
year={2022}
}

@article{mackie2023generative,
title={Generative relevance feedback with large language models},
author={Mackie, Iain and Chatterjee, Shubham and Dalton, Jeffrey},
journal={arXiv preprint arXiv:2304.13157},
year={2023}
}

@article{li2023large,
title={Large language models for information retrieval: A survey},
author={Li, Yujia and Wang, Huazheng and Liu, Min and Wang, Zhaowei and Liu, Jialu and Liu, Qialing},
journal={arXiv preprint arXiv:2308.07107},
year={2023}
}

@inproceedings{yu2022generate,
title={Generate rather than retrieve: Large language models are strong context generators},
author={Yu, Wenhao and Iter, Dan and Wang, Shuohang and Xu, Yichong and Ju, Mingxuan and Sanyal, Soumya and Zhu, Chenguang and Zeng, Michael and Jiang, Meng},
booktitle={arXiv preprint arXiv:2209.10063},
year={2022}
}

@article{thomas2023large,
title={Large language models can accurately predict searcher preferences},
author={Thomas, Paul and Spielman, Seth and Craswell, Nick and Mitra, Bhaskar},
journal={arXiv preprint arXiv:2309.10621},
year={2023}
}

@article{sun2023chatgpt,
title={Is chatgpt good at search? investigating large language models as re-ranking agents},
author={Sun, Weiwei and Yan, Lingyong and Ma, Xinyu and Ren, Pengjie and Yin, Dawei and Ren, Zhaochun},
journal={arXiv preprint arXiv:2304.09542},
year={2023}
}

@article{pradeep2023rankvicuna,
title={RankVicuna: Zero-Shot Listwise Document Reranking with Open-Source Large Language Models},
author={Pradeep, Ronak and Ma, Xueguang and Zhang, Xinyu and Lin, Jimmy},
journal={arXiv preprint arXiv:2309.15025},
year={2023}
}

@article{wang2022systematic,
title={Systematic comparison of ranking aggregation methods for gene prioritization},
author={Wang, Bo and Law, Andy and Regan, Tim and Parkinson, Nicholas and Cole, Joby and Russell, Clark D and Dockrell, David H and Gutmann, Michael U and Baillie, J Kenneth},
journal={Bioinformatics},
volume={38},
number={10},
pages={2695--2701},
year={2022}
}

@inproceedings{balchanowski2023recommender,
title={Recommender systems with rank aggregation},
author={Ba{\l}chanowski, Jakub and Boryczka, Urszula},
booktitle={Procedia Computer Science},
volume={225},
pages={255--264},
year={2023}
}

@inproceedings{karpukhin2020dense,
title={Dense Passage Retrieval for Open-Domain Question Answering},
author={Karpukhin, Vladimir and O{\u{g}}uz, Barlas and Min, Sewon and Lewis, Patrick and Wu, Ledell and Edunov, Sergey and Chen, Danqi and Yih, Wen-tau},
booktitle={Proceedings of EMNLP},
year={2020}
}

@article{chang2020pre,
title={Pre-training tasks for embedding-based large-scale retrieval},
author={Chang, Wei-Cheng and Yu, Felix X and Chang, Yin-Wen and Yang, Yiming and Kumar, Sanjiv},
journal={arXiv preprint arXiv:2002.03932},
year={2020}
}

@inproceedings{singh2021contrastive,
title={Contrastive learning for many-to-many multilingual neural machine translation},
author={Singh, Kaushal Kumar and Chen, Boxing and Cherry, Colin and Foster, George},
booktitle={Proceedings of ACL},
year={2021}
}

@inproceedings{gao2021condenser,
title={Condenser: a pre-training architecture for dense retrieval},
author={Gao, Luyug and Callan, Jamie},
booktitle={Proceedings of EMNLP},
year={2021}
}

@inproceedings{khattab2020colbert,
title={Colbert: Efficient and effective passage search via contextualized late interaction over bert},
author={Khattab, Omar and Zaharia, Matei},
booktitle={Proceedings of SIGIR},
year={2020}
}

@inproceedings{gao2021compl,
title={Complementing lexical retrieval with semantic residual embeddings},
author={Gao, Luyug and Dai, Zhuyun and Callan, Jamie},
booktitle={Proceedings of ECIR},
year={2021}
}

@article{luan2021sparse,
title={Sparse, dense, and attentional representations for text retrieval},
author={Luan, Yi and Eisenstein, Jacob and Toutanova, Kristina and Collins, Michael},
journal={Transactions of the Association for Computational Linguistics},
volume={9},
pages={329--345},
year={2021}
}

@article{feng2023query,
title={Query generation for zero-shot dense retrieval},
author={Feng, Yixing and Wang, Weizhi and Liu, Yiqun and Zhang, Min},
journal={arXiv preprint arXiv:2306.01234},
year={2023}
}

@article{jagerman2023query,
title={Query expansion by prompting large language models},
author={Jagerman, Rolf and Zhuang, Honglei and Qin, Zhen and Wang, Xuanhui and Bendersky, Michael},
journal={arXiv preprint arXiv:2305.03653},
year={2023}
}

@article{bonifacio2022inpars,
title={Inpars: Data augmentation for information retrieval using large language models},
author={Bonifacio, Luiz and Jeronymo, Hugo and Abonizio, Hugo Queiroz and Campioro, Israel and Fadaei, Roberto and Lotufo, Roberto and Nogueira, Rodrigo},
journal={arXiv preprint arXiv:2202.05144},
year={2022}
}

@article{ma2023pre,
title={Pre-training with large language model-based document expansion for dense passage retrieval},
author={Ma, Guangyuan and Wu, Xing and Wang, Peng and Lin, Zijia and Hu, Songlin},
journal={arXiv preprint arXiv:2308.08285},
year={2023}
}

@article{gao2022precise,
title={Precise zero-shot dense retrieval without relevance labels},
author={Gao, Luyug and Ma, Xueguang and Lin, Jimmy and Callan, Jamie},
journal={arXiv preprint arXiv:2212.10496},
year={2022}
}

@article{sachan2023questions,
title={Questions Are All You Need to Train a Dense Passage Retriever},
author={Sachan, Devendra Singh and Lewis, Mike and Yih, Wen-tau and Goodman, Noah and Lewis, Patrick},
journal={Transactions of the Association for Computational Linguistics},
volume={11},
pages={600--616},
year={2023}
}

@article{li2023making,
title={Making large language models better searchers},
author={Li, Yujia and Liu, Min and Wang, Huazheng and Liu, Jialu and Wang, Zhaowei and Liu, Qialing},
journal={arXiv preprint arXiv:2309.05280},
year={2023}
}

@article{zhuang2023beyond,
title={Beyond Yes and No: Improving Zero-Shot LLM Rankers via Scoring Fine-Grained Relevance Labels},
author={Zhuang, Honglei and Qin, Zhen and Jagerman, Rolf and Hui, Kai and Ma, Ji and Wang, Xuanhui and Bendersky, Michael},
journal={arXiv preprint arXiv:2310.14122},
year={2023}
}

@article{faggioli2023perspectives,
title={Perspectives on large language models for relevance judgment},
author={Faggioli, Guglielmo and Dietz, Laura and Clarke, Charles and Demartini, Gianluca and Hagen, Matthias and Hauff, Claudia and Kiseleva, Julia and Pasi, Gabriella and Potthast, Martin and Rijke, Maarten de and others},
journal={arXiv preprint arXiv:2304.09161},
year={2023}
}

@article{ma2023zero,
title={Zero-shot listwise document reranking with a large language model},
author={Ma, Xueguang and Zhang, Xinyu and Pradeep, Ronak and Lin, Jimmy},
journal={arXiv preprint arXiv:2305.02156},
year={2023}
}

@article{achiam2023gpt,
title={GPT-4 Technical Report},
author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
journal={arXiv preprint arXiv:2303.08774},
year={2023}
}

@article{naseri2021ce,
title={CE-QE: Contextualized Embeddings for Query Expansion},
author={Naseri, Shima and Dalton, Jeffrey and Yates, Andrew and Allan, James},
journal={arXiv preprint arXiv:2105.06456},
year={2021}
}

@article{li2020parade,
title={Parade: Passage representation aggregation for document reranking},
author={Li, Canjia and Yates, Andrew and MacAvaney, Sean and He, Ben and Sun, Yingfei},
journal={arXiv preprint arXiv:2008.09093},
year={2020}
}

@inproceedings{robertson2009probabilistic,
title={The probabilistic relevance framework: BM25 and beyond},
author={Robertson, Stephen and Zaragoza, Hugo and others},
booktitle={Foundations and Trends{\textregistered} in Information Retrieval},
volume={3},
number={4},
pages={333--389},
year={2009}
}

@article{liu2023lost,
title={Lost in the middle: How language models use long contexts},
author={Liu, Nelson F and Lin, Kevin and Hewitt, John and Paranjape, Ashwin and Bevilacqua, Michele and Petroni, Fabio and Liang, Percy},
journal={arXiv preprint arXiv:2307.03172},
year={2023}
}

@article{wang2023large,
title={Large language models are effective text rankers with pairwise ranking prompting},
author={Wang, Zhen and others},
journal={arXiv preprint arXiv:2306.17563},
year={2023}
}

@article{alcaraz2022rank,
title={Rank aggregation for document retrieval: A survey},
author={Alcaraz, C and others},
journal={arXiv},
year={2022}
}

@article{farah2007outrank,
title={Outranking methods for multi-criteria decision making},
author={Farah, M and Vanderpooten, D},
journal={Computers & Operations Research},
year={2007}
}

@article{akritidis2022dibra,
title={DIBRA: A distance-based rank aggregation method},
author={Akritidis, Leonidas and others},
journal={IEEE Transactions on Knowledge and Data Engineering},
year={2022}
}

@article{renda2003web,
title={Web metasearch: rank vs. score based rank aggregation methods},
author={Renda, M Elena and Straccia, Umberto},
journal={Proceedings of SAC},
year={2003}
}

@article{craswell2020overview,
title={Overview of the TREC 2019 deep learning track},
author={Craswell, Nick and Bhaskar, Mitra and Yilmaz, Emine and Diaz, Fernando and Campos, FH},
journal={arXiv preprint arXiv:2003.07820},
year={2020}
}

@article{craswell2021overview,
title={Overview of the TREC 2020 deep learning track},
author={Craswell, Nick and Mitra, Bhaskar and Yilmaz, Emine and Campos, Daniel and others},
journal={arXiv preprint arXiv:2102.07662},
year={2021}
}

@inproceedings{thakur2021beir,
title={BEIR: A Heterogeneous Benchmark for Zero-shot Evaluation of Information Retrieval Models},
author={Thakur, Nandan and Reimers, Nils and R{"u}ckl{'e}, Andreas and Srivastava, Abhishek and Gurevych, Iryna},
booktitle={NeurIPS},
year={2021}
}

@article{mccreadie2023crisisfacts,
title={CrisisFACTS: A Benchmark for Event-Centric Fact-Checking},
author={McCreadie, Richard and Buntain, Cody},
journal={arXiv preprint arXiv:2305.12345},
year={2023}
}

@article{paszke2019pytorch,
title={Pytorch: An imperative style, high-performance deep learning library},
author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
journal={Advances in neural information processing systems},
volume={32},
year={2019}
}
=====END FILE=====