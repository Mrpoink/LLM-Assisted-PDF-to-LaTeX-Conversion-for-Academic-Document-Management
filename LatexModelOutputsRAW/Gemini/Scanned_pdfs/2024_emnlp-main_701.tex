ACK

=====FILE: main.tex=====
\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{natbib}
\usepackage{authblk}
\usepackage{algorithm}
\usepackage{algorithmic}

\geometry{left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm}

\title{Lifelong Event Detection via Optimal Transport}

\author[1]{Viet Dao\thanks{Equal contribution.}}
\author[1]{Van-Cuong Pham\protect\footnotemark[1]}
\author[1]{Quyen Tran\protect\footnotemark[1]}
\author[1]{Thanh-Thien Le\protect\footnotemark[1]}
\author[2]{Linh Ngo Van}
\author[1]{Thien Huu Nguyen}

\affil[1]{Hanoi University of Science and Technology, VinAI Research, University of Oregon}
\affil[2]{Hanoi University of Science and Technology}

\date{}

\begin{document}

\maketitle

\begin{abstract}
Continual Event Detection (CED) poses a formidable challenge due to the catastrophic forgetting phenomenon, where learning new tasks (with new coming event types) hampers performance on previous ones. In this paper, we introduce a novel approach, Lifelong Event Detection via Optimal Transport (LEDOT), that leverages optimal transport principles to align the optimization of our classification module with the intrinsic nature of each class, as defined by their pre-trained language modeling. Our method integrates replay sets, prototype latent representations, and an innovative Optimal Transport component. Extensive experiments on MAVEN and ACE datasets demonstrate LEDOT's superior performance, consistently outperforming state-of-the-art baselines. The results underscore LEDOT as a pioneering solution in continual event detection, offering a more effective and nuanced approach to addressing catastrophic forgetting in evolving environments.
\end{abstract}

\input{sections/introduction}
\input{sections/background}
\input{sections/method}
\input{sections/experiments}
\input{sections/conclusion}

\section*{Acknowledgements}
This research has been supported by the Army Research Office (ARO) grant W911NF-21-1-0112, the NSF grant CNS-1747798 to the IUCRC Center for Big Learning, and the NSF grant # 2239570. This research is also supported in part by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via the HIATUS Program contract 2022-22072200003. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of ODNI, IARPA, or the U.S. Government.

\bibliographystyle{acl_natbib}
\begin{thebibliography}{99}

\bibitem{belouadah2019}
Eden Belouadah and Adrian Popescu. 2019.
\newblock Il2m: Class incremental learning with dual memory.
\newblock In \emph{2019 IEEE/CVF International Conference on Computer Vision (ICCV)}, pages 583--592.

\bibitem{cao2020}
Pengfei Cao, Yubo Chen, Jun Zhao, and Taifeng Wang. 2020.
\newblock Incremental event detection via knowledge consolidation networks.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)}, pages 707--717.

\bibitem{chaudhry2021}
Arslan Chaudhry, Albert Gordo, Puneet Dokania, Philip Torr, and David Lopez-Paz. 2021.
\newblock Using hindsight to anchor past knowledge in continual learning.
\newblock In \emph{Proceedings of the AAAI conference on artificial intelligence}, pages 6993--7001.

\bibitem{cui2021}
Li Cui, Deqing Yang, Jiaxin Yu, Chengwei Hu, Jiayang Cheng, Jingjie Yi, and Yanghua Xiao. 2021.
\newblock Refining sample embeddings with relation prototypes to enhance continual relation extraction.
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)}, pages 232--243. Association for Computational Linguistics.

\bibitem{cuturi2013}
Marco Cuturi. 2013.
\newblock Sinkhorn distances: Lightspeed computation of optimal transport.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume 26. Curran Associates, Inc.

\bibitem{devlin2019}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.
\newblock BERT: Pre-training of deep bidirectional transformers for language understanding.
\newblock In \emph{Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)}, pages 4171--4186. Association for Computational Linguistics.

\bibitem{frogner2015}
Charlie Frogner, Chiyuan Zhang, Hossein Mobahi, Mauricio Araya, and Tomaso A Poggio. 2015.
\newblock Learning with a wasserstein loss.
\newblock \emph{Advances in neural information processing systems}, 28.

\bibitem{gao2023}
Jun Gao, Huan Zhao, Changlong Yu, and Ruifeng Xu. 2023.
\newblock Exploring the feasibility of chatgpt for event extraction.
\newblock \emph{arXiv preprint arXiv:2303.03836}.

\bibitem{hai2024}
Nam Le Hai, Trang Nguyen, Linh Ngo Van, Thien Huu Nguyen, and Khoat Than. 2024.
\newblock Continual variational dropout: a view of auxiliary local variables in continual learning.
\newblock \emph{Machine Learning}, 113(1):281--323.

\bibitem{han2023}
Ridong Han, Tao Peng, Chaohao Yang, Benyou Wang, Lu Liu, and Xiang Wan. 2023.
\newblock Is information extraction solved by chatgpt? an analysis of performance, evaluation criteria, robustness and errors.

\bibitem{han2020}
Xu Han, Yi Dai, Tianyu Gao, Yankai Lin, Zhiyuan Liu, Peng Li, Maosong Sun, and Jie Zhou. 2020.
\newblock Continual relation learning via episodic memory activation and reconsolidation.
\newblock In \emph{Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics}, pages 6429--6440. Association for Computational Linguistics.

\bibitem{hinton2015}
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.
\newblock Distilling the knowledge in a neural network.

\bibitem{lai2023}
Viet Lai, Nghia Ngo, Amir Pouran Ben Veyseh, Hieu Man, Franck Dernoncourt, Trung Bui, and Thien Nguyen. 2023.
\newblock ChatGPT beyond English: Towards a comprehensive evaluation of large language models in multilingual learning.
\newblock In \emph{Findings of the Association for Computational Linguistics: EMNLP 2023}, pages 13171--13189. Association for Computational Linguistics.

\bibitem{le2024a}
Thanh-Thien Le, Viet Dao, Linh Van Nguyen, Thi-Nhung Nguyen, Linh Van Ngo, and Thien Huu Nguyen. 2024a.
\newblock Sharpseq: Empowering continual event detection through sharpness-aware sequential-task learning.
\newblock In \emph{2024 Annual Conference of the North American Chapter of the Association for Computational Linguistics}.

\bibitem{le2024b}
Thanh-Thien Le, Manh Nguyen, Tung Thanh Nguyen, Linh Ngo Van, and Thien Huu Nguyen. 2024b.
\newblock Continual relation extraction via sequential multi-task learning.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume 38, pages 18444--18452.

\bibitem{liu2022}
Minqian Liu, Shiyu Chang, and Lifu Huang. 2022.
\newblock Incremental prompting: Episodic memory prompt for lifelong event detection.
\newblock In \emph{Proceedings of the 29th International Conference on Computational Linguistics}, pages 2157--2165. International Committee on Computational Linguistics.

\bibitem{loshchilov2017}
Ilya Loshchilov and Frank Hutter. 2017.
\newblock Decoupled weight decay regularization.
\newblock \emph{arXiv preprint arXiv:1711.05101}.

\bibitem{lu2018}
Weiyi Lu and Thien Huu Nguyen. 2018.
\newblock Similar but not the same: Word sense disambiguation improves event detection via neural representation matching.
\newblock In \emph{Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing}, pages 4822--4828. Association for Computational Linguistics.

\bibitem{man2024a}
Hieu Man, Chien Van Nguyen, Nghia Trung Ngo, Linh Ngo, Franck Dernoncourt, and Thien Huu Nguyen. 2024a.
\newblock Hierarchical selection of important context for generative event causality identification with optimal transports.
\newblock In \emph{Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)}, pages 8122--8132. ELRA and ICCL.

\bibitem{man2024b}
Hieu Man, Chien Van Nguyen, Nghia Trung Ngo, Linh Ngo, Franck Dernoncourt, and Thien Huu Nguyen. 2024b.
\newblock Hierarchical selection of important context for generative event causality identification with optimal transports.
\newblock In \emph{Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)}, pages 8122--8132.

\bibitem{man2020}
Hieu Man Duc Trong, Duc Trong Le, Amir Pouran Ben Veyseh, Thuat Nguyen, and Thien Huu Nguyen. 2020.
\newblock Introducing a new dataset for event detection in cybersecurity texts.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)}, pages 5381--5390. Association for Computational Linguistics.

\bibitem{mccloskey1989}
Michael McCloskey and Neal J. Cohen. 1989.
\newblock Catastrophic interference in connectionist networks: The sequential learning problem.
\newblock In \emph{Psychology of Learning and Motivation}, volume 24, pages 109--165. Academic Press.

\bibitem{nguyen2023a}
Chien Nguyen, Linh Ngo, and Thien Nguyen. 2023a.
\newblock Retrieving relevant context to align representations for cross-lingual event detection.
\newblock In \emph{Findings of the Association for Computational Linguistics: ACL 2023}, pages 2157--2170.

\bibitem{nguyen2023b}
Huy Nguyen, Chien Nguyen, Linh Ngo, Anh Luu, and Thien Nguyen. 2023b.
\newblock A spectral viewpoint on continual relation extraction.
\newblock In \emph{Findings of the Association for Computational Linguistics: EMNLP 2023}, pages 9621--9629.

\bibitem{nguyen2016}
Thien Huu Nguyen, Kyunghyun Cho, and Ralph Grishman. 2016.
\newblock Joint event extraction via recurrent neural networks.
\newblock In \emph{Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, pages 300--309. Association for Computational Linguistics.

\bibitem{nguyen2015}
Thien Huu Nguyen and Ralph Grishman. 2015.
\newblock Event detection and domain adaptation with convolutional neural networks.
\newblock In \emph{Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)}, pages 365--371. Association for Computational Linguistics.

\bibitem{ouyang2022}
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Christiano, Jan Leike, and Ryan Lowe. 2022.
\newblock Training language models to follow instructions with human feedback.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume 35, pages 27730--27744. Curran Associates, Inc.

\bibitem{phan2022}
Hoang Phan, Anh Phan Tuan, Son Nguyen, Ngo Van Linh, and Khoat Than. 2022.
\newblock Reducing catastrophic forgetting in neural networks via gaussian mixture approximation.
\newblock In \emph{Pacific-Asia Conference on Knowledge Discovery and Data Mining}, pages 106--117. Springer.

\bibitem{qin2024}
Chengwei Qin, Ruirui Chen, Ruochen Zhao, Wenhan Xia, and Shafiq Joty. 2024.
\newblock Lifelong event detection with embedding space separation and compaction.

\bibitem{qiu2024}
Yunjian Qiu and Yan Jin. 2024.
\newblock Chatgpt and finetuned bert: A comparative study for developing intelligent design support systems.
\newblock \emph{Intelligent Systems with Applications}, 21:200308.

\bibitem{raffel2020}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock \emph{Journal of Machine Learning Research}, 21(140):1--67.

\bibitem{raffel2023}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2023.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.

\bibitem{ratcliff1990}
Roger Ratcliff. 1990.
\newblock Connectionist models of recognition memory: Constraints imposed by learning and forgetting functions.
\newblock \emph{Psychological Review}, 97(2):285--308.

\bibitem{rolnick2019}
David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lillicrap, and Gregory Wayne. 2019.
\newblock Experience replay for continual learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume 32.

\bibitem{saha2021}
Gobinda Saha, Isha Garg, and Kaushik Roy. 2021.
\newblock Gradient projection memory for continual learning.
\newblock \emph{arXiv preprint arXiv:2103.09762}.

\bibitem{shi2024}
Haizhou Shi, Zihao Xu, Hengyi Wang, Weiyi Qin, Wenyuan Wang, Yibin Wang, and Hao Wang. 2024.
\newblock Continual learning of large language models: A comprehensive survey.

\bibitem{sokar2021}
Ghada Sokar, Decebal Constantin Mocanu, and Mykola Pechenizkiy. 2021.
\newblock Spacenet: Make free space for continual learning.
\newblock \emph{Neurocomputing}, 439:1--11.

\bibitem{van2022}
Linh Ngo Van, Nam Le Hai, Hoang Pham, and Khoat Than. 2022.
\newblock Auxiliary local variables for improving regularization/prior approach in continual learning.
\newblock In \emph{Pacific-Asia conference on knowledge discovery and data mining}, pages 16--28. Springer.

\bibitem{walker2006}
Christopher Walker, Stephanie Strassel, Julie Medero, and Kazuaki Maeda. 2006.
\newblock ACE 2005 multilingual training corpus LDC2006T06.
\newblock Web Download. Philadelphia: Linguistic Data Consortium.

\bibitem{wang2020}
Xiaozhi Wang, Ziqi Wang, Xu Han, Wangyi Jiang, Rong Han, Zhiyuan Liu, Juanzi Li, Peng Li, Yankai Lin, and Jie Zhou. 2020.
\newblock Maven: A massive general domain event detection dataset.
\newblock \emph{arXiv preprint arXiv:2004.13590}.

\bibitem{wang2023}
Zitao Wang, Xinyi Wang, and Wei Hu. 2023.
\newblock Continual event extraction with semantic confusion rectification.

\bibitem{wu2019}
Yue Wu, Yinpeng Chen, Lijuan Wang, Yuancheng Ye, Zicheng Liu, Yandong Guo, and Yun Fu. 2019.
\newblock Large scale incremental learning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}.

\end{thebibliography}

\end{document}

=====END FILE=====

=====FILE: sections/introduction.tex=====
\section{Introduction}
Event Detection (ED) \citep{nguyen2016,nguyen2023a} presents a pivotal challenge in the domain of Information Extraction, tasked with identifying event triggers and their associated types from natural language text. However, the conventional ED training paradigm, characterized by its static nature, falls short in capturing the dynamic nature of real-world data. As highlighted by \citet{yu2021}, the ontology of events in ED research has been exhibiting a constant shift since its introduction, prompting the exploration of Continual Event Detection (CED), where data arrives continuously as a sequence of non-overlapping tasks.

Although large language models (LLMs) have recently emerged, showcasing the ability to tackle numerous problems using only prompts without the need for fine-tuning, they fall short in the domains of information extraction (IE) \citep{han2023,gao2023} and continual learning \citep{shi2024}. Continual event detection, in particular, remains a difficult task that is not effectively addressed by LLMs.

CED presents many issues, most notably the catastrophic forgetting \citep{mccloskey1989,ratcliff1990} phenomenon, where the training signal from new task hampers performance on past tasks. To provide a solution for this issue, numerous methods have been proposed, which usually fall into one of the three eminent approaches: Regularization-based \citep{chaudhry2021,saha2021,phan2022,van2022,hai2024}; Architecture-based \citep{yoon2017,sokar2021}; and Memory-based \citep{belouadah2019,rolnick2019}.

Out of these three, Memory-based methods have demonstrated superiority, leveraging access to the Replay buffer, a memory of limited size containing a portion of data from previously learned tasks for the model to rehearse during the training of new tasks. Despite the promise of Memory-based methods, challenges abound. First, the finite capacity of the Replay buffer results in the eviction of valuable information, leading to incomplete representations of past tasks and hence, inadequate generality. Furthermore, the process of sampling and replaying data might not be optimally curated, potentially hindering the model's ability to generalize across tasks effectively.

This setback arises because the conventional practice of discarding the original head of pre-trained language models (PLMs) during fine-tuning on downstream tasks overlooks valuable linguistic information encoded within it. In training the classifier module, state-of-the-art approaches \citep{qin2024,wang2023,liu2022,yu2021} often do so in isolation, devoid of any priors or foundations. Discarding the language modeling head in PLMs is highly wasteful. The language modeling head contains essential information about vocabulary distribution based on contextual representations. Losing this head sacrifices crucial linguistic nuances, making it harder to align the classifier module and ensure efficient fine-tuning.

Aligning our classifier module to this information is an essential but also formidable challenge. This alignment is crucial for ensuring a more efficient fine-tuning process, as it provides a foundational standard of learning that mitigates unnecessary overplasticity and prevents catastrophic forgetting.

To address the limitations discussed, this paper introduces a method to enhance Memory-based CED by integrating Optimal Transport (OT) principles, which provide a robust framework for measuring the distance between probability distributions. By incorporating OT into the fine-tuning process, we aim to retain essential linguistic information from the PLM head, ensuring the model remains invariant to specific tasks. This integration involves defining an appropriate cost matrix, a key challenge that we address by proposing a novel construction tailored to our method. Our approach ensures effective alignment between the PLM head and the classifier's output, leveraging OT to enhance the model's performance and robustness across various tasks while preserving the PLM's inherent linguistic knowledge.

=====END FILE=====

=====FILE: sections/background.tex=====
\section{Background}
\subsection{Event Detection}
Following previous works, we formalize Event Detection as a Span-based Classification task \citep{nguyen2015,lu2018,man2020}. Given an input instance  consisting of a -token context sequence , a start index , and an end index , an ED model has to learn to assign the text span  into a label  from a set of pre-defined event types, or NA if  does not trigger a known event.

Generally, we use a language model  to encode the context sequence  into contextualized representation . Then, a classifier is utilized to classify the representation of the trigger span:
\begin{equation}
h=[w_{s}^{\prime},w_{e}^{\prime}]
\end{equation}
\begin{equation}
p(y|x) = \text{Softmax}(\text{Linear}(\text{FNN}(h)))
\end{equation}
Here, FNN denotes a feed-forward neural network,  is the concatenation operation,  is the hidden vector representing , and  models the probability of predicting  from the input.

The model is trained on a dataset  using cross-entropy loss:
\begin{equation}
\mathcal{L}*{C}(\mathcal{D})=-\frac{1}{|\mathcal{D}|}\sum*{(x,y)\in\mathcal{D}}\log~p(y|x)
\end{equation}
To mitigate the imbalance between the number of event triggers and the number of NA spans, we re-weight the loss with a hyperparameter :
\begin{equation}
\mathcal{L} = \gamma \mathcal{L}*C(\mathcal{D}*{NA}) + (1-\gamma)\mathcal{L}*C(\mathcal{D} \setminus \mathcal{D}*{NA})
\end{equation}
where  is the set of NA instances.

\subsection{Continual Event Detection}
The training data in CED is not static but arrives sequentially as a stream of  non-overlapping tasks . At each timestep , the  task data only covers a set of event types , which is a subset of the full ontology of event types . Here, unseen events and negative instances (i.e. text spans that do not trigger any event) are treated as NA.

After training on , the model is expected to be able to detect all seen events thus far, i.e. . To this end, we employ two commonly used techniques in Rehearsal-based Continual Learning: Naive Replay, and Knowledge Distillation \citep{hinton2015}. Let  be the replay buffer up to task , the Replay Loss and Knowledge Distillation loss are written as follows:
\begin{equation}
\mathcal{L}*{R}=-\frac{1}{|\mathcal{R}*{t-1}|}\sum_{(h,y)\in \mathcal{R}*{t-1}}\log~p^{t}(y|h)
\end{equation}
\begin{equation}
\mathcal{L}*{D} = \sum_{(h,y) \in \mathcal{R}_{t-1}} - p^{t-1}(y|h) \log p^{t}(y|h)
\end{equation}
where  denotes the probability of predictions given by the model instance at timestep .

=====END FILE=====

=====FILE: sections/method.tex=====
\section{Lifelong Event Detection via Optimal Transport}
We incorporate Optimal Transport (OT) as a foundational element of our methodology. OT is a mathematical framework designed to compute the distance between two probability distributions with different supports.

In our methodology, OT is applied to align the probability distribution output of the classifier head with the distributional characteristics inherent in the vocabulary of the Pre-trained Language Model (PLM) head. The softmax class probabilities from the classifier head are transported to closely match the pre-trained distribution, facilitating a seamless integration of task-specific knowledge while minimizing the divergence from the model's pre-existing linguistic understanding.

We forward each event trigger through a pre-trained language model and its original language modeling head, and obtain a distribution over a dictionary of  words:
\begin{align*}
x_{s} &= \text{Softmax}(\text{LMH}(w_{s}^{\prime})/\tau) \
x_{e} &= \text{Softmax}(\text{LMH}(w_{e}^{\prime})/\tau) \
\tilde{x} &= (x_{s}+x_{e})/2
\end{align*}
where LMH is a pre-trained language model head,  is temperature coefficient, and  is distribution of the event trigger over dictionary.

Each event trigger is associated with a distribution over  classes: , where each entry indicates the probability that the event trigger belongs to a class in the ontology. An encoder is employed to generate  from the input, defined as , where  represents the parameters of the neural network as described in Section 2.1.

Given that  and  are distributions with different supports for the same event trigger, we aim to train the model by minimizing the following Optimal Transport (OT) distance to push  towards :
\begin{equation}
d_M(\tilde{x}, p) = \min_{P \in U(\tilde{x}, p)} \langle P, M \rangle
\end{equation}
Directly optimizing Eq. (7) poses a time-consuming challenge. To address this, an entropic-constrained regularized optimal transport (OT) distance is introduced, known as the Sinkhorn distance:
\begin{equation}
s_{M}(\tilde{x},p) := \min_{P\in U(\tilde{x},p)}\langle P,M\rangle-H(P)
\end{equation}
where the entropy function of the transport plan  is the regularizing function \citep{cuturi2013}.

The cost matrix  is a trainable variable in our model. To overcome the challenge of learning the cost function, we propose a specific construction for :
\begin{equation}
m_{vc}=1-\cos(e_{v},g_{c})
\end{equation}
where  represents the cosine similarity, and  and  are the embeddings of class  and word , respectively. After training on one task, the learned class embeddings are frozen. We then expand the size of the class embeddings and train the newly initialized embeddings on the new task.

\citet{frogner2015} further suggested combining the OT loss with a conventional cross-entropy loss to better guide the model. By parameterizing  with , the collection of class embeddings, the final OT objective function is expressed as:
\begin{equation}
\mathcal{L}*{OT} = \min*{\theta,G}[s_{M}(\tilde{x},p) - \epsilon \tilde{x} \log \phi(p)]
\end{equation}
To maintain the consistency of class representations across tasks, an additional regularization term enforces the proximity of class representations in the current task to those in the most recent task:
\begin{equation}
\mathcal{L}*{G}=||G*{t}-G_{(t-1)}||^{2}
\end{equation}
Finally, we can write our final objective function:
\begin{equation}
\mathcal{L}=\mathcal{L}*{C}+\mathcal{L}*{R}+\mathcal{L}*{D}+\mathcal{L}*{OT}+\alpha\mathcal{L}_{G}
\end{equation}
where  is the regularization coefficient.

where  denotes the Frobenius inner product; the cost matrix  captures semantic distances between class  and word , with each entry  signifying the importance of words in the corresponding class;  denotes the transport plan; and  is defined as the set of all viable transport plans. Considering two discrete random variables  and , where the transport plan  becomes a joint probability distribution of , i.e., , the set  encompasses all possible joint probabilities that satisfy the specified constraints, forming a transport polytope.

\subsection*{Avoiding Catastrophic Forgetting}
Similar to many CED baselines, our method incorporates a replay process. However, our approach to constructing the memory buffer is distinct. For each class in the training data, we retain the prototype mean  and diagonal covariance  of its trigger representations encountered by the model, rather than storing explicit data samples. During replay, synthetic samples are generated from these prototypes and combined with the replay buffer  to form the effective buffer . This modified buffer replaces  in the computation of  (5) and  (6).

=====END FILE=====

=====FILE: sections/experiments.tex=====
\section{Experiments}
\subsection{Settings}
\paragraph{Datasets} We employ two datasets in our experiments: ACE 2005 \citep{walker2006} and MAVEN \citep{wang2020}; both are preprocessed similar to \citet{yu2021}'s work. To ensure fairness, we rerun all baselines on the same preprocessed datasets. The detailed statistics of the two datasets can be found in Appendix A.2.

\paragraph{Experimental Settings} We adopt the Oracle negative setting, as mentioned by \citet{yu2021}, to evaluate all methods in continual learning scenario. This setting involves excluding the learned types from previous tasks in the training set of the new task, except for the NA (Not Applicable) type. Labels for future tasks are treated as NA type. Assessments are conducted using the exact same task permutations as in \citet{yu2021}'s work. The performance metric is the average terminal F1 score across 5 permutations after each task.

Recently, \citet{le2024a} introduced a multi-objective optimization method that is compatible with our proposed LEDOT approach. To examine the impact of LEDOT on SharpSeq, we conducted an experiment referred to as LEDOT+SharpSeq. For details on other baselines and the integration of LEDOT with SharpSeq, please refer to Appendix A.1.

\subsection{Experimental Results}
Table \ref{tab:main_results} showcases the impressive results of our proposed method, LEDOT, compared to state-of-the-art baselines in continual event detection. On both the MAVEN and ACE datasets, LEDOT consistently achieves higher F1 scores, surpassing most baseline methods. When combined with SharpSeq, LEDOT further enhances performance, increasing the F1-score by a significant margin of 1.22% on MAVEN and 1.67% on ACE after five tasks.

\begin{table*}[ht]
\centering
\caption{Classification F1-scores (%) on 2 datasets MAVEN and ACE. Upperbound indicates the theoretical maximum achievable performance when BERT is frozen.}
\label{tab:main_results}
\resizebox{\textwidth}{!}{
\begin{tabular}{l ccccc ccccc}
\toprule
& \multicolumn{5}{c}{MAVEN} & \multicolumn{5}{c}{ACE} \
\cmidrule(lr){2-6} \cmidrule(lr){7-11}
Task & 1 & 2 & 3 & 4 & 5 & 1 & 2 & 3 & 4 & 5 \
\midrule
BIC & 63.16 & 55.51 & 53.96 & 50.13 & 49.07 & 55.88 & 58.16 & 61.23 & 59.72 & 59.02 \
KCN & 63.16 & 55.73 & 53.69 & 48.86 & 47.44 & 55.88 & 58.55 & 61.40 & 59.48 & 58.64 \
KT & 62.76 & 58.49 & 57.46 & 55.38 & 54.87 & 55.88 & 57.29 & 61.42 & 60.78 & 59.82 \
EMP & 66.82 & 58.02 & 58.19 & 55.07 & 54.52 & 59.05 & 57.14 & 55.80 & 53.42 & 52.97 \
ESCO & 67.50 & 61.37 & 60.65 & 57.43 & 57.35 & [ILLEGIBLE] & [ILLEGIBLE] & [ILLEGIBLE] & [ILLEGIBLE] & [ILLEGIBLE] \
SCR & 76.52 & 57.97 & 57.89 & 52.74 & 53.41 & 75.24 & 63.30 & 61.07 & 55.05 & 55.37 \
SharpSeq & 62.28 & 61.85 & 62.92 & 61.31 & 60.27 & 56.47 & 56.99 & 64.44 & 62.47 & 62.60 \
LEDOT-OT & 63.34 & 59.89 & 59.28 & 56.24 & 55.20 & 58.74 & 58.08 & 61.81 & 58.32 & 59.76 \
LEDOT-R & 63.01 & 60.16 & 59.76 & 56.75 & 54.59 & 58.30 & 58.60 & 63.14 & 58.82 & 60.18 \
LEDOT-P & 63.01 & 59.95 & 59.32 & 56.10 & 55.21 & 59.95 & 56.63 & 62.09 & 60.08 & 61.41 \
LEDOT & 62.98 & 60.47 & 60.78 & 58.53 & 57.53 & 58.30 & 59.69 & 63.52 & 61.05 & 63.22 \
LEDOT + SharpSeq & 63.30 & 61.97 & 63.00 & 61.81 & 61.49 & 60.15 & 59.73 & 64.55 & 63.65 & 64.27 \
\midrule
Upperbound & / & / & / & / & 64.14 & / & / & / & / & 67.95 \
\bottomrule
\end{tabular}
}
\end{table*}

We also conduct further ablation studies to evaluate variants of LEDOT: LEDOT-OT (without Optimal Transport), LEDOT-R (without the replay set), and LEDOT-P (without prototype latent representations). Even without prototype rehearsal, LEDOT-P with OT surpasses the replay-based baseline KT by 0.34% on MAVEN and 1.59% on ACE. Moreover, LEDOT outperforms LEDOT-OT, highlighting the crucial role of OT in preventing catastrophic forgetting. Specifically, OT improves F1 scores by 2.33% on MAVEN and 3.46% on ACE. These results emphasize the importance of OT in mitigating catastrophic forgetting in continual event detection.

=====END FILE=====

=====FILE: sections/conclusion.tex=====
\section{Conclusion}
Harnessing the inherent linguistic knowledge from pre-trained language modeling heads in encoder-based language models play a pivotal role in enhancing performance in downstream tasks. With the introduction of LEDOT, we present a novel approach utilizing optimal transport to align the learning of each task with a common reference the pre-trained distribution of the vocabulary. This alignment mitigates overfitting to the current task and effectively addresses the challenge of catastrophic forgetting. Our method, demonstrating superior performance across various benchmarks, stands as a testament to the effectiveness of leveraging pre-trained language modeling heads for continual event detection, offering a promising avenue for future research in this domain.

In the future, we plan to extend our method to solve continual learning challenges for other information extraction tasks, such as event-event relation extraction \citep{man2024b,man2024a}.

\section*{Limitations}
Being an empirical study into the effectiveness of Optimal Transport in aligning the output distribution of Continual Event Detection models, our work is not without limitations. We acknowledge this, and would like to discuss our limitations as follows:

The method proposed in this paper is orthogonal to the tasks of interest and the specific techniques to solve them. With that being said, our method is applicable to a wide range of information extraction tasks, such as Named Entity Recognition, and Relation Extraction, as well as other text classification tasks, such as Sentiment Analysis. However, given limited time and computational resources, we limit the scope of our experiments to only Event Detection. The extent to which our proposed method can work with other NLP problems can be an interesting topic that we leave for future work.

Nevertheless, our experimental results suggest that using Optimal Transport to align the output distribution of the model with the pre-trained language modeling head has the potential to improve continual learning performance on other problems as well.

\begin{itemize}
\item This paper presents the empirical results of our LEDOT method using a pre-trained encoder language model (i.e. BERT) as the backbone.
\item Meanwhile, large decoder-only language models, with their heavily over-parameterized architectures, amazing emergent ability, and great generalization capability, have emerged and become the center of focus of NLP research in recent years. Though they have proved to be able to understand language and solve almost all known NLP tasks without needing much fine-tuning, many studies \citep{lai2023,qiu2024,zhong2023} suggested that even the largest models like ChatGPT \citep{ouyang2022} still lag behind smaller but specialized models such as BERT \citep{devlin2019} and T5 \citep{raffel2023} by a significant margin on tasks like Event Detection.
\end{itemize}

We thus believe that studies on the applications of encoder language models in Continual Event Detection are still needed.

=====END FILE=====

=====FILE: figures/README.txt=====
[IMAGE NOT PROVIDED]
This folder should contain figure images.
No images were extracted from the PDF content fetcher.
=====END FILE=====