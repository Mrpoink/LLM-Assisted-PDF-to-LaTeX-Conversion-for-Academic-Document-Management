=====FILE: main.tex=====
\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\usepackage{amsmath, amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{hyperref}
\usepackage{natbib}

\geometry{left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm}

\title{Multi-pass Decoding for Grammatical Error Correction}
\author{
Xiaoying Wang$^{1}$ \and Lingling Mu$^{1}$ \and Jingyi Zhang$^{2}$ \and Hongfei Xu$^{1}$\thanks{Corresponding author: Hongfei Xu. Email: hfxunlp@foxmail.com} \
$^{1}$Zhengzhou University, Henan, China \
$^{2}$Hasso Plattner Institute, University of Potsdam, Germany \
\texttt{{xywangnlp@gs, iellmu@}zzu.edu.cn} \
\texttt{Jingyi.Zhang@hpi.de}
}
\date{}

\begin{document}

\maketitle

\begin{abstract}
Sequence-to-sequence (seq2seq) models achieve comparable or better grammatical error correction performance compared to sequence-to-edit (seq2edit) models. Seq2edit models normally iteratively refine the correction result, while seq2seq models decode only once without aware of subsequent tokens. Iteratively refining the correction results of seq2seq models via Multi-Pass Decoding (MPD) may lead to better performance. However, MPD increases the inference costs. Deleting or replacing corrections in previous rounds may lose useful information in the source input. We present an early-stop mechanism to alleviate the efficiency issue. To address the source information loss issue, we propose to merge the source input with the previous round correction result into one sequence. Experiments on the CoNLL-14 test set and BEA-19 test set show that our approach can lead to consistent and significant improvements over strong BART and T5 baselines (+1.80, +1.35, and +2.02  for BART 12-2, large and T5 large respectively on CoNLL-14 and +2.99, +1.82, and +2.79 correspondingly on BEA-19), obtaining  scores of 68.41 and 75.36 on CoNLL-14 and BEA-19 respectively.
\end{abstract}

\section{Introduction}
Grammatical Error Correction (GEC) aims to correct grammatical errors in the given sentence \citep{Ng2013, Ng2014}. Nowadays, there are two mainstream GEC approaches. Sequence-to-edit (seq2edit) methods regard GEC as a sequence tagging task, where the model predicts edit tags (e.g., keep, delete, insert, replace, etc.) for each token iteratively for multiple rounds until all tokens are assigned the keep tag \citep{Malmi2019, Stahlberg2020, Omelianchuk2020, Yuan2021}. Seq2edit methods normally require to correct for a number of correction rounds to complete the correction. In contrast, Sequence-to-sequence (seq2seq) approaches consider the GEC task as Machine Translation (MT) from ungrammatical texts to grammatical texts \citep{Zhao2019, Kiyono2019, Wang2021, Li2022, Fang2023a}. The seq2seq model encodes the input sentence and auto-regressively decodes the corrected sentence. Current methods normally utilize the pre-trained models for better performance, such as BERT \citep{Devlin2019} and XLNet \citep{Yang2019} for seq2edit \citep{Omelianchuk2020}, and BART \citep{Lewis2020} and T5 \citep{Raffel2020} for seq2seq \citep{Kaneko2020, Liu2021}.

Seq2seq models lead to comparable or better performance than seq2edit approaches without using language-specific edit operations. However, current seq2seq GEC studies typically decode only once without aware of subsequent tokens. Multi-Pass Decoding (MPD) may enhance the performance through iterative refinements \citep{Ge2018}. Training MPD models to generate the gold reference given its correction results may also benefit its learning via self-correction \citep{Li2021}.

Multi-pass decoding leads to two problems: 1) iterative decoding increases the inference computational costs, and 2) deleting or replacing in previous correction rounds may incur information loss. We propose to introduce an early-stop mechanism to alleviate the efficiency issue. It takes the hidden representation of the end-of-sentence token (\texttt{<eos>}) as input, and stops MPD in cases: 1) the next round's correction result matches the current correction result, or 2) the next round's correction result has a larger edit distance to the reference. As for the information loss issue, we present methods to merge the source sentence and the previous round's correction output into a single sequence, as pre-trained models normally do not have multiple encoders for more than one inputs. We evaluate our approach on the CoNLL 2014 and BEA 2019 GEC shared tasks, and obtain significant improvements over the strong BART and T5 baselines, showing the effectiveness of our method.

\begin{itemize}
\item To improve the efficiency of multi-pass decoding, we present an early-stop mechanism to terminate the multi-pass decoding when the next decoding round would not lead to better correction result.
\item We propose source information fusion methods to address the information loss issue due to deleting or replacing edit operations in preceding correction rounds, and present comparison-based sequence merging approach to ensure the efficiency of source information fusion.
\item Our method brings about +1.80, +1.35, and +2.02  improvements over the strong BART 12-2, large and T5 baselines respectively on CoNLL-14 test set, and +1.75, +1.64, and +2.99, +1.82, and +2.79 correspondingly on the BEA-19 test set, showing the effectiveness of our approach.
\end{itemize}

\section{Preliminaries: Sequence-to-sequence GEC}
The seq2seq model  comprises an encoder and a decoder. It takes the input sequence  to correct, and generates the corrected sequence . The encoder takes the input sequence , and computes the contextual hidden state vectors :
\begin{equation}
h_{e} = \text{encoder}(x)
\end{equation}
The decoder generates the hidden state  based on the encoder hidden states  and the decoding history :
\begin{equation}
h_{d}^{k} = \text{decoder}(h_{e}, \hat{x}^{<k})
\end{equation}
where  is the -th token in the sequence.  is the start-of-sequence token \texttt{<sos>}.  means the token sequence from  to . The decoder classifier conditions on the decoder hidden state  and predicts the probability of each token in the vocabulary. The decoder selects the token with the highest probability for subsequent decoding steps:
\begin{equation}
\hat{x}^{k} = \text{classifier}(h_d^k)
\end{equation}

\begin{algorithm}[h]
\caption{Multi-pass decoding with early-stop}
\begin{algorithmic}[1]
\State \textbf{Input:} Input sentence to correct , GEC model , early-stop classifier , maximum number of decoding rounds , early-stop threshold ;
\State \textbf{Output:} Corrected sentence .
\State ;
\State ;
\If{}
\State ;
\Else
\For{ to }
\State ;
\State ;
\State ;
\If{ or }
\State \textbf{break};
\EndIf
\EndFor
\EndIf
\State \textbf{return} 
\end{algorithmic}
\end{algorithm}

The decoder repeats this process until the classifier produces the end-of-sequence token (\texttt{<eos>}) given the hidden state . Pre-training by reconstructing the corrupted text can compress the knowledge of large-scale corpus into model parameters. And fine-tuning pre-trained models (such as BART and T5) for GEC can lead to better performance \citep{Sun2021, Rothe2021}.

\section{Our Method}

\subsection{Multi-pass Decoding with Early-stop}
In the GEC task, the seq2seq GEC model  takes the input sentence  that might be incorrect, and generates the corrected sentence . Instead of using  as the final result, multi-pass decoding iteratively repeats the correction process, by feeding the correction result of the previous round  into the model and asking the model to correct  into , until . The termination condition involves decoding the same sequence twice. This increases the computational costs for inference while improving the performance. We train an early-stop mechanism together with the seq2seq model to address this issue.

The early-stop mechanism introduces a lightweight logistic regression classifier  to predict the probability of stopping the multi-pass decoding.  consists of a weight vector  and a bias scalar . During the decoding of , we take the decoder hidden representation  of the special end-of-sentence token (\texttt{<eos>}) to compute the early-stop probability:
\begin{equation}
p_{e} = \sigma(h_{t-1}^{\texttt{<eos>}} \cdot w_{e} + b_{e})
\end{equation}
where `$\cdot$'' and `'' are dot-product and sigmoid. We optimize the Binary Cross Entropy (BCE) loss between  and the early-stop label :
\begin{equation}
l_{e} = \text{BCE}(p_{e}, y_{e})
\end{equation}

In MPD training, we first decode , and label  of the previous decoding round based on ,  and the gold GEC reference .  is true if: 1)  equals to , or 2) the edit distance between  and  is larger than that with . The edit-distance condition aims to ensure that multi-pass decoding will not deteriorate the performance. To provide the training label of the current decoding round for the early-stop classifier , the decoding result of the next round  is always generated during training, to compare the edit distances between the reference with the current round decoding result  and the next round decoding result .

The training loss is the weighted combination of the original seq2seq generation loss  and :
\begin{equation}
l = l_{seq2seq} + \lambda * l_{e}
\end{equation}
We use Algorithm 1 for inference. We use a maximum number of decoding rounds  of 3, and early-stop if  or .  and  are default to 1 and 0.5 respectively. A  of 1 treats the correction task and the early-stop classifier equally during training. A threshold of 0.5 indicates to early-stop if the probability is larger than 0.5, which is reasonable for the binary classification task. The number of decoding rounds is tested on the development set, and using a value larger than 3 does not lead to better performance. We did not carefully tune  and  despite this may lead to better performance.

\begin{figure}[htbp]
\centering
\fbox{
\begin{minipage}{0.9\textwidth}
\centering
\vspace{1cm}
IMAGE NOT PROVIDED
\vspace{1cm}
\end{minipage}
}
\caption{Source information fusion. Example: Source "We go to the orchard and brought apples but forget pears". Decode () "We go to the orchard and bring apples but forget pears". Merge "We go to the orchard and brought bring apples but forget pears". Edit tags are assigned.}
\label{fig:fusion}
\end{figure}

\subsection{Source Information Fusion during Iterative Correction}
If the model deletes or replaces tokens in previous rounds, the original tokens are infeasible for thereafter correction rounds, even they might be valuable references for subsequent correction rounds. As shown in the example in Figure \ref{fig:fusion}, the model requires to correct: `We go to the orchard and brought apples, but forget pears.'' to: `We go to the orchard and buy apples, but forget pears.'' The model only fixes the tense of the verb `brought'' by replacing it with `bring'' in the first round. When the model correcting the semantic meaning of the verb `bring'' in the second round, choosing from `pick'' and `buy'' could be hard if it is not aware of the existence of the wrong verb `brought'' in the source input. Despite `brought'' is wrongly spelt, it encourages the model to select `buy'' instead of `pick'', as the past tense of `buy'' (`bought'') is closer to `brought'' than the past tense of `pick'' (`picked'').

Thus, keeping all source tokens feasible in all correction rounds may benefit the performance. But pre-trained seq2seq models normally do not have multiple encoders for both the source sentence  and the decoding result of the previous correction round . Concatenating  and  as the input of the encoder results in long and redundant sequences. The unchanged tokens also have two distant positions in the concatenated sequence. To encode  and  efficiently with the single encoder, we propose to merge  and  into a single sequence, as shown in Figure \ref{fig:fusion}. Specifically, we first compare  with  then extract the common and different segments, and finally merge the segments into a single sequence according to their orders in corresponding sequences. The merged sequence contains unchanged tokens, inserted tokens and deleted tokens with their original orders. Replacing can be regarded as an insertion plus a deletion.

We use edit tags or separated position encodings to distinguish tokens in the merged sequence. For edit tags, we use `e'' (equal), `d'' (delete) and ``i'' (insert) to represent the tokens' roles in the merged sequence, standing for tokens in both  and , appearing only in , and newly added to  respectively. We add an embedding layer for edit tags and add the edit embeddings to the word embeddings of the seq2seq model before encoder layers.

For position encoding, we use 2 position labels for the merged sequence: source position stands for the token's position in  and decode position for its position in . The position of the token is 0 if it does not appear in the sequence. To mitigate the gap between the new position embeddings and pre-trained models, the new position embeddings are initialized based on the pre-trained position embeddings. But we reduce the weights of position embeddings by half. This is because position embeddings are added twice when using the merged sequence as the input: once for the source position and another for the decode position.

\section{Experiments}
\subsection{Settings}
To test the effectiveness of our approach, we conducted experiments using the strong BART (12-2), BART (12-12) and T5 large baselines, and strictly followed the settings of \citet{Yakovlev2023} for data processing and BART fine-tuning. We used the same data set as \citet{Yakovlev2023}, and the models were fine-tuned for 3 stages following \citet{Omelianchuk2020}. Our Multi-Pass Decoding (MPD) method was only applied in the last stage. As this is more efficient than applying to all stages, and the model may produce more reasonable correction results ( is normally no worse than  compared to ) after the second stage. The original GEC training loss () was still kept. We implemented our approaches based on the Neutron implementation \citep{Xu2019} of the Transformer.

We evaluated on the CoNLL 2014 test set \citep{Ng2014} with M2 scorer \citep{Dahlmeier2012} and the BEA 2019 test set, and validated on the BEA 2019 (W&I+L) development set, and reported precision (P), recall (R) and  scores following common practices. Despite all these datasets are in English, they are widely used by the community, and we suggest that our approaches are language-agnostic and can be easily adapted to the other languages, as verified in Section 4.5.

\subsection{Main Results}
Based on the ablation studies, the MPD training only used single-pass decoding results, and the inference was multi-pass with early-stop (Section 4.3). We used both edit tags and position encoding for source information fusion (Section 4.4). Results on the CoNLL 2014 test set and BEA 2019 test set are shown in Tables \ref{tab:main_results} and \ref{tab:bea_dev} respectively.

Table \ref{tab:main_results} shows that: 1) the performance of the powerful LLaMa 2-7B Large Language Model (LLM) is far behind fine-tuned seq2edit and seq2seq methods even after fine-tuning, and 2) MPD can significantly and consistently improve the performance of all our baselines with different model sizes and settings (+1.80, +1.35 and +2.02  over BART 12-2, BART 12-12 and T5 large respectively). Results in Table \ref{tab:bea_dev} on the BEA-19 development set are also consistent. Although we only applied our methods to the widely used BART and T5 baselines, we suggest that our method is likely to bring about further improvements with more advanced baseline models.

\begin{table*}[t]
\centering
\caption{Main results. "*" and "\dag" denote our replication and using additional datasets respectively. BART (12-2) means the BART model with 12/2 encoder/decoder layers.}
\label{tab:main_results}
\resizebox{\textwidth}{!}{
\begin{tabular}{lcccccc}
\toprule
\multirow{2}{*}{Method} & \multicolumn{3}{c}{CoNLL 2014 (test)} & \multicolumn{3}{c}{BEA 2019 (test)} \
& P & R &  & P & R &  \
\midrule
LLaMa 2-7B (zero-shot) & 27.41 & 42.24 & 29.48 & 45.85 & 53.58 & 47.21 \
LLaMa 2-7B (fine-tune) & 65.64 & 41.81 & 58.92 & 66.28 & 49.15 & 61.96 \
\midrule
\multicolumn{7}{l}{\textit{Seq2edit}} \
PIE & 66.1 & 43.0 & 59.7 & 65.5 & 59.4 & 64.2 \
Lichtarge et al. (2019) & 66.7 & 40.6 & 59.8 & 72.3 & 61.4 & 69.8 \
Kiyono et al. (2019) & 72.4 & 46.1 & 65.0 & 68.8 & 63.4 & 67.7 \
Kaneko et al. (2020) & 72.6 & 46.4 & 65.2 & 79.2 & 53.9 & 72.4 \
ERRANT tags & 63.0 & 45.6 & 58.6 & 60.8 & 50.8 & 58.5 \
GECTOR & 77.5 & 40.1 & 65.3 & 79.4 & 54.5 & 72.8 \
Yuan et al. (2021) & 60.4 & 39.0 & 54.4 & 80.70 & 53.39 & 73.21 \
GST & 78.4 & 39.9 & 65.7 & 81.33 & 51.55 & 72.91 \
Tarnavskyi et al. (2022) & 76.1 & 41.6 & 65.3 & 61.8 & 52.1 & 59.5 \
Lai et al. (2022) & 70.73 & 43.88 & 63.01 & - & - & - \
LET & 61.2 & 40.9 & 55.6 & - & - & - \
\midrule
\multicolumn{7}{l}{\textit{Seq2seq}} \
Zhao et al. (2019) & 71.6 & 38.7 & 61.2 & 79.4 & 55.0 & 72.06 \
T5 large & 78.0 & 40.6 & 66.1 & 75.1 & 65.5 & 72.9 \
BIFI & 74.7 & 49.0 & 65.8 & 68.3 & 57.1 & 72.9 \
SynGEC & 69.2 & 49.8 & 67.6 & 73.5 & 55.9 & 65.6 \
BART (12-2) & 70.3 & 48.2 & 64.2 & 74.68 & 60.27 & 69.1 \
AMR-GEC & 71.62 & 48.74 & 64.4 & 65.10 & 32.29 & 71.27 \
BTR & 65.10 & 32.29 & 65.47 & 72.9 & 53.2 & 54.11 \
Cao et al. (2023a) & 73.2 & 37.8 & 54.11 & 76.8 & 64.8 & 67.9 \
GEC-DePenD & 74.8 & 50.0 & 61.6 & 77.1 & 66.7 & 74.1 \
TemplateGEC & 74.7 & 51.6 & 68.1 & 78.8 & 68.5 & 74.8 \
TransGEC & 75.0 & 53.2 & 68.6 & - & - & - \
Multimodal-GEC & 75.0 & 53.8 & 69.3 & - & - & 76.5 \
unsupervised GEC & - & - & 69.6 & - & - & - \
\midrule
BART (12-2)* & 72.56 & 44.73 & 64.53 & 69.62 & 63.56 & 68.32 \
\textbf{+ MPD} & \textbf{73.70} & \textbf{47.39} & \textbf{66.33} & \textbf{72.98} & \textbf{65.35} & \textbf{71.31} \
BART (12-12) & 72.04 & 52.55 & 67.06 & 73.14 & 64.65 & 71.27 \
\textbf{+ MPD} & \textbf{74.78} & \textbf{51.08} & \textbf{68.41} & \textbf{75.28} & \textbf{65.46} & \textbf{73.09} \
T5 large & 71.73 & 50.44 & 66.14 & 74.25 & 66.54 & 72.57 \
\textbf{+ MPD} & \textbf{74.77} & \textbf{50.34} & \textbf{68.16} & \textbf{77.81} & \textbf{66.95} & \textbf{75.36} \
\bottomrule
\end{tabular}
}
\end{table*}

\begin{table}[t]
\centering
\caption{Results on the BEA-19 development set.}
\label{tab:bea_dev}
\begin{tabular}{lccc}
\toprule
Method & P & R &  \
\midrule
BART (12-2)* & 69.69 & 50.27 & 64.69 \
\textbf{+ MPD} & \textbf{72.11} & \textbf{50.54} & \textbf{66.44} \
BART (12-12) & 71.62 & 49.73 & 65.82 \
\textbf{+ MPD} & \textbf{71.86} & \textbf{54.20} & \textbf{67.46} \
T5 large & 71.75 & 51.85 & 65.63 \
\textbf{+ MPD} & \textbf{71.69} & \textbf{54.33} & \textbf{67.38} \
\bottomrule
\end{tabular}
\end{table}

\subsection{Ablation Study for MPD Training and Inference}
In addition to training the model to generate the gold reference  given the input , the MPD training also takes the output of the previous decoding round  as the input. The output of the previous decoding round may be either the result of a single decoding round like \citet{Omelianchuk2020}, or the result of several decoding rounds until the inference termination condition. We study the effects of single-round and multi-round decoding for MPD training while using multi-pass decoding with early-stop for inference.

For single-round decoding in MPD training, we use the model to decode  into , and train the model to generate  given  and :
\begin{equation}
M(x, \hat{x}*{0}) \rightarrow r
\end{equation}
For multi-round decoding in MPD training, we start from  as  and iteratively decode  to  for several rounds until meeting the termination condition, and train the model to generate  given  and :
\begin{equation}
M(x, \hat{x}*{i}) \rightarrow r
\end{equation}

We also study the effects of the maximum number of decoding rounds with/without early-stop for MPD inference while using single-round decoding in MPD training. Additionally, we compare our simple early-stop mechanism with the policy network proposed by \citet{Geng2018}. \citet{Geng2018} employ reinforcement learning method to decide the number of decoding rounds based on the differences between the two consecutive decoding passes, and optimize the BLEU-based reward for machine translation. While in our experiment for the GEC task, we used the  score as the reward instead of BLEU.

To analyze the inference efficiency of our approach, we compare our method with the BART (12-4) baseline with vanilla fine-tuning and the ensemble of 2 vanilla BART (12-2) models initialized with different random seeds \citep{Tarnavskyi2022}. Both the BART (12-4) setting with 4 decoder layers and the ensemble can lead to better performance but slower inference speed compared to the BART (12-2) baseline.

Results in Table \ref{tab:ablation} show that: 1) for MPD training, both settings obtain similar performance, but the single-round decoding setting achieves slightly higher  scores while being more computationally efficient, 2) the performances of different numbers of maximum decoding rounds are also similar, larger  leads to slower inference, but the early-stop mechanism can mitigate this and bring about the best performance, 3) multi-pass decoding based on the policy network can also lead to consistent  improvements on the two shared tasks, but our simple early-stop method is more efficient than the policy network \citep{Geng2018} and leads to higher  scores, and 4) the performance of our MPD method with the BART (12-2) setting achieves better performance than both the BART (12-4) baseline with vanilla fine-tuning and the ensemble of 2 vanilla BART (12-2) models, and it is also faster than the BART (12-4) and the ensemble baselines for inference. This shows that our method can achieve better performance more efficiently.

Previous state-of-the-art multi-pass decoding study for NMT \citep{Geng2018} uses very complex reinforcement learning method to decide the required number of decoding rounds. The reinforcement learning training might be unstable and lead to unstable performances. Our supervised method directly trains the simple binary classifier based on the representation of the decoded sequence. We suggest that our early-stop method is easy to implement and very effective in practice.

\begin{table*}[t]
\centering
\caption{Results of various MPD training and inference settings. Speed is the inference speed on the BEA 2019 dev set.}
\label{tab:ablation}
\resizebox{\textwidth}{!}{
\begin{tabular}{lccccccc}
\toprule
\multirow{2}{*}{Setting} & \multicolumn{3}{c}{BEA 2019 dev} & \multicolumn{3}{c}{CONLL 2014 test} & \multirow{2}{*}{Speed} \
& P & R &  & P & R &  & \
\midrule
BART (12-2) & 69.69 & 50.27 & 64.69 & 72.56 & 44.73 & 64.53 & 1.00x \
BART (12-4) & 70.05 & 50.81 & 65.11 & 73.21 & 45.67 & 65.46 & 0.61x \
BART (12-2)*2 (Ensemble) & 70.15 & 50.73 & 65.16 & 73.48 & 45.98 & 65.50 & 0.49x \
\midrule
\multicolumn{8}{l}{\textit{Training}} \
Single-round & 72.11 & 50.54 & 66.44 & 73.70 & 47.39 & 66.33 & 0.83x \
Multi-round & 71.06 & 52.01 & 66.21 & 73.33 & 47.46 & 66.12 & - \
\midrule
\multicolumn{8}{l}{\textit{Inference}} \
Policy network (2018) & 71.32 & 50.35 & 65.84 & 73.01 & 46.95 & 65.71 & 0.27x \
without  () & 71.81 & 50.13 & 66.09 & 73.49 & 46.84 & 65.98 & 0.46x \
without  () & 71.39 & 50.34 & 65.88 & 73.04 & 46.91 & 65.72 & 0.41x \
without  () & 71.46 & 50.49 & 65.98 & 73.07 & 47.12 & 65.82 & 0.38x \
with  () & 72.11 & 50.54 & 66.44 & 73.70 & 47.39 & 66.33 & 0.83x \
\bottomrule
\end{tabular}
}
\end{table*}

\subsection{Effects of Source Information Fusion}
We test the effects of different source information fusion methods with the BART (12-2) setting, including: 1) using only  instead of both  and  for MPD inference ("None"), 2) sequence concatenation ("Concat"), 3) edit tags ("Edit"), 4) position encoding ("Pos"), and 5) both edit tags and position encoding ("Pos+Edit"). Results are shown in Table \ref{tab:fusion}.

Table \ref{tab:fusion} shows that: 1) vanilla MPD without source information fusion ("None") can already lead to +0.80 and +1.09  improvements on the BEA-19 development set and the CoNLL-14 test set respectively, showing the effectiveness of multi-pass decoding, 2) source information fusion through sequence concatenation ("Concat") can lead to +0.46 and +0.12  score improvements on the BEA 2019 development set and the CoNLL-14 test set respectively than without source information fusion ("None"), showing the positive effects of source information fusion, 3) both position encoding ("Pos") and edit tags ("Edit") bring about higher  scores than sequence concatenation ("Concat") while being more efficient, empirically showing the advantages of our sequence merging approach, and position encoding consistently brings about slightly better performance than edit tags, probably because of the pre-trained position embedding initialization, and 4) the combination of position encoding and edit tags ("Pos+Edit") leads to the best performance, but the difference is small compared to using only position encoding, probably because position encoding and edit tags provide similar information in denoting the roles of tokens in the two sequences despite in different forms and are complementary to some extent.

\begin{table}[t]
\centering
\caption{Results of source information fusion methods.}
\label{tab:fusion}
\resizebox{\textwidth}{!}{
\begin{tabular}{lcccccc}
\toprule
\multirow{2}{*}{Method} & \multicolumn{3}{c}{BEA 2019 dev} & \multicolumn{3}{c}{CONLL 2014 test} \
& P & R &  & P & R &  \
\midrule
BART (12-2) & 69.69 & 50.27 & 64.69 & 72.56 & 44.73 & 64.53 \
None & 69.66 & 52.82 & 65.49 & 71.30 & 49.77 & 65.62 \
Concat & 70.34 & 52.79 & 65.95 & 72.67 & 47.59 & 65.74 \
Edit & 70.73 & 52.65 & 66.18 & 72.80 & 47.83 & 65.92 \
Pos & 71.06 & 52.45 & 66.36 & 73.22 & 47.47 & 66.05 \
Pos+edit & 72.11 & 50.54 & 66.44 & 73.70 & 47.39 & 66.33 \
\bottomrule
\end{tabular}
}
\end{table}

\subsection{Verification on the Other Language}
We suggest that our approach is language-agnostic. To test its effectiveness on the other languages, we also conducted experiments on Chinese GEC datasets exactly following the experiment settings of \citet{Yang2024}. Specifically, we used the combination of the Lang-8 corpus provided by NLPCC 2018 \citep{Zhao2018b}, the HSK dataset and FCGEC training set \citep{Xu2022} as the training set, MuCGEC development set \citep{Zhang2022a} for validation, and tested on the NLPCC 2018 test set, FCGEC development set and NaCGEC test set \citep{Ma2022}. For evaluation metrics, we follow previous work and report word-level precision (P) / recall (R) / F-measure () performance on NLPCC18-Test using the official MaxMatch scorer \citep{Ng2014} and PKUNLP word segmentation tool. For the FCGEC development set and the NaCGEC test set, we report the character-level P/R/ scores using the ChERRANT scorer \citep{Zhang2022a}.

We use a large Transformer and the pre-trained BART model as the baselines. The batch size is 1024 and the maximum sentence length of training data is 128. The maximum number of training epochs is 20 and 10, respectively, and the beam size is 10. Results are shown in Tables 5 and 6. Tables 5 and 6 show similar phenomena as Tables 1 and 2. Our method also leads to consistent and significant improvements on all Chinese test sets (+2.06, +2.30, and +3.45  score improvements on the NLPCC 2018 test set, FCGEC development set and the NaCGEC test set respectively over the strong BART baseline).

\section{Related Work}
Seq2edit GEC. Seq2edit GEC methods \citep{Malmi2019, Awasthi2019, Stahlberg2020} iteratively assign edit operations to tokens, such as insertion, deletion, replacement, or language-specific transformations \citep{Omelianchuk2020}, etc., and improve the performance with self-correction \citep{Parnow2021}, type-based multi-turn training \citep{Lai2022}, decoupled error detection [MISSING CONTENT]

% \section{Conclusion}
% [MISSING CONTENT]

% \bibliography{refs}
% \bibliographystyle{acl_natbib}
\begin{thebibliography}{99}
\bibitem[Ng et al.(2013)]{Ng2013} Ng et al., 2013.
\bibitem[Ng et al.(2014)]{Ng2014} Ng et al., 2014.
\bibitem[Malmi et al.(2019)]{Malmi2019} Malmi et al., 2019.
\bibitem[Stahlberg and Kumar(2020)]{Stahlberg2020} Stahlberg and Kumar, 2020.
\bibitem[Omelianchuk et al.(2020)]{Omelianchuk2020} Omelianchuk et al., 2020.
\bibitem[Yuan et al.(2021)]{Yuan2021} Yuan et al., 2021.
\bibitem[Zhao et al.(2019)]{Zhao2019} Zhao et al., 2019.
\bibitem[Kiyono et al.(2019)]{Kiyono2019} Kiyono et al., 2019.
\bibitem[Wang et al.(2021)]{Wang2021} Wang et al., 2021.
\bibitem[Li et al.(2022)]{Li2022} Li et al., 2022.
\bibitem[Fang et al.(2023a)]{Fang2023a} Fang et al., 2023a.
\bibitem[Devlin et al.(2019)]{Devlin2019} Devlin et al., 2019.
\bibitem[Yang et al.(2019)]{Yang2019} Yang et al., 2019.
\bibitem[Lewis et al.(2020)]{Lewis2020} Lewis et al., 2020.
\bibitem[Raffel et al.(2020)]{Raffel2020} Raffel et al., 2020.
\bibitem[Kaneko et al.(2020)]{Kaneko2020} Kaneko et al., 2020.
\bibitem[Liu et al.(2021)]{Liu2021} Liu et al., 2021.
\bibitem[Ge et al.(2018)]{Ge2018} Ge et al., 2018.
\bibitem[Li et al.(2021)]{Li2021} Li et al., 2021.
\bibitem[Sun et al.(2021)]{Sun2021} Sun et al., 2021.
\bibitem[Rothe et al.(2021)]{Rothe2021} Rothe et al., 2021.
\bibitem[Yakovlev et al.(2023)]{Yakovlev2023} Yakovlev et al., 2023.
\bibitem[Xu and Liu(2019)]{Xu2019} Xu and Liu, 2019.
\bibitem[Dahlmeier and Ng(2012)]{Dahlmeier2012} Dahlmeier and Ng, 2012.
\bibitem[Geng et al.(2018)]{Geng2018} Geng et al., 2018.
\bibitem[Tarnavskyi et al.(2022)]{Tarnavskyi2022} Tarnavskyi et al., 2022.
\bibitem[Yang and Quan(2024)]{Yang2024} Yang and Quan, 2024.
\bibitem[Zhao et al.(2018b)]{Zhao2018b} Zhao et al., 2018b.
\bibitem[Xu et al.(2022)]{Xu2022} Xu et al., 2022.
\bibitem[Zhang et al.(2022a)]{Zhang2022a} Zhang et al., 2022a.
\bibitem[Ma et al.(2022)]{Ma2022} Ma et al., 2022.
\bibitem[Awasthi et al.(2019)]{Awasthi2019} Awasthi et al., 2019.
\bibitem[Parnow et al.(2021)]{Parnow2021} Parnow et al., 2021.
\bibitem[Lai et al.(2022)]{Lai2022} Lai et al., 2022.
\end{thebibliography}

\end{document}

=====END FILE=====