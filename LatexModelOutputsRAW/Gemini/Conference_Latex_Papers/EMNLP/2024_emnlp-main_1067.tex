ACK

=====FILE: main.tex=====
\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{times}
\usepackage{latexsym}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage{geometry}

% Adjust margins to approximate standard NLP paper format (e.g. ACL)
\geometry{left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm}

\title{Comparing a BERT Classifier and a GPT classifier for Detecting Connective Language Across Multiple Social Media}

\author{
Josephine Lukito\textsuperscript{1}, Bin Chen\textsuperscript{2}, Gina M. Masullo\textsuperscript{1}, and Natalie Jomini Stroud\textsuperscript{1} \
\textsuperscript{1}Center for Media Engagement, University of Texas at Austin \
\textsuperscript{2}The University of Hong Kong
}

\date{}

\begin{document}

\maketitle

\begin{abstract}
This study presents an approach for detecting connective language defined as language that facilitates engagement, understanding, and conversation-from social media discussions. We developed and evaluated two types of classifiers: BERT and GPT-3.5 turbo. Our results demonstrate that the BERT classifier significantly outperforms GPT-3.5 turbo in detecting connective language. Furthermore, our analysis confirms that connective language is distinct from related concepts measuring discourse qualities, such as politeness and toxicity. We also explore the potential of BERT-based classifiers for platform-agnostic tools. This research advances our understanding of the linguistic dimensions of online communication and proposes practical tools for detecting connective language across diverse digital environments.
\end{abstract}

\section{Introduction}

The growth and popularity of social media over the past two decades has created many opportunities for natural language processing and computational social science researchers to study short-form text. During this time, researchers have built a wide variety of text classifiers to understand social media posts, including for sentiment analysis (Wang et al., 2018), discrete emotion detection (Bakkialakshmi and Sudalaimuthu, 2022), life events identification (Cavalin et al., 2015), and even depression detection (Hosseini-Saravani et al., 2020).

Overwhelmingly, these efforts have focused on negative or unwanted online content. For example, research efforts have focused on the identification of misinformation, disinformation, or bot activity (Latha et al., 2022; Su et al., 2020; Srinivas et al., 2021). Similarly, there are hundreds of studies discussing NLP classifiers for malicious (Gharge and Chavan, 2017) or toxic language (Garlapati et al., 2022). At face value, the emphasis on building classifiers for unwanted content makes sense: one very common use case for NLP classifiers is to identify content for removal, whether it be spam messages (Garg and Girdhar, 2021) or content seen as toxic (Babakov et al., 2024).

And yet, there is little discussion regarding what desired language on social media would look like. Although NLP research has focused a great deal on building classifiers to remove unwanted content on social media, it has paid less attention to classifiers that detect wanted or desired content. To fill this gap, we advocate for and build a classifier for one such language feature: connectivity.

We define connective language as language features that express a willingness to talk with people who are not ideologically aligned, such as expressions of intellectual humility or openness to other perspectives. As we explain, connectivity is an essential aspect of human communication, and recent social science research highlights the importance of connective language to facilitate pro-democratic conversations (Overgaard et al., 2022). This research suggests that connective language can help facilitate discussion (Overgaard et al., 2021), empower citizens (Iranzo-Cabrera and Casero-Ripollés, 2023), and contribute to a healthier public square.

A connective language classifier could be used in multiple ways, such as allowing users to filter or sort content, awarding a badge to users employing the language, or recommending content on a platform. These use cases could help people identify others who are interested in having thoughtful exchanges.

Drawing from the literature in communication research and in natural language processing, this paper introduces and illustrates the use of a multi-platform connective language classifier. First, we build a human-labeled training set using a mix of social media messages from Reddit, Twitter, and Facebook. We use this novel training dataset to build a BERT classifier and a Generative AI (GPT-3.5 Turbo) classifier for connective language. Finally, we compare the connective language classifier to concepts for which there are existing classifiers, such as politeness, to show how they are semantically distinct.

\section{Related Work}

\subsection{Pro-Democratic NLP Efforts}
Given how much language and conversation, both political or otherwise, that occurs online and through digital platforms, natural language processing is increasingly important for pro-democratic efforts, from studying free speech efforts (Dore et al., 2023) and improving public service accessibility (Mariani et al., 2022) to encouraging citizen participation (Arana-Catania et al., 2021).

One pivotal area of NLP research is political opinion and information detection (Sen et al., 2020; Falk and Lapesa, 2022). These efforts can be used to decrease political animosity (Jia et al., 2024) and increase contact with different perspectives on a political issue (Reuver et al., 2021). While acknowledging that language models may themselves have political biases (Gover, 2023), they nevertheless can help citizens sort through the overwhelming amount of content now produced online.

\subsection{Polite, Civil, and Deliberative Language}
Identifying quality discourse has been a key feature of past research. Much of the work draws from deliberative theory (Habermas, 1991), which has been defined in numerous ways, but often includes the idea that interlocutors, treated equally, respectfully engage in fact-based discussions to reach consensus (Delli Carpini et al., 2004).

As summarized in Table 1, many past studies draw from this approach when analyzing discourse, whether in face-to-face conversations, within comment sections, or, most popular recently, on social media. Studies examine whether there is evidence of rational information exchange, including the citation of evidence, the presence of reasoned arguments, and whether people are asking genuine questions. Also consistent with some definitions of deliberation, past work has examined utterances that provide solutions or build toward consensus. Quality exchanges, according to several studies, also include interactivity and reciprocity among participants.

Beyond the informational content and the presence of interactivity, some studies also have looked at the tone of the conversation. Incivility, for instance, is seen as antithetic to deliberation (Freelon, 2015). Civility and respect characterize some operationalizations of quality discussion, yet most of the research looks for the presence of incivility and disrespect, as opposed to language indicating civility and respect. This is critical because a comment that does not use uncivil or disrespectful language is not necessarily civil and respectful.

The final discourse quality category we identified across studies, labeled Acknowledgment in Table 1, looks at how people treat others and others' arguments in a discussion. The concepts used vary broadly. Some involve acknowledging others' views, regardless of whether one is sympathetic. Others involve meta-reflection on the conversation overall. Yet others involve empathy for different viewpoints.

In a highly polarized context such as the United States, the opportunity for deliberation as conceived of by deliberative theorists is optimistic, but slim (e.g., Mutz, 2006). Political partisans routinely do not engage in deliberation, let alone agree upon facts, engage with each other, or respectfully work toward consensus. Rather than focusing on deliberation as solely important, scholars have noted that it may be better to consider related concepts other forms of desired language that may lead to (but are not necessarily) deliberation (Shugars, 2020; Overgaard et al., 2022). For example, identifying language that recognizes the humanity of the interlocutors or indicates an acknowledgement of differing opinions may help connect ideologically divergent groups, such as Democrats and Republicans in the United States.

Although a few concepts from Table 1 may hold promise, such as empathy and respect for counterarguments, it is equally important to consider (1) how these individual concepts may operate together to facilitate pro-democratic connectivity and (2) how one might computationally-detect such concepts.

A handful of NLP studies have sought to identify desired language styles, including polite language (Priya et al., 2024) and empathy (Zhou et al., 2021). These studies rely on background literature from social science disciplines, but leverage computational and NLP expertise to build pro-social classifiers that have the potential to improve online conversation (Kolhatkar et al., 2020).

\begin{table*}[t]
\centering
\small
\begin{tabular}{p{0.15\linewidth} p{0.3\linewidth} p{0.45\linewidth}}
\toprule
\textbf{Category} & \textbf{Description} & \textbf{Source(s)} \
\midrule
Rationality & Evidence & (Stromer-Galley, 2007; Halpern and Gibbs, 2013; Rowe, 2015; Esau et al., 2023) \
& Justification & (Steenbergen et al., 2003; Esau et al., 2017; Gold et al., 2017; Friess et al., 2021) \
\midrule
Questions & General questions & (Halpern and Gibbs, 2013; Ziegele et al., 2020; Esau et al., 2023; Murray et al., 2023) \
& Genuine questions & (Ziegele et al., 2020) \
& Inflammatory questions & (Del Valle et al., 2020; Ziegele et al., 2020) \
\midrule
Consensus/ Solutions & Working toward consensus & (Cappella et al., 2002; Menon et al., 2020) \
& Proposing solutions & (Del Valle et al., 2020) \
& Resolving conflicts & (Esau et al., 2023) \
\midrule
Interactivity/ Reciprocity & Replying & (Murray et al., 2023) \
& Referencing & (Friess and Eilders, 2015) \
\midrule
Respect/ Civility & Incivility & (Friess et al., 2021; Esau et al., 2023) \
& Interruption & (Jaidka et al., 2022) \
& Impoliteness & (Halpern and Gibbs, 2013; Esau et al., 2023) \
& Negative empathy & (Esau et al., 2017; Del Valle et al., 2020) \
& Civility & (Friess and Eilders, 2015) \
& Respect for others & (Steenbergen et al., 2003) \
\midrule
Acknowledgement & Value another's statement & (Freelon, 2015) \
& Respect for arguments & (Menon et al., 2020; Esau et al., 2023) \
\bottomrule
\end{tabular}
\caption{Related Work on Attributes of Quality Discourse}
\label{tab:related_work}
\end{table*}

\subsection{Connective Language}
Connective language is distinct from these past work in that it emphasizes linguistically building connections. It includes encouraging engagement, understanding, and conversation, using techniques such as expressing openness to alternative viewpoints. Although it has some aspects in common with the use of polite language, there are many forms of polite language that would not be connective (e.g. saying please). The idea also is related to (but distinct from) empathy, as connective posts are not about how one internalizes others' views. Rather, connective posts are about presenting one's own point in a manner that invites others to engage productively.

Research suggests that this type of language can reduce affective polarization. First, there's good evidence that exposure to sympathetic outparty members can curb affective polarization (Voelkel et al., 2023). Outpartisans writing connective posts should be seen as more sympathetic. Second, the use of humility one form of connective language can improve people's attitudes toward commenters from an opposing political party (Murray et al., 2021) and research on inter-group contact theory finds that positive interactions with individual outparty members can generalize to evaluations of the opposing party as a whole (Pettigrew and Tropp, 2013).

\section{Proposed Method}
To build a connective language classifier, we apply the following approach: first, we build a multi-platform dataset consisting of content from users who are likely to be engaging in discussion on a topic about which they disagree. This includes a mix of political topics (e.g., for whom should a citizen vote?) and apolitical discussion (e.g., should pineapple be a pizza topping?).

We then construct a gold-standard training set of connective language using human labelers. After achieving inter-coder agreement, four undergraduate students labeled 14,107 social media posts. We then use these messages to build a connective language BERT classifier. We compare this classifier to one built using GPT 3.5 turbo, a large-language model. We also analyze how connective language is distinct from other similar concepts, including politeness and constructiveness.

\subsection{Dataset}
To identify social media posts with connective language, we took an inductive approach. We first constructed a list of five Reddit and Twitter accounts that engaged in cross-cutting discussion that (1) did not alienate and (2) sometimes encouraged deliberation with ideologically-opposed social media users. These were: r/ChangeMyView, Olympia Snowe, Kathryn Murdoch, NoLabels, Braver Angels. From this list, the authors then derived eight attributes that could relate to connective language: humility, humanizing, common humanity, acknowledgement of emotions/thoughts, consensus building, reflective listening, reactivity, and truthfulness in conversation. These aligned with recommendations from journalists and organizational communication researchers (e.g., Feltman, 2011) for building trust.

Using these five examples and eight attributes, four undergraduate students were tasked with identifying similar accounts across Twitter. A total of 31 Twitter accounts were identified by the undergraduate coders and confirmed to contain connective language by the authors. These were: `The65Project'', `Preet Bharara'', `BarbMcQuade'', `mashagessen'', `ianbremmer'', `NateSilver538'', `Yascha\_Mounk'', `KHayhoe'', `uniteamerica'', `NickTroiano'', `KarenKornbluh'', `BrennanCenter'', `NowThisPolitics'', `kylegriffin1'', `politico'', `hrw'', `cliffordlevy'', `ZekeJMiller'', `CREWcrew'', `PhilipRucker'', `tribelaw'', `glennkirschner2'', `HeartlandSignal'', `nprpolitics'', `ezraklein'', `johnkingCNN'', `txpolproject'', `ap_politics'', `mattyglesias'', `HeerJeet'', `UNHumanRights'', `bbcpolitics''.

Additionally, we constructed a keyword-based query to supplement our user collection. The case-insensitive keyword query included the following 12 terms: imo, imho, inmyopinion, `in my opinion'', `I hear you'', `never thought about it'', `my perspective'', `see where you're coming from'', `see where ur coming from'', `thanks for sharing'', `complicated issue'', ``correct me if''. Posts from the original 31 accounts were subsampled for posts using the aforementioned 12 terms.

Public Twitter data from these accounts were gathered using the Twitter 2.0 Academic Track API from January 1, 2012 to December 31, 2022. To collect this data, we used two queries (one keyword-based and one user-based).

For Reddit, we considered posts published from January 1, 2012 to December 31, 2022, which were gathered from July 1 to 17, 2023 using Pushshift (Baumgartner et al., 2020) from the following subreddits: r/ChangeMyView and r/politics (two English-based subreddits, with the former including apolitical posts and the latter focused on political posts), using the above list of 12 query terms. Both subreddits are highly active with many users; at the time of the collection, r/ChangeMyView had 3.6 million followers and r/politics had 8.5 million followers in 2024.

For Facebook, we did not conduct a user-based query and simply queried for the use of the 12 terms across all public Facebook groups and pages available through Crowdtangle from January 1, 2012 to December 31, 2022. This collection was conducted from July 1 to 30, 2022.

To construct the dataset used to train this classifier, we took a subsample from each corpus and combined them into a English-language dataset that consisted of public Reddit submissions (), Twitter posts (), and Facebook posts ().

Using different query parameters for each data collection has become an increasingly common practice to account for temporal, discursive, and platform diversity (for similar collections, see (Avalle et al., 2024; Roccabruna et al., 2022)). Identifying information from this dataset, including the pseudonym or name of the account producing the content, has been removed from the dataset.

\subsection{Labeled Data}
To build a connective language classifier, we developed a codebook and hired four undergraduate students to code posts. The faculty co-authors initially conducted a comprehensive literature review on how various fields had conceptualized and operationalized concepts like connective language. A synthesis of this literature was developed into a preliminary codebook and shared with the students, who then brainstormed with the faculty authors to determine broad categories for operationalizing the concept of `connective posts'' versus `not connective posts.'' Then the students coded repeated random samples of 100 posts each drawn from our universe to practice coding and iterate on the coding guide, based on post content.

Next the students conducted eight rounds of coding, meeting weekly until they achieved a reliable Krippendorff's  (0.73) using a sample of 1,000 posts. Once the students achieved an inter-coder reliability above a 0.7 threshold, we then had students code 6,107 Reddit posts, 5,000 Twitter posts, and 3,000 Facebook posts, over three rounds, using the following coding guide:

A connective post was coded `1'' and defined as a post that: \begin{itemize} \item Encourages engagement, understanding, and conversation, sometimes by asking questions, or expressing openness to alternative views. \item Contains language that conveys openness by including phrases, such as `in my opinion,'' `imo,'' `imho,'' `in my viewpoint,'' `here's how I see it,'' `in my mind,'' `my 2 cents is.''
\item Other indicators of a connective posts include phrases such as `I respectfully disagree,'' `I disagree to an extent,'' `You're right about xxx,'' `I see where you're coming from,'' `You've changed my view,'' `I never thought about it like that,'' `Can you clarify,'' `I'm not trying to debate, but want to offer an opinion,'' `That's an interesting perspective,'' `I appreciate your feedback.''
\item Clarification: Hate speech (e.g., racist, sexist, homophobic, or xenophobic language) would invalidate a post as ``connective,'' but profanity alone would not.
\end{itemize}

A non-connective post was coded 0 and defined as a post that:
\begin{itemize}
\item Lacks any of the elements of connective posts described above or included hate speech.
\item Demonizes another person or is disrespectful to other points of view.
\item Contains no discussion.
\end{itemize}

To validate this operationalization of connective posts, accounting for variations in gender, race/ethnicity, and political beliefs, we conducted an online survey () and find little to no demographic differences across evaluations regarding connective language. These details can be found in the Appendix A.1.

\subsection{BERT Classifier}
Using human-labeled data, we trained a BERT (Bidirectional Encoder Representations from Transformers, Kenton and Toutanova, 2019) classifier to predict the presence of connective language in text content. Compared to traditional text classification methods, such as logistic regression and Naive Bayes models, a BERT classifier excels due to its deep understanding of context and language nuances (Shen and Liu, 2021; Shushkevich et al., 2022; Moreira et al., 2023), which is particularly useful in complex tasks, such as detecting connective language in texts.

\begin{figure}[h]
\centering
\fbox{\parbox{0.9\linewidth}{\centering IMAGE NOT PROVIDED}}
\caption{Pipeline of fine-tuning a BERT classifier for detecting connective language}
\label{fig:pipeline}
\end{figure}

As seen in Figure 1, we use the following approach: from the entire human-coded dataset, we first created a balanced sample () by undersampling the `1'' group, due to fewer instances of `1''s in the labeled data. A balanced dataset is crucial as it ensures that the model learns to recognize patterns associated with both classes equally, which leads to more accurate and generalizable results (Batista et al., 2004).

We then utilized the bert-base-uncased model (Devlin, 2018) for fine-tuning with our balanced labeled sample. The data was divided into training, validation, and test sets to effectively train the model while preventing overfitting. During training of the BERT classifier for binary classification, we employed TFBertForSequenceClassification with an Adam optimizer set at a learning rate of . Essential callbacks like EarlyStopping, ModelCheckpoint, and ReduceLROnPlateau were incorporated to enhance training efficiency and optimization on a MacBook Pro with an Apple M1 Pro chip. Default parameters from the scikit-learn package (Pedregosa et al., 2011) were used. The training process involved multiple iterations where the model predicted labels on the training data and these predictions were compared against the actual labels, continuing until the fine-tuned model demonstrated satisfactory precision and recall.

\subsection{Few-shot Classifier}
We employed a generative AI tool, specifically OpenAI's ``GPT 3.5 Turbo,'' accessed via the OpenAI API, to classify social media texts for connectivity\footnote{[https://platform.openai.com/docs/models/gpt-3-5-turbo](https://platform.openai.com/docs/models/gpt-3-5-turbo)}. The GPT 3.5 Turbo model is the most recently available version of OpenAI's language models, known for its enhanced speed and accuracy, which makes it ideal for real-time text classification tasks.

While social science research may benefit from the efficiency of large language models (Rosenbusch et al., 2023), LLMs may exhibit biases (Taubenfeld et al., 2024) and reliability issues (Majeed and Hwang, 2024).

The classification process involved a prompt that defined `connectivity'' and requested that the model classify an unlabeled post as either `1'' (connective) or ``0'' (non-connective). After several attempts (see Appendix A.2), the final prompt provided to the model was as follows:

\begin{quote}
Please perform a text annotation task: Below is the definition of `connectivity' and an unlabeled post. Your task is to classify the post based on whether it demonstrates connectivity. Respond only with `1' for connective or `0' for non-connective. Definition of Connectivity: Connectivity indicates the tone of a message. A post is considered connective if it shows a willingness to engage in conversation with others, especially those with differing opinions, uses hedging, or maintains a polite tone when sharing opinions or facts. Phrases like `in my honest opinion' are also markers of connective language. This definition is derived from the codebook used by the human coders. Here is the post: ``TEXT''
\end{quote}

We sampled a balanced set of 1000 texts (500 connective, 500 non-connective), stratified by platform, from our human-labeled dataset. We then compared the classifications made by the GPT model to the human labels, treating the human labels as actual values and the GPT's outputs as predictions.

\subsection{Comparing BERT and LLMs}
We choose to compare a BERT classifier and a GPT-based classifier as both are popular language models for building classifiers in the social sciences. While the BERT model has been used to build other political communication classifiers for topics such as deliberation (Fournier-Tombs and MacKenzie, 2021), GPT-based classifiers are comparatively newer. Furthermore, scholars have raised concerns about GPT 3.5's unreliability and tendency to produce biased outputs (Wang et al., 2023), especially when dealing with topics related to stereotyping and protected demographic groups. However, at the time of our study, it was unclear whether these biased outputs would also impact the ability to produce classifiers for normatively desired content (such as connective language).

\subsection{Comparison to Other Concepts}
To demonstrate the conceptual uniqueness of ``connectivity,'' we compared the result of connective language detection (human-labeled results) with several other related concepts, including politeness, civility, and a set of attributes related to political discussion quality such as constructiveness, justification, relevance, and reciprocity (Jaidka, 2022). Through correlation analysis between the score of connective language and other concepts for the same texts, we show the connectivity is a distinct attribute of political and social discussions.

For detecting toxicity, we employed the Perspective API\footnote{[https://support.perspectiveapi.com/](https://support.perspectiveapi.com/)}, a tool developed by Jigsaw and Google that uses machine learning models to identify and score the degree of perceived harmfulness or unpleasantness in written content. The output from Perspective API provides a set of scores for various sub-attributes, such as personal attacks, among others, in addition to an overall toxicity score. For our analysis, we specifically utilize the overall toxicity score, ranging from 0 (not toxic at all) to 1 (extremely toxic), to assess the general level of toxicity in the texts. This score synthesizes insights from all the sub-attributes into a single comprehensive measure, enabling a clear and focused evaluation of toxicity.

We also compare the classifier to the new Perspective API attributes, which are experimental: affinity, compassion, curiosity, nuance, personal story, reasoning, and respect.

To detect politeness, we utilized the R package ``politeness'' (Yeomans et al., 2023), a statistical tool designed to analyze linguistic cues and determine the levels of courtesy and respect present in text. We utilized the \texttt{politenessModel} function, which is a wrapper that can be used around a pre-trained model for detecting politeness from texts (Danescu-Niculescu-Mizil et al., 2013). This function outputs a score ranging from -1 to 1, where higher values represent higher politeness, and lower values indicate less politeness or rudeness.

In addition to toxicity and politeness, we also compared the connective language with a set of attributes related to the quality of political discussions proposed by Jaidka (2022). We are specifically concerned with six attributes that are related to connective language, constructiveness, justification, relevance, reciprocity, empathy/respect, and incivility. We used the classifiers featured in this paper to do the classifications. model, ``bert-base-uncased,'' analyzed 1,000 posts and demonstrated a precision of 0.85, recall of 0.84, and an F1-score of 0.85.

\section{Result}

\subsection{Descriptives}

\begin{table}[h]
\centering
\begin{tabular}{l l r r}
\toprule
\textbf{Platform} & \textbf{Connective} & \textbf{Count} & \textbf{Percentage} \
\midrule
Facebook & 0 & 1196 & 43.9% \
() & 1 & 1527 & 56.1% \
\midrule
Reddit & 0 & 2733 & 50.7% \
() & 1 & 2661 & 49.3% \
\midrule
Twitter & 0 & 1903 & 38.5% \
() & 1 & 3041 & 61.5% \
\bottomrule
\end{tabular}
\caption{Descriptive of Human-coded Posts by Platform}
\label{tab:descriptives}
\end{table}

Table \ref{tab:descriptives} provides a descriptive summary of human-coded posts used for training machine learning classifiers, showing the distribution of posts labeled as connective (1) and non-connective (0) across three major platforms: Facebook, Reddit, and Twitter. Notably, the data highlights variability in connective language usage, with Twitter exhibiting a higher percentage of connective posts (61.5%), compared to Reddit and Facebook.

\subsection{Model Evaluation: BERT vs GPT}
To evaluate and compare the performance of two classifiers, BERT and GPT-3.5 Turbo, we assessed their ability to predict whether social media posts convey ``connective language'' by comparing the predicted values from each classifier against the human-labeled results on the same data. The evaluation metrics used included precision, recall, and F1-score, as detailed in Table \ref{tab:evaluation}.

\begin{table}[h]
\centering
\begin{tabular}{l l l}
\toprule
\textbf{Metric} & \textbf{BERT} & \textbf{GPT} \
\midrule
\multicolumn{3}{l}{\textit{Overall ()}} \
Precision & 0.85 & 0.55 \
Recall & 0.84 & 0.42 \
F1 & 0.85 & 0.48 \
\midrule
\multicolumn{3}{l}{\textit{Facebook ()}} \
Precision & 0.92 & 0.51 \
Recall & 0.86 & 0.32 \
F1 & 0.89 & 0.40 \
\midrule
\multicolumn{3}{l}{\textit{Twitter ()}} \
Precision & 0.97 & 0.64 \
Recall & 0.99 & 0.22 \
F1 & 0.98 & 0.33 \
\midrule
\multicolumn{3}{l}{\textit{Reddit ()}} \
Precision & 0.81 & 0.55 \
Recall & 0.72 & 0.57 \
F1 & 0.76 & 0.56 \
\bottomrule
\end{tabular}
\caption{Evaluation metrics of BERT and GPT classifier by platform}
\label{tab:evaluation}
\end{table}

The BERT model, ``bert-base-uncased,'' analyzed 1,000 posts and demonstrated a precision of 0.85, recall of 0.84, and an F1-score of 0.85. In contrast, the GPT-3.5 Turbo model, when evaluating the same 1,000 posts, recorded lower scores across all metrics with a precision of 0.55, recall of 0.42, and F1-score of 0.48. These results indicate that the BERT model outperforms the GPT-3.5 Turbo in accurately identifying the conveyance of connective language in social media posts.

\subsection{Comparing Connectivity to Other Concepts}
We conducted a correlation analysis (see Table \ref{tab:correlations}) to explore the relationship between the new metric of connectivity and established measures within the context of political discussions. This analysis highlighted the unique aspects of the connectivity metric and its interactions with other key qualities of online discussions. The findings reveal that connectivity negatively correlates, with toxicity and incivility. Additionally, connective language identified with the BERT classifier shows a positive correlation with politeness, at 0.28, as well as empathy-respect, at 0.29. This implies that conversations with greater connectivity are also labeled as more polite and respectful, and less toxic or incivil.

Furthermore, weak to no negative correlations were found between connectivity and other concepts such as constructiveness, justification, relevance, and reciprocity. These findings provide robust evidence that connectivity captures elements of communication that are not fully addressed by traditional metrics. This distinctiveness is vital for a deeper understanding of the structural and relational dynamics that are often neglected in conventional content-focused analyses of online discussions.

\begin{table*}[t]
\centering
\small
\resizebox{\linewidth}{!}{
\begin{tabular}{l c c c c c c c c c c c c c}
\toprule
\textbf{Variable} & \textbf{M} & \textbf{SD} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{6} & \textbf{7} & \textbf{8} & \textbf{9} & \textbf{10} & \textbf{11} \
\midrule

1. Conn. (BERT) & 0.48 & 0.41 & & & & & & & & & & & \
2. Conn. (Human) & 0.50 & 0.50 & .73** & & & & & & & & & & \
3. Conn. (GPT) & 0.38 & 0.49 & .06* & .09** & & & & & & & & & \
4. Toxicity & 0.15 & 0.15 & -.10** & -.08* & -.12** & & & & & & & & \
5. Politeness & 0.01 & 0.50 & .28** & .27** & -.02 & -.24** & & & & & & & \
6. Constructiveness & -0.00 & 0.01 & -.19** & .18** & .04 & .06 & -.22** & & & & & & \
7. Justification & 0.05 & 0.02 & -.07* & -.05 & .24** & .01 & -.16** & .27** & & & & & \
8. Relevance & 0.07 & 0.02 & -.14** & -.09** & .24** & -.04 & -.11** & .16** & .84** & & & & \
9. Reciprocity & -0.00 & 0.01 & -.09** & -.06 & .04 & .06* & -.07* & .01 & -.12** & -.07* & & & \
10. Emp.-Respect & 0.01 & 0.01 & .29** & .23** & .12** & -.15** & .27** & -.24** & .05 & .01 & .04 & & \
11. Incivility & -0.02 & 0.01 & .12* & .10** & .15** & .16** & -.16** & .31** & .12** & -.15** & -.02 & -.45** & \
\bottomrule
\end{tabular}
}
\caption{Correlations Between Connectivity and Other Concepts}
\label{tab:correlations}
\end{table*}

Table 5 shows the results of a correlation test between three connective measurements: BERT, Human, and GPT, and seven measurements related to the `bridging system'' (Ovadya and Thorburn, 2023) computed by Perspective API: Affinity, Compassion, Curiosity, Nuance, Personal Story, Reasoning, and Respect. The results show that the measurements of connective language are, in some instances, weakly correlated with the `bridging'' measurements such as affinity and respect, yet the magnitude is modest, indicating the conceptual uniqueness of connective language.

\section{Discussion}
Connectivity emerged as an important attribute of online discussions. In this study, we proposed two types of classifiers to detect connective language from social media posts. First, we found that the BERT classifier outperforms GPT-3.5 turbo in classifying texts into connective and non-connective categories. This indicates the superior effectiveness of BERT in identifying connective language within political discussions.

Additionally, we found that connective language is conceptually distinct from other related concepts such as politeness, toxicity, constructiveness, reciprocity, among others, suggesting that connectivity represents a unique dimension of discourse quality. Furthermore, our results demonstrate the ability to use BERT to construct multi-platform classifiers, enhancing the versatility and applicability of our approach and potentially laying the foundation for platform-generalizable classifiers.

While our analysis did not necessarily find biases among the outputs of the GPT classifier, the decreased accuracy of this classifier may be a result of the more complex, nuanced, or new conceptualization of connectivity, as opposed to more overt or well-studied labels like sentiment or toxicity.

This classifier could be used to test whether those using connective language have more deliberative conversations, as theory would suggest (Delli Carpini et al., 2004); to evaluate the effects of exposure to social media posts that contain connectivity; and to examine practical ways of increasing connectivity to the extent that it has pro-democratic effects such as increasing understanding of alternative views.

\subsection{Limitations}
As with any study, we recognize that there are several limitations to this study that we were unable to address or were beyond the scope of our study. First, we constructed our sample in an effort to oversample for connective language. To do so, we sought out digital spaces where discussion and disagreement occurs, and we used keywords that literature suggests may be used when disagreement occurs. Therefore, the proportion of connective posts in our sample is not necessarily representative of a typical virtual conversation or topic. Future studies can [MISSING CONTENT]

\bibliographystyle{plain}
\bibliography{refs}

\appendix
\section{Appendix}
[MISSING APPENDIX CONTENT]

\end{document}
=====END FILE=====

=====FILE: refs.bib=====
% References reconstructed from citations found in the text.
% Full bibliography was not available in the source file content.

@article{wang2018,
author = {Wang, Y. and others},
title = {[MISSING TITLE]},
journal = {[MISSING JOURNAL]},
year = {2018}
}

@article{bakkialakshmi2022,
author = {Bakkialakshmi, V. and Sudalaimuthu, T.},
title = {[MISSING TITLE]},
journal = {[MISSING JOURNAL]},
year = {2022}
}

@inproceedings{cavalin2015,
author = {Cavalin, P. and others},
title = {[MISSING TITLE]},
booktitle = {[MISSING BOOKTITLE]},
year = {2015}
}

@inproceedings{hosseini2020,
author = {Hosseini-Saravani, S. H. and others},
title = {Depression Detection in Social Media Using a Psychoanalytical Technique for Feature Extraction and a Cognitive Based Classifier},
booktitle = {Advances in Computational Intelligence},
pages = {282--292},
year = {2020},
publisher = {Springer}
}

@article{latha2022,
author = {Latha, K. and others},
title = {[MISSING TITLE]},
journal = {[MISSING JOURNAL]},
year = {2022}
}

@article{su2020,
author = {Su, Y. and others},
title = {[MISSING TITLE]},
journal = {[MISSING JOURNAL]},
year = {2020}
}

@article{srinivas2021,
author = {Srinivas, S. and others},
title = {[MISSING TITLE]},
journal = {[MISSING JOURNAL]},
year = {2021}
}

@article{gharge2017,
author = {Gharge, S. and Chavan, M.},
title = {[MISSING TITLE]},
journal = {[MISSING JOURNAL]},
year = {2017}
}

@article{garlapati2022,
author = {Garlapati, S. and others},
title = {[MISSING TITLE]},
journal = {[MISSING JOURNAL]},
year = {2022}
}

@article{garg2021,
author = {Garg, N. and Girdhar, N.},
title = {[MISSING TITLE]},
journal = {[MISSING JOURNAL]},
year = {2021}
}

@article{babakov2024,
author = {Babakov, D. and others},
title = {[MISSING TITLE]},
journal = {[MISSING JOURNAL]},
year = {2024}
}

@article{overgaard2022,
author = {Overgaard, C. S. and others},
title = {[MISSING TITLE]},
journal = {[MISSING JOURNAL]},
year = {2022}
}

@article{iranzo2023,
author = {Iranzo-Cabrera, M. and Casero-Ripollés, A.},
title = {Political entrepreneurs in social media: Self-monitoring, authenticity and connective democracy. The case of Iñigo Er...},
journal = {[MISSING JOURNAL]},
year = {2023}
}

@book{habermas1991,
author = {Habermas, J.},
title = {The structural transformation of the public sphere: An inquiry into a category of bourgeois society},
publisher = {MIT Press},
year = {1991}
}

@article{delli2004,
author = {Delli Carpini, M. X. and others},
title = {[MISSING TITLE]},
journal = {[MISSING JOURNAL]},
year = {2004}
}

@article{halpern2013,
author = {Halpern, D. and Gibbs, J.},
title = {Social media as a catalyst for online deliberation? exploring the affordances of facebook and youtube for political expression},
journal = {Computers in human behavior},
volume = {29},
number = {3},
pages = {1159--1168},
year = {2013}
}

@article{gold2017,
author = {Gold, V. and others},
title = {Visual linguistic analysis of political discussions: Measuring deliberative quality},
journal = {Digital Scholarship in the Humanities},
volume = {32},
number = {1},
pages = {141--158},
year = {2017}
}

@article{gover2023,
author = {Gover, L.},
title = {Political bias in large language models},
journal = {The Commons: Puget Sound Journal of Politics},
volume = {4},
number = {1},
pages = {2},
year = {2023}
}

@inproceedings{kenton2019,
author = {Kenton, J. D. M. W. C. and Toutanova, K.},
title = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
booktitle = {NAACL-HLT},
year = {2019}
}

@article{devlin2018,
author = {Devlin, J.},
title = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
year = {2018}
}

@article{batista2004,
author = {Batista, G. E. and others},
title = {[MISSING TITLE]},
year = {2004}
}

@article{pedregosa2011,
author = {Pedregosa, F. and others},
title = {Scikit-learn: Machine Learning in Python},
journal = {Journal of Machine Learning Research},
year = {2011}
}

@article{yeomans2023,
author = {Yeomans, M. and others},
title = {[MISSING TITLE]},
year = {2023}
}

@inproceedings{danescu2013,
author = {Danescu-Niculescu-Mizil, C. and others},
title = {A computational approach to politeness with application to social factors},
booktitle = {ACL},
year = {2013}
}

@article{jaidka2022,
author = {Jaidka, K.},
title = {[MISSING TITLE]},
year = {2022}
}

@article{ovadya2023,
author = {Ovadya, A. and Thorburn, L.},
title = {[MISSING TITLE]},
year = {2023}
}

% Additional references found in text but incomplete in this list due to missing bibliography
% ...
=====END FILE=====