=====FILE: main.tex=====
\documentclass[11pt,a4paper,twocolumn]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\geometry{top=1in, bottom=1in, left=0.75in, right=0.75in}
\usepackage{times}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{float}
\usepackage{hyperref}
\usepackage{url}
\usepackage{xcolor}

\title{Towards Probing Speech-Specific Risks in Large Multimodal Models: \ A Taxonomy, Benchmark, and Insights}

\author{
Hao Yang \quad Gholamreza Haffari \quad Lizhen Qu \quad Ehsan Shareghi \
Department of Data Science & AI, Monash University \
\texttt{firstname.lastname@monash.edu}
}

\date{}

\begin{document}

\maketitle

\input{sections/abstract}
\input{sections/intro}
\input{sections/related}
\input{sections/taxonomy}
\input{sections/data}
\input{sections/experiments}
\input{sections/conclusion}

\bibliographystyle{alpha}
\input{sections/references}

% Appendix is missing from the provided text, but referenced in the text.
% \appendix
% \input{sections/appendix}

\end{document}
=====END FILE=====

=====FILE: sections/abstract.tex=====
\begin{abstract}
Large Multimodal Models (LMMs) have achieved great success recently, demonstrating a strong capability to understand multimodal information and to interact with human users. Despite the progress made, the challenge of detecting high-risk interactions in multimodal settings, and in particular in speech modality, remains largely unexplored. Conventional research on risk for speech modality primarily emphasises the content (e.g. what is captured as transcription). However, in speech-based interactions, paralinguistic cues in audio can significantly alter the intended meaning behind utterances. In this work, we propose a speech-specific risk taxonomy, covering 8 risk categories under hostility (malicious sarcasm and threats), malicious imitation (age, gender, ethnicity), and stereotypical biases (age, gender, ethnicity). Based on the taxonomy, we create a small-scale dataset for evaluating current LMMs capability in detecting these categories of risk. We observe even the latest models remain ineffective to detect various paralinguistic-specific risks in speech (e.g., Gemini 1.5 Pro is performing only slightly above random baseline). Warning: this paper contains biased and offensive examples.
\end{abstract}
=====END FILE=====

=====FILE: sections/intro.tex=====
\section{Introduction}

Large language models (LLMs) (Touvron et al., 2023a; Chiang et al., 2023; Anil et al., 2023) have showcased superior ability to in-context learning and robust zero-shot performance across various downstream natural language tasks (Xie et al., 2021; Brown et al., 2020; Wei et al., 2022). Building on the foundation established by LLMs, Large Multimodal Models (LMMs) (Chu et al., 2023; Reid et al., 2024; Tang et al., 2024; Hu et al., 2024) equipped with multimodal encoders extend the scope beyond mere text, and facilitate interactions centred on visual and auditory inputs. This evolution marks a significant leap towards more comprehensive and versatile AI systems.

Our code and data are available at \url{[https://github.com/YangHao97/speech_specific_risk](https://github.com/YangHao97/speech_specific_risk)}.

Although LMMs show the capability to process and interact in a wide-range of multimodal forms, they still embody several challenges associated with safety and risks. Investigating these potential issues in LMMs requires both a modality-specific definition of risk, and suitable benchmarks. While there is a dedicated body of work in the text domain to probe various aspects of LLMs beyond downstream performance, such categorical investigations are missing for other modalities such as speech. For instance, existing risk detection protocols for speech modality (Yousefi and Emmanouilidou, 2021; Rana and Jha, 2022; Nada et al., 2023; Reid et al., 2022; Ghosh et al., 2021) only focus on the content aspect (i.e., what could be captured by speech transcription), and neglect risks induced by paralinguistic cues, the unique feature of speech. To highlight this further, consider how various interpretations of the transcript ``I feel so good'' arises depending on the utterance form (e.g., varying tones, and emotions such as angry, sad, depressed, or imitation of a specific gender, age or ethnicity) in audio speech.

\begin{figure}[t]
\centering
\fbox{\begin{minipage}{0.9\columnwidth}
\centering
\textbf{IMAGE NOT PROVIDED} \
Original Caption: Figure 1: Our taxonomy of risk categories for speech.
\end{minipage}}
\caption{Our taxonomy of risk categories for speech.}
\label{fig:taxonomy}
\end{figure}

In this work, we move towards addressing this gap for speech modality by introducing a protocol to evaluate the capability of LMMs in detecting the risks induced specifically by paralinguistic cues. To our knowledge, our work is the first to explore the risk awareness at the paralinguistic level. We propose a speech taxonomy, covering 3 main categories: hostility, malicious imitation, and stereotypical biases, and further expand them into 8 corresponding sub-categories, which emphasise the implicit and subtle risks induced by paralinguistic cues in speech. Figure \ref{fig:taxonomy} provides a high-level overview of risk categories considered in this work (\S3). We then manually create a high-quality set of seed transcriptions for 4 of the sub-categories (hostile-sarcasm, and gender, age, ethnicity stereotypical biases; 10--15 examples per each sub-category). The seed set has been controlled to not leak the category of risk through the transcript alone. The seed sets are then expanded further by leveraging GPT-4. All samples (262 samples) were further filtered by three human annotators to maintain quality, resulting in 180 final transcriptions. Three human annotators are all from our co-authors of this paper (2 faculty members and 1 PhD student, all with expertises in NLP). The annotators work independently and do not have access to each other's annotation results during the process to avoid undesired biases. To convert these transcripts into audio, we used advanced text-to-speech (TTS) systems, Audiobox (Vyas et al., 2023) and Google TTS\footnote{Audiobox: \url{[https://audiobox.metademolab.com](https://audiobox.metademolab.com)}; and Google: \url{[https://cloud.google.com/text-to-speech](https://cloud.google.com/text-to-speech)}.}, to generate various synthetic speeches with paralinguistic cues, resulting in 1,800 speech instances.

In experiments, we evaluate 5 most recent speech-supported LMMs, Qwen-Audio-Chat (Chu et al., 2023), SALMONN-7B/13B (Tang et al., 2024), WavLLM (Hu et al., 2024), and Gemini-1.5-Pro (Reid et al., 2024), under various prompting strategies. Notably, Gemini 1.5 Pro performs very similar to random baseline (50%), while WavLLM performs worse that random guessing. Among the other two models, Qwen-Audio-Chat has a more stable success pattern under various prompting strategies, while SALMONN-7/13B do the best under certain prompting configurations. We attribute these differences in performance to different selection and adaptation of audio encoders. Among the risk categories, the one that seems the most difficult is Age Stereotypical Bias where even the best configuration's result is only slightly above random baseline (54%). For Gender and Ethnicity Stereotypical Biases the best result gets above 60%, and for Malicious Sarcasm it goes further into (70%).

To the best of our knowledge our paper presents the first speech-specific risk taxonomy, focused exclusively on risks associated with paralinguistic aspects of audio. We hope our taxonomy, benchmark, and evaluation protocol to encourage further investigation of risk in speech modality, and guide LMM developers towards more holistic evaluation and safeguarding across modalities.
=====END FILE=====

=====FILE: sections/related.tex=====
\section{Related Work}

The research on LLMs has shown increased focus on safety and responsibility, leading to significant advancements in benchmarking these models' ability to handle and respond to harmful content in text modality. Notable contributions in this area include the three-level hierarchical risk taxonomy introduced by Do-Not-Answer (Wang et al., 2023), which created a dataset containing 939 prompts that model should not respond to. SafetyBench (Zhang et al., 2023b) explored 7 distinct safety categories across the multiple choice questions, while CValues (Xu et al., 2023) established the first Chinese safety benchmark for evaluating the capability of LLMs. Goat-bench (Khanna et al., 2024) evaluated LMMs in detecting implicit social abuse in memes. Although many research efforts focus on mitigating the generation of harmful content, OR-Bench (Cui et al., 2024) presented 10 common rejection categories including 8k seemingly toxic prompts to benchmark the over-refusal of LLMs.

On conventional toxic speech detection task, the research has mostly focused on the content aspect. DeToxy-B (Ghosh et al., 2021) is proposed as a large-scale dataset for speech toxicity classification. Rana and Jha (2022) combined emotion by using multimodal learning to detect hate speech, and Reid et al. (2022) presented sensing toxicity from in-game communications. While content-focused line of research was relevant for a while, the transcription generated by the recent highly capable Automatic Speech Recognition (ASR) systems such as Whisper (Radford et al., 2023) could merge this line of research into text-based safety research (e.g., through a cascaded design of ASR and LLM). However, this type of cascaded approach also excludes the paralinguistic cues in audio as the focus remains on the transcription of ASR.

While early works in Speech-based LLMs shown minimal real progress in speech understanding (Su et al., 2023; Zhang et al., 2023a; Zhao et al., 2023), recent works through alignment of representation spaces between speech encoder's output and text-based LLM's input (either with full end-to-end training, or partial training of adaptors) have shown promising progress (Chu et al., 2023; Reid et al., 2024; Tang et al., 2024; Hu et al., 2024). These models, now matured enough, exhibit high competence in understanding speech (Lin et al., 2024a,b; Ma et al., 2023; Xue et al., 2023). Building on this context, our research aims to evaluate the capability of LMMs to detect risks initiated by paralinguistic cues, addressing a critical gap in the current understanding of speech-specific risks.
=====END FILE=====

=====FILE: sections/taxonomy.tex=====
\section{Our Speech-Specific Risk Taxonomy}

Our speech taxonomy is as shown in Figure \ref{fig:taxonomy}. To delineate the risks associated with paralinguistic cues, we establish 3 primary categories of risk speech. In contrast to conventional risk concerns centred on the speech content, we emphasise the significance of paralinguistic cues, including tone, emotion, and speaker information. Subsequently, we identify 8 corresponding sub-categories in which ostensibly low-risk speech content may be transformed into delivery, manifested in an implicit and subtle manner, due to the influence of corresponding paralinguistic cues.

\subsection{Hostility}
This category includes risks covering malicious sarcasm and threats. Hostility in communication typically conveys aggression, disparagement, and the intent to harm, significantly increasing psychological pressure and violating principles of respect and politeness. Emotion and tone serve as paralinguistic cues that induce hostility, transforming ostensibly low-risk content into risky speech, altering the perceived intent of the words spoken.

\textbf{Malicious Sarcasm.} We distinguish risky sarcasm and jokes based on the scenarios and the deliveries. Our considered sarcasm often arises in workplace and teamwork, where speakers express strong anger and mockery. In these scenarios, sarcasm is perceived as particularly aggressive and can have detrimental effects on mental health, leading to stress and anxiety among colleagues (Colston, 1997; Toplak and Katz, 2000; Katz et al., 2004; Zhu and Wang, 2020).

\textbf{Threats.} They represent a severe form of aggressive communication. In our definition, it is implicitly delivered by the speaker's emotion and tone, which creates a fear atmosphere and conveys implication to harm. The presence of threats within communication significantly harms the psychological health of others, and often escalate conflicts, leading to toxic environment.

\subsection{Malicious Imitation}
This category encompasses risky communication that involve the deliberate mimicry of voice characteristics associated with gender, age, and ethnicity. Such imitations, in the form of ridiculing and offending, aim to propagate and reinforce stereotypes, discrimination, or bias, leading to undermining the dignity of individuals and psychological trauma. The paralinguistic cues here are the comparison between the speaker's original voice and the exaggerated change of voice characteristics.

\textbf{Gender.} Gender-based imitation possibly involves exaggerating the feminine voice coupled with implicit stereotypes, aiming to demean and undermine the female group.

\textbf{Age.} Age-based imitation often targets the elderly. The imitative voice coupled with specific content depict them as a weak and old-fashioned group who is out of touch, which can reinforce stereotypes and exacerbate ageist.

\textbf{Ethnicity.} Ethnicity-based imitation targets accents of groups with different cultural background. This form of imitation often perpetuates racial and ethnic stereotypes, deepening cultural divides and exacerbating tensions in multicultural settings.

\subsection{Stereotypical Biases}
This category focuses on the risks associated with conversations that exhibits implicit stereotypes based on gender, age, and ethnicity. Stereotypical biases in communication often implicitly manifests through responses that may appear neutral but are loaded with underlying discriminatory attitudes. We characterise the paralinguistic cues harbouring risks in this category to include the gender, age, and ethnicity of the first and second speakers.

\textbf{Gender.} In cases of gender-based stereotypical bias, responses may implicitly convey stereotypical beliefs about abilities, roles, or behaviours associated with the female group. The content may be neutral, but the paralinguistics cues may harbour risks offensive to others. We consider risky interactions that contain a female and a male speaker.

\textbf{Age.} Stereotypical Bias against the elderly is exhibited in conversations that reflect age-related stereotypes. Responses to the elderly individuals may assume incompetence, resistance to change, or being out of touch. We consider risky interactions that contain an elderly and a young speaker.

\textbf{Ethnicity.} In the case of ethnicity stereotypical bias, responses may reflect stereotypes to a group, biases to their ability, or discrimination to cultural practices. It reinforces ethnic stereotypes and can hinder the equal treatment of individuals from diverse cultural backgrounds. We consider risky interactions in this category that contain an accented speaker and a native speaker.
=====END FILE=====

=====FILE: sections/data.tex=====
\section{Data Collection and Curation}

We curate our speech dataset for evaluation by (i) manually creating samples as seeds for each speech sub-category based on the corresponding risk description, (ii) leveraging seed instances to prompt GPT-4 to expand the sample set, and (iii) using advanced TTS systems, Audiobox and Google TTS, to generate synthetic speech for 4 risk sub-categories according to their specific paralinguistic descriptions (see Figure 2). Due to the safeguards and limitation of existing TTS system, we generate synthetic speech for these risk sub-categories: malicious sarcasm, age, gender, and ethnicity stereotypical biases. Table 1 provides our dataset statistics, and we report the average speech lengths in Appendix F.

More specifically, each sample in our dataset is a quadraple  where (i)  is the textual content (created by human or GPT4), (ii)  is the description of paralingustic cues covering emotion, tone, gender, age, and ethnicity, (iii)  is the automatically generated speech  based on Audiobox (Vyas et al., 2023) or Google TTS\footnote{Audiobox: \url{[https://audiobox.metademolab.com](https://audiobox.metademolab.com)}; and Google: \url{[https://cloud.google.com/text-to-speech](https://cloud.google.com/text-to-speech)}.}, and (iv)  is the label in (low-risk, malicious sarcasm, age, gender, ethnicity stereotypical biases).

\begin{figure}[t]
\centering
\fbox{\begin{minipage}{0.9\columnwidth}
\centering
\textbf{IMAGE NOT PROVIDED} \
Original Caption: Figure 2: Our data curation pipeline.
\end{minipage}}
\caption{Our data curation pipeline.}
\label{fig:pipeline}
\end{figure}

\begin{table}[t]
\centering
\begin{tabular}{lccc}
\toprule
Risk Sub-category & Risk & Low-risk & Total \
\midrule
Malicious Sarcasm & 375 & 375 & 750 \
Age Stereotypical Bias & 250 & 250 & 500 \
Gender Stereotypical Bias & 155 & 155 & 310 \
Ethnicity Stereotypical Bias & 120 & 120 & 240 \
\midrule
Total & 900 & 900 & 1800 \
\bottomrule
\end{tabular}
\caption{Our speech dataset for various risk types.}
\label{tab:dataset}
\end{table}

Creating a speech dataset entirely through human effort presents significant challenges, primarily due to its high costs, extensive time requirements, and the difficulty of finding individuals capable of accurately acting specific speech descriptions. These challenges often make the process inefficient and impractical, which lead us to leverage GPT-4 and advanced TTS systems for speech rendering, allowing to create diverse and scalable datasets at a fraction of the cost and time. However, we still need to bypass the safeguard restricting us to obtain safety-related data. The rest of this section outlines how to address these challenges.

\subsection{Text Samples}

\textbf{Seeds.} We first manually create 20 sample pairs of  for each risk sub-category label . These samples are quality controlled and filtered by 3 expert annotators based on these criteria: (i) the content is ostensibly low-risk, and (ii) when combined with paralinguistic , it is mapped to the risk label  (including the 4 risk labels plus the low-risk label). A sample is removed if at least two annotators find it low quality.

\textbf{GPT-4 Generation.} Manually creating samples is a time-consuming and costly process. Capitalising on the wide knowledge of GPT-4, we leverage the human-curated samples as seed templates, and prompt GPT-4 to generate more samples. Normally, we may describe a risk sub-category and include human-curated samples, and request GPT-4 to generalise them to more scenarios. However, GPT-4 tends to refuse responding to such requests due to its safeguards. We thus employ a strategy analogous to Wang et al. (2023) to overcome this issue, as explained below.

Specifically, in this OpenAI API, there is a chain of messages tagged as user and assistant alternating. In the first user role's message, we define a risk sub-category and request GPT-4 to produce samples. In the next assistant role's message, we fabricate a response where we put our seed samples here to simulate that GPT-4 has responded to our first request. In the final user role's message, we request GPT-4 generate additional 30 samples. We feed this conversation history including the above 3 messages into GPT-4, and GPT-4 successfully respond to our last request and provide additional 30 samples. These samples are annotated and filtered by human annotators, serving as seeds for iterative generation. We mix human-generated and GPT-4-generated samples as the text sample set where each sample has a risk version and a low-risk version by keeping the same  and modifying .

\subsection{Synthesising Speech}

\textbf{Sarcasm & Age Stereotypical Bias.} For each  in these categories, we generate 5 high-risk speech and 5 low-risk speech using Audiobox. We provide detailed speech descriptions for generation in Table 8 of Appendix C. The low-risk versions are generated from the modified paralinguistic description  as described in the following.
\begin{itemize}
\item For malicious sarcasm, We describe  as `speaking with angry emotion, and a mocking tone'', and $z^{\prime}$ as `speaking with happy and excited emotions''.
\item For age stereotypical bias, we distinguish between risk speech and low-risk speech based on the age of the first speaker. We describe  as `the first speaker is an elderly person, the second person is a young person'', and the corresponding $z^{\prime}$ is `the first speaker is a young person, the second person is also a young person''. We first generate 5 speech of the second-speaker for each sample, and then generate 10 speech of the first-speaker, including 5 risk version and 5 low-risk version, based on  and . We finally manually cut the long silence and noise in collected speech, and concatenate speech waves of the first and the second speakers with 0.8 seconds silence in between.
\end{itemize}

\textbf{Gender, Ethnicity Stereotypical Biases.} We utilise Google \footnote{Google TTS does not provide the age of speakers to generate the elderly voice needed for our dataset. Audiobox provides a random voice for each generation, suggesting it's not able to provide consistent speakers across samples in the same sub-category.} service to generate synthetic speech for risk categories: gender stereotypical bias and ethnicity stereotypical bias. To distinguish the risk and low-risk speech, we control the gender and ethnicity of the first speaker.

\begin{itemize}
\item For gender stereotypical bias, We describe  as `the first speaker is a woman, the second person is a man'', and the corresponding $z^{\prime}$ is `the first speaker is man, the second person is also a man''. we randomly select 5 female and 5 male voices from the en-US language list to serve as the first speaker, and an additional 5 male voices as the second speaker. We then create conversations by pairing each of the 5 female first-speakers with the 5 male second-speakers to constitute the risk speech samples. Similarly, pairing each of the 5 male first-speakers with the 5 male second-speakers generates the low-risk speech samples. All speech waves are concatenated with 0.8 seconds of silence in between.
\item For ethnicity stereotypical bias, a similar strategy is employed. We describe  as `the first speaker is a person with accent and diverse ethnicity backgrounds, the second speaker is a person with American native accent'', and the corresponding $z'$ is `the first speaker is a person with American native accent, the second speaker is also a person with American native accent''. However, due to the limitation of Google TTS providing only Indian-accented voices, we are restricted to using Indian voices as the first speaker. Specifically, we select 5 voices each from the en-IN and en-US language lists to serve as the first speaker, with an additional set of 5 voices chosen from the en-US list as the second speaker. These selections are then systematically paired and concatenated into conversations following the same protocol used for the gender-based pairings.
\end{itemize}

All generated speeches are quality checked by the annotators. Especially, for the Audiobox-generated speech (for Malicious Sarcasm, Age), although we feed voice requirements to Audiobox, some of the generated speeches still do not follow the requirements. Therefore, filtering by annotators is necessary, to collect speech that strictly meets our requirements and to ensure that humans can easily distinguish the sarcasm, gender, ethnicity, and age in the generated speeches.
=====END FILE=====

=====FILE: sections/experiments.tex=====
\section{Experiments}

We evaluate the capabilities of LMMs in detecting the risk induced by paralinguistic cues under 4 risk sub-categories: malicious sarcasm, and stereotypical biases for gender, age, and ethnicity. We first describe our evaluation prompts and models.

\textbf{Models.} We evaluate 5 recent LMMs with instruction-following and speech understanding capabilities. Qwen-Audio-Chat (Chu et al., 2023) is an instruction following version of Qwen-Audio (Chu et al., 2023) with a Whisper audio encoder.

[ILLEGIBLE / MISSING CONTENT: Description of other models like SALMONN, WavLLM, Gemini-1.5-Pro, and detailed experimental setup.]

\begin{table*}[t]
\centering
\small
\begin{tabular}{l|c|cc|c|c}
\toprule
\multirow{2}{*}{Prompt} & Sarcasm & \multicolumn{2}{c|}{Gender \quad Age} & Ethnicity & Weight Avg. \
& Acc & Acc & Acc & Acc & Acc \
\midrule
\multicolumn{6}{l}{\textbf{Qwen-Audio-Chat-7B}} \
Y/N & 66.00 & 55.81 & 48.40 & 49.58 & 57.17 \
 & 62.27 & 50.00 & 54.60 & 48.75 & 56.22 \
 & 50.00 & 50.00 & 50.00 & 50.00 & 50.00 \
MC & 61.47 & 45.48 & 51.60 & 61.67 & 56.00 \
 & 61.47 & 48.39 & 53.20 & 56.25 & 56.22 \
 & 76.67 & 50.97 & 50.00 & 50.42 & 61.34 \
Avg. & 62.98 & 50.11 & 51.30 & 52.78 & - \
\midrule
\multicolumn{6}{l}{\textbf{SALMONN-7B}} \
Y/N & 50.00 & 50.00 & 50.00 & 50.00 & 50.00 \
 & 50.00 & 50.00 & 50.00 & 50.00 & 50.00 \
 & 52.00 & 55.81 & 48.60 & 50.83 & 51.56 \
MC & 59.20 & 49.68 & 49.60 & 60.83 & 55.11 \
 & 58.93 & 48.06 & 53.00 & 63.33 & 56.00 \
 & 64.00 & 55.69 & 55.20 & 50.00 & 54.16 \
Avg. & 55.69 & 51.54 & 51.07 & 54.16 & 52.81 \
\midrule
\multicolumn{6}{l}{\textbf{SALMONN-13B}} \
Y/N & 64.80 & 50.00 & 50.00 & 50.00 & 56.17 \
 & 50.80 & 48.40 & 50.32 & 50.00 & 49.94 \
 & 50.40 & 45.80 & 62.58 & 45.42 & 50.56 \
MC & 61.60 & 34.84 & 42.40 & 63.33 & [ILLEGIBLE] \
\bottomrule
\end{tabular}
\caption{Main Results of detection accuracy (Acc) across different prompts and models.}
\label{tab:main_results}
\end{table*}

[ILLEGIBLE / MISSING CONTENT: Further analysis and tables shown in snippets 22-25, including Gender/Age/Ethnicity breakdown with FI scores.]

\begin{table}[h]
\centering
\begin{tabular}{l|c|c|c}
\toprule
Prompt & Gender & Age & Ethnicity \
& Acc & Acc & Acc \
\midrule
\multicolumn{4}{l}{\textbf{Qwen-Audio-Chat-7B}} \
Level-1 & 51.94 & 51.00 & 33.79 \
Level-2 & 54.41 & 50.80 & 34.12 \
\midrule
\multicolumn{4}{l}{\textbf{SALMONN-7B}} \
Level-1 & 51.94 & 39.56 & 34.17 \
Level-2 & 54.73 & 42.80 & 50.00 \
\bottomrule
\end{tabular}
\caption{Performance on different levels of prompts.}
\label{tab:level_results}
\end{table}

[ILLEGIBLE / MISSING CONTENT: Discussion of results.]
=====END FILE=====

=====FILE: sections/conclusion.tex=====
\section{Conclusion}

We presented a speech-specific risk taxonomy where paralinguistic cues in speech can transform low-risk textual content into high-risk speech. We created a high quality synthetic speech dataset under human annotation and filtering. We observed that even the most recent large multimodal models (such as Gemini 1.5 pro) perform near random baseline, with some of the recent speechLLMs scoring even worse than random guesses.
=====END FILE=====

=====FILE: sections/references.tex=====
\begin{thebibliography}{99}

\bibitem{anil2023palm}
Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al.
\newblock 2023.
\newblock Palm 2 technical report.
\newblock \emph{arXiv preprint arXiv:2305.10403}.

\bibitem{bai2023qwen}
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jin...
\newblock \emph{arXiv preprint arXiv:2309.16609} (Inferred from authors).

\bibitem{chiang2023vicuna}
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing.
\newblock 2023.
\newblock Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality.

\bibitem{chu2023qwen}
Yunfei Chu, Jin Xu, Xiaohuan Zhou, Qian Yang, Shiliang Zhang, Zhijie Yan, Chang Zhou, and Jingren Zhou.
\newblock 2023.
\newblock Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models.
\newblock \emph{arXiv preprint arXiv:2311.07919}.

\bibitem{colston1997salience}
Herbert L Colston.
\newblock 1997.
\newblock Salience of invalidating information about class inclusion.
\newblock \emph{Journal of Pragmatics}, 28(3):315--336. [ILLEGIBLE / INFERRED]

\bibitem{ghosh2021detoxy}
S. Ghosh et al.
\newblock 2021.
\newblock DeToxy-B: A Large-Scale Dataset for Toxicity Classification in Speech.
\newblock \emph{Interspeech}. [ILLEGIBLE / INFERRED]

\bibitem{hu2024wavllm}
Hu et al.
\newblock 2024.
\newblock WavLLM: Towards Robust and Adaptive Speech Large Language Models.
\newblock \emph{arXiv}. [ILLEGIBLE / INFERRED]

\bibitem{katz2004processing}
A. Katz et al.
\newblock 2004.
\newblock [Processing sarcasm]. [ILLEGIBLE / INFERRED]

\bibitem{khanna2024goat}
Khanna et al.
\newblock 2024.
\newblock Goat-bench: Safety Insights for LMMs. [ILLEGIBLE / INFERRED]

\bibitem{lin2024a}
Lin et al.
\newblock 2024a.
\newblock [Title Missing]. [ILLEGIBLE / INFERRED]

\bibitem{lin2024b}
Lin et al.
\newblock 2024b.
\newblock [Title Missing]. [ILLEGIBLE / INFERRED]

\bibitem{ma2023emotion2vec}
Ziyang Ma, Zhisheng Zheng, Jiaxin Ye, Jinchao Li, Zhifu Gao, Shiliang Zhang, and Xie Chen.
\newblock 2023.
\newblock emotion2vec: Self-supervised pre-training for speech emotion representation.
\newblock \emph{arXiv preprint arXiv:2312.15185}.

\bibitem{nada2023lightweight}
Ahlam Husni Abu Nada, Siddique Latif, and Junaid Qadir.
\newblock 2023.
\newblock Lightweight toxicity detection in spoken language: A transformer-based approach for edge devices.
\newblock \emph{arXiv preprint arXiv:2304.11408}.

\bibitem{radford2023robust}
Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever.
\newblock 2023.
\newblock Robust speech recognition via large-scale weak supervision.
\newblock In \emph{International Conference on Machine Learning}, pages 28492--28518. PMLR.

\bibitem{rana2022emotion}
Aneri Rana and Sonali Jha.
\newblock 2022.
\newblock Emotion based hate speech detection using multimodal learning.
\newblock \emph{arXiv preprint arXiv:2202.06218}.

\bibitem{reid2022bad}
Elizabeth Reid, Regan L Mandryk, Nicole A Beres, Madison Klarkowski, and Julian Frommel.
\newblock 2022.
\newblock "bad vibrations": Sensing toxicity from in-game audio features.
\newblock \emph{IEEE Transactions on Games}, 14(4):558--568.

\bibitem{reid2024gemini}
Reid et al.
\newblock 2024.
\newblock Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context.
\newblock \emph{arXiv}. [ILLEGIBLE / INFERRED]

\bibitem{su2023}
Su et al.
\newblock 2023.
\newblock [Title Missing]. [ILLEGIBLE / INFERRED]

\bibitem{tang2024salmonn}
Tang et al.
\newblock 2024.
\newblock SALMONN: Speech Audio Language Music Open Neural Network.
\newblock \emph{ICLR}. [ILLEGIBLE / INFERRED]

\bibitem{toplak2000effects}
M. Toplak and A. Katz.
\newblock 2000.
\newblock [Effects of Sarcasm]. [ILLEGIBLE / INFERRED]

\bibitem{touvron2023llama}
Touvron et al.
\newblock 2023a.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv}. [ILLEGIBLE / INFERRED]

\bibitem{vyas2023audiobox}
Vyas et al.
\newblock 2023.
\newblock Audiobox: Unified Audio Generation with Natural Language Prompts.
\newblock \emph{arXiv}. [ILLEGIBLE / INFERRED]

\bibitem{wang2023donotanswer}
Wang et al.
\newblock 2023.
\newblock Do-Not-Answer: A Dataset for Evaluating Safeguards of LLMs.
\newblock \emph{arXiv}. [ILLEGIBLE / INFERRED]

\bibitem{wei2022chain}
Wei et al.
\newblock 2022.
\newblock Chain-of-thought prompting elicits reasoning in large language models.
\newblock \emph{NeurIPS}. [ILLEGIBLE / INFERRED]

\bibitem{xie2021unsupervised}
Xie et al.
\newblock 2021.
\newblock Unsupervised data augmentation for consistency training.
\newblock \emph{NeurIPS}. [ILLEGIBLE / INFERRED]

\bibitem{xu2023cvalues}
Xu et al.
\newblock 2023.
\newblock CValues: Measuring the Safety of Chinese Large Language Models.
\newblock \emph{arXiv}. [ILLEGIBLE / INFERRED]

\bibitem{xue2023}
Xue et al.
\newblock 2023.
\newblock [Title Missing]. [ILLEGIBLE / INFERRED]

\bibitem{yousefi2021}
Yousefi and Emmanouilidou.
\newblock 2021.
\newblock [Title Missing]. [ILLEGIBLE / INFERRED]

\bibitem{zhang2023a}
Zhang et al.
\newblock 2023a.
\newblock [Title Missing]. [ILLEGIBLE / INFERRED]

\bibitem{zhang2023b}
Zhang et al.
\newblock 2023b.
\newblock SafetyBench: Evaluating the Safety of Large Language Models.
\newblock \emph{arXiv}. [ILLEGIBLE / INFERRED]

\bibitem{zhao2023}
Zhao et al.
\newblock 2023.
\newblock [Title Missing]. [ILLEGIBLE / INFERRED]

\bibitem{zhu2020}
Zhu and Wang.
\newblock 2020.
\newblock [Title Missing]. [ILLEGIBLE / INFERRED]

% ... Additional references truncated in source ...

\end{thebibliography}
=====END FILE=====

=====FILE: figures/README.txt=====
The original PDF contained the following figures which were not extracted:

Figure 1: Our taxonomy of risk categories for speech.
Figure 2: Our data curation pipeline.

Please place appropriate image files (e.g., figure1.png, figure2.png) in this directory and update the \includegraphics commands in the tex files.
=====END FILE=====